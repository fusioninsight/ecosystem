{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FusionInsight MRS \u751f\u6001\u5730\u56fe \u00b6 FusionInsight\u652f\u6301\u5f00\u6e90\u6807\u51c6\u7684Hadoop\u63a5\u53e3\uff0c\u53ef\u4ee5\u4e0e\u4ee5\u4e0b\u7b2c\u4e09\u65b9\u5de5\u5177\u8fdb\u884c\u5bf9\u63a5 \u7b2c\u4e09\u65b9\u5de5\u5177 FusionInsight \u6d89\u53ca\u9886\u57df \u5de5\u5177\u540d\u79f0 \u7248\u672c C50 C60 C70 C80 6.5 8.0 \u6570\u636e\u53ef\u89c6\u5316 FineBI 5.1 Hive Hive IBM Cognos 10.2.2fp4 Hive MicroStrategy 11.1.4 Hive Spark2x Hive Spark2x Oracle BIEE 11g Hive SparkSQL Hive SparkSQL ELK GaussDB 12c Hive SparkSQL Hive SparkSQL ELK GaussDB Hive SparkSQL ELK GaussDB Hive SparkSQL Power BI 2.75.5649.861 Hive Spark2x FTP-Server Hive FTP-Server Qlik Sense 3.2.4 Hive SparkSQL Hive SparkSQL Hive SparkSQL QlikView 12 Hive SparkSQL Hive SparkSQL Hive SparkSQL Hive SparkSQL Hive SparkSQL SSRS 2017 Hive SparkSQL Tableau 10.0.0 Hive SparkSQL 10.1.4 Hive SparkSQL 10.3.2 Hive SparkSQL 10.5.0 Hive SparkSQL Hive SparkSQL Hive SparkSQL \u6570\u636e\u5206\u6790 Alteryx 2018.2.5.48994 HDFS Hive SparkSQL HDFS Hive SparkSQL HDFS Hive SparkSQL HDFS Hive SparkSQL GeoMesa 2.3.1 HBase Rapidminer Studio 8.2.001 HDFS Hive MapReduce Spark SAS 9.4M3 HDFS Hive Yarn HDFS Hive Yarn HDFS Hive Yarn SSAS 2017 Hive Spark Splunk 7.2.4 HDFS Hive HDFS HDFS \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 Hive SparkSQL 8.7 Hive \u6570\u636e\u96c6\u6210 Apache NiFi 1.7.1 HDFS HBase Hive Spark Kafka Solr 1.9.2 HDFS HBase Hive Spark Kafka 1.12.0 HDFS HBase Hive Spark Kafka Confluent 4.1.0 HDFS Kafka HDFS Kafka 5.5.0 HDFS Kafka DataX 0.1 HDFS HDFS Denodo Platform 7.0 Hive Hive H2O.ai 3.24.0.2 HDFS GaussDB HDFS IBM InfoSphere CDC 11.3.3.1 HDFS IBM InfoSphere DataStage 11.3.1.0 HDFS Hive SparkSQL 11.5.0.2 HDFS Hive Phoenix SparkSQL Kafka GaussDB Informatica 10.0.0 HDFS HBase Hive HDFS HBase Hive HDFS HBase Hive Yarn HDFS Hive 10.2.2 HDFS HBase Hive HDFS Hive Informatica PowerCenter 10.2.0 HDFS Hive HDFS Hive Informatica PowerexChange CDC 10.2.0 Kafka Kafka Manager 1.3.3.21 Kafka Kafka Kettle 6.1 HDFS Hive HDFS Hive HDFS Hive HDFS Hive HDFS Hive Knime 3.6.1 HDFS Hive Spark 4.1.0 HDFS Hive Spark HDFS Hive OceanSource 1.0 HBase Hive Kafka ElasticSearch Redis GaussDB Oracle GoldenGate 12.2 HDFS HBase Flume Kafka HDFS HBase Flume Kafka HDFS Flume Kafka 12.3 HDFS HBase Flume Kafka HDFS HBase Flume Kafka Pentaho EE 7.1 HDFS Hive 8.0 HDFS Hive HDFS Hive SharePlex 9.2.1 Kafka Kafka Streamsets 3.16.1 HDFS Hive HBase Kafka HDFS Hive HBase Kafka Talend 6.4.1 HDFS HBase Hive HDFS HBase Hive HDFS Hive Hetu 7.0.1 HDFS HBase 7.2.1 HDFS HBase Hive HDFS HBase Hive Tibco BW 5.13 GaussDB debezium 1.0.0 Kafka 1.2.2 Kafka \u676d\u5dde\u5408\u4f17UTL 5.1 HDFS HBase Hive Kafka \u6570\u636e\u5e93 Apache Druid 0.14.2 HDFS Kafka 0.15.1 HDFS Kafka OpenTSDB 2.4.0 HBase SAP VORA 2.0 Spark 2.1 Spark \u676d\u5dde\u5408\u4f17UDB 6.1 GaussDB \u96c6\u6210\u5f00\u53d1\u73af\u5883 Anaconda 2-2019.03-Linux-x86_64 Spark2x Spark2x DBeaver 4.0.8 Hive Phoenix SparkSQL 4.2.1 Hive Phoenix SparkSQL 6.1.4 Hive Phoenix SparkSQL 6.3.4 Hive Phoenix SparkSQL Hetu Hive Phoenix SparkSQL Hetu DbVisualizer 10.0.1 Hive Phoenix SparkSQL 10.0.21 Hive Phoenix SparkSQL Hive Phoenix SparkSQL 9.5.7 Hive Phoenix SparkSQL HUE 4.0.1 HDFS HBase Hive Spark Jupyter Notebook 2.4.4.0 pySpark 2.7.16 Hive ELK Spark2x Hive ELK Spark2x HDFS Hive ELK Spark2x Hetu JupyterHub 1.0.0 Spark2x Spark2x Spark2x RStudio 3.4.1 SparkR SparkR SparkR SparkR Squirrel 3.7.1 Hive Phoenix SparkSQL 3.8.0 Hive Phoenix SparkSQL 3.9.1 Hive Phoenix SparkSQL Hive Phoenix SparkSQL Zeppelin 0.7.2 HBase Hive Spark SparkR 0.7.3 HBase Hive Spark SparkR HBase Hive Spark SparkR 0.8.0 HBase Hive Spark SparkR ELK 0.8.1 HBase Hive Spark2x GaussDB 0.9.0 Hive Spark2x GaussDB \u5176\u4ed6 Apache Airflow 1.10.6 HDFS Hive HeTu HDFS Hive Apache Livy 0.5.0 Spark2x 0.6.0 Spark2x 0.7.0 Spark2x GIS Tools for Hadoop 1.0 Hive MapReduce Kibana 6.1.3 ElasticSearch ElasticSearch Logstash 6.4.2 ElasticSearch 6.7.1 HDFS Kafka HDFS Kafka NeoKylin 6.9 OS 7.2 OS Tensorflow 1.15.0 HDFS HDFS 2.1.0 HDFS HDFS elasticsearch-head 1.0 ElasticSearch ElasticSearch filebeat 6.5.1 ElasticSearch ElasticSearch librdkafka 1.0 Kafka \u751f\u6001\u5bf9\u63a5\u5e38\u89c1\u95ee\u9898\u603b\u7ed3 1.0 HDFS HDFS SQL\u5206\u6790 Apache Drill 1.15.0 HDFS Hive HBase Kafka HDFS Hive HBase Kafka 1.17.0 HDFS Hive HBase Kafka HDFS Hive HBase Kafka Apache Kylin 1.6.0 HBase Hive 2.1.0 HBase Hive 2.3.1 HBase Hive 2.6.1 HBase Hive 3.0.1 Hive HBase Kafka Spark Kyligence Analytics Platform 2.2 HBase Hive 2.3 HBase Hive 2.4 HBase Hive 2.5 HBase Hive 3.0 HBase Hive Yarn Presto 0.155 HDFS Hive 0.184 HDFS Hive 0.196 HDFS Hive 0.210 HDFS Hive ElasticSearch HDFS Hive HDFS Hive","title":"Home"},{"location":"#fusioninsight-mrs","text":"FusionInsight\u652f\u6301\u5f00\u6e90\u6807\u51c6\u7684Hadoop\u63a5\u53e3\uff0c\u53ef\u4ee5\u4e0e\u4ee5\u4e0b\u7b2c\u4e09\u65b9\u5de5\u5177\u8fdb\u884c\u5bf9\u63a5 \u7b2c\u4e09\u65b9\u5de5\u5177 FusionInsight \u6d89\u53ca\u9886\u57df \u5de5\u5177\u540d\u79f0 \u7248\u672c C50 C60 C70 C80 6.5 8.0 \u6570\u636e\u53ef\u89c6\u5316 FineBI 5.1 Hive Hive IBM Cognos 10.2.2fp4 Hive MicroStrategy 11.1.4 Hive Spark2x Hive Spark2x Oracle BIEE 11g Hive SparkSQL Hive SparkSQL ELK GaussDB 12c Hive SparkSQL Hive SparkSQL ELK GaussDB Hive SparkSQL ELK GaussDB Hive SparkSQL Power BI 2.75.5649.861 Hive Spark2x FTP-Server Hive FTP-Server Qlik Sense 3.2.4 Hive SparkSQL Hive SparkSQL Hive SparkSQL QlikView 12 Hive SparkSQL Hive SparkSQL Hive SparkSQL Hive SparkSQL Hive SparkSQL SSRS 2017 Hive SparkSQL Tableau 10.0.0 Hive SparkSQL 10.1.4 Hive SparkSQL 10.3.2 Hive SparkSQL 10.5.0 Hive SparkSQL Hive SparkSQL Hive SparkSQL \u6570\u636e\u5206\u6790 Alteryx 2018.2.5.48994 HDFS Hive SparkSQL HDFS Hive SparkSQL HDFS Hive SparkSQL HDFS Hive SparkSQL GeoMesa 2.3.1 HBase Rapidminer Studio 8.2.001 HDFS Hive MapReduce Spark SAS 9.4M3 HDFS Hive Yarn HDFS Hive Yarn HDFS Hive Yarn SSAS 2017 Hive Spark Splunk 7.2.4 HDFS Hive HDFS HDFS \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 Hive SparkSQL 8.7 Hive \u6570\u636e\u96c6\u6210 Apache NiFi 1.7.1 HDFS HBase Hive Spark Kafka Solr 1.9.2 HDFS HBase Hive Spark Kafka 1.12.0 HDFS HBase Hive Spark Kafka Confluent 4.1.0 HDFS Kafka HDFS Kafka 5.5.0 HDFS Kafka DataX 0.1 HDFS HDFS Denodo Platform 7.0 Hive Hive H2O.ai 3.24.0.2 HDFS GaussDB HDFS IBM InfoSphere CDC 11.3.3.1 HDFS IBM InfoSphere DataStage 11.3.1.0 HDFS Hive SparkSQL 11.5.0.2 HDFS Hive Phoenix SparkSQL Kafka GaussDB Informatica 10.0.0 HDFS HBase Hive HDFS HBase Hive HDFS HBase Hive Yarn HDFS Hive 10.2.2 HDFS HBase Hive HDFS Hive Informatica PowerCenter 10.2.0 HDFS Hive HDFS Hive Informatica PowerexChange CDC 10.2.0 Kafka Kafka Manager 1.3.3.21 Kafka Kafka Kettle 6.1 HDFS Hive HDFS Hive HDFS Hive HDFS Hive HDFS Hive Knime 3.6.1 HDFS Hive Spark 4.1.0 HDFS Hive Spark HDFS Hive OceanSource 1.0 HBase Hive Kafka ElasticSearch Redis GaussDB Oracle GoldenGate 12.2 HDFS HBase Flume Kafka HDFS HBase Flume Kafka HDFS Flume Kafka 12.3 HDFS HBase Flume Kafka HDFS HBase Flume Kafka Pentaho EE 7.1 HDFS Hive 8.0 HDFS Hive HDFS Hive SharePlex 9.2.1 Kafka Kafka Streamsets 3.16.1 HDFS Hive HBase Kafka HDFS Hive HBase Kafka Talend 6.4.1 HDFS HBase Hive HDFS HBase Hive HDFS Hive Hetu 7.0.1 HDFS HBase 7.2.1 HDFS HBase Hive HDFS HBase Hive Tibco BW 5.13 GaussDB debezium 1.0.0 Kafka 1.2.2 Kafka \u676d\u5dde\u5408\u4f17UTL 5.1 HDFS HBase Hive Kafka \u6570\u636e\u5e93 Apache Druid 0.14.2 HDFS Kafka 0.15.1 HDFS Kafka OpenTSDB 2.4.0 HBase SAP VORA 2.0 Spark 2.1 Spark \u676d\u5dde\u5408\u4f17UDB 6.1 GaussDB \u96c6\u6210\u5f00\u53d1\u73af\u5883 Anaconda 2-2019.03-Linux-x86_64 Spark2x Spark2x DBeaver 4.0.8 Hive Phoenix SparkSQL 4.2.1 Hive Phoenix SparkSQL 6.1.4 Hive Phoenix SparkSQL 6.3.4 Hive Phoenix SparkSQL Hetu Hive Phoenix SparkSQL Hetu DbVisualizer 10.0.1 Hive Phoenix SparkSQL 10.0.21 Hive Phoenix SparkSQL Hive Phoenix SparkSQL 9.5.7 Hive Phoenix SparkSQL HUE 4.0.1 HDFS HBase Hive Spark Jupyter Notebook 2.4.4.0 pySpark 2.7.16 Hive ELK Spark2x Hive ELK Spark2x HDFS Hive ELK Spark2x Hetu JupyterHub 1.0.0 Spark2x Spark2x Spark2x RStudio 3.4.1 SparkR SparkR SparkR SparkR Squirrel 3.7.1 Hive Phoenix SparkSQL 3.8.0 Hive Phoenix SparkSQL 3.9.1 Hive Phoenix SparkSQL Hive Phoenix SparkSQL Zeppelin 0.7.2 HBase Hive Spark SparkR 0.7.3 HBase Hive Spark SparkR HBase Hive Spark SparkR 0.8.0 HBase Hive Spark SparkR ELK 0.8.1 HBase Hive Spark2x GaussDB 0.9.0 Hive Spark2x GaussDB \u5176\u4ed6 Apache Airflow 1.10.6 HDFS Hive HeTu HDFS Hive Apache Livy 0.5.0 Spark2x 0.6.0 Spark2x 0.7.0 Spark2x GIS Tools for Hadoop 1.0 Hive MapReduce Kibana 6.1.3 ElasticSearch ElasticSearch Logstash 6.4.2 ElasticSearch 6.7.1 HDFS Kafka HDFS Kafka NeoKylin 6.9 OS 7.2 OS Tensorflow 1.15.0 HDFS HDFS 2.1.0 HDFS HDFS elasticsearch-head 1.0 ElasticSearch ElasticSearch filebeat 6.5.1 ElasticSearch ElasticSearch librdkafka 1.0 Kafka \u751f\u6001\u5bf9\u63a5\u5e38\u89c1\u95ee\u9898\u603b\u7ed3 1.0 HDFS HDFS SQL\u5206\u6790 Apache Drill 1.15.0 HDFS Hive HBase Kafka HDFS Hive HBase Kafka 1.17.0 HDFS Hive HBase Kafka HDFS Hive HBase Kafka Apache Kylin 1.6.0 HBase Hive 2.1.0 HBase Hive 2.3.1 HBase Hive 2.6.1 HBase Hive 3.0.1 Hive HBase Kafka Spark Kyligence Analytics Platform 2.2 HBase Hive 2.3 HBase Hive 2.4 HBase Hive 2.5 HBase Hive 3.0 HBase Hive Yarn Presto 0.155 HDFS Hive 0.184 HDFS Hive 0.196 HDFS Hive 0.210 HDFS Hive ElasticSearch HDFS Hive HDFS Hive","title":"FusionInsight MRS \u751f\u6001\u5730\u56fe"},{"location":"Business_Intelligence/","text":"\u6570\u636e\u53ef\u89c6\u5316 \u00b6 FineBI 5.1 \u2194 6.5 5.1 \u2194 8.0 IBM Cognos 10.2.2fp4 \u2194 C60 MicroStrategy 11.1.4 \u2194 6.5 11.1.4 \u2194 8.0 Oracle BIEE 11g \u2194 C60 11g \u2194 C70 12c \u2194 C60 12c \u2194 C70 12c \u2194 6.5 12c \u2194 8.0 Power BI 2.75.5649.861 \u2194 6.5 2.75.5649.861 \u2194 8.0 Qlik Sense 3.2.4 \u2194 C70 3.2.4 \u2194 6.5 QlikView 12 \u2194 C60 12 \u2194 C70 12 \u2194 C80 12 \u2194 6.5 12 \u2194 8.0 SSRS 2017 \u2194 6.5 SmartBI 7.2.32464.17374 \u2194 C70 Tableau 10.0.0 \u2194 C50 10.1.4 \u2194 C60 10.3.2 \u2194 C70 10.5.0 \u2194 C80 10.5.0 \u2194 6.5 10.5.0 \u2194 8.0","title":"Index"},{"location":"Business_Intelligence/#_1","text":"FineBI 5.1 \u2194 6.5 5.1 \u2194 8.0 IBM Cognos 10.2.2fp4 \u2194 C60 MicroStrategy 11.1.4 \u2194 6.5 11.1.4 \u2194 8.0 Oracle BIEE 11g \u2194 C60 11g \u2194 C70 12c \u2194 C60 12c \u2194 C70 12c \u2194 6.5 12c \u2194 8.0 Power BI 2.75.5649.861 \u2194 6.5 2.75.5649.861 \u2194 8.0 Qlik Sense 3.2.4 \u2194 C70 3.2.4 \u2194 6.5 QlikView 12 \u2194 C60 12 \u2194 C70 12 \u2194 C80 12 \u2194 6.5 12 \u2194 8.0 SSRS 2017 \u2194 6.5 SmartBI 7.2.32464.17374 \u2194 C70 Tableau 10.0.0 \u2194 C50 10.1.4 \u2194 C60 10.3.2 \u2194 C70 10.5.0 \u2194 C80 10.5.0 \u2194 6.5 10.5.0 \u2194 8.0","title":"\u6570\u636e\u53ef\u89c6\u5316"},{"location":"Business_Intelligence/FineBI_5.1/","text":"FineBI\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 FineBI 5.1 \u2194 FusionInsight HD 6.5 (Hive) FineBI 5.1 \u2194 FusionInsight MRS 8.0 (Hive) \u5b89\u88c5FineBI \u00b6 \u5b89\u88c5FineBI, \u4ee5\u8def\u5f84 C:\\soft\\fineBI\\FineBI5.1 \u4e3a\u4f8b \u914d\u7f6eJDBC\u63a5\u53e3\u5bf9\u63a5Hive \u00b6 \u5c06\u5bf9\u63a5\u96c6\u7fa4\uff08\u7248\u672c6.5.1\uff09\u7684\u8ba4\u8bc1\u6587\u4ef6\u4e0b\u8f7d\u5230 C:\\651client \u6587\u4ef6\u5939\u4e0b\uff0c\u5305\u62ecuser.keytab\u548ckrb5.conf \u8bf4\u660e\uff1a\u5982\u679c\u662f\u5bf9\u63a5mrs 8.0\u7248\u672c\uff0c\u6839\u636e\u4e0a\u6b65\u628a\u5bf9\u5e94\u5ba2\u6237\u7aef\uff0c\u8ba4\u8bc1\u6587\u4ef6\u4e0b\u8f7d\u5230\u672c\u5730 \u5728FineBI\u7684bin\u76ee\u5f55\u4e0b\u627e\u5230\u914d\u7f6e\u6587\u4ef6finebi.vmoptions \u5e76\u5728\u8be5\u6587\u4ef6\u4e2d\u6dfb\u52a0\u914d\u7f6e\uff1a jaas.conf\u6587\u4ef6\u5185\u5bb9\u4e3a\uff1a \u4e0b\u8f7dFI HD6.5.1\u7684\u5ba2\u6237\u7aef\u5230\u672c\u5730\uff0c\u8865\u5168jar\u5305\uff0c\u627e\u5230Hive\\Beeline\\lib\u8def\u5f84\u53ef\u4ee5\u770b\u5230\u76f8\u5173\u7684jar\u5305 \u8bf4\u660e\uff1a \u5982\u679c\u662fmrs 8.0\u7248\u672c\uff0c\u5219\u9700\u8981\u7684\u9a71\u52a8jar\u5305\u4e3a \u5ba2\u6237\u7aef\u8def\u5f84\\Hive\\jdbc\u4e0b\u6240\u6709jar\u5305\u52a0\u4e0a\u5982\u4e0b\u989d\u5916\u4e09\u4e2ajar\u5305\uff08\u5982\u679c\u7f3a\u5c11\u7684\u8bdd\uff09 commons-lang-2.6.jar zookeeper-jute-3.5.6-hw-ei-302002.jar commons-collections-3.2.2.jar \u627e\u5230\u5e76\u8fdb\u5165FineBI\u76f8\u5173\u4f9d\u8d56\u8def\u5f84\uff0c\u5177\u4f53\u4e3a C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\lib , \u9700\u8981\u505a\u4ee5\u4e0b\u4e09\u4e2a\u64cd\u4f5c \u627e\u5230jar\u5305fine-bi-engine-third-5.1.jar\uff0c\u53f3\u952e\u4f7f\u7528winRAR\u6253\u5f00 \u8fdb\u5165org/apache\u76ee\u5f55\uff0c\u627e\u5230\u5e76\u5220\u9664zookeeper\u6587\u4ef6\u5939 \u5220\u9664FineBI\u81ea\u5e26\u7684zookeeper-3.4.6.jar \u5c06\u4e0a\u4e00\u6b65\u96c6\u7fa4\u5ba2\u6237\u7aefHive\\Beeline\\lib\u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u5230\u5f53\u524d\u6587\u4ef6\u5939\uff08 C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\lib \uff09 \u627e\u5230FineBI\u8def\u5f84 C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\resources ,\u5c06\u8ba4\u8bc1\u76f8\u5173\u6587\u4ef6user.keytab, krb5.conf, krb5.ini\u62f7\u8d1d\u5230\u6539\u76ee\u5f55\u4e0b \u542f\u52a8FineBI\uff0c\u627e\u5230\u7ba1\u7406\u7cfb\u7edf -> \u6570\u636e\u8fde\u63a5 -> \u65b0\u5efa\u6570\u636e\u8fde\u63a5 -> \u66f4\u591a\u6570\u636e\u8fde\u63a5 \u627e\u5230fusioninsight hd \u9009\u4e2d\u70b9\u786e\u5b9a \uff08hd 6.5\u5bf9\u63a5\uff09\u53c2\u8003\u4e0b\u56fe\u914d\u7f6e\u8fde\u63a5\u53c2\u6570 URL: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/651client/user.keytab \u9700\u8981\u6ce8\u610f\u7684\u662fuser.principal\u548cuser.keytab\u9700\u8981\u540c\u5b9e\u9645\u60c5\u51b5\u5339\u914d \u70b9\u51fb\u6d4b\u8bd5\u8fde\u63a5\u6d4b\u8bd5 \uff08mrs 8.0\u5bf9\u63a5\uff09\u53c2\u8003\u4e0b\u56fe\u914d\u7f6e\u8fde\u63a5\u53c2\u6570 1\uff1a ,172.16.10.132:24002,172.16.10.133:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM; 2\uff1a 172.16.10.131 3\uff1a 24002 4\uff1a jdbc:hive2://172.16.10.131:24002,172.16.10.132:24002,172.16.10.133:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM; \u8bf4\u660e\uff08\u91cd\u8981\uff09\uff1a \u914d\u7f6e1 + \u914d\u7f6e2 + \u914d\u7f6e3 \u4f1a\u751f\u6210\u914d\u7f6e4\u7684\u8fde\u63a5url,\u8981\u65f6\u523b\u4fdd\u6301\u77404\u4e2a\u914d\u7f6e\u7684\u6b63\u786e\u6027\uff0c\u5426\u5219\u5bf9\u63a5\u5931\u8d25 \u70b9\u51fb\u6d4b\u8bd5\u8fde\u63a5\u6d4b\u8bd5\uff1a \u70b9\u51fb\u521b\u5efa -> \u6dfb\u52a0\u6dfb\u52a0\u6570\u636e\u5e93\u8868 \u9009\u62e9\u4e00\u5f20\u8868\uff0c\u5982\u679c\u5df2\u7ecf\u9009\u62e9\u914d\u7f6e\u8fc7\u5219\u4e3a\u7070\u8272,\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u6570\u636e\u5217\u8868\u8def\u5f84 \u5728\u6570\u636e\u51c6\u5907->\u5bf9\u5e94\u7684\u6570\u636e\u5217\u8868\u8def\u5f84\u4e2d\u627e\u5230\u4e4b\u524d\u914d\u7f6e\u597d\u7684\u8868test","title":"5.1 <--> 8.0"},{"location":"Business_Intelligence/FineBI_5.1/#finebifusioninsight","text":"","title":"FineBI\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/FineBI_5.1/#_1","text":"FineBI 5.1 \u2194 FusionInsight HD 6.5 (Hive) FineBI 5.1 \u2194 FusionInsight MRS 8.0 (Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/FineBI_5.1/#finebi","text":"\u5b89\u88c5FineBI, \u4ee5\u8def\u5f84 C:\\soft\\fineBI\\FineBI5.1 \u4e3a\u4f8b","title":"\u5b89\u88c5FineBI"},{"location":"Business_Intelligence/FineBI_5.1/#jdbchive","text":"\u5c06\u5bf9\u63a5\u96c6\u7fa4\uff08\u7248\u672c6.5.1\uff09\u7684\u8ba4\u8bc1\u6587\u4ef6\u4e0b\u8f7d\u5230 C:\\651client \u6587\u4ef6\u5939\u4e0b\uff0c\u5305\u62ecuser.keytab\u548ckrb5.conf \u8bf4\u660e\uff1a\u5982\u679c\u662f\u5bf9\u63a5mrs 8.0\u7248\u672c\uff0c\u6839\u636e\u4e0a\u6b65\u628a\u5bf9\u5e94\u5ba2\u6237\u7aef\uff0c\u8ba4\u8bc1\u6587\u4ef6\u4e0b\u8f7d\u5230\u672c\u5730 \u5728FineBI\u7684bin\u76ee\u5f55\u4e0b\u627e\u5230\u914d\u7f6e\u6587\u4ef6finebi.vmoptions \u5e76\u5728\u8be5\u6587\u4ef6\u4e2d\u6dfb\u52a0\u914d\u7f6e\uff1a jaas.conf\u6587\u4ef6\u5185\u5bb9\u4e3a\uff1a \u4e0b\u8f7dFI HD6.5.1\u7684\u5ba2\u6237\u7aef\u5230\u672c\u5730\uff0c\u8865\u5168jar\u5305\uff0c\u627e\u5230Hive\\Beeline\\lib\u8def\u5f84\u53ef\u4ee5\u770b\u5230\u76f8\u5173\u7684jar\u5305 \u8bf4\u660e\uff1a \u5982\u679c\u662fmrs 8.0\u7248\u672c\uff0c\u5219\u9700\u8981\u7684\u9a71\u52a8jar\u5305\u4e3a \u5ba2\u6237\u7aef\u8def\u5f84\\Hive\\jdbc\u4e0b\u6240\u6709jar\u5305\u52a0\u4e0a\u5982\u4e0b\u989d\u5916\u4e09\u4e2ajar\u5305\uff08\u5982\u679c\u7f3a\u5c11\u7684\u8bdd\uff09 commons-lang-2.6.jar zookeeper-jute-3.5.6-hw-ei-302002.jar commons-collections-3.2.2.jar \u627e\u5230\u5e76\u8fdb\u5165FineBI\u76f8\u5173\u4f9d\u8d56\u8def\u5f84\uff0c\u5177\u4f53\u4e3a C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\lib , \u9700\u8981\u505a\u4ee5\u4e0b\u4e09\u4e2a\u64cd\u4f5c \u627e\u5230jar\u5305fine-bi-engine-third-5.1.jar\uff0c\u53f3\u952e\u4f7f\u7528winRAR\u6253\u5f00 \u8fdb\u5165org/apache\u76ee\u5f55\uff0c\u627e\u5230\u5e76\u5220\u9664zookeeper\u6587\u4ef6\u5939 \u5220\u9664FineBI\u81ea\u5e26\u7684zookeeper-3.4.6.jar \u5c06\u4e0a\u4e00\u6b65\u96c6\u7fa4\u5ba2\u6237\u7aefHive\\Beeline\\lib\u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u5230\u5f53\u524d\u6587\u4ef6\u5939\uff08 C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\lib \uff09 \u627e\u5230FineBI\u8def\u5f84 C:\\soft\\fineBI\\FineBI5.1\\webapps\\webroot\\WEB-INF\\resources ,\u5c06\u8ba4\u8bc1\u76f8\u5173\u6587\u4ef6user.keytab, krb5.conf, krb5.ini\u62f7\u8d1d\u5230\u6539\u76ee\u5f55\u4e0b \u542f\u52a8FineBI\uff0c\u627e\u5230\u7ba1\u7406\u7cfb\u7edf -> \u6570\u636e\u8fde\u63a5 -> \u65b0\u5efa\u6570\u636e\u8fde\u63a5 -> \u66f4\u591a\u6570\u636e\u8fde\u63a5 \u627e\u5230fusioninsight hd \u9009\u4e2d\u70b9\u786e\u5b9a \uff08hd 6.5\u5bf9\u63a5\uff09\u53c2\u8003\u4e0b\u56fe\u914d\u7f6e\u8fde\u63a5\u53c2\u6570 URL: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/651client/user.keytab \u9700\u8981\u6ce8\u610f\u7684\u662fuser.principal\u548cuser.keytab\u9700\u8981\u540c\u5b9e\u9645\u60c5\u51b5\u5339\u914d \u70b9\u51fb\u6d4b\u8bd5\u8fde\u63a5\u6d4b\u8bd5 \uff08mrs 8.0\u5bf9\u63a5\uff09\u53c2\u8003\u4e0b\u56fe\u914d\u7f6e\u8fde\u63a5\u53c2\u6570 1\uff1a ,172.16.10.132:24002,172.16.10.133:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM; 2\uff1a 172.16.10.131 3\uff1a 24002 4\uff1a jdbc:hive2://172.16.10.131:24002,172.16.10.132:24002,172.16.10.133:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM; \u8bf4\u660e\uff08\u91cd\u8981\uff09\uff1a \u914d\u7f6e1 + \u914d\u7f6e2 + \u914d\u7f6e3 \u4f1a\u751f\u6210\u914d\u7f6e4\u7684\u8fde\u63a5url,\u8981\u65f6\u523b\u4fdd\u6301\u77404\u4e2a\u914d\u7f6e\u7684\u6b63\u786e\u6027\uff0c\u5426\u5219\u5bf9\u63a5\u5931\u8d25 \u70b9\u51fb\u6d4b\u8bd5\u8fde\u63a5\u6d4b\u8bd5\uff1a \u70b9\u51fb\u521b\u5efa -> \u6dfb\u52a0\u6dfb\u52a0\u6570\u636e\u5e93\u8868 \u9009\u62e9\u4e00\u5f20\u8868\uff0c\u5982\u679c\u5df2\u7ecf\u9009\u62e9\u914d\u7f6e\u8fc7\u5219\u4e3a\u7070\u8272,\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u6570\u636e\u5217\u8868\u8def\u5f84 \u5728\u6570\u636e\u51c6\u5907->\u5bf9\u5e94\u7684\u6570\u636e\u5217\u8868\u8def\u5f84\u4e2d\u627e\u5230\u4e4b\u524d\u914d\u7f6e\u597d\u7684\u8868test","title":"\u914d\u7f6eJDBC\u63a5\u53e3\u5bf9\u63a5Hive"},{"location":"Business_Intelligence/IBM_Cognos/","text":"IBM Cognos\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM Cognos 10.2.2fp4 \u2194 FusionInsight HD V100R002C60U20 (Hive)","title":"10.2.2fp4 <--> C60"},{"location":"Business_Intelligence/IBM_Cognos/#ibm-cognosfusioninsight","text":"","title":"IBM Cognos\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/IBM_Cognos/#_1","text":"IBM Cognos 10.2.2fp4 \u2194 FusionInsight HD V100R002C60U20 (Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/","text":"MicroStrategy\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 MicroStrategy 11.1.4 \u2194 FusionInsight HD 6.5 (Hive/Spark2x) MicroStrategy 11.1.4 \u2194 FusionInsight MRS 8.0 (Hive/Spark2x) \u7b80\u4ecb \u00b6 MicroStrategy Desktop\u662f\u4e00\u6b3e\u529f\u80fd\u5f3a\u5927\u7684\u6570\u636e\u53d1\u73b0\u5de5\u5177\u3002\u4f7f\u7528MicroStrategy Desktop\u6765\u521b\u5efa\u81ea\u5b9a\u4e49\u7684\u4ea4\u4e92\u5f0f\u8fbe\u6790\u62a5\u544a\uff0c\u7528\u4e8e\u63a2\u7d22\u60a8\u7684\u4e1a\u52a1\u6570\u636e\u3002\u60a8\u53ef\u4ece\u8bb8\u591a\u4e0d\u540c\u7684\u6765\u6e90\u5bfc\u5165\u4e1a\u52a1\u6570\u636e\uff0c\u5305\u62ec\u672c\u5730\u6587\u4ef6\u3001\u6570\u636e\u5e93\u7b49\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cMicroStrategy Desktop\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHive\u3001Spark2x\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \uff08\u91cd\u8981\uff09\u5b8c\u6210MIT KERBEROS\u8ba4\u8bc1\uff0c\u5177\u4f53\u53c2\u89c1usionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->Hive\u5f00\u53d1\u6307\u5357->\u73af\u5883\u51c6\u5907->\u914d\u7f6eODBC\u6837\u4f8b\u5de5\u7a0b->Windows\u73af\u5883 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u53ef\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u521b\u5efa\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, first_name STRING, last_name STRING, subject_id INT, score FLOAT); INSERT INTO student VALUES (1,'Tom','Zhang',1,80); INSERT INTO student VALUES (2,'Sandy','Li',2,75); INSERT INTO student VALUES (3,'Benny','Chow',3,76); INSERT INTO student VALUES (4,'Tina','Wang',1,60); INSERT INTO student VALUES (5,'Tracy','Zhang',1,80); INSERT INTO student VALUES (6,'Andy','Li',2,79); INSERT INTO student VALUES (7,'Manson','Chow',3,86); INSERT INTO student VALUES (8,'Aurora','Wang',1,90); \u672c\u5730\u5df2\u5b58\u5728Subject.xlsx\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u672c\u6587\u4f7f\u7528keytab\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 ~ keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> </properties> </jdbc> MicroStrategy Desktop\u5bf9\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 MicroStrategy Desktop\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight\u7684Hive\u3001Spark2x\u7ec4\u4ef6\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u4ece https://www.microstrategy.com/us/get-started/desktop \u4e0b\u8f7dMicroStrategy Desktop\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6253\u5f00MicroStrategy Desktop\u754c\u9762\uff0c\u70b9\u51fb \u65b0\u8fbe\u6790\u62a5\u544a \u3002 \u70b9\u51fb\u201c\u8fbe\u6790\u62a5\u544a\u201d\u754c\u9762\u7684\u83dc\u5355\u680f \u6587\u4ef6->\u4fdd\u5b58 \u5c06\u62a5\u544a\u4fdd\u5b58\u4e3a FusionInsight.mstr \u3002 \u70b9\u51fb\u8fbe\u6790\u62a5\u544a\u201cFusionInsight\u201d\u754c\u9762\u7684 \u65b0\u6570\u636e \u6309\u94ae\u6dfb\u52a0\u6570\u636e\u3002 \u5728\u6570\u636e\u6e90\u4e2d\u9009\u62e9 \u6570\u636e\u5e93 \u3002 \u5bfc\u5165\u9009\u9879\u9009\u62e9 \u952e\u5165\u67e5\u8be2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c+\u201d\u6309\u94ae\u589e\u52a0\u6570\u636e\u6e90\u3002 \u52fe\u9009 \u65e0\u9700DSN\u7684\u6570\u636e\u6e90 \uff0c\u70b9\u51fb \u663e\u793a\u8fde\u63a5\u5b57\u7b26\u4e32 \u5e76 \u7f16\u8f91\u8fde\u63a5\u5b57\u7b26\u4e32 \u3002\u586b\u5199\u8fde\u63a5\u4fe1\u606f\u540e\u70b9\u51fb \u4fdd\u5b58 \u3002 Hive\u7684\u8fde\u63a5\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a \u6570 \u636e \u5e93\uff1aGeneric \u7248 \u672c\uff1aGeneric DBMS \u9a71\u52a8\u7a0b\u5e8f\uff1a\u9ed8\u8ba4\uff0c\u4e0d\u5f71\u54cd\u8fde\u63a5 \u8fde \u63a5 \u4e32\uff1aJDBC;DRIVER={com.huawei.fiber.FiberDriver};MSTR_JDBC_JAR_FOLDER=C:\\ecotesting\\Fiber\\lib;URL={jdbc:fiber://fiberconfig=C:\\ecotesting\\Fiber\\conf\\fiber.xml;defaultDriver=hive}; \u7528 \u6237\uff1a\u4e0d\u80fd\u4e3a\u7a7a\uff0c\u4f46\u662f\u4e0d\u7528\u4e8e\u8fde\u63a5\u9a8c\u8bc1\uff0c\u968f\u610f\u5199\uff0c\u4f8b\u5982\u201cA\u201d \u5bc6 \u7801\uff1a\u4e0d\u80fd\u4e3a\u7a7a\uff0c\u4f46\u662f\u4e0d\u7528\u4e8e\u8fde\u63a5\u9a8c\u8bc1\uff0c\u968f\u610f\u5199\uff0c\u4f8b\u5982\u201ctest\u201d \u6570\u636e\u6e90\u540d\u79f0\uff1afi_fiber_hive_jdbc\uff0c\u53ef\u81ea\u5b9a\u4e49 Spark2x\u7684\u8fde\u63a5\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a \u6570 \u636e \u5e93\uff1aGeneric \u7248 \u672c\uff1aGeneric DBMS \u9a71\u52a8\u7a0b\u5e8f\uff1a\u9ed8\u8ba4\uff0c\u4e0d\u5f71\u54cd\u8fde\u63a5 \u8fde \u63a5 \u4e32\uff1aJDBC;DRIVER={com.huawei.fiber.FiberDriver};MSTR_JDBC_JAR_FOLDER=C:\\ecotesting\\Fiber\\lib;URL={jdbc:fiber://fiberconfig=C:\\ecotesting\\Fiber\\conf\\fiber.xml;defaultDriver=spark2x}; \u7528 \u6237\uff1a\u4e0d\u80fd\u4e3a\u7a7a\uff0c\u4f46\u662f\u4e0d\u7528\u4e8e\u8fde\u63a5\u9a8c\u8bc1\uff0c\u968f\u610f\u5199\uff0c\u4f8b\u5982\u201cA\u201d \u5bc6 \u7801\uff1a\u4e0d\u80fd\u4e3a\u7a7a\uff0c\u4f46\u662f\u4e0d\u7528\u4e8e\u8fde\u63a5\u9a8c\u8bc1\uff0c\u968f\u610f\u5199\uff0c\u4f8b\u5982\u201ctest\u201d \u6570\u636e\u6e90\u540d\u79f0\uff1afi_fiber_spark2x_jdbc\uff0c\u53ef\u81ea\u5b9a\u4e49 \u8bf4\u660e\uff1a \u4e0a\u9762\u622a\u56fe\u663e\u793a\u7684\u662f\u5bf9\u63a5Hive\u7684\u8fde\u63a5\u4fe1\u606f\u3002 \u8fde\u63a5\u4fe1\u606f\u4e2d\uff0c\u4e3b\u8981\u662f\u201c\u8fde\u63a5\u4e32\u201d\u4fe1\u606f\u5f71\u54cd\u662f\u5426\u80fd\u8fde\u63a5\u6210\u529f\uff0c\u5176\u4ed6\u4fe1\u606f\u4e0d\u5f71\u54cd\u3002 Hive\u3001Spark2x\u8fde\u63a5\u4fe1\u606f\u7684\u552f\u4e00\u533a\u522b\u5728\u4e8edefaultDrive\u7684\u53d6\u503c\u3002 \u4ee5\u4e0b\u6307\u5bfc\u6b65\u9aa4\u4ee5Hive\u4e3a\u4f8b\uff0cSpark2x\u7c7b\u4f3c\u3002 \u5355\u51fb\u6570\u636e\u6e90 fi_fiber_hive_jdbc \uff0c\u8fde\u63a5\u6210\u529f\u5219\u8fd4\u56deFusionInsight\u7684\u6240\u6709\u547d\u540d\u7a7a\u95f4\u3002 \u9009\u62e9\u8868Student\u5b58\u5728\u7684\u547d\u540d\u7a7a\u95f4 default \uff0c\u53cc\u51fb\u8868 Student \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u9009\u62e9 \u4ee5\u5185\u5b58\u4e2d\u6570\u636e\u96c6\u7684\u5f62\u5f0f\u5bfc\u5165 \u3002 \u53f3\u51fb \u81ea\u5b9a\u4e49\u67e5\u8be2 \uff0c\u9009\u62e9 \u91cd\u547d\u540d \u4e3a \u201cStudent_Hive\u201d\u3002\u4eceSpark2x\u5bfc\u5165\u7684\u67e5\u8be2\u91cd\u547d\u540d\u4e3a \u201cStudent_Spark2x\u201d\u3002 MicroStrategy Desktop\u7f16\u8f91\u6570\u636e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4f7f\u7528MicroStrategy Desktop\u5408\u5e76\u4eceHive/Spark\u3001Excel\u5bfc\u5165\u7684\u591a\u4e2a\u6570\u636e\u6e90\uff0c\u8f93\u51fa\u62a5\u8868\u3002\u4ee5\u4e0b\u4ee5\u5408\u5e76Hive\u548cExcel\u6570\u636e\u6e90\u4e3a\u4f8b\uff0c\u7edf\u8ba1Student\u5404\u4e2a\u79d1\u76ee\u6210\u7ee9\u7684\u5e73\u5747\u503c\u3001\u6700\u5c0f\u503c\u3001\u6700\u5927\u503c\u3002\u5408\u5e76Spark2x\u548cExcel\u6570\u636e\u6e90\u64cd\u4f5c\u7c7b\u4f3c\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 MicroStrategy Desktop\u6210\u529f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u7ec4\u4ef6\u5e76\u5df2\u4eceHive\u6216\u8005Sparkx\u5bfc\u5165\u8868\u201cStudent\u201d\u7684\u6570\u636e\u5230MicroStrategy Desktop\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bfc\u5165Excel\u6570\u636e\u6e90\u3002 \u70b9\u51fb\u8fbe\u6790\u62a5\u544a\u201cFusionInsight\u201d\u754c\u9762\u7684 \u6dfb\u52a0\u6570\u636e \u6309\u94ae\u9009\u62e9 \u65b0\u5efa\u6570\u636e \u3002\u5728\u6570\u636e\u6e90\u4e2d\u9009\u62e9 \u6765\u81ea\u78c1\u76d8\u7684\u6587\u4ef6 \u3002 \u70b9\u51fb \u9009\u62e9\u6587\u4ef6 \u9009\u62e9\u672c\u5730\u6587\u4ef6 Subject.xlsx \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5efa\u7acb\u4e24\u4e2a\u6570\u636e\u6e90\u7684\u94fe\u63a5\u3002\u201cStudent_Hive\u201d\u7684\u201cSubject_id\u201d\u94fe\u63a5\u5230\u201cSubject.xlsx\u201d\u7684\u201cid\u201d\u3002 \u53f3\u952e\u201cStudent_Hive\u201d\u7684 Subject_id \u9009\u62e9 \u94fe\u63a5\u5230\u5176\u4ed6\u6570\u636e\u96c6 \u3002 \u9009\u62e9\u201cSubject.xlsx\u201d\u7684 id \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5728\u53f3\u4fa7\u201c\u56fe\u5e93\u201d\u4e2d\u9009\u62e9 \u6761\u5f62\u56fe \uff0c\u5c06\u201cStudent_Hive\u201d\u7684 Score \u62d6\u66f3\u81f3\u201c\u5782\u76f4\u201d\u6846\u4e2d\uff0c\u5c06\u201cSubject.xlsx\u201d\u7684 name \u62d6\u66f3\u81f3\u201c\u6c34\u5e73\u201d\u6846\u4e2d\u3002\u53f3\u952e\u201c\u5782\u76f4\u201d\u6846\u4e2d\u7684 Score \u5206\u522b\u9009\u62e9 \u805a\u5408\u4f9d\u636e->\u5e73\u5747 \u3001 \u805a\u5408\u4f9d\u636e->\u6700\u5c0f \u3001 \u805a\u5408\u4f9d\u636e->\u6700\u5927 \u3002\u5219\u56fe\u8868\u7684\u53ef\u89c6\u5316\u6548\u679c\u5982\u4e0b\u56fe\u6240\u793a: FAQ \u00b6 \u5b89\u88c5\u5b8cMicroStrategy Desktop\u540e\uff0c\u63d0\u793a\u4f7f\u7528\u524d\u9700\u8981\u5b89\u88c5.NET\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5b89\u88c5\u5b8cMicroStrategy Desktop\u540e\uff0c\u63d0\u793a\u201cThe installation process is complete. However, you need to install .NET before running Desktop on this machine. Click here for a step by step guide.\u201d\u3002 \u5982\u679c\u6ca1\u5b89\u88c5.NET\u76f4\u63a5\u542f\u52a8MicroStrategy Desktop\u5219\u4f1a\u8fd4\u56de\u201cThis application requires one of the following versions of the .NET Framework\u201d \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u70b9\u51fb\u63d0\u793a\u4fe1\u606f\u7684 Click here \u8df3\u8f6c\u81f3\u201cMicrosoft .NET Framework 4.7\u201d\u7684\u4e0b\u8f7d\u754c\u9762 https://www.microsoft.com/en-us/download/details.aspx?id=55167 \uff0c\u70b9\u51fb Download \u6309\u94ae\u4e0b\u8f7d NDP47-KB3186497-x86-x64-AllOS-ENU.exe \u3002 \u5b89\u88c5 NDP47-KB3186497-x86-x64-AllOS-ENU.exe \u3002\u5b89\u88c5\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u63d0\u793a\u201cSetup cannot continue because a dependent update needs to be installed before you can install this product on Windows 7, Windows Server 2008 R2, Windows 8 or Windows Server 2012.\u201d \u70b9\u51fb\u63d0\u793a\u4fe1\u606f\u4e2d\u7684 update \u8df3\u8f6c\u81f3 https://support.microsoft.com/zh-cn/help/4020302/the-net-framework-4-7-installation-is-blocked-on-windows-7-windows-ser \uff0c\u5728\u8be5\u9875\u9762\u7684\u201c\u89e3\u51b3\u65b9\u6cd5\u201d\u4e2d\uff0c\u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7684\u7248\u672c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u5bf9\u5e94\u7248\u672c\u7684d3dcompiler\u66f4\u65b0\u3002 Window 7 64\u4f4d\u64cd\u4f5c\u7cfb\u7edf\u4e0b\u8f7d\u5e76\u5b89\u88c5 Windows6.1-KB4019990-x64.msu \u540e\uff0c\u518d\u91cd\u65b0\u5b89\u88c5 NDP47-KB3186497-x86-x64-AllOS-ENU.exe \u3002\u4e4b\u540e\u5219\u53ef\u4ee5\u6210\u529f\u542f\u52a8MicroStrategy Desktop\u3002","title":"11.1.4 <--> 8.0"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#microstrategyfusioninsight","text":"","title":"MicroStrategy\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_1","text":"MicroStrategy 11.1.4 \u2194 FusionInsight HD 6.5 (Hive/Spark2x) MicroStrategy 11.1.4 \u2194 FusionInsight MRS 8.0 (Hive/Spark2x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_2","text":"MicroStrategy Desktop\u662f\u4e00\u6b3e\u529f\u80fd\u5f3a\u5927\u7684\u6570\u636e\u53d1\u73b0\u5de5\u5177\u3002\u4f7f\u7528MicroStrategy Desktop\u6765\u521b\u5efa\u81ea\u5b9a\u4e49\u7684\u4ea4\u4e92\u5f0f\u8fbe\u6790\u62a5\u544a\uff0c\u7528\u4e8e\u63a2\u7d22\u60a8\u7684\u4e1a\u52a1\u6570\u636e\u3002\u60a8\u53ef\u4ece\u8bb8\u591a\u4e0d\u540c\u7684\u6765\u6e90\u5bfc\u5165\u4e1a\u52a1\u6570\u636e\uff0c\u5305\u62ec\u672c\u5730\u6587\u4ef6\u3001\u6570\u636e\u5e93\u7b49\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cMicroStrategy Desktop\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHive\u3001Spark2x\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \uff08\u91cd\u8981\uff09\u5b8c\u6210MIT KERBEROS\u8ba4\u8bc1\uff0c\u5177\u4f53\u53c2\u89c1usionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->Hive\u5f00\u53d1\u6307\u5357->\u73af\u5883\u51c6\u5907->\u914d\u7f6eODBC\u6837\u4f8b\u5de5\u7a0b->Windows\u73af\u5883 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u53ef\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u521b\u5efa\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, first_name STRING, last_name STRING, subject_id INT, score FLOAT); INSERT INTO student VALUES (1,'Tom','Zhang',1,80); INSERT INTO student VALUES (2,'Sandy','Li',2,75); INSERT INTO student VALUES (3,'Benny','Chow',3,76); INSERT INTO student VALUES (4,'Tina','Wang',1,60); INSERT INTO student VALUES (5,'Tracy','Zhang',1,80); INSERT INTO student VALUES (6,'Andy','Li',2,79); INSERT INTO student VALUES (7,'Manson','Chow',3,86); INSERT INTO student VALUES (8,'Aurora','Wang',1,90); \u672c\u5730\u5df2\u5b58\u5728Subject.xlsx\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#fiber","text":"","title":"Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_4","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u672c\u6587\u4f7f\u7528keytab\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#keytab","text":"\u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> </properties> </jdbc>","title":"\u64cd\u4f5c\u6b65\u9aa4 ~ keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#microstrategy-desktopfiber","text":"","title":"MicroStrategy Desktop\u5bf9\u63a5Fiber"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_6","text":"MicroStrategy Desktop\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight\u7684Hive\u3001Spark2x\u7ec4\u4ef6\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_7","text":"\u4ece https://www.microstrategy.com/us/get-started/desktop \u4e0b\u8f7dMicroStrategy Desktop\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_8","text":"\u6253\u5f00MicroStrategy Desktop\u754c\u9762\uff0c\u70b9\u51fb \u65b0\u8fbe\u6790\u62a5\u544a \u3002 \u70b9\u51fb\u201c\u8fbe\u6790\u62a5\u544a\u201d\u754c\u9762\u7684\u83dc\u5355\u680f \u6587\u4ef6->\u4fdd\u5b58 \u5c06\u62a5\u544a\u4fdd\u5b58\u4e3a FusionInsight.mstr \u3002 \u70b9\u51fb\u8fbe\u6790\u62a5\u544a\u201cFusionInsight\u201d\u754c\u9762\u7684 \u65b0\u6570\u636e \u6309\u94ae\u6dfb\u52a0\u6570\u636e\u3002 \u5728\u6570\u636e\u6e90\u4e2d\u9009\u62e9 \u6570\u636e\u5e93 \u3002 \u5bfc\u5165\u9009\u9879\u9009\u62e9 \u952e\u5165\u67e5\u8be2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c+\u201d\u6309\u94ae\u589e\u52a0\u6570\u636e\u6e90\u3002 \u52fe\u9009 \u65e0\u9700DSN\u7684\u6570\u636e\u6e90 \uff0c\u70b9\u51fb \u663e\u793a\u8fde\u63a5\u5b57\u7b26\u4e32 \u5e76 \u7f16\u8f91\u8fde\u63a5\u5b57\u7b26\u4e32 \u3002\u586b\u5199\u8fde\u63a5\u4fe1\u606f\u540e\u70b9\u51fb \u4fdd\u5b58 \u3002 Hive\u7684\u8fde\u63a5\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a \u6570 \u636e \u5e93\uff1aGeneric \u7248 \u672c\uff1aGeneric DBMS \u9a71\u52a8\u7a0b\u5e8f\uff1a\u9ed8\u8ba4\uff0c\u4e0d\u5f71\u54cd\u8fde\u63a5 \u8fde \u63a5 \u4e32\uff1aJDBC;DRIVER={com.huawei.fiber.FiberDriver};MSTR_JDBC_JAR_FOLDER=C:\\ecotesting\\Fiber\\lib;URL={jdbc:fiber://fiberconfig=C:\\ecotesting\\Fiber\\conf\\fiber.xml;defaultDriver=hive}; \u7528 \u6237\uff1a\u4e0d\u80fd\u4e3a\u7a7a\uff0c\u4f46\u662f\u4e0d\u7528\u4e8e\u8fde\u63a5\u9a8c\u8bc1\uff0c\u968f\u610f\u5199\uff0c\u4f8b\u5982\u201cA\u201d \u5bc6 \u7801\uff1a\u4e0d\u80fd\u4e3a\u7a7a\uff0c\u4f46\u662f\u4e0d\u7528\u4e8e\u8fde\u63a5\u9a8c\u8bc1\uff0c\u968f\u610f\u5199\uff0c\u4f8b\u5982\u201ctest\u201d \u6570\u636e\u6e90\u540d\u79f0\uff1afi_fiber_hive_jdbc\uff0c\u53ef\u81ea\u5b9a\u4e49 Spark2x\u7684\u8fde\u63a5\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a \u6570 \u636e \u5e93\uff1aGeneric \u7248 \u672c\uff1aGeneric DBMS \u9a71\u52a8\u7a0b\u5e8f\uff1a\u9ed8\u8ba4\uff0c\u4e0d\u5f71\u54cd\u8fde\u63a5 \u8fde \u63a5 \u4e32\uff1aJDBC;DRIVER={com.huawei.fiber.FiberDriver};MSTR_JDBC_JAR_FOLDER=C:\\ecotesting\\Fiber\\lib;URL={jdbc:fiber://fiberconfig=C:\\ecotesting\\Fiber\\conf\\fiber.xml;defaultDriver=spark2x}; \u7528 \u6237\uff1a\u4e0d\u80fd\u4e3a\u7a7a\uff0c\u4f46\u662f\u4e0d\u7528\u4e8e\u8fde\u63a5\u9a8c\u8bc1\uff0c\u968f\u610f\u5199\uff0c\u4f8b\u5982\u201cA\u201d \u5bc6 \u7801\uff1a\u4e0d\u80fd\u4e3a\u7a7a\uff0c\u4f46\u662f\u4e0d\u7528\u4e8e\u8fde\u63a5\u9a8c\u8bc1\uff0c\u968f\u610f\u5199\uff0c\u4f8b\u5982\u201ctest\u201d \u6570\u636e\u6e90\u540d\u79f0\uff1afi_fiber_spark2x_jdbc\uff0c\u53ef\u81ea\u5b9a\u4e49 \u8bf4\u660e\uff1a \u4e0a\u9762\u622a\u56fe\u663e\u793a\u7684\u662f\u5bf9\u63a5Hive\u7684\u8fde\u63a5\u4fe1\u606f\u3002 \u8fde\u63a5\u4fe1\u606f\u4e2d\uff0c\u4e3b\u8981\u662f\u201c\u8fde\u63a5\u4e32\u201d\u4fe1\u606f\u5f71\u54cd\u662f\u5426\u80fd\u8fde\u63a5\u6210\u529f\uff0c\u5176\u4ed6\u4fe1\u606f\u4e0d\u5f71\u54cd\u3002 Hive\u3001Spark2x\u8fde\u63a5\u4fe1\u606f\u7684\u552f\u4e00\u533a\u522b\u5728\u4e8edefaultDrive\u7684\u53d6\u503c\u3002 \u4ee5\u4e0b\u6307\u5bfc\u6b65\u9aa4\u4ee5Hive\u4e3a\u4f8b\uff0cSpark2x\u7c7b\u4f3c\u3002 \u5355\u51fb\u6570\u636e\u6e90 fi_fiber_hive_jdbc \uff0c\u8fde\u63a5\u6210\u529f\u5219\u8fd4\u56deFusionInsight\u7684\u6240\u6709\u547d\u540d\u7a7a\u95f4\u3002 \u9009\u62e9\u8868Student\u5b58\u5728\u7684\u547d\u540d\u7a7a\u95f4 default \uff0c\u53cc\u51fb\u8868 Student \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u9009\u62e9 \u4ee5\u5185\u5b58\u4e2d\u6570\u636e\u96c6\u7684\u5f62\u5f0f\u5bfc\u5165 \u3002 \u53f3\u51fb \u81ea\u5b9a\u4e49\u67e5\u8be2 \uff0c\u9009\u62e9 \u91cd\u547d\u540d \u4e3a \u201cStudent_Hive\u201d\u3002\u4eceSpark2x\u5bfc\u5165\u7684\u67e5\u8be2\u91cd\u547d\u540d\u4e3a \u201cStudent_Spark2x\u201d\u3002","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#microstrategy-desktop","text":"","title":"MicroStrategy Desktop\u7f16\u8f91\u6570\u636e"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_9","text":"\u4f7f\u7528MicroStrategy Desktop\u5408\u5e76\u4eceHive/Spark\u3001Excel\u5bfc\u5165\u7684\u591a\u4e2a\u6570\u636e\u6e90\uff0c\u8f93\u51fa\u62a5\u8868\u3002\u4ee5\u4e0b\u4ee5\u5408\u5e76Hive\u548cExcel\u6570\u636e\u6e90\u4e3a\u4f8b\uff0c\u7edf\u8ba1Student\u5404\u4e2a\u79d1\u76ee\u6210\u7ee9\u7684\u5e73\u5747\u503c\u3001\u6700\u5c0f\u503c\u3001\u6700\u5927\u503c\u3002\u5408\u5e76Spark2x\u548cExcel\u6570\u636e\u6e90\u64cd\u4f5c\u7c7b\u4f3c\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_10","text":"MicroStrategy Desktop\u6210\u529f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u7ec4\u4ef6\u5e76\u5df2\u4eceHive\u6216\u8005Sparkx\u5bfc\u5165\u8868\u201cStudent\u201d\u7684\u6570\u636e\u5230MicroStrategy Desktop\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#_11","text":"\u5bfc\u5165Excel\u6570\u636e\u6e90\u3002 \u70b9\u51fb\u8fbe\u6790\u62a5\u544a\u201cFusionInsight\u201d\u754c\u9762\u7684 \u6dfb\u52a0\u6570\u636e \u6309\u94ae\u9009\u62e9 \u65b0\u5efa\u6570\u636e \u3002\u5728\u6570\u636e\u6e90\u4e2d\u9009\u62e9 \u6765\u81ea\u78c1\u76d8\u7684\u6587\u4ef6 \u3002 \u70b9\u51fb \u9009\u62e9\u6587\u4ef6 \u9009\u62e9\u672c\u5730\u6587\u4ef6 Subject.xlsx \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5efa\u7acb\u4e24\u4e2a\u6570\u636e\u6e90\u7684\u94fe\u63a5\u3002\u201cStudent_Hive\u201d\u7684\u201cSubject_id\u201d\u94fe\u63a5\u5230\u201cSubject.xlsx\u201d\u7684\u201cid\u201d\u3002 \u53f3\u952e\u201cStudent_Hive\u201d\u7684 Subject_id \u9009\u62e9 \u94fe\u63a5\u5230\u5176\u4ed6\u6570\u636e\u96c6 \u3002 \u9009\u62e9\u201cSubject.xlsx\u201d\u7684 id \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5728\u53f3\u4fa7\u201c\u56fe\u5e93\u201d\u4e2d\u9009\u62e9 \u6761\u5f62\u56fe \uff0c\u5c06\u201cStudent_Hive\u201d\u7684 Score \u62d6\u66f3\u81f3\u201c\u5782\u76f4\u201d\u6846\u4e2d\uff0c\u5c06\u201cSubject.xlsx\u201d\u7684 name \u62d6\u66f3\u81f3\u201c\u6c34\u5e73\u201d\u6846\u4e2d\u3002\u53f3\u952e\u201c\u5782\u76f4\u201d\u6846\u4e2d\u7684 Score \u5206\u522b\u9009\u62e9 \u805a\u5408\u4f9d\u636e->\u5e73\u5747 \u3001 \u805a\u5408\u4f9d\u636e->\u6700\u5c0f \u3001 \u805a\u5408\u4f9d\u636e->\u6700\u5927 \u3002\u5219\u56fe\u8868\u7684\u53ef\u89c6\u5316\u6548\u679c\u5982\u4e0b\u56fe\u6240\u793a:","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Business_Intelligence/MicroStrategy_11.1.4/#faq","text":"\u5b89\u88c5\u5b8cMicroStrategy Desktop\u540e\uff0c\u63d0\u793a\u4f7f\u7528\u524d\u9700\u8981\u5b89\u88c5.NET\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5b89\u88c5\u5b8cMicroStrategy Desktop\u540e\uff0c\u63d0\u793a\u201cThe installation process is complete. However, you need to install .NET before running Desktop on this machine. Click here for a step by step guide.\u201d\u3002 \u5982\u679c\u6ca1\u5b89\u88c5.NET\u76f4\u63a5\u542f\u52a8MicroStrategy Desktop\u5219\u4f1a\u8fd4\u56de\u201cThis application requires one of the following versions of the .NET Framework\u201d \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u70b9\u51fb\u63d0\u793a\u4fe1\u606f\u7684 Click here \u8df3\u8f6c\u81f3\u201cMicrosoft .NET Framework 4.7\u201d\u7684\u4e0b\u8f7d\u754c\u9762 https://www.microsoft.com/en-us/download/details.aspx?id=55167 \uff0c\u70b9\u51fb Download \u6309\u94ae\u4e0b\u8f7d NDP47-KB3186497-x86-x64-AllOS-ENU.exe \u3002 \u5b89\u88c5 NDP47-KB3186497-x86-x64-AllOS-ENU.exe \u3002\u5b89\u88c5\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u63d0\u793a\u201cSetup cannot continue because a dependent update needs to be installed before you can install this product on Windows 7, Windows Server 2008 R2, Windows 8 or Windows Server 2012.\u201d \u70b9\u51fb\u63d0\u793a\u4fe1\u606f\u4e2d\u7684 update \u8df3\u8f6c\u81f3 https://support.microsoft.com/zh-cn/help/4020302/the-net-framework-4-7-installation-is-blocked-on-windows-7-windows-ser \uff0c\u5728\u8be5\u9875\u9762\u7684\u201c\u89e3\u51b3\u65b9\u6cd5\u201d\u4e2d\uff0c\u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7684\u7248\u672c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u5bf9\u5e94\u7248\u672c\u7684d3dcompiler\u66f4\u65b0\u3002 Window 7 64\u4f4d\u64cd\u4f5c\u7cfb\u7edf\u4e0b\u8f7d\u5e76\u5b89\u88c5 Windows6.1-KB4019990-x64.msu \u540e\uff0c\u518d\u91cd\u65b0\u5b89\u88c5 NDP47-KB3186497-x86-x64-AllOS-ENU.exe \u3002\u4e4b\u540e\u5219\u53ef\u4ee5\u6210\u529f\u542f\u52a8MicroStrategy Desktop\u3002","title":"FAQ"},{"location":"Business_Intelligence/Oracle_BIEE/","text":"Oracle BIEE\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Oracle BIEE 11g \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Oracle BIEE 11g \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Oracle BIEE 12c \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight HD 6.5 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight MRS 8.0 (Hive/SparkSQL) Linux\u73af\u5883\u5b89\u88c5OBIEE \u00b6 \u5b89\u88c5OS \u00b6 \u5b89\u88c5RedHat6.5\u64cd\u4f5c\u7cfb\u7edf\uff0cdesktop\u7248 \u521b\u5efa\u7528\u6237oracle \u5b89\u88c5jdk8 \u00b6 \u83b7\u53d6jdk8\u5b89\u88c5\u5305\uff0c\u6267\u884c\u5b89\u88c5 \u5b89\u88c5Weblogic \u00b6 \u521b\u5efaoracle home\u76ee\u5f55\uff1a umask 027 mkdir -p /Oracle/Middleware/Oracle_Home chown -R oracle:oracle /Oracle/ \u4e0a\u4f20weblogic\u5b89\u88c5\u5305\uff0c\u89e3\u538b \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 \u5b89\u88c5BI Server \u00b6 \u4e0a\u4f20OBIEE\u5b89\u88c5\u5305\uff0c\u89e3\u538b chmod 755 bi_platform-12.2.1.2.0_linux64.bin \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 ./bi_platform-12.2.1.2.0_linux64.bin \u8865\u9f50lib\u5305 yum install -y compat-libcap1 compat-libstdc++-33 libstdc++-devel gcc gcc-c++ libaio-devel \u53d6\u6d88\u5f53\u524d\u5b89\u88c5\uff0c\u91cd\u65b0\u8fd0\u884c\u5b89\u88c5\u7a0b\u5e8f \u5b89\u88c5Oracle Database 12c \u00b6 \u5b89\u88c5\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b89\u88c5\u76ee\u5f55 mkdir -p /Oracle/database chown -R oracle:oracle /Oracle \u4e0b\u8f7dOracle Database 12c\u5b89\u88c5\u5305\uff0c\u89e3\u538b\u5f97\u5230database\u6587\u4ef6\u5939 chmod -R 755 database/ cd database/ su oracle ./runInstaller \u53ea\u5b89\u88c5\u5355\u5b9e\u4f8b\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b9e\u4f8b cd /Oracle/database/product/12.1.0/dbhome_1/bin/ ./dbca \u5b57\u7b26\u96c6\u9009\u62e9AL32UTF8\uff0c\u4e0d\u52fe\u9009\u201ccreate as container database\u201d \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi ~/.bash_profile ORACLE_BASE = /Oracle/database ORACLE_HOME = $ORACLE_BASE /product/12.1.0/dbhome_1 ORACLE_SID = orcl ORACLE_TERM = xterm PATH = $PATH : $ORACLE_HOME /bin export ORACLE_BASE export ORACLE_HOME export ORACLE_SID export ORACLE_TERM export PATH \u5bfc\u5165\u73af\u5883\u53d8\u91cf source ~/.bash_profile \u914d\u7f6e\u76d1\u542c\u7a0b\u5e8f\u548c\u7f51\u7edc\u670d\u52a1\u540d netca Listener\u7aef\u53e3\u8bbe\u4e3a\u9ed8\u8ba4\u503c1521 \u7f51\u7edc\u670d\u52a1\u540d\u914d\u7f6e\u4e3a ORCL \u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f \u4e3b\u673a\u91cd\u542f\u540e\uff0c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f su oracle source ~/.bash_profile lsnrctl start sqlplus / as sysdba sqlplus\u754c\u9762\u6267\u884c startup \u4f7f\u7528RCU\u521b\u5efaSchema \u00b6 \u542f\u52a8rcu cd /Oracle/Middleware/Oracle_Home/oracle_common/bin/ ./rcu \u914d\u7f6eBI Server \u00b6 \u6267\u884c\u914d\u7f6e cd /Oracle/Middleware/Oracle_Home/bi/bin ./config.sh \u5b89\u88c5BI Client \u00b6 \u5728Win7(64 bit)\u7cfb\u7edf\u4e0a\u5b89\u88c5BI Client \u5bf9\u63a5Hive \u00b6 \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eKerberos\u8ba4\u8bc1 \u4ece http://web.mit.edu/kerberos/ \u4e0b\u8f7d\u5b89\u88c5kfw-4.1 \u5b89\u88c5\u914d\u7f6eHive ODBC Driver \u4e0b\u8f7d\u5b89\u88c5Hive ODBC Driver\uff08Windows\u7248\u672c\uff09\uff0c \u4e0b\u8f7d\u5730\u5740 \u5728BI\u5ba2\u6237\u7aef\u6240\u5728\u7684Windows\u673a\u5668\u4e0a\u914d\u7f6e\u7cfb\u7edfDSN \u6d4b\u8bd5ODBC\u8fde\u63a5 BI \u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 Client\u7aef\u6253\u5f00Oracle BI \u7ba1\u7406\u5de5\u5177 \u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684DSN\uff0c\u7528\u6237\u540d\u53e3\u4ee4\u4efb\u610f\u8f93\u5165\uff0c\u4f46\u4e0d\u80fd\u4e3a\u7a7a \u7981\u7528BI Server\u9ad8\u901f\u7f13\u5b58 \u00b6 \u767b\u5f55Weblogic\u57df\u7ba1\u7406\u754c\u9762 http://162.1.115.81:9500/em \u914d\u7f6e\u4e2d\u7981\u7528\u9ad8\u901f\u7f13\u5b58 \u4e0a\u4f20RPD\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u5ba2\u6237\u7aef cmd \u5207\u6362\u5230 E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bitools\\bin \u76ee\u5f55 \u6267\u884c\u547d\u4ee4\u4e0a\u4f20RPD datamodel.cmd uploadrpd -U weblogic -P Huawei123 -I E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bifoundation\\server\\obiee-hive.rpd -W Huawei@123 -S 162.1.115.81 -N 9502 -SI ssi \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eKerberos\u8ba4\u8bc1 mv /etc/krb5.conf /etc/krb5.conf.bak \u5c06FusionInsight\u96c6\u7fa4\u7684krb5.conf\u4e0a\u4f20\u5230/etc\u76ee\u5f55\u4e0b kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eCloudera Hive ODBC Driver yum install -y unixODBC \u4e0b\u8f7dHive ODBC Driver\uff08Linux\u7248\u672c\uff09 \u4e0b\u8f7d\u5730\u5740 \u5b89\u88c5Hive ODBC Driver rpm -Uvh ClouderaHiveODBC-2.5.5.1006-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u4e0eClient\u7aef\u751f\u6210\u7684RPD\u6587\u4ef6\u7684DSN\u540d\u79f0\u548c\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 mv /etc/odbc.ini /etc/odbc.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbc.ini /etc/ vi /etc/odbc.ini \u4fee\u6539odbc\u914d\u7f6e\u6587\u4ef6 vi /opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini mv /etc/odbcinst.ini /etc/odbcinst.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbcinst.ini /etc/ \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile export LD_LIBRARY_PATH=/usr/lib64:/opt/cloudera/hiveodbc/lib/64 export ODBCINI=/etc/odbc.ini export ODBCSYSINI=/etc export SIMBAINI=/opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Cloudera Hive DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core cp odbc.ini odbc.ini.bak vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e \u00b6 \u6253\u5f00BI Analytics\u754c\u9762 http://162.1.115.81:9502/analytics \u521b\u5efa\u5206\u6790 \u9009\u62e9\u5f85\u5206\u6790\u7684\u5217\u62d6\u5230\u53f3\u4fa7\u533a\u57df \u70b9\u51fb\u201c\u7ed3\u679c\u201d\u9875\u7b7e\uff0c\u68c0\u7d22\u6240\u9009\u5217\u6570\u636e \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u4fdd\u5b58\u6309\u94ae\uff0c\u4fdd\u5b58\u67e5\u8be2\u7ed3\u679c \u521b\u5efa\u53ef\u89c6\u5206\u6790\u5668\u9879\u76ee \u6dfb\u52a0\u6570\u636e\u6e90 \u9009\u53d6\u6570\u636e\u663e\u793a\u5f62\u5f0f \u6dfb\u52a0\u8ba1\u7b97 \u5bf9\u63a5Spark SQL \u00b6 \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 Kerberos\u8ba4\u8bc1 Kerberos\u83b7\u53d6\u8ba4\u8bc1\u7968\u636e \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7d\u5b89\u88c5 Simba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6eDSN\uff1a \u6d4b\u8bd5ODBC\u8fde\u63a5\uff1a BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 \u65b0\u5efaobiee-spark.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 Sample Simba Spark DSN \u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u4e0a\u4f20RDP \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 Kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7dSimba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 rpm -Uvh SimbaSparkODBC-1.2.2.1002-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u589e\u52a0Sample Simba Spark DSN\uff0c\u4e0eClient\u7aef\u914d\u7f6e\u76f8\u540c vi /etc/odbc.ini \u4fee\u6539odbcinst.ini\uff0c vi /etc/odbcinist.ini \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Simba Spark DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e \u00b6 \u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e \u5bf9\u63a5LibrA/ELK \u00b6 \u914d\u7f6eLibrA\u4e0eELK\u7684\u65b9\u5f0f\u6ca1\u6709\u533a\u522b\uff0c\u4ee5\u4e0b\u4ee5\u5bf9\u63a5ELK\u4e3a\u4f8b\u8fdb\u884c\u64cd\u4f5c \u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN \u00b6 \u914d\u7f6eobiee\u5ba2\u6237\u7aef\u7684ODBC\u9a71\u52a8 \u6309\u7167ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684\u6307\u5bfc\u5b89\u88c5\u914d\u7f6eELK\u7684windows\u9a71\u52a8 \u914d\u7f6eDSN\uff0c\u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u4fdd\u5b58ODBC\u8fde\u63a5 BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP \u00b6 \u65b0\u5efaobiee-elk.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 PostgreSQL35W \u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef \u00b6 \u4e0a\u4f20RDP \u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN \u00b6 \u53c2\u8003LibrA/ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684Linux\u4e0b\u914d\u7f6e\u6570\u636e\u6e90\u7ae0\u8282\uff0c\u5b8c\u6210obiee\u8282\u70b9\u4e0b\u7684ODBC\u9a71\u52a8\u7684\u5b89\u88c5 \u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u786e\u4fddODBC\u9a71\u52a8\u5b89\u88c5\u6210\u529f isql -v PostgreSQL35W BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u5728ODBC Data Sources\u90e8\u5206\u589e\u52a0PostgreSQL35W\u7684DSN \u5728\u6587\u4ef6\u672b\u5c3e\u589e\u52a0PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e\u6700\u540e\u4e00\u884cDriverUnicodeType=1\u9700\u8981\u52a0\u4e0a\uff0c\u5426\u5219obiee\u67e5\u8be2\u7684\u65f6\u5019\u4f1a\u62a5\u9519[nQSError: 12010] Communication error connecting to remote end point: address = obiee; port = 9514. (HY000) \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh \u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e \u00b6 \u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"12c <--> 8.0"},{"location":"Business_Intelligence/Oracle_BIEE/#oracle-bieefusioninsight","text":"","title":"Oracle BIEE\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/Oracle_BIEE/#_1","text":"Oracle BIEE 11g \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Oracle BIEE 11g \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Oracle BIEE 12c \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight HD 6.5 (Hive/SparkSQL/ELK/GaussDB) Oracle BIEE 12c \u2194 FusionInsight MRS 8.0 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Oracle_BIEE/#linuxobiee","text":"","title":"Linux\u73af\u5883\u5b89\u88c5OBIEE"},{"location":"Business_Intelligence/Oracle_BIEE/#os","text":"\u5b89\u88c5RedHat6.5\u64cd\u4f5c\u7cfb\u7edf\uff0cdesktop\u7248 \u521b\u5efa\u7528\u6237oracle","title":"\u5b89\u88c5OS"},{"location":"Business_Intelligence/Oracle_BIEE/#jdk8","text":"\u83b7\u53d6jdk8\u5b89\u88c5\u5305\uff0c\u6267\u884c\u5b89\u88c5","title":"\u5b89\u88c5jdk8"},{"location":"Business_Intelligence/Oracle_BIEE/#weblogic","text":"\u521b\u5efaoracle home\u76ee\u5f55\uff1a umask 027 mkdir -p /Oracle/Middleware/Oracle_Home chown -R oracle:oracle /Oracle/ \u4e0a\u4f20weblogic\u5b89\u88c5\u5305\uff0c\u89e3\u538b \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762","title":"\u5b89\u88c5Weblogic"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server","text":"\u4e0a\u4f20OBIEE\u5b89\u88c5\u5305\uff0c\u89e3\u538b chmod 755 bi_platform-12.2.1.2.0_linux64.bin \u4ee5oracle\u7528\u6237\u767b\u5f55\u56fe\u5f62\u754c\u9762 ./bi_platform-12.2.1.2.0_linux64.bin \u8865\u9f50lib\u5305 yum install -y compat-libcap1 compat-libstdc++-33 libstdc++-devel gcc gcc-c++ libaio-devel \u53d6\u6d88\u5f53\u524d\u5b89\u88c5\uff0c\u91cd\u65b0\u8fd0\u884c\u5b89\u88c5\u7a0b\u5e8f","title":"\u5b89\u88c5BI Server"},{"location":"Business_Intelligence/Oracle_BIEE/#oracle-database-12c","text":"\u5b89\u88c5\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b89\u88c5\u76ee\u5f55 mkdir -p /Oracle/database chown -R oracle:oracle /Oracle \u4e0b\u8f7dOracle Database 12c\u5b89\u88c5\u5305\uff0c\u89e3\u538b\u5f97\u5230database\u6587\u4ef6\u5939 chmod -R 755 database/ cd database/ su oracle ./runInstaller \u53ea\u5b89\u88c5\u5355\u5b9e\u4f8b\u6570\u636e\u5e93\u8f6f\u4ef6 \u521b\u5efa\u6570\u636e\u5e93\u5b9e\u4f8b cd /Oracle/database/product/12.1.0/dbhome_1/bin/ ./dbca \u5b57\u7b26\u96c6\u9009\u62e9AL32UTF8\uff0c\u4e0d\u52fe\u9009\u201ccreate as container database\u201d \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi ~/.bash_profile ORACLE_BASE = /Oracle/database ORACLE_HOME = $ORACLE_BASE /product/12.1.0/dbhome_1 ORACLE_SID = orcl ORACLE_TERM = xterm PATH = $PATH : $ORACLE_HOME /bin export ORACLE_BASE export ORACLE_HOME export ORACLE_SID export ORACLE_TERM export PATH \u5bfc\u5165\u73af\u5883\u53d8\u91cf source ~/.bash_profile \u914d\u7f6e\u76d1\u542c\u7a0b\u5e8f\u548c\u7f51\u7edc\u670d\u52a1\u540d netca Listener\u7aef\u53e3\u8bbe\u4e3a\u9ed8\u8ba4\u503c1521 \u7f51\u7edc\u670d\u52a1\u540d\u914d\u7f6e\u4e3a ORCL \u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f \u4e3b\u673a\u91cd\u542f\u540e\uff0c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u6570\u636e\u5e93\u548c\u76d1\u542c\u7a0b\u5e8f su oracle source ~/.bash_profile lsnrctl start sqlplus / as sysdba sqlplus\u754c\u9762\u6267\u884c startup","title":"\u5b89\u88c5Oracle Database 12c"},{"location":"Business_Intelligence/Oracle_BIEE/#rcuschema","text":"\u542f\u52a8rcu cd /Oracle/Middleware/Oracle_Home/oracle_common/bin/ ./rcu","title":"\u4f7f\u7528RCU\u521b\u5efaSchema"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server_1","text":"\u6267\u884c\u914d\u7f6e cd /Oracle/Middleware/Oracle_Home/bi/bin ./config.sh","title":"\u914d\u7f6eBI Server"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-client","text":"\u5728Win7(64 bit)\u7cfb\u7edf\u4e0a\u5b89\u88c5BI Client","title":"\u5b89\u88c5BI Client"},{"location":"Business_Intelligence/Oracle_BIEE/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn","text":"\u914d\u7f6eKerberos\u8ba4\u8bc1 \u4ece http://web.mit.edu/kerberos/ \u4e0b\u8f7d\u5b89\u88c5kfw-4.1 \u5b89\u88c5\u914d\u7f6eHive ODBC Driver \u4e0b\u8f7d\u5b89\u88c5Hive ODBC Driver\uff08Windows\u7248\u672c\uff09\uff0c \u4e0b\u8f7d\u5730\u5740 \u5728BI\u5ba2\u6237\u7aef\u6240\u5728\u7684Windows\u673a\u5668\u4e0a\u914d\u7f6e\u7cfb\u7edfDSN \u6d4b\u8bd5ODBC\u8fde\u63a5","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-rdp","text":"Client\u7aef\u6253\u5f00Oracle BI \u7ba1\u7406\u5de5\u5177 \u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684DSN\uff0c\u7528\u6237\u540d\u53e3\u4ee4\u4efb\u610f\u8f93\u5165\uff0c\u4f46\u4e0d\u80fd\u4e3a\u7a7a","title":"BI \u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#bi-server_2","text":"\u767b\u5f55Weblogic\u57df\u7ba1\u7406\u754c\u9762 http://162.1.115.81:9500/em \u914d\u7f6e\u4e2d\u7981\u7528\u9ad8\u901f\u7f13\u5b58","title":"\u7981\u7528BI Server\u9ad8\u901f\u7f13\u5b58"},{"location":"Business_Intelligence/Oracle_BIEE/#rpd","text":"\u5ba2\u6237\u7aef cmd \u5207\u6362\u5230 E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bitools\\bin \u76ee\u5f55 \u6267\u884c\u547d\u4ee4\u4e0a\u4f20RPD datamodel.cmd uploadrpd -U weblogic -P Huawei123 -I E:\\Oracle\\Middleware\\Oracle_Home\\bi\\bifoundation\\server\\obiee-hive.rpd -W Huawei@123 -S 162.1.115.81 -N 9502 -SI ssi","title":"\u4e0a\u4f20RPD\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_1","text":"\u914d\u7f6eKerberos\u8ba4\u8bc1 mv /etc/krb5.conf /etc/krb5.conf.bak \u5c06FusionInsight\u96c6\u7fa4\u7684krb5.conf\u4e0a\u4f20\u5230/etc\u76ee\u5f55\u4e0b kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eCloudera Hive ODBC Driver yum install -y unixODBC \u4e0b\u8f7dHive ODBC Driver\uff08Linux\u7248\u672c\uff09 \u4e0b\u8f7d\u5730\u5740 \u5b89\u88c5Hive ODBC Driver rpm -Uvh ClouderaHiveODBC-2.5.5.1006-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u4e0eClient\u7aef\u751f\u6210\u7684RPD\u6587\u4ef6\u7684DSN\u540d\u79f0\u548c\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 mv /etc/odbc.ini /etc/odbc.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbc.ini /etc/ vi /etc/odbc.ini \u4fee\u6539odbc\u914d\u7f6e\u6587\u4ef6 vi /opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini mv /etc/odbcinst.ini /etc/odbcinst.ini.bak cp /opt/cloudera/hiveodbc/Setup/odbcinst.ini /etc/ \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile export LD_LIBRARY_PATH=/usr/lib64:/opt/cloudera/hiveodbc/lib/64 export ODBCINI=/etc/odbc.ini export ODBCSYSINI=/etc export SIMBAINI=/opt/cloudera/hiveodbc/Setup/cloudera.hiveodbc.ini \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Cloudera Hive DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core cp odbc.ini odbc.ini.bak vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#hive_1","text":"\u6253\u5f00BI Analytics\u754c\u9762 http://162.1.115.81:9502/analytics \u521b\u5efa\u5206\u6790 \u9009\u62e9\u5f85\u5206\u6790\u7684\u5217\u62d6\u5230\u53f3\u4fa7\u533a\u57df \u70b9\u51fb\u201c\u7ed3\u679c\u201d\u9875\u7b7e\uff0c\u68c0\u7d22\u6240\u9009\u5217\u6570\u636e \u70b9\u51fb\u53f3\u4e0a\u89d2\u7684\u4fdd\u5b58\u6309\u94ae\uff0c\u4fdd\u5b58\u67e5\u8be2\u7ed3\u679c \u521b\u5efa\u53ef\u89c6\u5206\u6790\u5668\u9879\u76ee \u6dfb\u52a0\u6570\u636e\u6e90 \u9009\u53d6\u6570\u636e\u663e\u793a\u5f62\u5f0f \u6dfb\u52a0\u8ba1\u7b97","title":"\u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e"},{"location":"Business_Intelligence/Oracle_BIEE/#spark-sql","text":"","title":"\u5bf9\u63a5Spark SQL"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_2","text":"Kerberos\u8ba4\u8bc1 Kerberos\u83b7\u53d6\u8ba4\u8bc1\u7968\u636e \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7d\u5b89\u88c5 Simba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6eDSN\uff1a \u6d4b\u8bd5ODBC\u8fde\u63a5\uff1a","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#birdp","text":"\u65b0\u5efaobiee-spark.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 Sample Simba Spark DSN","title":"BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#rdp","text":"\u4e0a\u4f20RDP","title":"\u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_3","text":"Kerberos\u8ba4\u8bc1 su oracle kinit test_cn \u5b89\u88c5\u914d\u7f6eSimba Spark ODBC Driver \u4e0b\u8f7dSimba Spark ODBC Driver\uff1a \u4e0b\u8f7d\u5730\u5740 rpm -Uvh SimbaSparkODBC-1.2.2.1002-1.el6.x86_64.rpm \u4fee\u6539DSN\u914d\u7f6e\uff0c\u589e\u52a0Sample Simba Spark DSN\uff0c\u4e0eClient\u7aef\u914d\u7f6e\u76f8\u540c vi /etc/odbc.ini \u4fee\u6539odbcinst.ini\uff0c vi /etc/odbcinist.ini \u914d\u7f6e\u73af\u5883\u53d8\u91cf vi /etc/profile \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6d4b\u8bd5ODBC\u8fde\u63a5 su oracle isql -v 'Sample Simba Spark DSN' BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#spark","text":"\u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"\u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e"},{"location":"Business_Intelligence/Oracle_BIEE/#libraelk","text":"\u914d\u7f6eLibrA\u4e0eELK\u7684\u65b9\u5f0f\u6ca1\u6709\u533a\u522b\uff0c\u4ee5\u4e0b\u4ee5\u5bf9\u63a5ELK\u4e3a\u4f8b\u8fdb\u884c\u64cd\u4f5c","title":"\u5bf9\u63a5LibrA/ELK"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_4","text":"\u914d\u7f6eobiee\u5ba2\u6237\u7aef\u7684ODBC\u9a71\u52a8 \u6309\u7167ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684\u6307\u5bfc\u5b89\u88c5\u914d\u7f6eELK\u7684windows\u9a71\u52a8 \u914d\u7f6eDSN\uff0c\u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u4fdd\u5b58ODBC\u8fde\u63a5","title":"\u914d\u7f6e\u5ba2\u6237\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#birdp_1","text":"\u65b0\u5efaobiee-elk.rdp\uff0cDSN\u9009\u62e9\u4e0a\u4e00\u6b65\u914d\u7f6e\u7684 PostgreSQL35W","title":"BI\u7ba1\u7406\u5de5\u5177\u65b0\u5efaRDP"},{"location":"Business_Intelligence/Oracle_BIEE/#rdp_1","text":"\u4e0a\u4f20RDP","title":"\u4e0a\u4f20RDP\u6587\u4ef6\u5230\u670d\u52a1\u7aef"},{"location":"Business_Intelligence/Oracle_BIEE/#dsn_5","text":"\u53c2\u8003LibrA/ELK\u7684\u4ea7\u54c1\u6587\u6863\u7684Linux\u4e0b\u914d\u7f6e\u6570\u636e\u6e90\u7ae0\u8282\uff0c\u5b8c\u6210obiee\u8282\u70b9\u4e0b\u7684ODBC\u9a71\u52a8\u7684\u5b89\u88c5 \u6d4b\u8bd5ODBC\u8fde\u63a5\uff0c\u786e\u4fddODBC\u9a71\u52a8\u5b89\u88c5\u6210\u529f isql -v PostgreSQL35W BI\u57df\u914d\u7f6e\u7cfb\u7edfODBC cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/config/fmwconfig/bienv/core vi odbc.ini \u5728ODBC Data Sources\u90e8\u5206\u589e\u52a0PostgreSQL35W\u7684DSN \u5728\u6587\u4ef6\u672b\u5c3e\u589e\u52a0PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e PostgreSQL35W\u7684DSN\u7684\u8be6\u7ec6\u914d\u7f6e\u6700\u540e\u4e00\u884cDriverUnicodeType=1\u9700\u8981\u52a0\u4e0a\uff0c\u5426\u5219obiee\u67e5\u8be2\u7684\u65f6\u5019\u4f1a\u62a5\u9519[nQSError: 12010] Communication error connecting to remote end point: address = obiee; port = 9514. (HY000) \u91cd\u542fOBIS su oracle cd /Oracle/Middleware/Oracle_Home/user_projects/domains/bi/bitools/bin ./stop.sh ./start.sh","title":"\u914d\u7f6e\u670d\u52a1\u7aef\u7cfb\u7edfDSN"},{"location":"Business_Intelligence/Oracle_BIEE/#spark_1","text":"\u53c2\u8003 \u670d\u52a1\u7aef\u5206\u6790Hive\u6570\u636e","title":"\u670d\u52a1\u7aef\u5206\u6790Spark\u6570\u636e"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/","text":"PowerBI\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Power BI 2.75.5649.861 \u2194 FusionInsight HD 6.5 (Hive/Spark2x/FTP-Server) Power BI 2.75.5649.861 \u2194 FusionInsight MRS 8.0 (Hive/FTP-Server) \u7b80\u4ecb \u00b6 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHive\u3001Spark2x\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, first_name STRING, last_name STRING, subject_id INT, score FLOAT); INSERT INTO student VALUES (1,'Tom','Zhang',1,80); INSERT INTO student VALUES (2,'Sandy','Li',2,75); INSERT INTO student VALUES (3,'Benny','Chow',3,76); INSERT INTO student VALUES (4,'Tina','Wang',1,60); INSERT INTO student VALUES (5,'Tracy','Zhang',1,80); INSERT INTO student VALUES (6,'Andy','Li',2,79); INSERT INTO student VALUES (7,'Manson','Chow',3,86); INSERT INTO student VALUES (8,'Aurora','Wang',1,90); \u672c\u5730\u5df2\u5b58\u5728Subject.xlsx\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a \u4ece https://www.microsoft.com/en-us/download/details.aspx?id=58494 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u7684Power BI Desktop\u5e76\u5b89\u88c5\u3002\u672c\u6587\u7248\u672c\u4e3a PBIDesktopSetup_x64.exe \u3002 \u8bf4\u660e\uff1a\u53ea\u5728\u672c\u5730\u521b\u5efa\u62a5\u8868\uff0c\u4e0d\u9700\u8981\u6ce8\u518c\u8d26\u53f7\u3002\u5982\u679c\u9700\u8981\u53d1\u5e03\u62a5\u8868\u4e0e\u4ed6\u4eba\u5171\u4eab\uff0c\u5219\u9700\u8981\u6ce8\u518c\u8d26\u53f7\u3002 \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002\u672c\u6587\u7248\u672c\u4e3a kfw-4.1-amd64.msi \u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bf4\u660e\uff1a C:\\ProgramData \u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u5728\u201c\u6587\u4ef6\u5939\u548c\u641c\u7d22\u9009\u9879->\u67e5\u770b\u201d\u4e2d\u8bbe\u7f6e\u201c\u663e\u793a\u9690\u85cf\u7684\u6587\u4ef6\u3001\u6587\u4ef6\u5939\u6216\u9a71\u52a8\u5668\u201d\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u91cd\u542f\u673a\u5668\u8ba9\u65b0\u589e\u7684\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u8bf4\u660e\uff1a\u7968\u636e\u8fc7\u671f\u540e\u9700\u8981\u91cd\u65b0\u83b7\u53d6\u3002 \u914d\u7f6eHive\u6570\u636e\u6e90 \u00b6 Power BI\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3\u3002 \u4ece https://www.microsoft.com/en-us/download/details.aspx?id=40886 \u4e0b\u8f7dMicrosoft Hive ODBC Driver\u5e76\u5b89\u88c5\u3002\u672c\u6587\u7248\u672c\u4e3a HiveODBC64.msi \u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Microsoft Hive ODBC Driver -> 64-bit ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Microsoft Hive ODBC Driver -> Finish \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff08\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u503c\uff09\uff1a Data Source Name: ms_hive_odbc\uff0c\u53ef\u81ea\u5b9a\u4e49\u3002 Host(s): 172.16.4.21\uff0cHive Service\u4e3b\u8282\u70b9 Port\uff1a21066\uff0cHive Service\u7aef\u53e3 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a Thrift Transport: SASL SSL Options: \u53d6\u6d88\u52fe\u9009\u201cEnable SSL\u201d \u8bf4\u660e\uff1aAdvanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb Test \u6309\u94ae\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u8fd4\u56de\u201cSUCCESS\u201d\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Power BI\u5bf9\u63a5Hive \u00b6 Power BI\u542f\u52a8\u540e\u70b9\u51fb Get data \u6216\u8005 home->Get Data->More \u3002 \u5728\u641c\u7d22\u6846\u8f93\u5165 odbc \u540e\u9009\u62e9 ODBC \uff0c\u70b9\u51fb Connect \u3002 \u201cData source name (DSN)\u201d \u9009\u62e9 ms_hive_odbc \uff0c\u70b9\u51fb OK \u3002 \u9009\u62e9 Windows \uff0c\u70b9\u51fb Connect \u3002 \u52fe\u9009 default \u6570\u636e\u5e93\u7684\u8868 student \uff0c\u70b9\u51fb Load \u3002 \u9009\u62e9 Data \u89c6\u56fe\u5373\u53ef\u9884\u89c8\u8868\u7684\u6570\u636e\u3002 \u914d\u7f6eSpark\u6570\u636e\u6e90 \u00b6 \u4ece https://www.microsoft.com/en-us/download/details.aspx?id=49883 \u4e0b\u8f7dMicrosoft Spark ODBC Driver\u5e76\u5b89\u88c5\u3002\u672c\u6587\u7248\u672c\u4e3a SparkODBC64.msi \u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Microsoft Spark ODBC Driver -> 64-bit ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Microsoft Spark ODBC Driver -> Finish \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff08\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u503c\uff09\uff1a Data Source Name: ms_spark2x_odbc\uff0c\u53ef\u81ea\u5b9a\u4e49\u3002 Spark Serve Type: SparkThriftServer(Spark1.1 and later) Host(s): 172.16.4.22\uff0cSpark2x\u7684JDBCServer2x\u4e3b\u8282\u70b9 Port\uff1a22550\uff0c\u4e3a\u5c5e\u6027hive.sever2.thrift.port\u7684\u503c Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a Thrift Transport: SASL SSL Options: \u53d6\u6d88\u52fe\u9009\u201cEnable SSL\u201d \u8bf4\u660e\uff1aAdvanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb Test \u6309\u94ae\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u8fd4\u56de\u201cSUCCESS\u201d\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Spark2x\u6210\u529f\u3002 Power BI\u5bf9\u63a5Spark2x \u00b6 Power BI\u5bf9\u63a5Spark2x\u6709\u4e24\u79cd\u65b9\u5f0f\u3002\u53ef\u4ee5\u9009\u62e9\u901a\u8fc7Spark ODBC\u5bf9\u63a5\uff0c\u6216\u8005\u901a\u8fc7Power BI\u63d0\u4f9b\u7684Spark\u65b9\u5f0f\u5bf9\u63a5\u3002 ODBC \u00b6 Power BI\u542f\u52a8\u540e\uff0c\u70b9\u51fb Get data \u6216\u8005 home->Get Data->More \u3002 \u5728\u641c\u7d22\u6846\u8f93\u5165 odbc \u540e\u9009\u62e9 ODBC \uff0c\u70b9\u51fb Connect \u3002 \u201cData source name (DSN)\u201d \u9009\u62e9 ms_spark2x_odbc \uff0c\u70b9\u51fb OK \u3002 \u9009\u62e9 Windows \uff0c\u70b9\u51fb Connect \u3002 \u52fe\u9009 default \u6570\u636e\u5e93\u7684\u8868 student \uff0c\u70b9\u51fb Load \u3002 \u9009\u62e9 Data \u89c6\u56fe\u5373\u53ef\u9884\u89c8\u8868\u7684\u6570\u636e\u3002 Spark \u00b6 Power BI\u542f\u52a8\u540e\uff0c\u70b9\u51fb Get data \u6216\u8005 home->Get Data->More \u3002 \u5728\u641c\u7d22\u6846\u8f93\u5165 spark \u540e\u9009\u62e9 Spark \uff0c\u70b9\u51fb Connect \u3002 \u201cServer\u201d\u8f93\u5165Spark2x\u7684JDBCServer2x\u4e3b\u8282\u70b9IP\uff0c\u4f8b\u5982 172.16.4.22 \uff0c\u201cProtocol\u201d\u9009\u62e9 Standard \uff0c\u70b9\u51fb OK \u3002 \u70b9\u51fb Windows \uff0c\u9009\u62e9 Use my current credentials \uff0c\u201cRealm\u201d\u8f93\u5165 HADOOP.COM \uff0c\u201cHost FQDN\u201d\u8f93\u5165 hadoop.hadoop.com \uff0c\u201cService Name\u201d\u8f93\u5165 spark2x \uff0c\u70b9\u51fb Connect \u3002 \u52fe\u9009\u8868 student \uff0c\u70b9\u51fb Load \u3002 \u9009\u62e9 Data \u89c6\u56fe\u5373\u53ef\u9884\u89c8\u8868\u7684\u6570\u636e\u3002 Power BI\u5bf9\u63a5FTP-Server \u00b6 \u767b\u5f55FusionInsight Manger\uff0c\u4fee\u6539FTP-Server\u7684\u914d\u7f6e ftp-enabled=true \u4fdd\u5b58\u540e\uff0c\u70b9\u51fb \u66f4\u591a->\u91cd\u542f \u91cd\u542fFTP-Server\u3002 \u767b\u5f55FusionInsight\u5ba2\u6237\u7aef\uff0c\u521b\u5efa\u6587\u4ef6powerbi_hdfs.txt\u5e76\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 vi /opt/powerbi_hdfs.txt hdfs dfs -put /opt/powerbi_hdfs.txt /tmp Power BI\u542f\u52a8\u540e\uff0c\u70b9\u51fb Get data \u6216\u8005 home->Get Data->More \u3002 \u5728\u641c\u7d22\u6846\u8f93\u5165 web \u540e\u9009\u62e9 Web \uff0c\u70b9\u51fb Connect \u3002 \u9009\u62e9 Basic \uff0c\u201cURL\u201d\u8f93\u5165 ftp://172.16.4.21:22021/tmp/powerbi_hdfs.txt \u3002 \u70b9\u51fb FTP \uff0c\u8f93\u5165FusionInsight\u7528\u6237\u540d developuser \u548c\u5bf9\u5e94\u7684\u5bc6\u7801\uff0c\u70b9\u51fb Connect \u3002 \u70b9\u51fb Load \u52a0\u8f7d\u6570\u636e\u3002 \u9009\u62e9 Data \u89c6\u56fe\u5373\u53ef\u9884\u89c8\u6570\u636e\u3002 Power BI\u8c03\u6574\u548c\u5408\u5e76\u591a\u4e2a\u6570\u636e\u6e90\u3002 \u00b6 \u4f7f\u7528Power BI\u8c03\u6574\u548c\u5408\u5e76\u4eceHive/Spark\u3001Excel\u5bfc\u5165\u7684\u591a\u4e2a\u6570\u636e\u6e90\uff0c\u8f93\u51fa\u62a5\u8868\u3002\u4ee5\u4e0b\u4ee5\u8c03\u6574\u3001\u5408\u5e76Hive\u548cExcel\u6570\u636e\u6e90\u4e3a\u4f8b\u3002\u8c03\u6574\u3001\u5408\u5e76Spark\u3001FTP-Server\u548cExcel\u6570\u636e\u6e90\u64cd\u4f5c\u7c7b\u4f3c\u3002 \u5bfc\u5165Excel\u6570\u636e\u6e90\u3002 \u70b9\u51fb home->Get Data->Excel \u5bfc\u5165\u672c\u5730\u6587\u4ef6Subject.xlsx\u3002 \u52fe\u9009 Subject \uff0c\u70b9\u51fb Load \u3002 \u70b9\u51fb home->Edit Queries \u8fdb\u5165Power Query\u7f16\u8f91\u5668\u8fdb\u884c\u8c03\u6574\u548c\u7ec4\u5408\u64cd\u4f5c\u3002 \u5408\u5e76\u5217\uff1a \u5728Power Query\u7f16\u8f91\u5668\u4e2d\uff0c\u6309\u4e0bCtrl\u952e\u9009\u4e2d\u67e5\u8be2student\u7684first_name\u548clast_name\u5217\uff0c\u7136\u540e\u70b9\u51fb Transform->Merge Columns \u5c06\u8fd9\u4e24\u5217\u5408\u5e76\uff0c\u5e76\u547d\u540d\u4e3a name \u3002 \u5408\u5e76\u67e5\u8be2\uff1a \u5728Power Query\u7f16\u8f91\u5668\u4e2d\uff0c\u9009\u4e2d\u67e5\u8be2student\uff0c\u70b9\u51fb home->Merge Queries \uff0c \u9009\u4e2d student.subject_id \u548c Subject.id \uff0c\u201cJoin Kind\u201d\u9009\u62e9 Left Outer \uff0c\u70b9\u51fb OK \u3002 \u70b9\u51fb \u6309\u94ae\u53ef\u5c55\u5f00\u9690\u85cf\u7684\u5217\u3002 \u5220\u9664\u5217\uff1a \u70b9\u51fb home->Remove Cloumns \u53ef\u5c06\u4e0d\u9700\u8981\u7684\u5217\u5220\u9664\u3002\u5c06id\u3001subject_id\u3001subject.id\u3001subject.description\u5220\u9664\u3002 \u91cd\u547d\u540d\u5217\uff1a \u53cc\u51fb\u5217\u540d\u201csubject.name\u201d\u91cd\u547d\u540d\u4e3a\u201csubject_name\u201d\u3002 \u70b9\u51fb Close & Apply \u5173\u95edPower Query\u7f16\u8f91\u5668\u5e76\u5e94\u7528\u4fee\u6539\u3002 \u521b\u5efa\u62a5\u8868\u3002\u9009\u62e9 \u62a5\u8868 \u89c6\u56fe\u3002\u4f9d\u6b21\u52fe\u9009\u67e5\u8be2student\u7684 Subject_name \u3001 score \u3002\u201c\u503c\u201d\u9009\u62e9 Average of score \uff0c\u5e76\u70b9\u51fb\u56fe\u8868\u53f3\u4e0a\u89d2\u7684 ... \u6309\u94ae\u9009\u62e9 Show data \u3002\u62a5\u8868\u663e\u793a\u5982\u4e0b\uff1a","title":"2.75.5649.861 <--> 8.0"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#powerbifusioninsight","text":"","title":"PowerBI\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#_1","text":"Power BI 2.75.5649.861 \u2194 FusionInsight HD 6.5 (Hive/Spark2x/FTP-Server) Power BI 2.75.5649.861 \u2194 FusionInsight MRS 8.0 (Hive/FTP-Server)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#_2","text":"","title":"\u7b80\u4ecb"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHive\u3001Spark2x\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, first_name STRING, last_name STRING, subject_id INT, score FLOAT); INSERT INTO student VALUES (1,'Tom','Zhang',1,80); INSERT INTO student VALUES (2,'Sandy','Li',2,75); INSERT INTO student VALUES (3,'Benny','Chow',3,76); INSERT INTO student VALUES (4,'Tina','Wang',1,60); INSERT INTO student VALUES (5,'Tracy','Zhang',1,80); INSERT INTO student VALUES (6,'Andy','Li',2,79); INSERT INTO student VALUES (7,'Manson','Chow',3,86); INSERT INTO student VALUES (8,'Aurora','Wang',1,90); \u672c\u5730\u5df2\u5b58\u5728Subject.xlsx\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a \u4ece https://www.microsoft.com/en-us/download/details.aspx?id=58494 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u7684Power BI Desktop\u5e76\u5b89\u88c5\u3002\u672c\u6587\u7248\u672c\u4e3a PBIDesktopSetup_x64.exe \u3002 \u8bf4\u660e\uff1a\u53ea\u5728\u672c\u5730\u521b\u5efa\u62a5\u8868\uff0c\u4e0d\u9700\u8981\u6ce8\u518c\u8d26\u53f7\u3002\u5982\u679c\u9700\u8981\u53d1\u5e03\u62a5\u8868\u4e0e\u4ed6\u4eba\u5171\u4eab\uff0c\u5219\u9700\u8981\u6ce8\u518c\u8d26\u53f7\u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#windowskerberos","text":"\u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002\u672c\u6587\u7248\u672c\u4e3a kfw-4.1-amd64.msi \u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bf4\u660e\uff1a C:\\ProgramData \u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u5728\u201c\u6587\u4ef6\u5939\u548c\u641c\u7d22\u9009\u9879->\u67e5\u770b\u201d\u4e2d\u8bbe\u7f6e\u201c\u663e\u793a\u9690\u85cf\u7684\u6587\u4ef6\u3001\u6587\u4ef6\u5939\u6216\u9a71\u52a8\u5668\u201d\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u91cd\u542f\u673a\u5668\u8ba9\u65b0\u589e\u7684\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u8bf4\u660e\uff1a\u7968\u636e\u8fc7\u671f\u540e\u9700\u8981\u91cd\u65b0\u83b7\u53d6\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#hive","text":"Power BI\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3\u3002 \u4ece https://www.microsoft.com/en-us/download/details.aspx?id=40886 \u4e0b\u8f7dMicrosoft Hive ODBC Driver\u5e76\u5b89\u88c5\u3002\u672c\u6587\u7248\u672c\u4e3a HiveODBC64.msi \u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Microsoft Hive ODBC Driver -> 64-bit ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Microsoft Hive ODBC Driver -> Finish \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff08\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u503c\uff09\uff1a Data Source Name: ms_hive_odbc\uff0c\u53ef\u81ea\u5b9a\u4e49\u3002 Host(s): 172.16.4.21\uff0cHive Service\u4e3b\u8282\u70b9 Port\uff1a21066\uff0cHive Service\u7aef\u53e3 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a Thrift Transport: SASL SSL Options: \u53d6\u6d88\u52fe\u9009\u201cEnable SSL\u201d \u8bf4\u660e\uff1aAdvanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb Test \u6309\u94ae\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u8fd4\u56de\u201cSUCCESS\u201d\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002","title":"\u914d\u7f6eHive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#power-bihive","text":"Power BI\u542f\u52a8\u540e\u70b9\u51fb Get data \u6216\u8005 home->Get Data->More \u3002 \u5728\u641c\u7d22\u6846\u8f93\u5165 odbc \u540e\u9009\u62e9 ODBC \uff0c\u70b9\u51fb Connect \u3002 \u201cData source name (DSN)\u201d \u9009\u62e9 ms_hive_odbc \uff0c\u70b9\u51fb OK \u3002 \u9009\u62e9 Windows \uff0c\u70b9\u51fb Connect \u3002 \u52fe\u9009 default \u6570\u636e\u5e93\u7684\u8868 student \uff0c\u70b9\u51fb Load \u3002 \u9009\u62e9 Data \u89c6\u56fe\u5373\u53ef\u9884\u89c8\u8868\u7684\u6570\u636e\u3002","title":"Power BI\u5bf9\u63a5Hive"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#spark","text":"\u4ece https://www.microsoft.com/en-us/download/details.aspx?id=49883 \u4e0b\u8f7dMicrosoft Spark ODBC Driver\u5e76\u5b89\u88c5\u3002\u672c\u6587\u7248\u672c\u4e3a SparkODBC64.msi \u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Microsoft Spark ODBC Driver -> 64-bit ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Microsoft Spark ODBC Driver -> Finish \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff08\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u503c\uff09\uff1a Data Source Name: ms_spark2x_odbc\uff0c\u53ef\u81ea\u5b9a\u4e49\u3002 Spark Serve Type: SparkThriftServer(Spark1.1 and later) Host(s): 172.16.4.22\uff0cSpark2x\u7684JDBCServer2x\u4e3b\u8282\u70b9 Port\uff1a22550\uff0c\u4e3a\u5c5e\u6027hive.sever2.thrift.port\u7684\u503c Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a Thrift Transport: SASL SSL Options: \u53d6\u6d88\u52fe\u9009\u201cEnable SSL\u201d \u8bf4\u660e\uff1aAdvanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb Test \u6309\u94ae\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u8fd4\u56de\u201cSUCCESS\u201d\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Spark2x\u6210\u529f\u3002","title":"\u914d\u7f6eSpark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#power-bispark2x","text":"Power BI\u5bf9\u63a5Spark2x\u6709\u4e24\u79cd\u65b9\u5f0f\u3002\u53ef\u4ee5\u9009\u62e9\u901a\u8fc7Spark ODBC\u5bf9\u63a5\uff0c\u6216\u8005\u901a\u8fc7Power BI\u63d0\u4f9b\u7684Spark\u65b9\u5f0f\u5bf9\u63a5\u3002","title":"Power BI\u5bf9\u63a5Spark2x"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#odbc","text":"Power BI\u542f\u52a8\u540e\uff0c\u70b9\u51fb Get data \u6216\u8005 home->Get Data->More \u3002 \u5728\u641c\u7d22\u6846\u8f93\u5165 odbc \u540e\u9009\u62e9 ODBC \uff0c\u70b9\u51fb Connect \u3002 \u201cData source name (DSN)\u201d \u9009\u62e9 ms_spark2x_odbc \uff0c\u70b9\u51fb OK \u3002 \u9009\u62e9 Windows \uff0c\u70b9\u51fb Connect \u3002 \u52fe\u9009 default \u6570\u636e\u5e93\u7684\u8868 student \uff0c\u70b9\u51fb Load \u3002 \u9009\u62e9 Data \u89c6\u56fe\u5373\u53ef\u9884\u89c8\u8868\u7684\u6570\u636e\u3002","title":"ODBC"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#spark_1","text":"Power BI\u542f\u52a8\u540e\uff0c\u70b9\u51fb Get data \u6216\u8005 home->Get Data->More \u3002 \u5728\u641c\u7d22\u6846\u8f93\u5165 spark \u540e\u9009\u62e9 Spark \uff0c\u70b9\u51fb Connect \u3002 \u201cServer\u201d\u8f93\u5165Spark2x\u7684JDBCServer2x\u4e3b\u8282\u70b9IP\uff0c\u4f8b\u5982 172.16.4.22 \uff0c\u201cProtocol\u201d\u9009\u62e9 Standard \uff0c\u70b9\u51fb OK \u3002 \u70b9\u51fb Windows \uff0c\u9009\u62e9 Use my current credentials \uff0c\u201cRealm\u201d\u8f93\u5165 HADOOP.COM \uff0c\u201cHost FQDN\u201d\u8f93\u5165 hadoop.hadoop.com \uff0c\u201cService Name\u201d\u8f93\u5165 spark2x \uff0c\u70b9\u51fb Connect \u3002 \u52fe\u9009\u8868 student \uff0c\u70b9\u51fb Load \u3002 \u9009\u62e9 Data \u89c6\u56fe\u5373\u53ef\u9884\u89c8\u8868\u7684\u6570\u636e\u3002","title":"Spark"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#power-biftp-server","text":"\u767b\u5f55FusionInsight Manger\uff0c\u4fee\u6539FTP-Server\u7684\u914d\u7f6e ftp-enabled=true \u4fdd\u5b58\u540e\uff0c\u70b9\u51fb \u66f4\u591a->\u91cd\u542f \u91cd\u542fFTP-Server\u3002 \u767b\u5f55FusionInsight\u5ba2\u6237\u7aef\uff0c\u521b\u5efa\u6587\u4ef6powerbi_hdfs.txt\u5e76\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 vi /opt/powerbi_hdfs.txt hdfs dfs -put /opt/powerbi_hdfs.txt /tmp Power BI\u542f\u52a8\u540e\uff0c\u70b9\u51fb Get data \u6216\u8005 home->Get Data->More \u3002 \u5728\u641c\u7d22\u6846\u8f93\u5165 web \u540e\u9009\u62e9 Web \uff0c\u70b9\u51fb Connect \u3002 \u9009\u62e9 Basic \uff0c\u201cURL\u201d\u8f93\u5165 ftp://172.16.4.21:22021/tmp/powerbi_hdfs.txt \u3002 \u70b9\u51fb FTP \uff0c\u8f93\u5165FusionInsight\u7528\u6237\u540d developuser \u548c\u5bf9\u5e94\u7684\u5bc6\u7801\uff0c\u70b9\u51fb Connect \u3002 \u70b9\u51fb Load \u52a0\u8f7d\u6570\u636e\u3002 \u9009\u62e9 Data \u89c6\u56fe\u5373\u53ef\u9884\u89c8\u6570\u636e\u3002","title":"Power BI\u5bf9\u63a5FTP-Server"},{"location":"Business_Intelligence/PowerBI_2.75.5649.861/#power-bi","text":"\u4f7f\u7528Power BI\u8c03\u6574\u548c\u5408\u5e76\u4eceHive/Spark\u3001Excel\u5bfc\u5165\u7684\u591a\u4e2a\u6570\u636e\u6e90\uff0c\u8f93\u51fa\u62a5\u8868\u3002\u4ee5\u4e0b\u4ee5\u8c03\u6574\u3001\u5408\u5e76Hive\u548cExcel\u6570\u636e\u6e90\u4e3a\u4f8b\u3002\u8c03\u6574\u3001\u5408\u5e76Spark\u3001FTP-Server\u548cExcel\u6570\u636e\u6e90\u64cd\u4f5c\u7c7b\u4f3c\u3002 \u5bfc\u5165Excel\u6570\u636e\u6e90\u3002 \u70b9\u51fb home->Get Data->Excel \u5bfc\u5165\u672c\u5730\u6587\u4ef6Subject.xlsx\u3002 \u52fe\u9009 Subject \uff0c\u70b9\u51fb Load \u3002 \u70b9\u51fb home->Edit Queries \u8fdb\u5165Power Query\u7f16\u8f91\u5668\u8fdb\u884c\u8c03\u6574\u548c\u7ec4\u5408\u64cd\u4f5c\u3002 \u5408\u5e76\u5217\uff1a \u5728Power Query\u7f16\u8f91\u5668\u4e2d\uff0c\u6309\u4e0bCtrl\u952e\u9009\u4e2d\u67e5\u8be2student\u7684first_name\u548clast_name\u5217\uff0c\u7136\u540e\u70b9\u51fb Transform->Merge Columns \u5c06\u8fd9\u4e24\u5217\u5408\u5e76\uff0c\u5e76\u547d\u540d\u4e3a name \u3002 \u5408\u5e76\u67e5\u8be2\uff1a \u5728Power Query\u7f16\u8f91\u5668\u4e2d\uff0c\u9009\u4e2d\u67e5\u8be2student\uff0c\u70b9\u51fb home->Merge Queries \uff0c \u9009\u4e2d student.subject_id \u548c Subject.id \uff0c\u201cJoin Kind\u201d\u9009\u62e9 Left Outer \uff0c\u70b9\u51fb OK \u3002 \u70b9\u51fb \u6309\u94ae\u53ef\u5c55\u5f00\u9690\u85cf\u7684\u5217\u3002 \u5220\u9664\u5217\uff1a \u70b9\u51fb home->Remove Cloumns \u53ef\u5c06\u4e0d\u9700\u8981\u7684\u5217\u5220\u9664\u3002\u5c06id\u3001subject_id\u3001subject.id\u3001subject.description\u5220\u9664\u3002 \u91cd\u547d\u540d\u5217\uff1a \u53cc\u51fb\u5217\u540d\u201csubject.name\u201d\u91cd\u547d\u540d\u4e3a\u201csubject_name\u201d\u3002 \u70b9\u51fb Close & Apply \u5173\u95edPower Query\u7f16\u8f91\u5668\u5e76\u5e94\u7528\u4fee\u6539\u3002 \u521b\u5efa\u62a5\u8868\u3002\u9009\u62e9 \u62a5\u8868 \u89c6\u56fe\u3002\u4f9d\u6b21\u52fe\u9009\u67e5\u8be2student\u7684 Subject_name \u3001 score \u3002\u201c\u503c\u201d\u9009\u62e9 Average of score \uff0c\u5e76\u70b9\u51fb\u56fe\u8868\u53f3\u4e0a\u89d2\u7684 ... \u6309\u94ae\u9009\u62e9 Show data \u3002\u62a5\u8868\u663e\u793a\u5982\u4e0b\uff1a","title":"Power BI\u8c03\u6574\u548c\u5408\u5e76\u591a\u4e2a\u6570\u636e\u6e90\u3002"},{"location":"Business_Intelligence/QlikSense/","text":"Qlik Sense\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Qlik Sense 3.2.4 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Qlik Sense 3.2.4 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL)","title":"3.2.4 <--> 6.5"},{"location":"Business_Intelligence/QlikSense/#qlik-sensefusioninsight","text":"","title":"Qlik Sense\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/QlikSense/#_1","text":"Qlik Sense 3.2.4 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Qlik Sense 3.2.4 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/QlikView/","text":"QlikView\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 QlikView 12 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight MRS 8.0 (Hive/SparkSQL) \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\uff0c\u5730\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684**\u521b\u5efa\u7528\u6237**\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201csparkdemo\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230C:\\ProgramData\\MIT\\Kerberos5\u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u547d\u4ee4\u884c\u8fdb\u5165\u5230MIT Kerberos\u5b89\u88c5\u8def\u5f84\uff0c\u627e\u5230\u53ef\u6267\u884c\u6587\u4ef6kinit.exe\uff0c\u4f8b\u5982\u672c\u6587\u8def\u5f84\u4e3a\uff1a C:\\Program Files\\MIT\\Kerberos\\bin \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a kinit -k -t /path_to_userkeytab/user.keytab UserName \u5176\u4e2dpath_to_userkeytab\u4e3a\u5b58\u653e\u7528\u6237keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0cuser.keytab\u4e3a\u7528\u6237\u7684keytab\uff0cUserName\u4e3a\u7528\u6237\u540d\u3002 \u914d\u7f6eHive\u6570\u636e\u6e90 \u00b6 QlikView\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3 \u4e0b\u8f7d\u5b89\u88c5Hive ODBC\u9a71\u52a8 \u00b6 \u4ece\u4ee5\u4e0b\u5730\u5740\u4e0b\u8f7d\u9a71\u52a8\u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\uff1a \u4e0b\u8f7d\u5730\u5740 \u914d\u7f6e\u7528\u6237DSN \u00b6 \u5728OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668\u9875\u9762\u7684\u7528\u6237DSN\u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb\u6dfb\u52a0\uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Cloudera ODBC Driver for Apache Hive \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u914d\u7f6eHive\u6570\u636e\u6e90\u3002 Data Source Name\uff1a\u4e3a\u81ea\u5b9a\u4e49\u53c2\u6570 Host(s)\uff1a HiveServer\u7684\u4e1a\u52a1ip Port\uff1a Hive Service\u7aef\u53e3\uff0c21066 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a hive Realm\uff1a \u7559\u7a7a \u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\u5219\u8868\u793a\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb OK \u8fde\u63a5Hive\u6570\u636e\u6e90 \u00b6 \u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728**\u8fde\u63a5\u5230\u6570\u636e\u6e90**\u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90hive_odbc\uff0c\u7136\u540e\u70b9\u51fb**\u786e\u5b9a**\uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002 \u914d\u7f6eSpark\u6570\u636e\u6e90 \u00b6 QlivView\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\uff0c\u5bf9\u63a5SparkSQL\u7684thrift\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5b89\u88c5Spark\u7684ODBC\u9a71\u52a8 \u00b6 \u5728Simba\u5b98\u7f51\u4e0b\u8f7dSpark ODBC\u9a71\u52a8\uff0c\u6839\u636e\u7528\u6237\u81ea\u8eab\u64cd\u4f5c\u7cfb\u7edf\u9009\u62e932bit\u621664bit\uff0cData Source\u9009\u62e9Spark SQL\uff0c\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u6839\u636e\u5b89\u88c5\u5ba2\u6237\u7aef\u63d0\u793a\u5b89\u88c5\u5ba2\u6237\u7aef\u3002 \u914d\u7f6e\u7528\u6237DSN \u00b6 \u5728 OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\u7684 \u7528\u6237DSN \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Simba Spark ODBC Driver \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728 Simba Spark ODBC Driver DSN Setup \u9875\u9762\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\u3002 Data Source Name\uff1a \u81ea\u5b9a\u4e49 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a \u7559\u7a7a\uff0c Host(s)\uff1a JDBCServer(\u4e3b)\u7684\u4e1a\u52a1ip\uff0c Port\uff1a SparkThriftServer\u5ba2\u6237\u7aef\u7aef\u53e3\u53f723040\u3002 \u8bbe\u7f6e\u5b8c\u6bd5\u540e\u70b9\u51fb Advanced Options \uff0c\u5728\u5f39\u51fa\u7684 Advanced Options \u9875\u9762\u4e2d\uff0c\u52fe\u9009 Use Native Query \u548c Get Tables With Query \uff0c\u7136\u540e\u70b9\u51fb OK \u56de\u5230 Simba Spark ODBC Driver DSN Setup \uff0c\u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u9000\u51fa\u9875\u9762\uff0c\u5426\u5219\u5c06\u5f39\u51fa\u5931\u8d25\u5bf9\u8bdd\u6846\u3002 \u56de\u5230 Simba Spark ODBC Driver DSN Setup \u9875\u9762\uff0c\u70b9\u51fb OK \uff0c\u56de\u5230 ODBC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u5e76\u9000\u51fa\u914d\u7f6e\u3002 \u8fde\u63a5Spark\u6570\u636e\u6e90 \u00b6 \u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728 \u8fde\u63a5\u5230\u6570\u636e\u6e90 \u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90spark_odbc\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002 FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662f Host(s) \u3001 Port \u3001 Host FQDN \u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u5f55\u5165","title":"12 <--> 8.0"},{"location":"Business_Intelligence/QlikView/#qlikviewfusioninsight","text":"","title":"QlikView\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/QlikView/#_1","text":"QlikView 12 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL) QlikView 12 \u2194 FusionInsight MRS 8.0 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/QlikView/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\uff0c\u5730\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684**\u521b\u5efa\u7528\u6237**\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201csparkdemo\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230C:\\ProgramData\\MIT\\Kerberos5\u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u547d\u4ee4\u884c\u8fdb\u5165\u5230MIT Kerberos\u5b89\u88c5\u8def\u5f84\uff0c\u627e\u5230\u53ef\u6267\u884c\u6587\u4ef6kinit.exe\uff0c\u4f8b\u5982\u672c\u6587\u8def\u5f84\u4e3a\uff1a C:\\Program Files\\MIT\\Kerberos\\bin \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a kinit -k -t /path_to_userkeytab/user.keytab UserName \u5176\u4e2dpath_to_userkeytab\u4e3a\u5b58\u653e\u7528\u6237keytab\u6587\u4ef6\u7684\u8def\u5f84\uff0cuser.keytab\u4e3a\u7528\u6237\u7684keytab\uff0cUserName\u4e3a\u7528\u6237\u540d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Business_Intelligence/QlikView/#hive","text":"QlikView\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3","title":"\u914d\u7f6eHive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#hive-odbc","text":"\u4ece\u4ee5\u4e0b\u5730\u5740\u4e0b\u8f7d\u9a71\u52a8\u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\uff1a \u4e0b\u8f7d\u5730\u5740","title":"\u4e0b\u8f7d\u5b89\u88c5Hive ODBC\u9a71\u52a8"},{"location":"Business_Intelligence/QlikView/#dsn","text":"\u5728OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668\u9875\u9762\u7684\u7528\u6237DSN\u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb\u6dfb\u52a0\uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Cloudera ODBC Driver for Apache Hive \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u914d\u7f6eHive\u6570\u636e\u6e90\u3002 Data Source Name\uff1a\u4e3a\u81ea\u5b9a\u4e49\u53c2\u6570 Host(s)\uff1a HiveServer\u7684\u4e1a\u52a1ip Port\uff1a Hive Service\u7aef\u53e3\uff0c21066 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a hive Realm\uff1a \u7559\u7a7a \u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\u5219\u8868\u793a\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb OK","title":"\u914d\u7f6e\u7528\u6237DSN"},{"location":"Business_Intelligence/QlikView/#hive_1","text":"\u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728**\u8fde\u63a5\u5230\u6570\u636e\u6e90**\u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90hive_odbc\uff0c\u7136\u540e\u70b9\u51fb**\u786e\u5b9a**\uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002","title":"\u8fde\u63a5Hive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#spark","text":"QlivView\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\uff0c\u5bf9\u63a5SparkSQL\u7684thrift\u63a5\u53e3\u3002","title":"\u914d\u7f6eSpark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#sparkodbc","text":"\u5728Simba\u5b98\u7f51\u4e0b\u8f7dSpark ODBC\u9a71\u52a8\uff0c\u6839\u636e\u7528\u6237\u81ea\u8eab\u64cd\u4f5c\u7cfb\u7edf\u9009\u62e932bit\u621664bit\uff0cData Source\u9009\u62e9Spark SQL\uff0c\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u6839\u636e\u5b89\u88c5\u5ba2\u6237\u7aef\u63d0\u793a\u5b89\u88c5\u5ba2\u6237\u7aef\u3002","title":"\u4e0b\u8f7d\u5b89\u88c5Spark\u7684ODBC\u9a71\u52a8"},{"location":"Business_Intelligence/QlikView/#dsn_1","text":"\u5728 OBDC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\u7684 \u7528\u6237DSN \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u914d\u7f6e\u7528\u6237\u6570\u636e\u6e90\u3002 \u5728 \u521b\u5efa\u6570\u636e\u6e90 \u9875\u9762\uff0c\u627e\u5230 Simba Spark ODBC Driver \uff0c\u9009\u4e2d\u540e\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728 Simba Spark ODBC Driver DSN Setup \u9875\u9762\u4e2d\u914d\u7f6eSpark\u6570\u636e\u6e90\u3002 Data Source Name\uff1a \u81ea\u5b9a\u4e49 Mechanism\uff1a Kerberos Host FQDN\uff1a hadoop.hadoop.com Service Name\uff1a spark Realm\uff1a \u7559\u7a7a\uff0c Host(s)\uff1a JDBCServer(\u4e3b)\u7684\u4e1a\u52a1ip\uff0c Port\uff1a SparkThriftServer\u5ba2\u6237\u7aef\u7aef\u53e3\u53f723040\u3002 \u8bbe\u7f6e\u5b8c\u6bd5\u540e\u70b9\u51fb Advanced Options \uff0c\u5728\u5f39\u51fa\u7684 Advanced Options \u9875\u9762\u4e2d\uff0c\u52fe\u9009 Use Native Query \u548c Get Tables With Query \uff0c\u7136\u540e\u70b9\u51fb OK \u56de\u5230 Simba Spark ODBC Driver DSN Setup \uff0c\u70b9\u51fb Test \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u9000\u51fa\u9875\u9762\uff0c\u5426\u5219\u5c06\u5f39\u51fa\u5931\u8d25\u5bf9\u8bdd\u6846\u3002 \u56de\u5230 Simba Spark ODBC Driver DSN Setup \u9875\u9762\uff0c\u70b9\u51fb OK \uff0c\u56de\u5230 ODBC\u6570\u636e\u6e90\u7ba1\u7406\u5668 \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u5e76\u9000\u51fa\u914d\u7f6e\u3002","title":"\u914d\u7f6e\u7528\u6237DSN"},{"location":"Business_Intelligence/QlikView/#spark_1","text":"\u6253\u5f00QlikView 12\uff0c \u65b0\u5efa \u4e00\u4e2a\u6587\u6863 \u5173\u95ed\u5f39\u51fa\u7684\u5165\u95e8\u5411\u5bfc \u5728\u5de5\u5177\u680f\u4e2d\u6253\u5f00 \u7f16\u8f91\u811a\u672c \u6309\u94ae \u5728\u5f39\u51fa\u7684 \u7f16\u8f91\u811a\u672c \u9875\u9762\u4e0b\u65b9\uff0c\u70b9\u51fb \u6570\u636e \u6807\u7b7e\u9875\uff0c\u5728 \u6570\u636e\u5e93 \u7684\u4e0b\u62c9\u680f\u4e2d\u627e\u5230 OCBC \uff0c\u70b9\u51fb \u8fde\u63a5 \uff1b \u5728 \u8fde\u63a5\u5230\u6570\u636e\u6e90 \u9875\u9762\uff0c\u9009\u62e9\u4e0a\u9762\u914d\u7f6e\u7684\u6570\u636e\u6e90spark_odbc\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff1b \u5728 \u7f16\u8f91\u811a\u672c \u9875\u9762\u7684 \u6570\u636e \u6807\u7b7e\u9875\u4e2d\uff0c\u70b9\u51fb \u9009\u62e9 \u6309\u94ae \u5728 \u521b\u5efaSelect\u8bed\u53e5 \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u60f3\u8981\u5bfc\u5165\u7684 \u6570\u636e\u5e93\u8868\u683c \uff0c\u5728 \u5b57\u6bb5 \u4e2d\u9009\u62e9*\uff0c\u5219\u5bfc\u5165\u5b8c\u6574\u8868\u683c\uff0c\u5176\u4f59\u9009\u9879\u5219\u5bfc\u5165\u5176\u5bf9\u5e94\u7684\u8868\u683c\uff0c\u7136\u540e\u70b9\u51fb \u786e\u5b9a \uff08\u793a\u4f8b\u4e2d\u9009\u62e9*\uff09\uff1b \u56de\u5230 \u7f16\u8f91\u811a\u672c \u9875\u9762\uff0c\u70b9\u51fb \u786e\u5b9a \u56de\u5230QlikView\u5de5\u4f5c\u8868\u9875\u9762\uff0c\u70b9\u51fb \u91cd\u65b0\u52a0\u8f7d \uff0c\u5219\u53ef\u4ee5\u5c06\u6570\u636e\u5e93\u8868\u683c\u5bfc\u5165\u5230QlikView\u4e2d\u3002 \u7136\u540e\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u5236\u56fe\u5236\u8868\u5206\u6790\u7b49\u5904\u7406\uff0c\u5177\u4f53\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003QlikView\u5b98\u7f51\u7684\u4f7f\u7528\u6307\u5357\u3002","title":"\u8fde\u63a5Spark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/QlikView/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662f Host(s) \u3001 Port \u3001 Host FQDN \u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u5f55\u5165","title":"FAQ"},{"location":"Business_Intelligence/SSRS/","text":"SSRS\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SSRS 2017 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL) \u5b89\u88c5\u914d\u7f6e \u00b6 \u4e0b\u8f7d\u5b89\u88c5 \u00b6 Microsoft SQL Server 2017 Reporting Services \u4e0b\u8f7d: \u70b9\u51fb\u4e0b\u8f7d \u5b89\u88c5\u8fc7\u7a0b\uff1a \u70b9\u51fb\u67e5\u770b\u5b89\u88c5\u6307\u5bfc \u8f6f\u4ef6\u914d\u7f6e \u00b6 \u5b89\u88c5\u5b8c\u6210\u540e\u6253\u5f00Report Server Configuration Manager\uff0c\u9009\u62e9 Web Service URL , \u8bbe\u7f6e\u670d\u52a1\u7aef\u53e3\u3001\u865a\u62df\u76ee\u5f55 \u7b49\uff0c\u70b9\u51fb \u5e94\u7528 \u5b8c\u6210\u8bbe\u7f6e\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u6570\u636e\u9a71\u52a8\u53ca\u8ba4\u8bc1\u5de5\u5177\u5b89\u88c5 \u00b6 \u4e0b\u8f7d\u548c\u5b89\u88c532bit \u548c64 bit\u7684Microsoft ODBC\u9a71\u52a8: Microsoft Hive ODBC Driver\u4e0b\u8f7d\uff1a \u70b9\u51fb\u4e0b\u8f7d Microsoft Spark ODBC Driver\u4e0b\u8f7d\u5730\u5740\uff1a \u70b9\u51fb\u4e0b\u8f7d \u4e0b\u8f7d\u548c\u5b89\u88c5MIT Kerberos\u8ba4\u8bc1\u5ba2\u6237\u7aef: \u70b9\u51fb\u4e0b\u8f7d \u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\u3002\u8bf7\u8bb0\u4f4f\u5b89\u88c5\u8def\u5f84\uff0c\u4f8b\u5982\uff1a C:\\Program Files\\MIT\\Kerberos \u3002 \u53c2\u8003FusionInsight HD 6.5 \u6587\u6863\uff0c \u5e94\u7528\u5f00\u53d1\u6307\u5357 -> \u5b89\u5168\u6a21\u5f0f -> \u5b89\u5168\u8ba4\u8bc1 \u521b\u5efa\u5bf9\u63a5\u8d26\u53f7\u53ca\u914d\u7f6e\u76f8\u5173\u6743\u9650\u3002 \u4f7f\u7528\u5bf9\u63a5\u8d26\u53f7\u767b\u5f55FusionInsight\u7ba1\u7406\u754c\u9762\uff0c\u9f20\u6807\u505c\u7559\u5728\u53f3\u4e0a\u89d2\u663e\u793a\u7528\u6237\u5904\uff0c\u5728\u4e0b\u62c9\u663e\u793a\u6846\u4e2d\u9009\u62e9 \u4e0b\u8f7d\u7528\u6237\u51ed\u636e \uff0c\u9009\u62e9\u96c6\u7fa4\u5e76\u786e\u8ba4\u4e0b\u8f7d\uff0c\u4e0b\u8f7d\u89e3\u538b\u540e\u5305\u62ec krb5.conf \u548c user.keytab \u4e24\u4e2a\u6587\u4ef6\u3002 \u91cd\u547d\u540dkrb5.conf\u6587\u4ef6\u4e3a krb5.ini \uff0c\u5e76\u62f7\u8d1d\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002\u201cC:\\ProgramData\u201d\u76ee\u5f55\u4e00\u822c\u662f\u9690\u85cf\u7684\uff0c\u9700\u8981\u8bbe\u7f6e\u663e\u793a\u9690\u85cf\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6\u3002\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002\u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5_CONFIG \uff0c\u53d8\u91cf\u503c\u4e3a C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini \u3002 \u91cd\u542f\u670d\u52a1\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1\u3002\u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d\uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb **OK**\u5b8c\u6210\u8ba4\u8bc1\u3002 \u914d\u7f6eSpark\u548cHive ODBC DSN \u00b6 \u914d\u7f6eSpark ODBC\u3000DSN \u00b6 \u6253\u5f00Windows ODBC\u914d\u7f6e\u5de5\u5177\uff0c\u5728 System DSN \u4e2d\uff0c\u5206\u522b\u914d\u7f6e Sample Microsoft Hive DSN \u548c Sample Microsoft Spark DSN \uff0c\u76f8\u5173\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff0c\u6839\u636e\u5b9e\u9645\u73af\u5883\u66ff\u6362HOST\u5730\u5740\u3002 Data Source Name: Sample Microsoft Spark DSN Spark Serve Type: SparkThriftServer(Spark1.1 and later) Host(s): 172.16.11.22\uff0cSpark2x\u7684JDBCServer2x\u4e3b\u8282\u70b9 Port\uff1a22550 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a Thrift Transport: SASL \u53c2\u8003\u914d\u7f6e\uff1a \u5728 SSL OPTIONS \u4e2d\u53d6\u6d88\u52fe\u9009SSL\u6821\u9a8c\uff0c\u8bbe\u7f6e\u5982\u4e0b\u56fe\uff1a \u914d\u7f6eHIVE ODBC DSN \u00b6 HIVE DSN \u914d\u7f6e\u53c2\u8003\u9009\u9879\u5982\u4e0b\uff0c\u6839\u636e\u5b9e\u9645\u73af\u5883\u66ff\u6362HOST\u5730\u5740 Data Source Name: Sample Microsoft Hive DSN Host(s): 172.16.11.21\uff0cHive Service\u4e3b\u8282\u70b9 Port\uff1a21066\uff0cHive Service\u7aef\u53e3 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a Thrift Transport: SASL SSL Options: \u53d6\u6d88\u52fe\u9009\u201cEnable SSL\u201d \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a \u529f\u80fd\u9a8c\u8bc1 \u00b6 \u51c6\u5907\u6d4b\u8bd5\u6570\u636e \u00b6 \u901a\u8fc7beeline\u521b\u5efa\u6570\u636e\u5e93\u548c\u6570\u636e\u8868\uff0c\u5e76\u63d2\u5165\u90e8\u5206\u6d4b\u8bd5\u6570\u636e\u3002\u672c\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u521b\u5efa\u9500\u552e\u6d4b\u8bd5\u8868\uff0c\u5305\u62ec\u5458\u5de5ID\uff0c\u59d3\u540d name\uff0c\u5b63\u5ea6 quarter\uff0c\u9500\u552e\u989d sales\u7b49\u5217\uff0c\u63d2\u5165\u4e86\u90e8\u5206\u6d4b\u8bd5\u6570\u636e\uff0c\u901a\u8fc7SSRS\u62a5\u8868\u6765\u663e\u793a\u5458\u5de5\u6bcf\u4e2a\u5b63\u5ea6\u7684\u9500\u552e\u989d\u4ee5\u53ca\u5bf9\u6bd4\u76f4\u65b9\u56fe\u3002 create database sales; create table quarterTable(id string,name string, quarter int, sales bigint); insert into quarterTable values('00271715','tiekui',1,1000000); insert into quarterTable values('00271715','tiekui',2,4000000); insert into quarterTable values('00271715','tiekui',3,6000000); insert into quarterTable values('00271715','tiekui',4,4700000); insert into quarterTable values('00201234','zhangsan',1,1000000); insert into quarterTable values('00201234','zhangsan',2,2000000); insert into quarterTable values('00201234','zhangsan',3,3000000); insert into quarterTable values('00201234','zhangsan',4,3200000); \u521b\u5efaHive\u6570\u636e\u6e90 \u00b6 \u6253\u5f00Report Builder\u8f6f\u4ef6\uff0c\u53f3\u952e Data Sources \uff0c\u70b9\u51fb add data source ,\u8bbe\u7f6e Data Source Name \uff0c\u9009\u62e9 use a connection embedded in my report \uff0c select connection type \u4e0b\u62c9\u6761\u4e2d\u9009\u62e9 ODBC \uff0c\u70b9\u51fb build \uff0c\u5728\u5f39\u51fa\u6846\u4e2d\uff0c Data Source Specification \u4e2d\u9009\u62e9 use connection string \uff0c\u70b9\u51fb build \u6309\u94ae\uff0c\u5f39\u51fa\u6846\u4e2d\u9009\u62e9 Machine Data Source \uff0c\u9009\u62e9 Sample Microsoft Hive DSN \uff0c\u70b9\u51fb OK \u5b8c\u6210\u914d\u7f6e. \u521b\u5efaSpark2X \u6570\u636e\u6e90 \u00b6 \u6253\u5f00Report Builder\u8f6f\u4ef6\uff0c\u53f3\u952e Data Sources \uff0c\u70b9\u51fb add data source ,\u8bbe\u7f6e Data Source Name \uff0c\u9009\u62e9 use a connection embedded in my report \uff0c select connection type \u4e0b\u62c9\u6761\u4e2d\u9009\u62e9 ODBC \uff0c\u70b9\u51fb build \uff0c\u5728\u5f39\u51fa\u6846\u4e2d\uff0c Data Source Specification \u4e2d\u9009\u62e9 use connection string \uff0c\u70b9\u51fb build \u6309\u94ae\uff0c\u5f39\u51fa\u6846\u4e2d\u9009\u62e9 Machine Data Source \uff0c\u9009\u62e9 Sample Microsoft Spark DSN \uff0c\u70b9\u51fb OK \u5b8c\u6210\u914d\u7f6e. \u521b\u5efa\u6570\u636e\u96c6 \u00b6 \u5728Report Builder\u5de6\u4fa7\u89c6\u56fe\u4e2d\uff0c\u53f3\u952e Data Sets ,\u9009\u62e9 add dataset\" \uff0c\u8bbe\u7f6e\u6570\u636e\u96c6\u540d\u79f0\uff0c\u9009\u62e9 use a dataset embedded in my report \uff0c\u9009\u62e9data sources\u4e3a\u4e4b\u524d\u914d\u7f6e\u7684Hive\u6216\u8005Spark Data Sources\u3002\u5728Query Designer\u4e2d\u8f93\u5165SQL\u8bed\u53e5\u7b5b\u9009\u6570\u636e\u5217\uff0c\u4f8b\u5982 select * from sales.quarterTable ,\u70b9\u51fb OK ,\u5b8c\u6210\u914d\u7f6e\u3002\u5b8c\u6210\u8be5\u64cd\u4f5c\u4ee5\u540e\uff0c\u5c06\u5728\u5de6\u4fa7\u51fa\u73b0\u6570\u636e\u96c6\u7684\u5217\uff0c\u7528\u4e8e\u540e\u7eed\u7684\u6570\u636e\u5206\u6790\u3002 \u8bbe\u8ba1\u62a5\u8868 \u00b6 \u6839\u636e\u9700\u6c42\u8bbe\u8ba1\u62a5\u8868\uff0c\u672c\u6d4b\u8bd5\u8bbe\u8ba1\u7684\u56fe\u6807\u5982\u4e0b\uff0c\u5206\u522b\u57fa\u4e8eHive\u548cSpark\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u6570\u636e\u8868\u683c\uff0c\u521b\u5efa\u8fc7\u7a0b\u5982\u4e0b\uff1a \u9009\u62e9 Report Builder -> insert -> chart wizard \uff0c\u9009\u62e9Hive\u6570\u636e\u6e90\uff0c\u9009\u62e9 Bar \uff0ccatergories \u4f7f\u7528 quarter \u5217\uff0cseries \u4f7f\u7528 id \u5217\uff0cvalue\u9009\u62e9 sum(quaterTable_sales) \u70b9\u51fb Next \u548c Finish \uff0c\u5b8c\u6210\u914d\u7f6e\u3002\u4f7f\u7528\u76f8\u540c\u7684\u65b9\u5f0f\uff0c\u4f7f\u7528spark\u6570\u636e\u6e90\u521b\u5efa\u4e00\u4e2a\u8868\u683c\u3002 \u521b\u5efa\u540e\u7684\u6548\u679c\u5982\u4e0b\u56fe\uff1a \u70b9\u51fbReport Builder \u5de5\u5177\u5de6\u4e0a\u89e3 RUN \u6309\u94ae\u6d4b\u8bd5\uff0c\u5c06\u83b7\u53d6\u6570\u636e\u5e76\u751f\u6210\u62a5\u8868\uff0c\u6d4b\u8bd5\u7ed3\u679c\u5982\u4e0b\uff1a \u53d1\u5e03\u5230SSRS \u00b6 \u6d4b\u8bd5\u5b8c\u6210\u540e\uff0c\u9009\u62e9\u5de6\u4e0a\u89d2 File -> Publish Report Parts -> publish all report parts with defalt settings \uff0cReport Server\u5730\u5740\u4e3a http://{reportserver_ip}/ReportServer ,\u4e0a\u4f20\u5b8c\u6210\u540e\uff0c\u6253\u5f00Report Server\u7f51\u7ad9 http://{ip}/ReportServer , \u9009\u62e9\u5bf9\u5e94\u7684\u6587\u4ef6\uff0c\u67e5\u770b\u62a5\u8868\u7ed3\u679c\u5982\u4e0b\uff0c\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u67e5\u770b\u5230\u62a5\u8868\u6570\u636e\u3002 \u9a8c\u8bc1\u5b8c\u6210\uff0cSSRS\u62a5\u8868\u901a\u8fc7Hive ODBC\u548cSpark ODBC\u83b7\u53d6\u6570\u636e\u5e76\u6b63\u786e\u5448\u73b0\u3002","title":"2017 <--> 6.5"},{"location":"Business_Intelligence/SSRS/#ssrsfusioninsight","text":"","title":"SSRS\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/SSRS/#_1","text":"SSRS 2017 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/SSRS/#_2","text":"","title":"\u5b89\u88c5\u914d\u7f6e"},{"location":"Business_Intelligence/SSRS/#_3","text":"Microsoft SQL Server 2017 Reporting Services \u4e0b\u8f7d: \u70b9\u51fb\u4e0b\u8f7d \u5b89\u88c5\u8fc7\u7a0b\uff1a \u70b9\u51fb\u67e5\u770b\u5b89\u88c5\u6307\u5bfc","title":"\u4e0b\u8f7d\u5b89\u88c5"},{"location":"Business_Intelligence/SSRS/#_4","text":"\u5b89\u88c5\u5b8c\u6210\u540e\u6253\u5f00Report Server Configuration Manager\uff0c\u9009\u62e9 Web Service URL , \u8bbe\u7f6e\u670d\u52a1\u7aef\u53e3\u3001\u865a\u62df\u76ee\u5f55 \u7b49\uff0c\u70b9\u51fb \u5e94\u7528 \u5b8c\u6210\u8bbe\u7f6e\u3002","title":"\u8f6f\u4ef6\u914d\u7f6e"},{"location":"Business_Intelligence/SSRS/#_5","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Business_Intelligence/SSRS/#_6","text":"\u4e0b\u8f7d\u548c\u5b89\u88c532bit \u548c64 bit\u7684Microsoft ODBC\u9a71\u52a8: Microsoft Hive ODBC Driver\u4e0b\u8f7d\uff1a \u70b9\u51fb\u4e0b\u8f7d Microsoft Spark ODBC Driver\u4e0b\u8f7d\u5730\u5740\uff1a \u70b9\u51fb\u4e0b\u8f7d \u4e0b\u8f7d\u548c\u5b89\u88c5MIT Kerberos\u8ba4\u8bc1\u5ba2\u6237\u7aef: \u70b9\u51fb\u4e0b\u8f7d","title":"\u6570\u636e\u9a71\u52a8\u53ca\u8ba4\u8bc1\u5de5\u5177\u5b89\u88c5"},{"location":"Business_Intelligence/SSRS/#kerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\u3002\u8bf7\u8bb0\u4f4f\u5b89\u88c5\u8def\u5f84\uff0c\u4f8b\u5982\uff1a C:\\Program Files\\MIT\\Kerberos \u3002 \u53c2\u8003FusionInsight HD 6.5 \u6587\u6863\uff0c \u5e94\u7528\u5f00\u53d1\u6307\u5357 -> \u5b89\u5168\u6a21\u5f0f -> \u5b89\u5168\u8ba4\u8bc1 \u521b\u5efa\u5bf9\u63a5\u8d26\u53f7\u53ca\u914d\u7f6e\u76f8\u5173\u6743\u9650\u3002 \u4f7f\u7528\u5bf9\u63a5\u8d26\u53f7\u767b\u5f55FusionInsight\u7ba1\u7406\u754c\u9762\uff0c\u9f20\u6807\u505c\u7559\u5728\u53f3\u4e0a\u89d2\u663e\u793a\u7528\u6237\u5904\uff0c\u5728\u4e0b\u62c9\u663e\u793a\u6846\u4e2d\u9009\u62e9 \u4e0b\u8f7d\u7528\u6237\u51ed\u636e \uff0c\u9009\u62e9\u96c6\u7fa4\u5e76\u786e\u8ba4\u4e0b\u8f7d\uff0c\u4e0b\u8f7d\u89e3\u538b\u540e\u5305\u62ec krb5.conf \u548c user.keytab \u4e24\u4e2a\u6587\u4ef6\u3002 \u91cd\u547d\u540dkrb5.conf\u6587\u4ef6\u4e3a krb5.ini \uff0c\u5e76\u62f7\u8d1d\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002\u201cC:\\ProgramData\u201d\u76ee\u5f55\u4e00\u822c\u662f\u9690\u85cf\u7684\uff0c\u9700\u8981\u8bbe\u7f6e\u663e\u793a\u9690\u85cf\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6\u3002\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002\u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5_CONFIG \uff0c\u53d8\u91cf\u503c\u4e3a C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini \u3002 \u91cd\u542f\u670d\u52a1\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1\u3002\u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d\uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb **OK**\u5b8c\u6210\u8ba4\u8bc1\u3002","title":"\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Business_Intelligence/SSRS/#sparkhive-odbc-dsn","text":"","title":"\u914d\u7f6eSpark\u548cHive ODBC DSN"},{"location":"Business_Intelligence/SSRS/#spark-odbc-dsn","text":"\u6253\u5f00Windows ODBC\u914d\u7f6e\u5de5\u5177\uff0c\u5728 System DSN \u4e2d\uff0c\u5206\u522b\u914d\u7f6e Sample Microsoft Hive DSN \u548c Sample Microsoft Spark DSN \uff0c\u76f8\u5173\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff0c\u6839\u636e\u5b9e\u9645\u73af\u5883\u66ff\u6362HOST\u5730\u5740\u3002 Data Source Name: Sample Microsoft Spark DSN Spark Serve Type: SparkThriftServer(Spark1.1 and later) Host(s): 172.16.11.22\uff0cSpark2x\u7684JDBCServer2x\u4e3b\u8282\u70b9 Port\uff1a22550 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a Thrift Transport: SASL \u53c2\u8003\u914d\u7f6e\uff1a \u5728 SSL OPTIONS \u4e2d\u53d6\u6d88\u52fe\u9009SSL\u6821\u9a8c\uff0c\u8bbe\u7f6e\u5982\u4e0b\u56fe\uff1a","title":"\u914d\u7f6eSpark ODBC\u3000DSN"},{"location":"Business_Intelligence/SSRS/#hive-odbc-dsn","text":"HIVE DSN \u914d\u7f6e\u53c2\u8003\u9009\u9879\u5982\u4e0b\uff0c\u6839\u636e\u5b9e\u9645\u73af\u5883\u66ff\u6362HOST\u5730\u5740 Data Source Name: Sample Microsoft Hive DSN Host(s): 172.16.11.21\uff0cHive Service\u4e3b\u8282\u70b9 Port\uff1a21066\uff0cHive Service\u7aef\u53e3 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a Thrift Transport: SASL SSL Options: \u53d6\u6d88\u52fe\u9009\u201cEnable SSL\u201d \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a","title":"\u914d\u7f6eHIVE ODBC DSN"},{"location":"Business_Intelligence/SSRS/#_7","text":"","title":"\u529f\u80fd\u9a8c\u8bc1"},{"location":"Business_Intelligence/SSRS/#_8","text":"\u901a\u8fc7beeline\u521b\u5efa\u6570\u636e\u5e93\u548c\u6570\u636e\u8868\uff0c\u5e76\u63d2\u5165\u90e8\u5206\u6d4b\u8bd5\u6570\u636e\u3002\u672c\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u521b\u5efa\u9500\u552e\u6d4b\u8bd5\u8868\uff0c\u5305\u62ec\u5458\u5de5ID\uff0c\u59d3\u540d name\uff0c\u5b63\u5ea6 quarter\uff0c\u9500\u552e\u989d sales\u7b49\u5217\uff0c\u63d2\u5165\u4e86\u90e8\u5206\u6d4b\u8bd5\u6570\u636e\uff0c\u901a\u8fc7SSRS\u62a5\u8868\u6765\u663e\u793a\u5458\u5de5\u6bcf\u4e2a\u5b63\u5ea6\u7684\u9500\u552e\u989d\u4ee5\u53ca\u5bf9\u6bd4\u76f4\u65b9\u56fe\u3002 create database sales; create table quarterTable(id string,name string, quarter int, sales bigint); insert into quarterTable values('00271715','tiekui',1,1000000); insert into quarterTable values('00271715','tiekui',2,4000000); insert into quarterTable values('00271715','tiekui',3,6000000); insert into quarterTable values('00271715','tiekui',4,4700000); insert into quarterTable values('00201234','zhangsan',1,1000000); insert into quarterTable values('00201234','zhangsan',2,2000000); insert into quarterTable values('00201234','zhangsan',3,3000000); insert into quarterTable values('00201234','zhangsan',4,3200000);","title":"\u51c6\u5907\u6d4b\u8bd5\u6570\u636e"},{"location":"Business_Intelligence/SSRS/#hive","text":"\u6253\u5f00Report Builder\u8f6f\u4ef6\uff0c\u53f3\u952e Data Sources \uff0c\u70b9\u51fb add data source ,\u8bbe\u7f6e Data Source Name \uff0c\u9009\u62e9 use a connection embedded in my report \uff0c select connection type \u4e0b\u62c9\u6761\u4e2d\u9009\u62e9 ODBC \uff0c\u70b9\u51fb build \uff0c\u5728\u5f39\u51fa\u6846\u4e2d\uff0c Data Source Specification \u4e2d\u9009\u62e9 use connection string \uff0c\u70b9\u51fb build \u6309\u94ae\uff0c\u5f39\u51fa\u6846\u4e2d\u9009\u62e9 Machine Data Source \uff0c\u9009\u62e9 Sample Microsoft Hive DSN \uff0c\u70b9\u51fb OK \u5b8c\u6210\u914d\u7f6e.","title":"\u521b\u5efaHive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/SSRS/#spark2x","text":"\u6253\u5f00Report Builder\u8f6f\u4ef6\uff0c\u53f3\u952e Data Sources \uff0c\u70b9\u51fb add data source ,\u8bbe\u7f6e Data Source Name \uff0c\u9009\u62e9 use a connection embedded in my report \uff0c select connection type \u4e0b\u62c9\u6761\u4e2d\u9009\u62e9 ODBC \uff0c\u70b9\u51fb build \uff0c\u5728\u5f39\u51fa\u6846\u4e2d\uff0c Data Source Specification \u4e2d\u9009\u62e9 use connection string \uff0c\u70b9\u51fb build \u6309\u94ae\uff0c\u5f39\u51fa\u6846\u4e2d\u9009\u62e9 Machine Data Source \uff0c\u9009\u62e9 Sample Microsoft Spark DSN \uff0c\u70b9\u51fb OK \u5b8c\u6210\u914d\u7f6e.","title":"\u521b\u5efaSpark2X \u6570\u636e\u6e90"},{"location":"Business_Intelligence/SSRS/#_9","text":"\u5728Report Builder\u5de6\u4fa7\u89c6\u56fe\u4e2d\uff0c\u53f3\u952e Data Sets ,\u9009\u62e9 add dataset\" \uff0c\u8bbe\u7f6e\u6570\u636e\u96c6\u540d\u79f0\uff0c\u9009\u62e9 use a dataset embedded in my report \uff0c\u9009\u62e9data sources\u4e3a\u4e4b\u524d\u914d\u7f6e\u7684Hive\u6216\u8005Spark Data Sources\u3002\u5728Query Designer\u4e2d\u8f93\u5165SQL\u8bed\u53e5\u7b5b\u9009\u6570\u636e\u5217\uff0c\u4f8b\u5982 select * from sales.quarterTable ,\u70b9\u51fb OK ,\u5b8c\u6210\u914d\u7f6e\u3002\u5b8c\u6210\u8be5\u64cd\u4f5c\u4ee5\u540e\uff0c\u5c06\u5728\u5de6\u4fa7\u51fa\u73b0\u6570\u636e\u96c6\u7684\u5217\uff0c\u7528\u4e8e\u540e\u7eed\u7684\u6570\u636e\u5206\u6790\u3002","title":"\u521b\u5efa\u6570\u636e\u96c6"},{"location":"Business_Intelligence/SSRS/#_10","text":"\u6839\u636e\u9700\u6c42\u8bbe\u8ba1\u62a5\u8868\uff0c\u672c\u6d4b\u8bd5\u8bbe\u8ba1\u7684\u56fe\u6807\u5982\u4e0b\uff0c\u5206\u522b\u57fa\u4e8eHive\u548cSpark\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u6570\u636e\u8868\u683c\uff0c\u521b\u5efa\u8fc7\u7a0b\u5982\u4e0b\uff1a \u9009\u62e9 Report Builder -> insert -> chart wizard \uff0c\u9009\u62e9Hive\u6570\u636e\u6e90\uff0c\u9009\u62e9 Bar \uff0ccatergories \u4f7f\u7528 quarter \u5217\uff0cseries \u4f7f\u7528 id \u5217\uff0cvalue\u9009\u62e9 sum(quaterTable_sales) \u70b9\u51fb Next \u548c Finish \uff0c\u5b8c\u6210\u914d\u7f6e\u3002\u4f7f\u7528\u76f8\u540c\u7684\u65b9\u5f0f\uff0c\u4f7f\u7528spark\u6570\u636e\u6e90\u521b\u5efa\u4e00\u4e2a\u8868\u683c\u3002 \u521b\u5efa\u540e\u7684\u6548\u679c\u5982\u4e0b\u56fe\uff1a \u70b9\u51fbReport Builder \u5de5\u5177\u5de6\u4e0a\u89e3 RUN \u6309\u94ae\u6d4b\u8bd5\uff0c\u5c06\u83b7\u53d6\u6570\u636e\u5e76\u751f\u6210\u62a5\u8868\uff0c\u6d4b\u8bd5\u7ed3\u679c\u5982\u4e0b\uff1a","title":"\u8bbe\u8ba1\u62a5\u8868"},{"location":"Business_Intelligence/SSRS/#ssrs","text":"\u6d4b\u8bd5\u5b8c\u6210\u540e\uff0c\u9009\u62e9\u5de6\u4e0a\u89d2 File -> Publish Report Parts -> publish all report parts with defalt settings \uff0cReport Server\u5730\u5740\u4e3a http://{reportserver_ip}/ReportServer ,\u4e0a\u4f20\u5b8c\u6210\u540e\uff0c\u6253\u5f00Report Server\u7f51\u7ad9 http://{ip}/ReportServer , \u9009\u62e9\u5bf9\u5e94\u7684\u6587\u4ef6\uff0c\u67e5\u770b\u62a5\u8868\u7ed3\u679c\u5982\u4e0b\uff0c\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u67e5\u770b\u5230\u62a5\u8868\u6570\u636e\u3002 \u9a8c\u8bc1\u5b8c\u6210\uff0cSSRS\u62a5\u8868\u901a\u8fc7Hive ODBC\u548cSpark ODBC\u83b7\u53d6\u6570\u636e\u5e76\u6b63\u786e\u5448\u73b0\u3002","title":"\u53d1\u5e03\u5230SSRS"},{"location":"Business_Intelligence/SmartBI/","text":"SmartBI\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SmartBI 7.2.32464.17374 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL)","title":"7.2.32464.17374 <--> C70"},{"location":"Business_Intelligence/SmartBI/#smartbifusioninsight","text":"","title":"SmartBI\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/SmartBI/#_1","text":"SmartBI 7.2.32464.17374 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Tableau/","text":"Tableau\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Tableau 10.0.0 \u2194 FusionInsight HD V100R002C30 (Hive/SparkSQL) Tableau 10.0.0 \u2194 FusionInsight HD V100R002C50 (Hive/SparkSQL) Tableau 10.1.4 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Tableau 10.3.2 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight MRS 8.0 (Hive/SparkSQL) \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctableau\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002 \u914d\u7f6eHive\u6570\u636e\u6e90 \u00b6 Tableau\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a \u4e0b\u8f7d\u5730\u5740 \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb\u4e2d\u7684Test\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Tableau\u4f7f\u7528\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201c\u5176\u4ed6\u6570\u636e\u5e93\uff08ODBC\uff09\u201d\uff1b DSN\u9009\u62e9hive_odbc\uff08\u4e0a\u4e00\u6b65\u4e2d\u8bbe\u7f6eODBC\u7684\u540d\u79f0\uff09\uff0c\u70b9\u51fb\u8fde\u63a5\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\uff0c\u7136\u540e\u767b\u9646\u3002 \u67e5\u8be2\u767e\u4e07\u7ea7\u6570\u636e\u8868\u6570\u636e \u67e5\u8be2\u591a\u8868\u6570\u636e \u914d\u7f6eSpark\u6570\u636e\u6e90 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5spark\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u521b\u5efaDSN\uff08Data Source Name\uff09 \u6253\u5f00 C:\\Program Files\\Simba Spark ODBC Driver\\lib\\DriverConfiguration64.exe \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 Tableau\u4f7f\u7528Spark\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201cSpark SQL\u201d\uff0c\u4f5c\u5982\u4e0b\u914d\u7f6e\uff1a \u5176\u4e2d\u670d\u52a1\u5668\u4e3aJDBCServer(\u4e3b)\u7684\u4e1a\u52a1IP\u3002 \u7aef\u53e3\u4e3aFusionInsight\u4e2dSpark\u670d\u52a1\u914d\u7f6e\uff0c\u5bfc\u51fa\u670d\u52a1\u914d\u7f6e\u6587\u4ef6\uff0c\u5176\u4e2d hive.server2.thrift.port \u5bf9\u5e94\u503c\u3002 \u70b9\u51fb\u201c\u767b\u5f55\u201d\uff0c\u8fdb\u5165tableau\u9875\u9762\uff0c\u9009\u62e9\u67b6\u6784\u548c\u8868\uff0c\u7ed3\u679c\u5982\u4e0b\u3002 \u7528Tableau\u505a\u5b9e\u65f6\u8fde\u63a5\uff0c\u6253\u5f00\u5de5\u4f5c\u7c3f\uff0c\u5bf9\u8be5\u8868\u8fdb\u884c\u56fe\u5f62\u5316\u5206\u6790\u3002 \u6027\u80fd\u6d4b\u8bd5 \u67e5\u8be2\u5305\u542b\u767e\u4e07\u6761\u6570\u636e\u7684\u8868web_sales \u591a\u8868\u5173\u8054\u67e5\u8be2\uff1astore_sales\u548citem\u8868\u505a\u5173\u8054\u67e5\u8be2 \u589e\u52a0customer_address\u8868 \u67e5\u8be2\u7ed3\u679c\uff1a FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662fHost(s)\u3001Port\u3001Host FQDN\u7b49\u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u8f93\u5165 \u7968\u636e24\u5c0f\u65f6\u540e\u8fc7\u671f\uff0c\u65e0\u6cd5\u518d\u8bbf\u95ee\u6570\u636e \u5728windows\u4e0a\u65b0\u589e\u5b9a\u65f6\u4efb\u52a1\uff0c\u5b9a\u65f6\u6267\u884ckinit\u547d\u4ee4\uff0c\u5237\u65b0kerberos\u7968\u636e","title":"10.5.0 <--> 8.0"},{"location":"Business_Intelligence/Tableau/#tableaufusioninsight","text":"","title":"Tableau\u5bf9\u63a5FusionInsight"},{"location":"Business_Intelligence/Tableau/#_1","text":"Tableau 10.0.0 \u2194 FusionInsight HD V100R002C30 (Hive/SparkSQL) Tableau 10.0.0 \u2194 FusionInsight HD V100R002C50 (Hive/SparkSQL) Tableau 10.1.4 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) Tableau 10.3.2 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight HD 6.5 (Hive/SparkSQL) Tableau 10.5.0 \u2194 FusionInsight MRS 8.0 (Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Business_Intelligence/Tableau/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctableau\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Business_Intelligence/Tableau/#hive","text":"Tableau\u4e2d\u914d\u7f6eHive\u6570\u636e\u6e90\uff0c\u5bf9\u63a5Hive\u7684ODBC\u63a5\u53e3\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a \u4e0b\u8f7d\u5730\u5740 \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u914d\u7f6eODBC\u9a71\u52a8 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 User DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb\u4e2d\u7684Test\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Tableau\u4f7f\u7528\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201c\u5176\u4ed6\u6570\u636e\u5e93\uff08ODBC\uff09\u201d\uff1b DSN\u9009\u62e9hive_odbc\uff08\u4e0a\u4e00\u6b65\u4e2d\u8bbe\u7f6eODBC\u7684\u540d\u79f0\uff09\uff0c\u70b9\u51fb\u8fde\u63a5\uff0c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\uff0c\u7136\u540e\u767b\u9646\u3002 \u67e5\u8be2\u767e\u4e07\u7ea7\u6570\u636e\u8868\u6570\u636e \u67e5\u8be2\u591a\u8868\u6570\u636e","title":"\u914d\u7f6eHive\u6570\u636e\u6e90"},{"location":"Business_Intelligence/Tableau/#spark","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5spark\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a http://www.tableau.com/support/drivers \u521b\u5efaDSN\uff08Data Source Name\uff09 \u6253\u5f00 C:\\Program Files\\Simba Spark ODBC Driver\\lib\\DriverConfiguration64.exe \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 Tableau\u4f7f\u7528Spark\u6570\u636e\u6e90 Tableau\u542f\u52a8\u540e\u4f1a\u8fdb\u5165\u8fde\u63a5\u9009\u62e9\u754c\u9762\uff0c\u70b9\u51fb\u201c\u66f4\u591a\u670d\u52a1\u5668\u201d\uff0c\u518d\u70b9\u51fb\u201cSpark SQL\u201d\uff0c\u4f5c\u5982\u4e0b\u914d\u7f6e\uff1a \u5176\u4e2d\u670d\u52a1\u5668\u4e3aJDBCServer(\u4e3b)\u7684\u4e1a\u52a1IP\u3002 \u7aef\u53e3\u4e3aFusionInsight\u4e2dSpark\u670d\u52a1\u914d\u7f6e\uff0c\u5bfc\u51fa\u670d\u52a1\u914d\u7f6e\u6587\u4ef6\uff0c\u5176\u4e2d hive.server2.thrift.port \u5bf9\u5e94\u503c\u3002 \u70b9\u51fb\u201c\u767b\u5f55\u201d\uff0c\u8fdb\u5165tableau\u9875\u9762\uff0c\u9009\u62e9\u67b6\u6784\u548c\u8868\uff0c\u7ed3\u679c\u5982\u4e0b\u3002 \u7528Tableau\u505a\u5b9e\u65f6\u8fde\u63a5\uff0c\u6253\u5f00\u5de5\u4f5c\u7c3f\uff0c\u5bf9\u8be5\u8868\u8fdb\u884c\u56fe\u5f62\u5316\u5206\u6790\u3002 \u6027\u80fd\u6d4b\u8bd5 \u67e5\u8be2\u5305\u542b\u767e\u4e07\u6761\u6570\u636e\u7684\u8868web_sales \u591a\u8868\u5173\u8054\u67e5\u8be2\uff1astore_sales\u548citem\u8868\u505a\u5173\u8054\u67e5\u8be2 \u589e\u52a0customer_address\u8868 \u67e5\u8be2\u7ed3\u679c\uff1a","title":"\u914d\u7f6eSpark\u6570\u636e\u6e90"},{"location":"Business_Intelligence/Tableau/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 ODBC\u8fde\u63a5\u5931\u8d25 \u5e38\u89c1\u60c5\u51b5\u662fHost(s)\u3001Port\u3001Host FQDN\u7b49\u7684\u8f93\u5165\u6570\u636e\u6709\u8bef\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u8fdb\u884c\u8f93\u5165 \u7968\u636e24\u5c0f\u65f6\u540e\u8fc7\u671f\uff0c\u65e0\u6cd5\u518d\u8bbf\u95ee\u6570\u636e \u5728windows\u4e0a\u65b0\u589e\u5b9a\u65f6\u4efb\u52a1\uff0c\u5b9a\u65f6\u6267\u884ckinit\u547d\u4ee4\uff0c\u5237\u65b0kerberos\u7968\u636e","title":"FAQ"},{"location":"Data_Analysis/","text":"\u6570\u636e\u5206\u6790 \u00b6 Alteryx 2018.2.5.48994 \u2194 C60 2018.2.5.48994 \u2194 C80 2018.2.5.48994 \u2194 6.5 GeoMesa 2.3.1 \u2194 6.5 Rapidminer Studio 8.2.001 \u2194 C80 SAS 9.4M3 \u2194 C60 9.4M3 \u2194 C70 9.4M3 \u2194 C80 SSAS 2017 \u2194 6.5 Splunk 7.2.4 \u2194 C80 7.2.4 \u2194 6.5 7.2.4 \u2194 8.0 \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u2194 C60 8.7 \u2194 8.0","title":"Index"},{"location":"Data_Analysis/#_1","text":"Alteryx 2018.2.5.48994 \u2194 C60 2018.2.5.48994 \u2194 C80 2018.2.5.48994 \u2194 6.5 GeoMesa 2.3.1 \u2194 6.5 Rapidminer Studio 8.2.001 \u2194 C80 SAS 9.4M3 \u2194 C60 9.4M3 \u2194 C70 9.4M3 \u2194 C80 SSAS 2017 \u2194 6.5 Splunk 7.2.4 \u2194 C80 7.2.4 \u2194 6.5 7.2.4 \u2194 8.0 \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u2194 C60 8.7 \u2194 8.0","title":"\u6570\u636e\u5206\u6790"},{"location":"Data_Analysis/Alteryx/","text":"Alteryx\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Alteryx 2018.2.5.48994 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight HD 6.5 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/SparkSQL) \u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528Kerbers\u8ba4\u8bc1\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002 \u914d\u7f6eSpark ODBC \u8fde\u63a5 \u00b6 \u5728\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u914d\u7f6eSpark ODBC\u9a71\u52a8 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a https://www.tableau.com/support/drivers \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Simba Spark ODBC Driver -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aSpark ODBC\u8fde\u63a5\u6210\u529f\u3002 \u5728Alteryx\u4f7f\u7528Spark\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aApache Spark ODBC COnnection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aApache Spark ODBC Write->Driver: \u9ed8\u8ba4 Connection String\uff1aNew database connection\uff0c\u9009\u62e9Spark DSN\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC->Simba Spark Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Spark DSN\uff1aSimba Spark \uff08System\uff09\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dSpark\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a \u914d\u7f6eHive ODBC\u6570\u636e\u6e90 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Hive\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a \u4e0b\u8f7d\u5730\u5740 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Alteryx\u4f7f\u7528Hive\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aHive Connection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aHive ODBC Write->Driver: Hive ODBC Connection String\uff1aNew database connection\uff0c\u9009\u62e9Hive DSN\uff0c\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u5728\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Hive DSN\uff1aSample Cloudera Hive DSN(System)\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dHive\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer\uff1a \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a \u914d\u7f6eHDFS\u6570\u636e\u6e90 \u00b6 HDFS\u662f\u901a\u8fc7WebHDFS\u8fde\u63a5\uff0c\u524d\u63d0\u6761\u4ef6\u662f\u83b7\u53d6MIT Kerberos Ticket\uff0c\u5e76\u5728Manager\u4e2d\u4fee\u6539HDFS\u7684\u914d\u7f6e\uff1a dfs.http.policy \u4fee\u6539\u4e3aHTTP_AND_HTTPS\uff0c\u91cd\u542fHDFS\u3002 \u5728Alteryx\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Hadoop \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a Server\uff1aWebHDFS Host\uff1a HDFS\u6240\u5728\u670d\u52a1\u5668IP Port: \u914d\u7f6e\u6587\u4ef6\u4e2ddfs.namenode.http.port\u5bf9\u5e94\u7aef\u53e3 User Name & Password\uff1a Kerberos \u8ba4\u8bc1\u7528\u6237\u540d\u53ca\u5bc6\u7801 Kerberos: Kerberos MIT \u70b9\u51fbTest\uff0c\u51fa\u73b0Connection successful \u8868\u660e\u8fde\u63a5\u6210\u529f\u3002 \u5f39\u51fa\u96c6\u7fa4\u4e2d\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5185\u5bb9\uff0c\u76ee\u524d\u652f\u6301Avro\u548cCSV\u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u9700\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002 \u9009\u62e9\u76f8\u5e94\u6587\u4ef6\uff0c\u8fde\u63a5\u6210\u529f\uff0cRefresh\u4e4b\u540e\u5de6\u4fa7\u83dc\u5355\u663e\u793a\u6587\u4ef6\u5185\u5bb9\u9884\u89c8\uff1a Join \u64cd\u4f5c\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b: FAQ \u00b6 \u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 \u6d4b\u8bd5\u8fde\u63a5\u65f6\u51fa\u73b0Default Kerberos ticket is expired Kerberos MIT ticket \u8fc7\u671f\uff0c\u9700\u8981\u91cd\u65b0\u83b7\u5f97\uff0c\u83b7\u53d6\u4e00\u6b21\u6709\u6548\u671f\u4e3a10h.","title":"2018.2.5.48994 <--> 8.0"},{"location":"Data_Analysis/Alteryx/#alteryxfusioninsight","text":"","title":"Alteryx\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/Alteryx/#_1","text":"Alteryx 2018.2.5.48994 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight HD 6.5 (HDFS/Hive/SparkSQL) Alteryx 2018.2.5.48994 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/Alteryx/#windowskerberos","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos \u4e0b\u8f7d\u7f51\u5740\uff1a http://web.mit.edu/kerberos/dist/#kfw-4.0 \u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672ckfw-4.1-amd64.msi\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u628akrb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002 \u91cd\u542f\u673a\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u4f7f\u7528Kerbers\u8ba4\u8bc1\u7684\u7528\u6237\u540d\u5bc6\u7801\u767b\u5f55\uff0c\u7528\u6237\u540d\u7684\u683c\u5f0f\u4e3a\uff1a\u7528\u6237\u540d@Kerberos\u57df\u540d\u3002 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002","title":"\u914d\u7f6eWindows\u7684kerberos\u8ba4\u8bc1"},{"location":"Data_Analysis/Alteryx/#spark-odbc","text":"\u5728\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u914d\u7f6eSpark ODBC\u9a71\u52a8 \u4e0b\u8f7d\u5e76\u5b89\u88c5ODBC\u9a71\u52a8\uff1a https://www.tableau.com/support/drivers \u6839\u636e\u64cd\u4f5c\u7cfb\u7edf\u7c7b\u578b\u9009\u62e9\u5bf9\u5e94\u7684ODBC\u7248\u672c\uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Simba Spark ODBC Driver -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf\uff0c Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a \u70b9\u51fb\u201cAdvanced Options\u201d\uff0c\u52fe\u9009\u5982\u4e0b\u9009\u9879\uff1a \u70b9\u51fbOK\uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aSpark ODBC\u8fde\u63a5\u6210\u529f\u3002 \u5728Alteryx\u4f7f\u7528Spark\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aApache Spark ODBC COnnection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aApache Spark ODBC Write->Driver: \u9ed8\u8ba4 Connection String\uff1aNew database connection\uff0c\u9009\u62e9Spark DSN\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC->Simba Spark Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Spark DSN\uff1aSimba Spark \uff08System\uff09\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dSpark\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a","title":"\u914d\u7f6eSpark ODBC \u8fde\u63a5"},{"location":"Data_Analysis/Alteryx/#hive-odbc","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Hive\u7684ODBC\u9a71\u52a8 ODBC\u9a71\u52a8\u4e0b\u8f7d\u5730\u5740\uff1a \u4e0b\u8f7d\u5730\u5740 \u521b\u5efaDSN(Data Source Name)\uff1a\u9009\u62e9 \u5f00\u59cb -> Simba Spark ODBC Driver -> ODBC Administrator \u3002 \u9009\u62e9 System DSN -> Add -> Cloudera ODBC Driver for Apache Hive -> Finish \u6309\u5b9e\u9645\u914d\u7f6e\u76f8\u5e94\u7684\u53d8\u91cf Host(s): Hive Service\u4e3b\u8282\u70b9 Port\uff1aHive Service\u7aef\u53e321066 Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a \u5982\u4e0b\u56fe\uff1a Advanced Options\u4e0d\u9700\u8981\u8fdb\u884c\u914d\u7f6e\u9ed8\u8ba4\u7684\u53c2\u6570\u5373\u53ef\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fbTest\u8fdb\u884c\u6d4b\u8bd5\u8fde\u63a5\uff0c\u5982\u679c\u51fa\u73b0\u4e0b\u56fe\uff0c\u5219\u8868\u793aODBC\u8fde\u63a5Hive\u6210\u529f\u3002 Alteryx\u4f7f\u7528Hive\u6570\u636e\u6e90 Alteryx\u542f\u52a8\u540e\u9009\u62e9Options->Advanced Options->Manage In-DB Connections \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a DataSource\uff1aHive Connection Type\uff1aSystem Connections: \u9996\u6b21\u4f7f\u7528\u9009new Connection Name: \u81ea\u5b9a\u4e49 Read->Driver\uff1aHive ODBC Write->Driver: Hive ODBC Connection String\uff1aNew database connection\uff0c\u9009\u62e9Hive DSN\uff0c\u586b\u5199\u7528\u6237\u540d\u5bc6\u7801 \u5728\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Other Databases->ODBC Data Source Name \u9009\u62e9\u5728\u914d\u7f6eODBC\u9a71\u52a8\u65f6\u65b0\u5efa\u7684Hive DSN\uff1aSample Cloudera Hive DSN(System)\uff0c\u586b\u5165\u7528\u6237\u540d\u5bc6\u7801\uff1a \u70b9\u51fbOK\uff0cAlteryx\u4f1a\u8fde\u63a5\u81f3\u96c6\u7fa4,\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u663e\u793a\u7684\u662f\u96c6\u7fa4\u4e2dHive\u4e2d\u7684\u6570\u636e\u8868\uff0c\u9009\u62e9\u4e00\u4e2a\u6570\u636e\u8868\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f8b\u5982Customer\uff1a \u5bfc\u5165\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b,Refresh\u4e4b\u540e\u5728\u5de6\u4fa7\u53ef\u4ee5\u770b\u5230\u6570\u636e\u9884\u89c8\uff1a \u518d\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u6e90\uff0c\u6267\u884cjoin\u64cd\u4f5c\uff0c\u6210\u529f\u540e\u7ed3\u679c\u5982\u4e0b\uff1a","title":"\u914d\u7f6eHive ODBC\u6570\u636e\u6e90"},{"location":"Data_Analysis/Alteryx/#hdfs","text":"HDFS\u662f\u901a\u8fc7WebHDFS\u8fde\u63a5\uff0c\u524d\u63d0\u6761\u4ef6\u662f\u83b7\u53d6MIT Kerberos Ticket\uff0c\u5e76\u5728Manager\u4e2d\u4fee\u6539HDFS\u7684\u914d\u7f6e\uff1a dfs.http.policy \u4fee\u6539\u4e3aHTTP_AND_HTTPS\uff0c\u91cd\u542fHDFS\u3002 \u5728Alteryx\u4e3b\u754c\u9762\u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165Input Data\u5de5\u5177\uff0c\u5728\u5de6\u4fa7Connect a file or database \u4e2d\u70b9\u51fb\u4e0b\u62c9\u83dc\u5355\uff0c\u9009\u62e9Hadoop \u5728\u5f39\u51fa\u7684\u754c\u9762\u4e2d\u586b\u5199\u914d\u7f6e\uff1a Server\uff1aWebHDFS Host\uff1a HDFS\u6240\u5728\u670d\u52a1\u5668IP Port: \u914d\u7f6e\u6587\u4ef6\u4e2ddfs.namenode.http.port\u5bf9\u5e94\u7aef\u53e3 User Name & Password\uff1a Kerberos \u8ba4\u8bc1\u7528\u6237\u540d\u53ca\u5bc6\u7801 Kerberos: Kerberos MIT \u70b9\u51fbTest\uff0c\u51fa\u73b0Connection successful \u8868\u660e\u8fde\u63a5\u6210\u529f\u3002 \u5f39\u51fa\u96c6\u7fa4\u4e2d\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5185\u5bb9\uff0c\u76ee\u524d\u652f\u6301Avro\u548cCSV\u683c\u5f0f\u7684\u6587\u4ef6\uff0c\u9700\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002 \u9009\u62e9\u76f8\u5e94\u6587\u4ef6\uff0c\u8fde\u63a5\u6210\u529f\uff0cRefresh\u4e4b\u540e\u5de6\u4fa7\u83dc\u5355\u663e\u793a\u6587\u4ef6\u5185\u5bb9\u9884\u89c8\uff1a Join \u64cd\u4f5c\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b:","title":"\u914d\u7f6eHDFS\u6570\u636e\u6e90"},{"location":"Data_Analysis/Alteryx/#faq","text":"\u627e\u4e0d\u5230C:\\ProgramData\\MIT\\Kerberos5\u6587\u4ef6\u5939 C:\\ProgramData\u4e00\u822c\u5c5e\u4e8e\u9690\u85cf\u6587\u4ef6\u5939\uff0c\u8bbe\u7f6e\u6587\u4ef6\u5939\u9690\u85cf\u53ef\u89c1\u6216\u8005\u4f7f\u7528\u641c\u7d22\u529f\u80fd\u5373\u53ef\u89e3\u51b3\u95ee\u9898\u3002 \u8fde\u63a5\u6210\u529f\u65e0\u6570\u636e\u5e93\u6743\u9650 \u8fde\u63a5\u6240\u4f7f\u7528\u7684\u7528\u6237\u9700\u8981\u6709\u6570\u636e\u5e93\u7684\u6743\u9650\uff0c\u5426\u5219\u5c06\u5bfc\u81f4ODBC\u8fde\u63a5\u6210\u529f\u5374\u65e0\u6cd5\u8bfb\u53d6\u6570\u636e\u5e93\u5185\u5bb9\u3002 \u6d4b\u8bd5\u8fde\u63a5\u65f6\u51fa\u73b0Default Kerberos ticket is expired Kerberos MIT ticket \u8fc7\u671f\uff0c\u9700\u8981\u91cd\u65b0\u83b7\u5f97\uff0c\u83b7\u53d6\u4e00\u6b21\u6709\u6548\u671f\u4e3a10h.","title":"FAQ"},{"location":"Data_Analysis/GeoMesa_2.3.1/","text":"GeoMesa\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 GeoMesa 2.3.1 \u2194 FusionInsight HD 6.5 (HBase) \u7b80\u4ecb \u00b6 GeoMesa\u662fApache\u5f00\u6e90\u5de5\u5177\u5957\u4ef6\uff0c\u53ef\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u5206\u6790\uff0c\u4f7f\u60a8\u53ef\u4ee5\u7ba1\u7406\u548c\u5206\u6790\u5de8\u5927\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u4f8b\u5982IoT\uff0c\u793e\u4ea4\u5a92\u4f53\uff0c\u8ddf\u8e2a\u548c\u79fb\u52a8\u7535\u8bdd\u7b49\u5e94\u7528\u7a0b\u5e8f\u3002 GeoMesa\u5728\u6d41\u884c\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u4e4b\u4e0a\u63d0\u4f9b\u65f6\u7a7a\u6570\u636e\u6301\u4e45\u6027\u6765\u5b9e\u73b0\u5bf9\u70b9\uff0c\u7ebf\u548c\u9762\u6570\u636e\u7684\u5927\u89c4\u6a21\u5b58\u50a8\u3002\u5b83\u5141\u8bb8\u901a\u8fc7\u5145\u5206\u5229\u7528\u5730\u7406\u5c5e\u6027\u6765\u6307\u5b9a\u8ddd\u79bb\u548c\u533a\u57df\u7684\u67e5\u8be2\u6765\u5feb\u901f\u8bbf\u95ee\u6b64\u6570\u636e\u3002\u901a\u8fc7\u8bf8\u5982GeoServer\u4e4b\u7c7b\u7684\u5730\u7406\u4fe1\u606f\u670d\u52a1\u5668\uff0cGeoMesa\u901a\u8fc7\u652f\u6301\u901a\u8fc7\u6807\u51c6OGC\uff08\u5f00\u653e\u5730\u7406\u7a7a\u95f4\u8054\u76df\uff09API\u548c\u534f\u8bae\uff08\u4f8b\u5982WFS\u548cWMS\uff09\u8bbf\u95ee\u5176\u6570\u636e\u5e93\u548c\u6d41\u529f\u80fd\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u4e0e\u73b0\u6709\u5730\u56fe\u5ba2\u6237\u7aef\u7684\u96c6\u6210\u3002\u8fd9\u4e9b\u754c\u9762\u8fd8\u4f7fGeoMesa\u53ef\u4ee5\u9a71\u52a8\u5730\u56fe\u7528\u6237\u754c\u9762\u5e76\u63d0\u4f9b\u6570\u636e\u4ee5\u8fdb\u884c\u5206\u6790\uff0c\u4f8b\u5982\u67e5\u8be2\uff0c\u76f4\u65b9\u56fe\uff0c\u70ed\u56fe\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Linux\u64cd\u4f5c\u7cfb\u7edf\uff0cGeoMesa HBase\u5bf9\u63a5FusionInsight HD\u7684HBase\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002\u672c\u6587\u4f7f\u7528\u7684\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 /opt \u76ee\u5f55\u4e0b\u3002 \u5728 /opt \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5b89\u88c5\u90e8\u7f72GeoMesa HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728\u5df2\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\u7684\u8282\u70b9\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4eceGithub\u4e0b\u8f7d\u5df2\u7ecf\u7f16\u8bd1\u597d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4f8b\u5982\uff1a wget \"https://github.com/locationtech/geomesa/releases/download/geomesa_2.11-$VERSION/geomesa-hbase_2.11-$VERSION-bin.tar.gz\" \uff0c\u89e3\u538b\u81f3\u76ee\u6807\u76ee\u5f55 /opt \u3002 cd /opt wget \"https://github.com/locationtech/geomesa/releases/download/geomesa_2.11-2.3.1/geomesa-hbase_2.11-2.3.1-bin.tar.gz\" tar -xvf geomesa-hbase_2.11-2.3.1-bin.tar.gz \u8bf4\u660e\uff1a \u5982\u679cwget\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6267\u884c\u201cyum install wget\u201d\u8fdb\u884c\u5b89\u88c5\u3002 \u672c\u6587\u4f7f\u7528\u7684\u201c$VERSION\u201d\u4e3a 2.3.1\u3002 \u53ef\u4ee5\u9009\u62e9\u4ece https://github.com/locationtech/geomesa/releases \u4e0b\u8f7d\u76f8\u5e94\u7248\u672c\u7684geomesa-hbase_2.11-$VERSION-bin.tar.gz\u518d\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u3002 \u90e8\u7f72GeoMesa Distributed Runtime JAR \u5c06 /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar \u62f7\u8d1d\u81f3FusionInsight\u7684HBase\u7ec4\u4ef6\u7684\u6bcf\u4e2aRegionServer\u8282\u70b9\u7684HBase\u7b2c\u4e09\u65b9jar\u5305\u7ba1\u7406\u76ee\u5f55\uff0c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.21:/opt/huawei/Bigdata/third_lib/HBase/ scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.22:/opt/huawei/Bigdata/third_lib/HBase/ scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.23:/opt/huawei/Bigdata/third_lib/HBase/ \u8bf4\u660e\uff1aFusionInsight\u7684HBase\u7ec4\u4ef6\u7684RegionServer\u8282\u70b9\u7684IP\u5206\u522b\u4e3a172.16.4.21\u3001172.16.4.22\u3001172.16.4.23\u3002 \u767b\u5f55FusionInsight\u7684HBase\u7ec4\u4ef6\u7684\u6bcf\u4e2aRegionServer\u8282\u70b9\uff0c\u4fee\u6539\u201cgeomesa-hbase-distributed-runtime_2.11-2.3.1.jar\u201d\u7684\u6240\u6709\u8005\u548c\u7ec4\u4e3a omm:wheel \u3002 \u5176\u4e2d\u4e00\u4e2a\u96c6\u7fa4\u8282\u70b9\u4fee\u6539\u793a\u4f8b\u5982\u4e0b\uff1a chown omm:wheel /opt/huawei/Bigdata/third_lib/HBase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar \u767b\u5f55FusionInsight Manage\u91cd\u542fHBase\u670d\u52a1\u3002 \u914d\u7f6eGeoMesa\u3001Hadoop\u548cHBase\u7684\u73af\u5883\u53d8\u91cf\u4ee5\u4fbf\u80fd\u591f\u4f7f\u7528HBase\u547d\u4ee4\u884c\u5de5\u5177 geomesa-hbase \u3002 vi ~/.bash_profile \u589e\u52a0\u7684\u73af\u5883\u53d8\u91cf\u793a\u4f8b\u5982\u4e0b\uff1a export GEOMESA_HBASE_HOME=/opt/geomesa-hbase_2.11-2.3.1 export PATH=${PATH}:${GEOMESA_HBASE_HOME}/bin export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u6267\u884c source \u547d\u4ee4\u4f7fGeoMesa\u3001Hadoop\u548cHBase\u7684\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 source ~/.bash_profile source /opt/hadoopclient/bigdata_env echo $GEOMESA_HBASE_HOME $HADOOP_HOME $HBASE_HOME $JAVA_TOOL_OPTIONS \u8c03\u8bd5GeoMesa\u6837\u4f8b\u811a\u672c \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u7684\u8282\u70b9\u8c03\u8bd5GeoMesa\u7684\u6837\u4f8b\u811a\u672cgeomesa-tutorials-hbase-quickstart\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u3002 \u5df2\u5b89\u88c5Maven\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4e0b\u8f7dGeoMesa\u7684\u6837\u4f8b\u811a\u672c\u3002 \u5fc5\u987b\u4e0b\u8f7d\u4e0eGeoMesa\u7248\u672c\u76f8\u5bf9\u5e94\u7684\u7248\u672c\u3002 cd /opt git clone https://github.com/geomesa/geomesa-tutorials.git cd geomesa-tutorials git checkout tags/geomesa-tutorials-2.3.1 -b geomesa-tutorials-2.3.1 \u8bf4\u660e\uff1a\u5982\u679cgit\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6267\u884c\u201cyum install git\u201d\u8fdb\u884c\u5b89\u88c5\u3002 \u4fee\u6539 /opt/geomesa-tutorials/pom.xml \u7684zookeeper\u7248\u672c\u4e3aFusionInsight\u4f7f\u7528\u7684\u76f8\u540c\u7248\u672c\uff0c\u4f8b\u5982 3.5.1 \u3002 \u5c06FusionInsight\u7684HBase\u7ec4\u4ef6\u76f8\u5173\u7684core-site.xml\u3001hdfs-site.xml\u548chbase-site.xml\u62f7\u8d1d\u81f3 /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources \u76ee\u5f55\u3002 cp /opt/hadoopclient/HBase/hbase/conf/core-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources cp /opt/hadoopclient/HBase/hbase/conf/hdfs-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources cp /opt/hadoopclient/HBase/hbase/conf/hbase-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources vi /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources/hbase-site.xml \u5728 /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources/hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.geomesa.principal \u548c hbase.geomesa.keytab \u3002 <property> <name>hbase.geomesa.principal</name> <value>developuser</value> </property> <property> <name>hbase.geomesa.keytab</name> <value>/opt/user.keytab</value> </property> \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6784\u5efa\u201cgeomesa-tutorials-hbase\u201d\u5de5\u7a0b\u3002 cd /opt/geomesa-tutorials mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8bf4\u660e\uff1a\u5fc5\u987b\u5728 /opt/geomesa-tutorials \u76ee\u5f55\u6267\u884c\u201cmvn clean install\u201d\u547d\u4ee4\uff0c\u5426\u5219\u4f1a\u8fd4\u56de\u201c[ERROR] Could not find the selected project in the reactor: geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart\u201d \u5982\u679c\u6784\u5efa\u8fc7\u7a0b\u4e2d\u8fd4\u56de\u201cFailed to execute goal on project geomesa-tutorials-hbase: Could not resolve dependencies for project org.geomesa.example:geomesa-tutorials-hbase:pom:2.3.1: Could not find artifact org.apache.zookeeper:zookeeper:jar:3.5.1 in locationtech-releases ( https://repo.locationtech.org/content/groups/releases ) \u201d\uff0c\u5219 \u5c06 /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar \u62f7\u8d1d\u81f3 /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 \u518d\u91cd\u65b0\u6784\u5efa\u201cgeomesa-tutorials-hbase-quickstart\u201d\u5de5\u7a0b\u3002 mkdir -p /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8bf4\u660e\uff1a\u5982\u679c\u6784\u5efa\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u8fd4\u56de\u9519\u8bef\uff0c\u4e5f\u9700\u8981\u64cd\u4f5c\u6b64\u6b65\u9aa4\u3002 \u6784\u5efa\u5de5\u7a0b\u6210\u529f\u5219\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fd0\u884c\u201cgeomesa-tutorials-hbase\u201d\u5de5\u7a0b\u3002 java -cp /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/target/geomesa-tutorials-hbase-quickstart-2.3.1.jar org.geomesa.example.hbase.HBaseQuickStart --namespace GEOMESA --hbase.catalog GEOMESA.GEOMESA_HBASE \u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u5219\u8868\u793a\u6210\u529f\uff1a \u767b\u5f55FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u67e5\u8be2\u65b0\u5efa\u4e94\u5f20\u8868\u3002 hbase shell list GeoMesa\u6570\u636e\u53ef\u89c6\u5316 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4f7f\u7528GeoMesa HBase\u5de5\u5177\u53d1\u884c\u7248\u7684 geomesa-hbase export \u547d\u4ee4\u663e\u793aGeoMesa\u6837\u4f8b\u811a\u672c\u63d0\u53d6\u7684\u6570\u636e\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 GeoMesa\u6837\u4f8b\u811a\u672c\u5df2\u8c03\u8bd5\u6210\u529f\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 cd /opt geomesa-hbase export --output-format leaflet --feature-name gdelt-quickstart --catalog GEOMESA.GEOMESA_HBASE \u4e0b\u8f7d geomesa-hbase export \u547d\u4ee4\u4ea7\u751f\u7684 /opt/index.html \u6587\u4ef6\uff0c\u5e76\u7528\u6d4f\u89c8\u5668\u6253\u5f00\u3002 FAQ \u00b6 \u5982\u4f55\u5b89\u88c5Maven \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u65f6\u8fd4\u56de\u201cmvn: command not found\u201d\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728 /usr/local \u76ee\u5f55\u4e0b\u5b89\u88c5maven\u3002 cd /usr/local wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.6.0/binaries/apache-maven-3.6.0-bin.tar.gz tar -xvf apache-maven-3.6.0-bin.tar.gz \u914d\u7f6eMaven\u7684\u73af\u5883\u53d8\u91cf\u3002 vi ~/.bash_profile \u589e\u52a0\u7684\u73af\u5883\u53d8\u91cf\u793a\u4f8b\u5982\u4e0b\uff1a export MAVEN_HOME=/usr/local/apache-maven-3.6.0 export PATH=${PATH}:${GEOMESA_HBASE_HOME}/bin:$MAVEN_HOME/bin \u6267\u884c source \u547d\u4ee4\u4f7f\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 source ~/.bash_profile \u8fd0\u884c org.geomesa.example.hbase.HBaseQuickStart \u65f6\u8fd4\u56dejava.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c java -Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -cp geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/target/geomesa-tutorials-hbase-quickstart-2.3.1.jar org.geomesa.example.hbase.HBaseQuickStart --namespace GEOMESA --hbase.catalog GEOMESA.GEOMESA_HBASE \u65f6\u8fd4\u56de\u201cjava.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher\u201d\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u4e0d\u80fd\u4f7f\u7528\u5f00\u6e90\u7684zookeeper.jar\u5305\uff0c\u5fc5\u987b\u4f7f\u7528FusionInsight\u4f7f\u7528\u7684zookeeper\u7684jar\u5305\u3002\u5c06 /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar \u62f7\u8d1d\u81f3 /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 \u518d\u91cd\u65b0\u6784\u5efa\u201cgeomesa-tutorials-hbase-quickstart\u201d\u5de5\u7a0b\u3002\u64cd\u4f5c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a mkdir -p /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cd /opt/geomesa-tutorials mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8fd0\u884c geomesa-hbase export \u547d\u4ee4\u65f6\u8fd4\u56deCan't process features serialized with an older version \u3010\u95ee\u9898\u63cf\u8ff0\u3011 GeoMesa\u7248\u672c\u7684\u7248\u672c\u4e3a2.3.1\uff0cGeoMesa HBase\u7684\u6837\u4f8b\u811a\u672c\u7684\u7248\u672c\u4e3a2.4.0\u3002\u6267\u884c geomesa-hbase export --output-format leaflet --feature-name gdelt-quickstart --catalog GEOMEA_HBASE \u65f6\u8fd4\u56dejava.lang.IllegalArgumentException: Can't process features serialized with an older version\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u4e0b\u8f7dGeoMesa\u7684\u6837\u4f8b\u811a\u672c\u65f6\u786e\u4fdd\u4e0b\u8f7d\u4e0eGeoMesa HBase\u7248\u672c\u76f8\u5bf9\u5e94\u7684\u7248\u672c\u3002\u5982\u679c\u4e0d\u662f\u76f8\u540c\u7248\u672c\uff0c\u6267\u884c git checkout \u547d\u4ee4\u5207\u6362\u6837\u4f8b\u811a\u672c\u7248\u672c\u4e0eGeoMesa HBase\u7248\u672c\u4e00\u81f4\u3002 cd /opt/geomesa-tutorials git checkout tags/geomesa-tutorials-2.3.1 -b geomesa-tutorials-2.3.1","title":"2.3.1 <--> 6.5"},{"location":"Data_Analysis/GeoMesa_2.3.1/#geomesafusioninsight","text":"","title":"GeoMesa\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_1","text":"GeoMesa 2.3.1 \u2194 FusionInsight HD 6.5 (HBase)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_2","text":"GeoMesa\u662fApache\u5f00\u6e90\u5de5\u5177\u5957\u4ef6\uff0c\u53ef\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5730\u7406\u7a7a\u95f4\u5206\u6790\uff0c\u4f7f\u60a8\u53ef\u4ee5\u7ba1\u7406\u548c\u5206\u6790\u5de8\u5927\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u4f8b\u5982IoT\uff0c\u793e\u4ea4\u5a92\u4f53\uff0c\u8ddf\u8e2a\u548c\u79fb\u52a8\u7535\u8bdd\u7b49\u5e94\u7528\u7a0b\u5e8f\u3002 GeoMesa\u5728\u6d41\u884c\u7684\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u4e4b\u4e0a\u63d0\u4f9b\u65f6\u7a7a\u6570\u636e\u6301\u4e45\u6027\u6765\u5b9e\u73b0\u5bf9\u70b9\uff0c\u7ebf\u548c\u9762\u6570\u636e\u7684\u5927\u89c4\u6a21\u5b58\u50a8\u3002\u5b83\u5141\u8bb8\u901a\u8fc7\u5145\u5206\u5229\u7528\u5730\u7406\u5c5e\u6027\u6765\u6307\u5b9a\u8ddd\u79bb\u548c\u533a\u57df\u7684\u67e5\u8be2\u6765\u5feb\u901f\u8bbf\u95ee\u6b64\u6570\u636e\u3002\u901a\u8fc7\u8bf8\u5982GeoServer\u4e4b\u7c7b\u7684\u5730\u7406\u4fe1\u606f\u670d\u52a1\u5668\uff0cGeoMesa\u901a\u8fc7\u652f\u6301\u901a\u8fc7\u6807\u51c6OGC\uff08\u5f00\u653e\u5730\u7406\u7a7a\u95f4\u8054\u76df\uff09API\u548c\u534f\u8bae\uff08\u4f8b\u5982WFS\u548cWMS\uff09\u8bbf\u95ee\u5176\u6570\u636e\u5e93\u548c\u6d41\u529f\u80fd\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e86\u4e0e\u73b0\u6709\u5730\u56fe\u5ba2\u6237\u7aef\u7684\u96c6\u6210\u3002\u8fd9\u4e9b\u754c\u9762\u8fd8\u4f7fGeoMesa\u53ef\u4ee5\u9a71\u52a8\u5730\u56fe\u7528\u6237\u754c\u9762\u5e76\u63d0\u4f9b\u6570\u636e\u4ee5\u8fdb\u884c\u5206\u6790\uff0c\u4f8b\u5982\u67e5\u8be2\uff0c\u76f4\u65b9\u56fe\uff0c\u70ed\u56fe\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Linux\u64cd\u4f5c\u7cfb\u7edf\uff0cGeoMesa HBase\u5bf9\u63a5FusionInsight HD\u7684HBase\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002\u672c\u6587\u4f7f\u7528\u7684\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 /opt \u76ee\u5f55\u4e0b\u3002 \u5728 /opt \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; };","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Analysis/GeoMesa_2.3.1/#geomesa-hbase","text":"","title":"\u5b89\u88c5\u90e8\u7f72GeoMesa HBase"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_4","text":"\u5728\u5df2\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\u7684\u8282\u70b9\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_6","text":"\u4eceGithub\u4e0b\u8f7d\u5df2\u7ecf\u7f16\u8bd1\u597d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4f8b\u5982\uff1a wget \"https://github.com/locationtech/geomesa/releases/download/geomesa_2.11-$VERSION/geomesa-hbase_2.11-$VERSION-bin.tar.gz\" \uff0c\u89e3\u538b\u81f3\u76ee\u6807\u76ee\u5f55 /opt \u3002 cd /opt wget \"https://github.com/locationtech/geomesa/releases/download/geomesa_2.11-2.3.1/geomesa-hbase_2.11-2.3.1-bin.tar.gz\" tar -xvf geomesa-hbase_2.11-2.3.1-bin.tar.gz \u8bf4\u660e\uff1a \u5982\u679cwget\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6267\u884c\u201cyum install wget\u201d\u8fdb\u884c\u5b89\u88c5\u3002 \u672c\u6587\u4f7f\u7528\u7684\u201c$VERSION\u201d\u4e3a 2.3.1\u3002 \u53ef\u4ee5\u9009\u62e9\u4ece https://github.com/locationtech/geomesa/releases \u4e0b\u8f7d\u76f8\u5e94\u7248\u672c\u7684geomesa-hbase_2.11-$VERSION-bin.tar.gz\u518d\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u3002 \u90e8\u7f72GeoMesa Distributed Runtime JAR \u5c06 /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar \u62f7\u8d1d\u81f3FusionInsight\u7684HBase\u7ec4\u4ef6\u7684\u6bcf\u4e2aRegionServer\u8282\u70b9\u7684HBase\u7b2c\u4e09\u65b9jar\u5305\u7ba1\u7406\u76ee\u5f55\uff0c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.21:/opt/huawei/Bigdata/third_lib/HBase/ scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.22:/opt/huawei/Bigdata/third_lib/HBase/ scp /opt/geomesa-hbase_2.11-2.3.1/dist/hbase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar root@172.16.4.23:/opt/huawei/Bigdata/third_lib/HBase/ \u8bf4\u660e\uff1aFusionInsight\u7684HBase\u7ec4\u4ef6\u7684RegionServer\u8282\u70b9\u7684IP\u5206\u522b\u4e3a172.16.4.21\u3001172.16.4.22\u3001172.16.4.23\u3002 \u767b\u5f55FusionInsight\u7684HBase\u7ec4\u4ef6\u7684\u6bcf\u4e2aRegionServer\u8282\u70b9\uff0c\u4fee\u6539\u201cgeomesa-hbase-distributed-runtime_2.11-2.3.1.jar\u201d\u7684\u6240\u6709\u8005\u548c\u7ec4\u4e3a omm:wheel \u3002 \u5176\u4e2d\u4e00\u4e2a\u96c6\u7fa4\u8282\u70b9\u4fee\u6539\u793a\u4f8b\u5982\u4e0b\uff1a chown omm:wheel /opt/huawei/Bigdata/third_lib/HBase/geomesa-hbase-distributed-runtime_2.11-2.3.1.jar \u767b\u5f55FusionInsight Manage\u91cd\u542fHBase\u670d\u52a1\u3002 \u914d\u7f6eGeoMesa\u3001Hadoop\u548cHBase\u7684\u73af\u5883\u53d8\u91cf\u4ee5\u4fbf\u80fd\u591f\u4f7f\u7528HBase\u547d\u4ee4\u884c\u5de5\u5177 geomesa-hbase \u3002 vi ~/.bash_profile \u589e\u52a0\u7684\u73af\u5883\u53d8\u91cf\u793a\u4f8b\u5982\u4e0b\uff1a export GEOMESA_HBASE_HOME=/opt/geomesa-hbase_2.11-2.3.1 export PATH=${PATH}:${GEOMESA_HBASE_HOME}/bin export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u6267\u884c source \u547d\u4ee4\u4f7fGeoMesa\u3001Hadoop\u548cHBase\u7684\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 source ~/.bash_profile source /opt/hadoopclient/bigdata_env echo $GEOMESA_HBASE_HOME $HADOOP_HOME $HBASE_HOME $JAVA_TOOL_OPTIONS","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Analysis/GeoMesa_2.3.1/#geomesa","text":"","title":"\u8c03\u8bd5GeoMesa\u6837\u4f8b\u811a\u672c"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_7","text":"\u5728\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u7684\u8282\u70b9\u8c03\u8bd5GeoMesa\u7684\u6837\u4f8b\u811a\u672cgeomesa-tutorials-hbase-quickstart\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_8","text":"\u5df2\u5b8c\u6210\u5b89\u88c5\u90e8\u7f72GeoMesa HBase\u3002 \u5df2\u5b89\u88c5Maven\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_9","text":"\u4e0b\u8f7dGeoMesa\u7684\u6837\u4f8b\u811a\u672c\u3002 \u5fc5\u987b\u4e0b\u8f7d\u4e0eGeoMesa\u7248\u672c\u76f8\u5bf9\u5e94\u7684\u7248\u672c\u3002 cd /opt git clone https://github.com/geomesa/geomesa-tutorials.git cd geomesa-tutorials git checkout tags/geomesa-tutorials-2.3.1 -b geomesa-tutorials-2.3.1 \u8bf4\u660e\uff1a\u5982\u679cgit\u672a\u5b89\u88c5\uff0c\u53ef\u4ee5\u6267\u884c\u201cyum install git\u201d\u8fdb\u884c\u5b89\u88c5\u3002 \u4fee\u6539 /opt/geomesa-tutorials/pom.xml \u7684zookeeper\u7248\u672c\u4e3aFusionInsight\u4f7f\u7528\u7684\u76f8\u540c\u7248\u672c\uff0c\u4f8b\u5982 3.5.1 \u3002 \u5c06FusionInsight\u7684HBase\u7ec4\u4ef6\u76f8\u5173\u7684core-site.xml\u3001hdfs-site.xml\u548chbase-site.xml\u62f7\u8d1d\u81f3 /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources \u76ee\u5f55\u3002 cp /opt/hadoopclient/HBase/hbase/conf/core-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources cp /opt/hadoopclient/HBase/hbase/conf/hdfs-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources cp /opt/hadoopclient/HBase/hbase/conf/hbase-site.xml /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources vi /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources/hbase-site.xml \u5728 /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/src/main/resources/hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.geomesa.principal \u548c hbase.geomesa.keytab \u3002 <property> <name>hbase.geomesa.principal</name> <value>developuser</value> </property> <property> <name>hbase.geomesa.keytab</name> <value>/opt/user.keytab</value> </property> \u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6784\u5efa\u201cgeomesa-tutorials-hbase\u201d\u5de5\u7a0b\u3002 cd /opt/geomesa-tutorials mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8bf4\u660e\uff1a\u5fc5\u987b\u5728 /opt/geomesa-tutorials \u76ee\u5f55\u6267\u884c\u201cmvn clean install\u201d\u547d\u4ee4\uff0c\u5426\u5219\u4f1a\u8fd4\u56de\u201c[ERROR] Could not find the selected project in the reactor: geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart\u201d \u5982\u679c\u6784\u5efa\u8fc7\u7a0b\u4e2d\u8fd4\u56de\u201cFailed to execute goal on project geomesa-tutorials-hbase: Could not resolve dependencies for project org.geomesa.example:geomesa-tutorials-hbase:pom:2.3.1: Could not find artifact org.apache.zookeeper:zookeeper:jar:3.5.1 in locationtech-releases ( https://repo.locationtech.org/content/groups/releases ) \u201d\uff0c\u5219 \u5c06 /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar \u62f7\u8d1d\u81f3 /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 \u518d\u91cd\u65b0\u6784\u5efa\u201cgeomesa-tutorials-hbase-quickstart\u201d\u5de5\u7a0b\u3002 mkdir -p /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8bf4\u660e\uff1a\u5982\u679c\u6784\u5efa\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u8fd4\u56de\u9519\u8bef\uff0c\u4e5f\u9700\u8981\u64cd\u4f5c\u6b64\u6b65\u9aa4\u3002 \u6784\u5efa\u5de5\u7a0b\u6210\u529f\u5219\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fd0\u884c\u201cgeomesa-tutorials-hbase\u201d\u5de5\u7a0b\u3002 java -cp /opt/geomesa-tutorials/geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/target/geomesa-tutorials-hbase-quickstart-2.3.1.jar org.geomesa.example.hbase.HBaseQuickStart --namespace GEOMESA --hbase.catalog GEOMESA.GEOMESA_HBASE \u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u5219\u8868\u793a\u6210\u529f\uff1a \u767b\u5f55FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u67e5\u8be2\u65b0\u5efa\u4e94\u5f20\u8868\u3002 hbase shell list","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Analysis/GeoMesa_2.3.1/#geomesa_1","text":"","title":"GeoMesa\u6570\u636e\u53ef\u89c6\u5316"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_10","text":"\u4f7f\u7528GeoMesa HBase\u5de5\u5177\u53d1\u884c\u7248\u7684 geomesa-hbase export \u547d\u4ee4\u663e\u793aGeoMesa\u6837\u4f8b\u811a\u672c\u63d0\u53d6\u7684\u6570\u636e\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_11","text":"GeoMesa\u6837\u4f8b\u811a\u672c\u5df2\u8c03\u8bd5\u6210\u529f\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Analysis/GeoMesa_2.3.1/#_12","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 cd /opt geomesa-hbase export --output-format leaflet --feature-name gdelt-quickstart --catalog GEOMESA.GEOMESA_HBASE \u4e0b\u8f7d geomesa-hbase export \u547d\u4ee4\u4ea7\u751f\u7684 /opt/index.html \u6587\u4ef6\uff0c\u5e76\u7528\u6d4f\u89c8\u5668\u6253\u5f00\u3002","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Analysis/GeoMesa_2.3.1/#faq","text":"\u5982\u4f55\u5b89\u88c5Maven \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u65f6\u8fd4\u56de\u201cmvn: command not found\u201d\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728 /usr/local \u76ee\u5f55\u4e0b\u5b89\u88c5maven\u3002 cd /usr/local wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.6.0/binaries/apache-maven-3.6.0-bin.tar.gz tar -xvf apache-maven-3.6.0-bin.tar.gz \u914d\u7f6eMaven\u7684\u73af\u5883\u53d8\u91cf\u3002 vi ~/.bash_profile \u589e\u52a0\u7684\u73af\u5883\u53d8\u91cf\u793a\u4f8b\u5982\u4e0b\uff1a export MAVEN_HOME=/usr/local/apache-maven-3.6.0 export PATH=${PATH}:${GEOMESA_HBASE_HOME}/bin:$MAVEN_HOME/bin \u6267\u884c source \u547d\u4ee4\u4f7f\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 source ~/.bash_profile \u8fd0\u884c org.geomesa.example.hbase.HBaseQuickStart \u65f6\u8fd4\u56dejava.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c java -Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -cp geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart/target/geomesa-tutorials-hbase-quickstart-2.3.1.jar org.geomesa.example.hbase.HBaseQuickStart --namespace GEOMESA --hbase.catalog GEOMESA.GEOMESA_HBASE \u65f6\u8fd4\u56de\u201cjava.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher\u201d\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u4e0d\u80fd\u4f7f\u7528\u5f00\u6e90\u7684zookeeper.jar\u5305\uff0c\u5fc5\u987b\u4f7f\u7528FusionInsight\u4f7f\u7528\u7684zookeeper\u7684jar\u5305\u3002\u5c06 /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar \u62f7\u8d1d\u81f3 /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 \u518d\u91cd\u65b0\u6784\u5efa\u201cgeomesa-tutorials-hbase-quickstart\u201d\u5de5\u7a0b\u3002\u64cd\u4f5c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a mkdir -p /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.1 cd /opt/geomesa-tutorials mvn clean install -pl geomesa-tutorials-hbase/geomesa-tutorials-hbase-quickstart -am \u8fd0\u884c geomesa-hbase export \u547d\u4ee4\u65f6\u8fd4\u56deCan't process features serialized with an older version \u3010\u95ee\u9898\u63cf\u8ff0\u3011 GeoMesa\u7248\u672c\u7684\u7248\u672c\u4e3a2.3.1\uff0cGeoMesa HBase\u7684\u6837\u4f8b\u811a\u672c\u7684\u7248\u672c\u4e3a2.4.0\u3002\u6267\u884c geomesa-hbase export --output-format leaflet --feature-name gdelt-quickstart --catalog GEOMEA_HBASE \u65f6\u8fd4\u56dejava.lang.IllegalArgumentException: Can't process features serialized with an older version\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u4e0b\u8f7dGeoMesa\u7684\u6837\u4f8b\u811a\u672c\u65f6\u786e\u4fdd\u4e0b\u8f7d\u4e0eGeoMesa HBase\u7248\u672c\u76f8\u5bf9\u5e94\u7684\u7248\u672c\u3002\u5982\u679c\u4e0d\u662f\u76f8\u540c\u7248\u672c\uff0c\u6267\u884c git checkout \u547d\u4ee4\u5207\u6362\u6837\u4f8b\u811a\u672c\u7248\u672c\u4e0eGeoMesa HBase\u7248\u672c\u4e00\u81f4\u3002 cd /opt/geomesa-tutorials git checkout tags/geomesa-tutorials-2.3.1 -b geomesa-tutorials-2.3.1","title":"FAQ"},{"location":"Data_Analysis/RapidMiner/","text":"RapidMiner\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Rapidminer Studio 8.2.001 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/MapReduce/Spark) \u51c6\u5907\u5de5\u4f5c \u00b6 \u4e0b\u8f7d\u5b89\u88c5RapidMiner Studio, \u5f53\u524d\u6700\u65b0\u7248\u672c\u4e3a8.2.001,\u4e0b\u8f7d\u5730\u5740 https://rapidminer.com/ \u5b89\u88c5\u5b8c\u6210\u540e\u5728\u4e3b\u754c\u9762\u9876\u90e8\u83dc\u5355\u680f\u9009\u62e9 Extensions->Marketplace ,\u641c\u7d22 radoop ,\u5b89\u88c5\u540e\u91cd\u542frapidminer \u4fee\u6539\u672c\u5730host\u6587\u4ef6\uff0c\u8def\u5f84\u4e3aC:\\Windows\\System32\\drivers\\etc\uff0c\u52a0\u5165\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9IP\u4e0e\u4e3b\u673a\u540d\u5bf9\u5e94\u5173\u7cfb\uff0c\u4fdd\u5b58\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\u3002 \u51c6\u5907FusionInsight\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u4ee5\u53cajar\u5305 \u5728\u96c6\u7fa4\u7684Manager\u4e2d\uff0c\u9009\u62e9\u670d\u52a1->\u4e0b\u8f7d\u5ba2\u6237\u7aef->\u5b8c\u6574\u5ba2\u6237\u7aef \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cYarn\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u5939\u91cc\uff0c\u4f8b\u5982\u547d\u540d\u4e3aconfig\u3002 \u6253\u5f00 yarn-site.xml ,\u5220\u9664\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e <property> <name>audit.service.name</name> <value>Yarn</value> </property> \u8fdb\u5165Spark\u7ec4\u4ef6\u7684Jar\u5305\u76ee\u5f55\u201c\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars\u201d\uff0c\u5c06\u6240\u6709jar\u5305\u590d\u5236\u51fa\u6765\uff0c\u4fdd\u5b58\u5728\u672c\u673a\u67d0\u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:/jars \u3002 \u96c6\u7fa4\u914d\u7f6e \u00b6 \u914d\u7f6eUDP\u7aef\u53e3\u7ed1\u5b9a \u4e0b\u8f7d\u5b89\u88c5UDP\u7aef\u53e3\u7ed1\u5b9a\u5de5\u5177uredir\uff0c\u4e0b\u8f7d\u5730\u5740 https://github.com/troglobit/uredir \u7f16\u8bd1\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5206\u522b\u4e0a\u4f20\u81f3KDC\u670d\u52a1\u6240\u5728\u7684\u4e3b\u5907\u8282\u70b9(\u53ef\u5728krb5.conf\u6587\u4ef6\u4e2d\u67e5\u770b)\uff0c\u8fdb\u5165uredir\u6267\u884c\u6587\u4ef6\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u7aef\u53e3\u7ed1\u5b9a,\u5176\u4e2dIP\u4e3a\u6240\u5728\u8282\u70b9IP ./uredir IP:88 IP:21732 \u914d\u7f6eRadoop\u4f9d\u8d56jar\u5305 \u5728Radoop\u6587\u6863\u4e2d\u5fc3\uff0c\u4e0b\u8f7dRadoop\u4f9d\u8d56jar\u5305\uff0c\u4e0b\u8f7d\u5730\u5740 https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,\u4e0b\u8f7d\u4e0e\u5b89\u88c5\u7684RapidMiner\u7248\u672c\u5bf9\u5e94\u7684jar\u5305\u3002 \u5c06jar\u5305\u4e0a\u4f20\u81f3\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u76f8\u540c\u7684\u8def\u5f84\u4e0b\uff0c\u4f8b\u5982/usr/local/lib/radoop/ \u5728\u96c6\u7fa4HiveServer\u6240\u5728\u8282\u70b9\uff0c\u5206\u522b\u4e0a\u4f20Radoop\u7684jar\u5305\u81f3\u4ee5\u4e0b\u8def\u5f84\uff0c\u5e76\u4fee\u6539\u6240\u6709\u8005\u548c\u6267\u884c\u6743\u9650 Hive\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib\"\uff0c Mapreduce\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\uff1a\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib\" cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar * \u5728FusionInsight Manager \u754c\u9762\u6dfb\u52a0Hive\u767d\u540d\u5355\u914d\u7f6e radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name \u9700\u8981\u4ee5 | \u5206\u5272 * \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u91cd\u542fHiveServer \u521b\u5efaRadoop UDF\u51fd\u6570 \u5728\u4e3b\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a #cd /opt/hadoopclient #source bigdata_env #kinit developuser \u8f93\u5165developuser\u7528\u6237\u5bc6\u7801\uff0c\u6267\u884cbeeline\uff0c\u8fdb\u5165Hive Hive\u4e2d\u521b\u5efa\u6570\u636e\u5e93\uff0c\u4f8b\u5982\u521b\u5efa\u6570\u636e\u5e93rapidminer,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a create database rapidminer\uff1b use rapidminer\uff1b DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep'; RapidMiner\u914d\u7f6e \u00b6 \u5728RapidMiner\u4e2d\uff0c\u83dc\u5355\u9009\u62e9Connections->Manage Radoop Connections \u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u9009\u62e9New Connections->Import Hadoop Configuration Files\uff0c\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939\uff0c\u70b9\u51fbImport Configuration \u5bfc\u5165\u6210\u529f\u540e\u70b9\u51fbNext\uff0c\u8fdb\u5165\u8fde\u63a5\u914d\u7f6e\u7a97\u53e3\uff0c\u6839\u636e\u5de6\u4fa7\u83dc\u5355\u680f\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199\uff1a Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory\uff1aSpark\u7ec4\u4ef6\u7684jars\u5305 Client Principal\uff1a Kerberos\u7528\u6237\u540d@HADOOP.com Keytab File: \u4eceManager\u4e0b\u8f7d\u7684keytab\u6587\u4ef6 KDC Address\uff1a \u96c6\u7fa4KDC\u6240\u5728\u670d\u52a1\u5668IP REALM\uff1a HADOOP.COM Kerberos Config File: \u4eceManager\u4e0b\u8f7d\u7684krb5\u914d\u7f6e\u6587\u4ef6 Hadoop\uff1a \u5728\u5de6\u4e0a\u89d2\u641c\u7d22\u6846\u4e2d\u641c\u7d22split\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.input.fileinputformat.split.maxsize\u53c2\u6570 \u641c\u7d22classpath\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.application.classpath\u53c2\u6570 Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive\uff08or libs\uff09Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1a\u6dfb\u52a0spark.driver.extraJavaOptions\u548cspark.executor.extraJavaOptions\u4e24\u4e2a\u53c2\u6570 \u53c2\u6570value\u5728Manager\uff0cServices->Spark2X Configuration->\u6240\u6709\u914d\u7f6e\uff0c\u641c\u7d22extraJavaOptions\uff0c\u9009\u62e9Spark2x->SparkResource2x\u4e2d\u7684\u8fd9\u4e24\u4e2a\u53c2\u6570\u503c\uff0c\u5c06\u5176\u4e2d\u4f7f\u7528\u7684\u6240\u6709\u201c./\u201d\u76f8\u5bf9\u8def\u5f84\u66ff\u6362\u4e3a\u670d\u52a1\u7aefSpark\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4f8b\u5982\u201c/opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc\u201d Hive\uff1a Hive Version\uff1aHive Server2 Hive Server Address\uff1aHive \u670d\u52a1\u6240\u5728\u8282\u70b9IP Hive Port\uff1a 21066 Database Name\uff1a \u5728Hive\u4e2d\u521b\u5efa\u7684Radoop Function\u6240\u5728\u7684\u6570\u636e\u5e93\u540d\u79f0 Customer database for UDFs: \u540cDatabase Name \u70b9\u51fbOK->Proced Anyway->Save \u6d4b\u8bd5\u8fde\u63a5 \u00b6 \u70b9\u51fbConfigure,\u5728Global\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eGlobal\u6d4b\u8bd5\u6210\u529f \u5728Hadoop\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHadoop\u6d4b\u8bd5\u6210\u529f \u5728Spark\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eSpark\u6d4b\u8bd5\u6210\u529f \u5728Hive\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHive\u6d4b\u8bd5\u6210\u529f \u5728Manage Radoop Connections \u7a97\u53e3\uff0c\u9009\u4e2d\u6240\u521b\u5efa\u7684\u8fde\u63a5\uff0c\u70b9\u51fbFull test\u8fdb\u884c\u5b8c\u6574\u6d4b\u8bd5\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u5b8c\u6574\u6d4b\u8bd5\u901a\u8fc7 Radoop\u6837\u4f8b\u8fd0\u884c \u00b6 \u5728RapidMiner Studio \u4e3b\u9875\u9762\uff0cHelp->Tutorials->User Hadoop->Rapidminer Radoop \u6839\u636eTutorials\u7684\u6307\u5bfc\u8fd0\u884c\u6837\u4f8b\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a FAQ \u00b6 \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0c\u63d0\u793aICMP port unreachable/Error retrieving Hive object list\u95ee\u9898 \u68c0\u67e5\u96c6\u7fa4\u4e2d\u7aef\u53e3\u7ed1\u5b9a\u7a0b\u5e8f\u662f\u5426\u6b63\u5e38\u8fd0\u884c\uff0c\u7ed1\u5b9a\u7684\u7aef\u53e3\u662f\u5426\u6b63\u786e\u3002RapidMiner\u5728\u6d4b\u8bd5\u65f6\uff0c\u4f1a\u4e0e\u96c6\u7fa4\u768488\u7aef\u53e3\u8fde\u63a5\u8fdb\u884cKerberos\u8ba4\u8bc1\uff0c\u800cFusionInsight\u5e73\u53f0\u5bf9\u7aef\u53e3\u8fdb\u884c\u4e86\u89c4\u5212\uff0cKerberos\u8ba4\u8bc1\u4f7f\u7528\u7684\u7aef\u53e3\u662f21732\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u63d0\u793aGSS initiate failed \u68c0\u67e5\u672c\u5730host\u6587\u4ef6\u662f\u5426\u6dfb\u52a0\u4e86\u96c6\u7fa4IP\u4e0e\u4e3b\u673a\u540d\u7684\u5bf9\u5e94\u5173\u7cfb\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u5c06\u5404\u79cd\u7248\u672c\u90fd\u6d4b\u8bd5\u4e86\u4e00\u904d\uff0c\u6700\u540e\u63d0\u793aSpark test failed \u68c0\u67e5\u6dfb\u52a0\u7684\u4e24\u4e2aAdvanced Parameters\u662f\u5426\u586b\u5199\u6b63\u786e\uff0c\u5176value\u503c\u4e2d\u7684\u7edd\u5bf9\u8def\u5f84\u5bf9\u4e8e\u6bcf\u4e2a\u96c6\u7fa4\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5f53\u96c6\u7fa4\u91cd\u88c5\u540e\u9700\u8981\u4fee\u6539\u8be5\u503c\u3002","title":"8.2.001 <--> C80"},{"location":"Data_Analysis/RapidMiner/#rapidminerfusioninsight","text":"","title":"RapidMiner\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/RapidMiner/#_1","text":"Rapidminer Studio 8.2.001 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/MapReduce/Spark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/RapidMiner/#_2","text":"\u4e0b\u8f7d\u5b89\u88c5RapidMiner Studio, \u5f53\u524d\u6700\u65b0\u7248\u672c\u4e3a8.2.001,\u4e0b\u8f7d\u5730\u5740 https://rapidminer.com/ \u5b89\u88c5\u5b8c\u6210\u540e\u5728\u4e3b\u754c\u9762\u9876\u90e8\u83dc\u5355\u680f\u9009\u62e9 Extensions->Marketplace ,\u641c\u7d22 radoop ,\u5b89\u88c5\u540e\u91cd\u542frapidminer \u4fee\u6539\u672c\u5730host\u6587\u4ef6\uff0c\u8def\u5f84\u4e3aC:\\Windows\\System32\\drivers\\etc\uff0c\u52a0\u5165\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9IP\u4e0e\u4e3b\u673a\u540d\u5bf9\u5e94\u5173\u7cfb\uff0c\u4fdd\u5b58\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u89d2\u8272\u4e0e\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u89d2\u8272\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Spark\uff0cHive\uff0cHDFS\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201cdevelopuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\u3002 \u51c6\u5907FusionInsight\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u4ee5\u53cajar\u5305 \u5728\u96c6\u7fa4\u7684Manager\u4e2d\uff0c\u9009\u62e9\u670d\u52a1->\u4e0b\u8f7d\u5ba2\u6237\u7aef->\u5b8c\u6574\u5ba2\u6237\u7aef \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cYarn\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230\u4e00\u4e2a\u6587\u4ef6\u5939\u91cc\uff0c\u4f8b\u5982\u547d\u540d\u4e3aconfig\u3002 \u6253\u5f00 yarn-site.xml ,\u5220\u9664\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e <property> <name>audit.service.name</name> <value>Yarn</value> </property> \u8fdb\u5165Spark\u7ec4\u4ef6\u7684Jar\u5305\u76ee\u5f55\u201c\\FusionInsight_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.1.0.tar.gz\\spark\\jars\u201d\uff0c\u5c06\u6240\u6709jar\u5305\u590d\u5236\u51fa\u6765\uff0c\u4fdd\u5b58\u5728\u672c\u673a\u67d0\u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:/jars \u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Analysis/RapidMiner/#_3","text":"\u914d\u7f6eUDP\u7aef\u53e3\u7ed1\u5b9a \u4e0b\u8f7d\u5b89\u88c5UDP\u7aef\u53e3\u7ed1\u5b9a\u5de5\u5177uredir\uff0c\u4e0b\u8f7d\u5730\u5740 https://github.com/troglobit/uredir \u7f16\u8bd1\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5206\u522b\u4e0a\u4f20\u81f3KDC\u670d\u52a1\u6240\u5728\u7684\u4e3b\u5907\u8282\u70b9(\u53ef\u5728krb5.conf\u6587\u4ef6\u4e2d\u67e5\u770b)\uff0c\u8fdb\u5165uredir\u6267\u884c\u6587\u4ef6\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u7aef\u53e3\u7ed1\u5b9a,\u5176\u4e2dIP\u4e3a\u6240\u5728\u8282\u70b9IP ./uredir IP:88 IP:21732 \u914d\u7f6eRadoop\u4f9d\u8d56jar\u5305 \u5728Radoop\u6587\u6863\u4e2d\u5fc3\uff0c\u4e0b\u8f7dRadoop\u4f9d\u8d56jar\u5305\uff0c\u4e0b\u8f7d\u5730\u5740 https://docs.rapidminer.com/latest/radoop/installation/operation-and-maintenance.html ,\u4e0b\u8f7d\u4e0e\u5b89\u88c5\u7684RapidMiner\u7248\u672c\u5bf9\u5e94\u7684jar\u5305\u3002 \u5c06jar\u5305\u4e0a\u4f20\u81f3\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u76f8\u540c\u7684\u8def\u5f84\u4e0b\uff0c\u4f8b\u5982/usr/local/lib/radoop/ \u5728\u96c6\u7fa4HiveServer\u6240\u5728\u8282\u70b9\uff0c\u5206\u522b\u4e0a\u4f20Radoop\u7684jar\u5305\u81f3\u4ee5\u4e0b\u8def\u5f84\uff0c\u5e76\u4fee\u6539\u6240\u6709\u8005\u548c\u6267\u884c\u6743\u9650 Hive\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib\"\uff0c Mapreduce\u670d\u52a1\u7aef\u7684lib\u8def\u5f84\uff1a\"/opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib\" cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hive-1.3.0/hive-1.3.0/lib chown omm:wheel radoop_hive-v4.jar chown omm:wheel rapidminer_libs-8.2.0.jar chmod 700 radoop_hive-v4.jar chmod 700 rapidminer_libs-8.2.0.jar cd /opt/huawei/Bigdata/FusionInsight_HD_V100R002C80SPC200/install/FusionInsight-Hadoop-2.7.2/hadoop/share/hadoop/mapreduce/lib chown omm:ficommon radoop_hive-v4.jar chown omm:ficommon rapidminer_libs-8.2.0.jar chmod 750 radoop_hive-v4.jar chmod 750 rapidminer_libs-8.2.0.jar * \u5728FusionInsight Manager \u754c\u9762\u6dfb\u52a0Hive\u767d\u540d\u5355\u914d\u7f6e radoop\\.operation\\.id|mapred\\.job\\.name|hive\\.warehouse\\.subdir\\.inherit\\.perms|hive\\.exec\\.max\\.dynamic\\.partitions|hive\\.exec\\.max\\.dynamic\\.partitions\\.pernode|spark\\.app\\.name \u9700\u8981\u4ee5 | \u5206\u5272 * \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u91cd\u542fHiveServer \u521b\u5efaRadoop UDF\u51fd\u6570 \u5728\u4e3b\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff1a #cd /opt/hadoopclient #source bigdata_env #kinit developuser \u8f93\u5165developuser\u7528\u6237\u5bc6\u7801\uff0c\u6267\u884cbeeline\uff0c\u8fdb\u5165Hive Hive\u4e2d\u521b\u5efa\u6570\u636e\u5e93\uff0c\u4f8b\u5982\u521b\u5efa\u6570\u636e\u5e93rapidminer,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a create database rapidminer\uff1b use rapidminer\uff1b DROP FUNCTION IF EXISTS r3_add_file; DROP FUNCTION IF EXISTS r3_apply_model; DROP FUNCTION IF EXISTS r3_correlation_matrix; DROP FUNCTION IF EXISTS r3_esc; DROP FUNCTION IF EXISTS r3_gaussian_rand; DROP FUNCTION IF EXISTS r3_greatest; DROP FUNCTION IF EXISTS r3_is_eq; DROP FUNCTION IF EXISTS r3_least; DROP FUNCTION IF EXISTS r3_max_index; DROP FUNCTION IF EXISTS r3_nth; DROP FUNCTION IF EXISTS r3_pivot_collect_avg; DROP FUNCTION IF EXISTS r3_pivot_collect_count; DROP FUNCTION IF EXISTS r3_pivot_collect_max; DROP FUNCTION IF EXISTS r3_pivot_collect_min; DROP FUNCTION IF EXISTS r3_pivot_collect_sum; DROP FUNCTION IF EXISTS r3_pivot_createtable; DROP FUNCTION IF EXISTS r3_score_naive_bayes; DROP FUNCTION IF EXISTS r3_sum_collect; DROP FUNCTION IF EXISTS r3_which; DROP FUNCTION IF EXISTS r3_sleep; CREATE FUNCTION r3_add_file AS 'eu.radoop.datahandler.hive.udf.GenericUDFAddFile'; CREATE FUNCTION r3_apply_model AS 'eu.radoop.datahandler.hive.udf.GenericUDTFApplyModel'; CREATE FUNCTION r3_correlation_matrix AS 'eu.radoop.datahandler.hive.udf.GenericUDAFCorrelationMatrix'; CREATE FUNCTION r3_esc AS 'eu.radoop.datahandler.hive.udf.GenericUDFEscapeChars'; CREATE FUNCTION r3_gaussian_rand AS 'eu.radoop.datahandler.hive.udf.GenericUDFGaussianRandom'; CREATE FUNCTION r3_greatest AS 'eu.radoop.datahandler.hive.udf.GenericUDFGreatest'; CREATE FUNCTION r3_is_eq AS 'eu.radoop.datahandler.hive.udf.GenericUDFIsEqual'; CREATE FUNCTION r3_least AS 'eu.radoop.datahandler.hive.udf.GenericUDFLeast'; CREATE FUNCTION r3_max_index AS 'eu.radoop.datahandler.hive.udf.GenericUDFMaxIndex'; CREATE FUNCTION r3_nth AS 'eu.radoop.datahandler.hive.udf.GenericUDFNth'; CREATE FUNCTION r3_pivot_collect_avg AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotAvg'; CREATE FUNCTION r3_pivot_collect_count AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotCount'; CREATE FUNCTION r3_pivot_collect_max AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMax'; CREATE FUNCTION r3_pivot_collect_min AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotMin'; CREATE FUNCTION r3_pivot_collect_sum AS 'eu.radoop.datahandler.hive.udf.GenericUDAFPivotSum'; CREATE FUNCTION r3_pivot_createtable AS 'eu.radoop.datahandler.hive.udf.GenericUDTFCreatePivotTable'; CREATE FUNCTION r3_score_naive_bayes AS 'eu.radoop.datahandler.hive.udf.GenericUDFScoreNaiveBayes'; CREATE FUNCTION r3_sum_collect AS 'eu.radoop.datahandler.hive.udf.GenericUDAFSumCollect'; CREATE FUNCTION r3_which AS 'eu.radoop.datahandler.hive.udf.GenericUDFWhich'; CREATE FUNCTION r3_sleep AS 'eu.radoop.datahandler.hive.udf.GenericUDFSleep';","title":"\u96c6\u7fa4\u914d\u7f6e"},{"location":"Data_Analysis/RapidMiner/#rapidminer","text":"\u5728RapidMiner\u4e2d\uff0c\u83dc\u5355\u9009\u62e9Connections->Manage Radoop Connections \u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u9009\u62e9New Connections->Import Hadoop Configuration Files\uff0c\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939\uff0c\u70b9\u51fbImport Configuration \u5bfc\u5165\u6210\u529f\u540e\u70b9\u51fbNext\uff0c\u8fdb\u5165\u8fde\u63a5\u914d\u7f6e\u7a97\u53e3\uff0c\u6839\u636e\u5de6\u4fa7\u83dc\u5355\u680f\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199\uff1a Global\uff1a Hadoop Version\uff1aOther\uff08Hadoop 2X line\uff09 Additional Libraries Directory\uff1aSpark\u7ec4\u4ef6\u7684jars\u5305 Client Principal\uff1a Kerberos\u7528\u6237\u540d@HADOOP.com Keytab File: \u4eceManager\u4e0b\u8f7d\u7684keytab\u6587\u4ef6 KDC Address\uff1a \u96c6\u7fa4KDC\u6240\u5728\u670d\u52a1\u5668IP REALM\uff1a HADOOP.COM Kerberos Config File: \u4eceManager\u4e0b\u8f7d\u7684krb5\u914d\u7f6e\u6587\u4ef6 Hadoop\uff1a \u5728\u5de6\u4e0a\u89d2\u641c\u7d22\u6846\u4e2d\u641c\u7d22split\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.input.fileinputformat.split.maxsize\u53c2\u6570 \u641c\u7d22classpath\uff0c\u5728\u641c\u7d22\u7ed3\u679c\u4e2d\u53d6\u6d88\u52fe\u9009mapreduce.application.classpath\u53c2\u6570 Spark\uff1a Spark Version\uff1aSpark2.1 Spark Archive\uff08or libs\uff09Path: local:///opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/install/FusionInsight-Spark2x-2.1.0/spark/jars Spark Resource Allocation Policy\uff1aStatic\uff0cDefault Configuration Advanced Spark Parameters\uff1a\u6dfb\u52a0spark.driver.extraJavaOptions\u548cspark.executor.extraJavaOptions\u4e24\u4e2a\u53c2\u6570 \u53c2\u6570value\u5728Manager\uff0cServices->Spark2X Configuration->\u6240\u6709\u914d\u7f6e\uff0c\u641c\u7d22extraJavaOptions\uff0c\u9009\u62e9Spark2x->SparkResource2x\u4e2d\u7684\u8fd9\u4e24\u4e2a\u53c2\u6570\u503c\uff0c\u5c06\u5176\u4e2d\u4f7f\u7528\u7684\u6240\u6709\u201c./\u201d\u76f8\u5bf9\u8def\u5f84\u66ff\u6362\u4e3a\u670d\u52a1\u7aefSpark\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u4f8b\u5982\u201c/opt/huawei/Bigdata/FusionInsight_Spark2x_V100R002C80SPC200/1_21_SparkResource2x/etc\u201d Hive\uff1a Hive Version\uff1aHive Server2 Hive Server Address\uff1aHive \u670d\u52a1\u6240\u5728\u8282\u70b9IP Hive Port\uff1a 21066 Database Name\uff1a \u5728Hive\u4e2d\u521b\u5efa\u7684Radoop Function\u6240\u5728\u7684\u6570\u636e\u5e93\u540d\u79f0 Customer database for UDFs: \u540cDatabase Name \u70b9\u51fbOK->Proced Anyway->Save","title":"RapidMiner\u914d\u7f6e"},{"location":"Data_Analysis/RapidMiner/#_4","text":"\u70b9\u51fbConfigure,\u5728Global\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eGlobal\u6d4b\u8bd5\u6210\u529f \u5728Hadoop\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHadoop\u6d4b\u8bd5\u6210\u529f \u5728Spark\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eSpark\u6d4b\u8bd5\u6210\u529f \u5728Hive\u9875\u9762\uff0c\u70b9\u51fbTest\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660eHive\u6d4b\u8bd5\u6210\u529f \u5728Manage Radoop Connections \u7a97\u53e3\uff0c\u9009\u4e2d\u6240\u521b\u5efa\u7684\u8fde\u63a5\uff0c\u70b9\u51fbFull test\u8fdb\u884c\u5b8c\u6574\u6d4b\u8bd5\uff0cTest Results\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u5b8c\u6574\u6d4b\u8bd5\u901a\u8fc7","title":"\u6d4b\u8bd5\u8fde\u63a5"},{"location":"Data_Analysis/RapidMiner/#radoop","text":"\u5728RapidMiner Studio \u4e3b\u9875\u9762\uff0cHelp->Tutorials->User Hadoop->Rapidminer Radoop \u6839\u636eTutorials\u7684\u6307\u5bfc\u8fd0\u884c\u6837\u4f8b\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a","title":"Radoop\u6837\u4f8b\u8fd0\u884c"},{"location":"Data_Analysis/RapidMiner/#faq","text":"\u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0c\u63d0\u793aICMP port unreachable/Error retrieving Hive object list\u95ee\u9898 \u68c0\u67e5\u96c6\u7fa4\u4e2d\u7aef\u53e3\u7ed1\u5b9a\u7a0b\u5e8f\u662f\u5426\u6b63\u5e38\u8fd0\u884c\uff0c\u7ed1\u5b9a\u7684\u7aef\u53e3\u662f\u5426\u6b63\u786e\u3002RapidMiner\u5728\u6d4b\u8bd5\u65f6\uff0c\u4f1a\u4e0e\u96c6\u7fa4\u768488\u7aef\u53e3\u8fde\u63a5\u8fdb\u884cKerberos\u8ba4\u8bc1\uff0c\u800cFusionInsight\u5e73\u53f0\u5bf9\u7aef\u53e3\u8fdb\u884c\u4e86\u89c4\u5212\uff0cKerberos\u8ba4\u8bc1\u4f7f\u7528\u7684\u7aef\u53e3\u662f21732\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u63d0\u793aGSS initiate failed \u68c0\u67e5\u672c\u5730host\u6587\u4ef6\u662f\u5426\u6dfb\u52a0\u4e86\u96c6\u7fa4IP\u4e0e\u4e3b\u673a\u540d\u7684\u5bf9\u5e94\u5173\u7cfb\u3002 \u6d4b\u8bd5Spark\u65f6\uff0c\u5c06\u5404\u79cd\u7248\u672c\u90fd\u6d4b\u8bd5\u4e86\u4e00\u904d\uff0c\u6700\u540e\u63d0\u793aSpark test failed \u68c0\u67e5\u6dfb\u52a0\u7684\u4e24\u4e2aAdvanced Parameters\u662f\u5426\u586b\u5199\u6b63\u786e\uff0c\u5176value\u503c\u4e2d\u7684\u7edd\u5bf9\u8def\u5f84\u5bf9\u4e8e\u6bcf\u4e2a\u96c6\u7fa4\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5f53\u96c6\u7fa4\u91cd\u88c5\u540e\u9700\u8981\u4fee\u6539\u8be5\u503c\u3002","title":"FAQ"},{"location":"Data_Analysis/SAS_9.4M3/","text":"SAS\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SAS 9.4M3 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Yarn) SAS 9.4M3 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/Hive/Yarn) SAS 9.4M3 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/Yarn) \u652f\u6301\u4ee5\u4e0b\u7ec4\u4ef6\u5bf9\u63a5\uff08SAS Access for Hadoop\u3001SAS HPA\u3001SAS EP\uff09 \u5bf9\u63a5\u6307\u5bfc \u00b6 9.4M3 \u2194 C60 9.4M3 \u2194 C70 9.4M3 \u2194 C80","title":"9.4M3 <--> C80"},{"location":"Data_Analysis/SAS_9.4M3/#sasfusioninsight","text":"","title":"SAS\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/SAS_9.4M3/#_1","text":"SAS 9.4M3 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Yarn) SAS 9.4M3 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/Hive/Yarn) SAS 9.4M3 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/Yarn) \u652f\u6301\u4ee5\u4e0b\u7ec4\u4ef6\u5bf9\u63a5\uff08SAS Access for Hadoop\u3001SAS HPA\u3001SAS EP\uff09","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/SAS_9.4M3/#_2","text":"9.4M3 \u2194 C60 9.4M3 \u2194 C70 9.4M3 \u2194 C80","title":"\u5bf9\u63a5\u6307\u5bfc"},{"location":"Data_Analysis/SSAS/","text":"SQL Server Analysis Services \u5bf9\u63a5 FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 SSAS 2017 \u2194 FusionInsight HD 6.5 (Hive/Spark) \u6982\u8ff0 \u00b6 SQL Server Analysis Service (SSAS) \u662f\u5fae\u8f6f\u65d7\u4e0b\u7684\u6570\u636e\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9b\u4f01\u4e1a\u7ea7\u7684\u8bed\u4e49\u6570\u636e\u6a21\u578b\uff0c\u7528\u4e8e\u652f\u6491\u62a5\u8868\u5206\u6790\u548c\u5546\u4e1a\u51b3\u7b56\u3002 SSAS \u5e38\u7528\u7684\u4e24\u79cd\u6a21\u578b\u4e3a\u8868\u683c\u6a21\u578b\u548c\u591a\u7ef4\u6a21\u578b\uff0c\u4f7f\u7528\u8868\u683c\u6a21\u578b\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7ODBC\u9a71\u52a8\u76f4\u63a5\u5bf9\u63a5Hive\u6570\u636e\u6e90\uff0c\u5982\u679c\u662f\u591a\u7ef4\u6a21\u578b\uff0c\u9700\u8981\u901a\u8fc7SQL Server\u94fe\u63a5\u670d\u52a1\u5668\u4f5c\u8f6c\u6362\u540e\uff0c\u624d\u80fd\u5bf9\u63a5Hive\u6570\u636e\u6e90\uff0c\u8be6\u7ec6\u5bf9\u63a5\u8fc7\u7a0b\u53c2\u8003\u540e\u7eed\u529f\u80fd\u9a8c\u8bc1\u8fc7\u7a0b\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u8f6f\u4ef6\u5b89\u88c5 \u00b6 \u63d0\u524d\u5b89\u88c5\u4ee5\u4e0b\u8f6f\u4ef6\u7528\u4e8eSSAS\u6d4b\u8bd5\u9a8c\u8bc1\uff1a visual studio 2019\uff0c\u9700\u8981\u5b89\u88c5 Microsoft Analysis Services Projects \u6269\u5c55\u5305\u4ee5\u53cadata storage and processing \u5de5\u5177\u5305\u3002 Power BI Desktop\uff0c\u7528\u4e8e\u8c03\u7528SSAS\u6570\u636e\u6e90\uff0c\u8fdb\u884c\u6570\u636e\u5c55\u73b0\u3002 SSMS, \u7528\u4e8eSQL Server\u548cSSAS\u6570\u636e\u5e93\u7ba1\u7406\u3002 SQL Server 2017\uff0c\u9700\u8981\u521b\u5efa\u4e24\u4e2a\u5b9e\u4f8b\uff0c\u5206\u522b\u652f\u6301\u8fd0\u884c\u8868\u683c\u6a21\u578b\u548c\u591a\u7ef4\u6a21\u578b\u7684SSAS\u3002 SSAS 2017\uff0c\u521b\u5efa\u4e24\u4e2a\u5b9e\u4f8b\uff0c\u5206\u522b\u8fd0\u884c\u8868\u683c\u6a21\u578b\u6a21\u5f0f\u548c\u591a\u7ef4\u6a21\u578b\u6a21\u5f0f\u3002 \u6570\u636e\u6e90\u9a71\u52a8\u51c6\u5907 \u00b6 \u4e0b\u8f7d\u548c\u5b89\u88c532bit \u548c64 bit\u7684Microsoft ODBC\u9a71\u52a8: Microsoft Hive ODBC Driver\u4e0b\u8f7d\uff1a \u70b9\u51fb\u4e0b\u8f7d Microsoft Spark ODBC Driver\u4e0b\u8f7d\u5730\u5740\uff1a \u70b9\u51fb\u4e0b\u8f7d \u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u4e0b\u8f7d\u548c\u5b89\u88c5MIT Kerberos\u8ba4\u8bc1\u5ba2\u6237\u7aef: \u70b9\u51fb\u4e0b\u8f7d \u914d\u7f6eKerberos\u8ba4\u8bc1\u8fc7\u7a0b\uff1a \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\u3002\u8bf7\u8bb0\u4f4f\u5b89\u88c5\u8def\u5f84\uff0c\u4f8b\u5982\uff1a\u201cC:\\Program Files\\MIT\\Kerberos\u201d\u3002 \u53c2\u8003FusionInsight HD 6.5 \u6587\u6863\uff0c\u201c\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1\u201d\u914d\u7f6e\u5bf9\u63a5\u8d26\u53f7\u53ca\u76f8\u5173\u6743\u9650\u3002 \u4f7f\u7528\u5bf9\u63a5\u8d26\u53f7\u767b\u5f55FusionInsight\u7ba1\u7406\u754c\u9762\uff0c\u9f20\u6807\u505c\u7559\u5728\u53f3\u4e0a\u89d2\u663e\u793a\u7528\u6237\u5904\uff0c\u5728\u4e0b\u62c9\u663e\u793a\u6846\u4e2d\u9009\u62e9\u201c\u4e0b\u8f7d\u7528\u6237\u51ed\u636e\u201d\uff0c\u9009\u62e9\u96c6\u7fa4\u5e76\u786e\u8ba4\u4e0b\u8f7d\uff0c\u4e0b\u8f7d\u89e3\u538b\u540e\u5305\u62eckrb5.conf\u548cuser.keytab\u4e24\u4e2a\u6587\u4ef6\u3002 \u91cd\u547d\u540dkrb5.conf\u6587\u4ef6\u4e3akrb5.ini\uff0c\u5e76\u62f7\u8d1d\u5230\u201cC:\\ProgramData\\MIT\\Kerberos5\u201d\u76ee\u5f55\u4e0b\u3002\u201cC:\\ProgramData\u201d\u76ee\u5f55\u4e00\u822c\u662f\u9690\u85cf\u7684\uff0c\u9700\u8981\u8bbe\u7f6e\u663e\u793a\u9690\u85cf\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6\u3002\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002\u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5_CONFIG\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\ProgramData\\MIT\\Kerberos5\\krb5.ini\u201d,\u91cd\u542f\u670d\u52a1\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1\u3002\u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002 \u914d\u7f6eSpark ODBC\u3000DSN \u00b6 \u6253\u5f00Windows ODBC\u914d\u7f6e\u5de5\u5177\uff0c\u5728 System DSN \u4e2d\uff0c\u5206\u522b\u914d\u7f6e Sample Microsoft Hive DSN \u548c Sample Microsoft Spark DSN \uff0c\u76f8\u5173\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff0c\u6839\u636e\u5b9e\u9645\u73af\u5883\u66ff\u6362HOST\u5730\u5740\u3002 Data Source Name: Sample Microsoft Spark DSN Spark Serve Type: SparkThriftServer(Spark1.1 and later) Host(s): 172.16.11.22\uff0cSpark2x\u7684JDBCServer2x\u4e3b\u8282\u70b9 Port\uff1a22550 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a Thrift Transport: SASL \u53c2\u8003\u914d\u7f6e\uff1a \u5728 SSL OPTIONS \u4e2d\u53d6\u6d88\u52fe\u9009SSL\u6821\u9a8c\uff0c\u8bbe\u7f6e\u5982\u4e0b\u56fe\uff1a \u914d\u7f6eHIVE ODBC DSN \u00b6 HIVE DSN \u914d\u7f6e\u53c2\u8003\u9009\u9879\u5982\u4e0b\uff0c\u6839\u636e\u5b9e\u9645\u73af\u5883\u66ff\u6362HOST\u5730\u5740 Data Source Name: Sample Microsoft Hive DSN Host(s): 172.16.11.21\uff0cHive Service\u4e3b\u8282\u70b9 Port\uff1a21066\uff0cHive Service\u7aef\u53e3 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a Thrift Transport: SASL SSL Options: \u53d6\u6d88\u52fe\u9009\u201cEnable SSL\u201d \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a \u529f\u80fd\u9a8c\u8bc1\u4e00\uff1a\u521b\u5efa\u8868\u683c\u6a21\u578b \u00b6 \u9a8c\u8bc1\u4f7f\u7528Hive\u6570\u636e\u6e90\u521b\u5efa\u4e00\u4e2aSSAS\u8868\u683c\u6a21\u578b\u3002 \u51c6\u5907\u6d4b\u8bd5\u6570\u636e\u3002 \u00b6 \u5728beeline\u6a21\u5f0f\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efa\u6d4b\u8bd5\u8868\u53ca\u76f8\u5173\u6570\u636e\u3002 create database ssas; create table city(ct_id string,ct_value string); insert into city values('sz', 'shenzhen'); insert into city values('bj', 'beijing'); insert into city values('sh', 'shanghai'); create table customer(cs_id string, cs_name string, cs_age int, cs_city string); insert into customer values('1','tk', 32, 'sz'); insert into customer values('2','ht', 18, 'bj'); insert into customer values('3','zhh', 21, 'sh'); insert into customer values('4','hhx', 22, 'sz'); create table store(st_id string, st_city string); insert into store values('store1', 'sz'); insert into store values('store2', 'bj'); insert into store values('store3', 'sh'); create table goods(gd_id string, gd_name string, gd_price int); insert into goods values('bd','bread',10); insert into goods values('mk','milk',6); create table date_dim(date_id string,dt_date date,year int,month int,day int); insert into date_dim values('2010','20101001',2010,10,1); insert into date_dim values('2011','20110501',2011,5,1); create table orders(od_cs_id string,od_st_id string, od_dd_id string,od_gd_id string, gd_price int,quantity int, income int); insert into orders values('1','store1','2010','bd',10,30,300); insert into orders values('2','store2','2011','mk',6,5,30); insert into orders values('3','store3','2010','mk',6,10,60); insert into orders values('3','store3','2011','bd',10,3,30); insert into orders values('2','store2','2010','mk',6,30,180); insert into orders values('1','store1','2011','bd',10,20,200); \u521b\u5efaProject \u00b6 \u6253\u5f00visual studio \u5de5\u5177\uff0c File -> new -> Project -> Analysis services tabular project \uff0c\u70b9\u51fb next ,\u8bbe\u7f6e\u9879\u76ee\u540d\u79f0\uff0c\u9009\u62e9\u6846\u67b6\u4e3a .NET Framework 4.7.2 \uff0c\u70b9\u51fb create , \u9009\u62e9 Integrated workspace\" \uff0c\u517c\u5bb9\u6a21\u5f0f\u8bbe\u7f6e\u4e3a SQL Server 2017/ Azure Analysis Services(1400) , \u70b9\u51fb OK \u5b8c\u6210\u9879\u76ee\u521b\u5efa\u3002 \u6dfb\u52a0\u6570\u636e \u00b6 \u5728\u53f3\u4fa7Tabular Model Explorer \u4e2d\uff0c\u53f3\u952e DATA SOURCE -> New DataSource -> ODBC -> \u70b9\u51fb Connect . \u65b0\u7684\u5f39\u51fa\u7a97\u53e3\u4e2d\uff0c\u9009\u62e9DSN\uff0c\u6bd4\u5982\u914d\u7f6e\u7684 Sample Microsoft Hive DSN \uff0c \u70b9\u51fb Ok \u3002 \u9009\u62e9 windows -> impersonation mode \u4e3a impersonate service account \uff0c\u70b9\u51fb Connect \u3002 \u5bfc\u5165\u8868\u683c \u00b6 \u53f3\u952e SQL\u6570\u636e\u6e90\uff0c\u9009\u62e9 import new tables \uff0c\u52fe\u9009\u521b\u5efa\u7684\u6240\u6709\u89c6\u56fe\uff0c\u70b9\u51fb load \u52a0\u8f7d\u6570\u636e\uff0c\u6267\u884c\u6210\u529f\u540e\u5982\u4e0b\u56fe\uff1a \u521b\u5efa\u6a21\u578b \u00b6 \u521b\u5efa\u6a21\u578b\u7684\u5b9e\u4f53\u5173\u8054\u5173\u7cfb\uff0c\u5982\u4e0b\u56fe\uff1a \u70b9\u51fb\u5de5\u5177\u680f \u542f\u52a8 \uff0c\u5b8c\u6210\u6a21\u578b\u521b\u5efa\u548c\u90e8\u7f72\uff0c\u6a21\u578b\u521b\u5efa\u5b8c\u6210\u540e\u5982\u4e0b\u56fe\uff1a \u62a5\u8868\u5206\u6790 \u00b6 \u4f7f\u7528Power BI Desktop \u52a0\u8f7dssas\u6a21\u578b\u5e76\u8fdb\u884c\u62a5\u8868\u5c55\u793a \u6253\u5f00Power BI Desktop\u8f6f\u4ef6\uff0c\u9009\u62e9 Get Data \uff0c\u9009\u62e9 SQL Server Analysis Service Database \uff0c\u70b9\u51fb Connect , \u8f93\u5165 SSAS database server\u4fe1\u606f\uff0cDatabase\u53ef\u4ee5\u4e0d\u586b\uff0c\u8fde\u63a5\u65b9\u5f0f\u4f7f\u7528 Connect Live \u65b9\u5f0f\uff0c\u70b9\u51fb OK \u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u9009\u62e9\u5bf9\u5e94\u7684\u6a21\u578b\uff0c\u5e76\u70b9\u51fb OK \u8fdb\u5165\u4e0b\u4e00\u6b65\uff0c\u6a21\u578b\u5c06\u88ab\u5bfc\u5165\uff0c\u6a21\u578b\u4e2d\u7684\u8868\u53ca\u5b57\u6bb5\u4fe1\u606f\u5c06\u5728\u53f3\u4fa7Fields\u6a21\u5757\u4e2d\u663e\u793a\u3002 \u8bbe\u8ba1\u62a5\u8868\uff0c\u672c\u6848\u4f8b\u4e2d\u8bbe\u8ba1\u4e00\u4e2a\u76f4\u65b9\u56fe\uff0c\u5c55\u793a\u5404\u57ce\u5e02\u9762\u5305\u548c\u725b\u5976\u7684\u9500\u552e\u60c5\u51b5\uff1a \u53f3\u952e\u56fe\u8868\uff0c\u9009\u62e9 show data \uff0c\u53ef\u4ee5\u67e5\u770b\u8be6\u7ec6\u6570\u636e\uff0c\u5982\u4e0b\u56fe\uff1a Power BI Desktop \u80fd\u591f\u6b63\u786e\u8bfb\u53d6SSAS\u6a21\u578b\u6570\u636e\u5e76\u62a5\u8868\u5448\u73b0\uff0c\u9a8c\u8bc1\u5b8c\u6210\u3002 \u529f\u80fd\u9a8c\u8bc1\u4e8c\uff1a\u521b\u5efa\u591a\u7ef4\u6a21\u578b\u6570\u636e\u6e90\u89c6\u56fe \u00b6 \u7531\u4e8eMicrosoft Visual Studo\u591a\u7ef4\u6a21\u578b\u6570\u636e\u6e90\u4ee5\u652f\u6301Microsoft OLE DB\u4e3a\u4e3b\uff0c\u5e76\u4e0d\u652f\u6301ODBC\uff0c\u6240\u4ee5\u5bf9\u63a5Hive\u6570\u636e\u6e90\u65f6\uff0c\u76f4\u63a5\u4f7f\u7528Microsoft Hive/Spark ODBC\u662f\u65e0\u6cd5\u5bf9\u63a5\u4f7f\u7528\u7684\u3002\u9700\u8981\u5728SQL Server \u670d\u52a1\u5668\u4e2d\uff0c\u5c06Hive/Spark \u6dfb\u52a0\u4e3a\u5916\u90e8\u94fe\u63a5\u670d\u52a1\u5668\uff0c\u9a71\u52a8\u9009\u7528 Microsoft OLEDB Provider for ODBC Driver\uff0c\u540c\u65f6\u5728SQL Server\u4e2d\u521b\u5efa\u89c6\u56fe\u6765\u8c03\u7528Hive\u8868\u3002\u8fd9\u79cd\u65b9\u5f0f\u95f4\u63a5\u7684\u901a\u8fc7SQL Server\u505a\u4e00\u4e2a\u8f6c\u6362\uff0c\u5b9e\u73b0\u5bf9Hive\u6570\u636e\u6e90\u7684\u8bbf\u95ee \u3002 \u4e0b\u56fe\u4e3a\u591a\u7ef4\u6570\u636e\u6a21\u578b\u652f\u6301\u7684\u6570\u636e\u6e90\u9a71\u52a8\u5217\u8868\uff0c\u4e0d\u652f\u6301ODBC\u7c7b\u578b\u9a71\u52a8\u3002 \u591a\u7ef4\u6a21\u578b\u5efa\u6a21\u6d89\u53ca\u7684\u5de5\u5177\u53ca\u6d41\u7a0b\u53c2\u8003\u5982\u4e0b\u56fe\uff1a Microsoft\u5b98\u65b9SSAS\u5bf9\u63a5Hive/Spark\u6700\u4f73\u5b9e\u8df5: \u70b9\u51fb\u67e5\u770b \u51c6\u5907\u6d4b\u8bd5\u6570\u636e\u3002 \u00b6 \u5728beeline\u6a21\u5f0f\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efa\u6d4b\u8bd5\u8868\u53ca\u76f8\u5173\u6570\u636e\u3002 create database ssas; create table city(ct_id string,ct_value string); insert into city values('sz', 'shenzhen'); insert into city values('bj', 'beijing'); insert into city values('sh', 'shanghai'); create table customer(cs_id string, cs_name string, cs_age int, cs_city string); insert into customer values('1','tk', 32, 'sz'); insert into customer values('2','ht', 18, 'bj'); insert into customer values('3','zhh', 21, 'sh'); insert into customer values('4','hhx', 22, 'sz'); create table store(st_id string, st_city string); insert into store values('store1', 'sz'); insert into store values('store2', 'bj'); insert into store values('store3', 'sh'); create table goods(gd_id string, gd_name string, gd_price int); insert into goods values('bd','bread',10); insert into goods values('mk','milk',6); create table date_dim(date_id string,dt_date date,year int,month int,day int); insert into date_dim values('2010','20101001',2010,10,1); insert into date_dim values('2011','20110501',2011,5,1); create table orders(od_cs_id string,od_st_id string, od_dd_id string,od_gd_id string, gd_price int,quantity int, income int); insert into orders values('1','store1','2010','bd',10,30,300); insert into orders values('2','store2','2011','mk',6,5,30); insert into orders values('3','store3','2010','mk',6,10,60); insert into orders values('3','store3','2011','bd',10,3,30); insert into orders values('2','store2','2010','mk',6,30,180); insert into orders values('1','store1','2011','bd',10,20,200); \u521b\u5efa linked server \u00b6 \u5728SQL SERVER \u6570\u636e\u5e93\u4e2d\u94fe\u63a5\u6570\u636e\u5e93 \u6253\u5f00SQL Server Management Studio, \u9009\u62e9 file -> object Explorer \uff0c\u9009\u62e9 database engine ,\u8fde\u63a5\u672c\u673a\u5df2\u7ecf\u5b89\u88c5\u5b8c\u6210\u7684SQL Server\u3002 \u5de6\u4fa7\u8d44\u6e90\u680f\u4e2d\uff0c\u53f3\u952e server objects -> Linked Servers \uff0c\u9009\u62e9 New Linked Server \uff0c\u8f93\u5165\u94fe\u63a5\u670d\u52a1\u5668\u7684\u8fde\u63a5\u4fe1\u606f\uff0c\u70b9\u51fb OK \u5b8c\u6210\u914d\u7f6e\u3002 Linked Server: HIVELINK ,\u53ef\u81ea\u5b9a\u4e49 Provider: Microsoft OLEDB Provider for ODBC Driver Data Source: Sample Microsoft Hive DSN #\u5982\u679c\u662fSpark,\u5219\u6b64\u5904\u66ff\u6362\u4e3aSpark\u7684DSN,\u5982Sample Microsoft Spark DSN \u5176\u5b83\u53ef\u7559\u7a7a \u914d\u7f6e\u53c2\u8003\u5982\u4e0b\u56fe\uff1a \u521b\u5efa\u89c6\u56fe \u00b6 \u5728SSMS\u4e2d\uff0c\u6253\u5f00\u4e00\u4e2aSQL\u67e5\u8be2\u7a97\u53e3\uff0c\u5206\u522b\u6267\u884c\u4ee5\u4e0b\u6bcf\u6761\u8bed\u53e5\uff0c\u521b\u5efaHive\u8868\u5bf9\u5e94\u7684\u89c6\u56fe\uff1a create view vw_ssas_city AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.city;') create view vw_ssas_customer AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.customer;') create view vw_ssas_date_dim AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.date_dim;') create view vw_ssas_goods AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.goods;') create view vw_ssas_orders AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.orders;') create view vw_ssas_store AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.store;') create view vw_ssas_store AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.store;') \u521b\u5efaProject \u00b6 \u6253\u5f00visual studio \u5de5\u5177\uff0c File -> new -> Project -> Analysis Services Multidimensional and Data Mining Project \uff0c\u70b9\u51fb next ,\u8bbe\u7f6e\u9879\u76ee\u540d\u79f0\uff0c\u70b9\u51fb create , \u5b8c\u6210\u9879\u76ee\u521b\u5efa\u3002 \u5bfc\u5165\u6570\u636e\u7528\u4e8e\u5efa\u6a21 \u00b6 \u5728\u53f3\u4fa7 Solution Explorer \u4e2d\uff0c\u53f3\u952e Data Sources -> New Data Source \uff0c\u70b9\u51fb NEXT , \u9009\u62e9 Create a data source based on an existing or a new connection ,\u70b9\u51fb new , \u8bbe\u7f6e\u6570\u636e\u5e93\u8fde\u63a5\u3002 Connection Manager \u7a97\u53e3\u4e2d\uff0c\u9009\u62e9\u6216\u8005\u8f93\u5165 Server Name \u7684IP\u6216\u8005\u57df\u540d\uff0cDatabase \u9009\u62e9 master(\u521b\u5efa\u7684\u89c6\u56fe\u6240\u5728\u7684DB) \uff0c\u70b9\u51fb next \u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 Impersonate Information \u9875\u9762\u4e2d\uff0c\u53ef\u4ee5\u9009\u62e9 use service as an account , \u8bbe\u7f6edatabase \u540d\u79f0\uff0c\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 \u53f3\u4fa7 Solution Explorer\u4e2d\uff0c\u53f3\u952e Data Source Views \uff0c\u9009\u62e9 New Data Source View , \u70b9\u51fb Next , \u4e0b\u4e00\u9875\u9762\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb Next \u3002 Select Tables and Views \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u9700\u8981\u5305\u542b\u7684\u89c6\u56fe\uff0c\u6dfb\u52a0\u5230\u53f3\u4fa7 included Objects \u4e2d\uff0c\u70b9\u51fb next \u8fdb\u5165\u4e0b\u4e00\u6b65\uff0c\u70b9\u51fb Finish \u5b8c\u6210\u8bbe\u7f6e\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6b64\u65f6\u6570\u636e\u6e90\u89c6\u56fe\u5df2\u7ecf\u6210\u529f\u83b7\u53d6Hive\u6570\u636e\u6e90\u4e2d\u7684\u76f8\u5173\u8868\u683c\u6570\u636e\u4fe1\u606f\u3002","title":"2017 <--> 6.5"},{"location":"Data_Analysis/SSAS/#sql-server-analysis-services-fusioninsight-hd","text":"","title":"SQL Server Analysis Services \u5bf9\u63a5 FusionInsight HD"},{"location":"Data_Analysis/SSAS/#_1","text":"SSAS 2017 \u2194 FusionInsight HD 6.5 (Hive/Spark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/SSAS/#_2","text":"SQL Server Analysis Service (SSAS) \u662f\u5fae\u8f6f\u65d7\u4e0b\u7684\u6570\u636e\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9b\u4f01\u4e1a\u7ea7\u7684\u8bed\u4e49\u6570\u636e\u6a21\u578b\uff0c\u7528\u4e8e\u652f\u6491\u62a5\u8868\u5206\u6790\u548c\u5546\u4e1a\u51b3\u7b56\u3002 SSAS \u5e38\u7528\u7684\u4e24\u79cd\u6a21\u578b\u4e3a\u8868\u683c\u6a21\u578b\u548c\u591a\u7ef4\u6a21\u578b\uff0c\u4f7f\u7528\u8868\u683c\u6a21\u578b\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7ODBC\u9a71\u52a8\u76f4\u63a5\u5bf9\u63a5Hive\u6570\u636e\u6e90\uff0c\u5982\u679c\u662f\u591a\u7ef4\u6a21\u578b\uff0c\u9700\u8981\u901a\u8fc7SQL Server\u94fe\u63a5\u670d\u52a1\u5668\u4f5c\u8f6c\u6362\u540e\uff0c\u624d\u80fd\u5bf9\u63a5Hive\u6570\u636e\u6e90\uff0c\u8be6\u7ec6\u5bf9\u63a5\u8fc7\u7a0b\u53c2\u8003\u540e\u7eed\u529f\u80fd\u9a8c\u8bc1\u8fc7\u7a0b\u3002","title":"\u6982\u8ff0"},{"location":"Data_Analysis/SSAS/#_3","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Analysis/SSAS/#_4","text":"\u63d0\u524d\u5b89\u88c5\u4ee5\u4e0b\u8f6f\u4ef6\u7528\u4e8eSSAS\u6d4b\u8bd5\u9a8c\u8bc1\uff1a visual studio 2019\uff0c\u9700\u8981\u5b89\u88c5 Microsoft Analysis Services Projects \u6269\u5c55\u5305\u4ee5\u53cadata storage and processing \u5de5\u5177\u5305\u3002 Power BI Desktop\uff0c\u7528\u4e8e\u8c03\u7528SSAS\u6570\u636e\u6e90\uff0c\u8fdb\u884c\u6570\u636e\u5c55\u73b0\u3002 SSMS, \u7528\u4e8eSQL Server\u548cSSAS\u6570\u636e\u5e93\u7ba1\u7406\u3002 SQL Server 2017\uff0c\u9700\u8981\u521b\u5efa\u4e24\u4e2a\u5b9e\u4f8b\uff0c\u5206\u522b\u652f\u6301\u8fd0\u884c\u8868\u683c\u6a21\u578b\u548c\u591a\u7ef4\u6a21\u578b\u7684SSAS\u3002 SSAS 2017\uff0c\u521b\u5efa\u4e24\u4e2a\u5b9e\u4f8b\uff0c\u5206\u522b\u8fd0\u884c\u8868\u683c\u6a21\u578b\u6a21\u5f0f\u548c\u591a\u7ef4\u6a21\u578b\u6a21\u5f0f\u3002","title":"\u8f6f\u4ef6\u5b89\u88c5"},{"location":"Data_Analysis/SSAS/#_5","text":"\u4e0b\u8f7d\u548c\u5b89\u88c532bit \u548c64 bit\u7684Microsoft ODBC\u9a71\u52a8: Microsoft Hive ODBC Driver\u4e0b\u8f7d\uff1a \u70b9\u51fb\u4e0b\u8f7d Microsoft Spark ODBC Driver\u4e0b\u8f7d\u5730\u5740\uff1a \u70b9\u51fb\u4e0b\u8f7d","title":"\u6570\u636e\u6e90\u9a71\u52a8\u51c6\u5907"},{"location":"Data_Analysis/SSAS/#kerberos","text":"\u4e0b\u8f7d\u548c\u5b89\u88c5MIT Kerberos\u8ba4\u8bc1\u5ba2\u6237\u7aef: \u70b9\u51fb\u4e0b\u8f7d \u914d\u7f6eKerberos\u8ba4\u8bc1\u8fc7\u7a0b\uff1a \u4e0b\u8f7d\u5e76\u5b89\u88c5MIT Kerberos\u3002\u8bf7\u8bb0\u4f4f\u5b89\u88c5\u8def\u5f84\uff0c\u4f8b\u5982\uff1a\u201cC:\\Program Files\\MIT\\Kerberos\u201d\u3002 \u53c2\u8003FusionInsight HD 6.5 \u6587\u6863\uff0c\u201c\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1\u201d\u914d\u7f6e\u5bf9\u63a5\u8d26\u53f7\u53ca\u76f8\u5173\u6743\u9650\u3002 \u4f7f\u7528\u5bf9\u63a5\u8d26\u53f7\u767b\u5f55FusionInsight\u7ba1\u7406\u754c\u9762\uff0c\u9f20\u6807\u505c\u7559\u5728\u53f3\u4e0a\u89d2\u663e\u793a\u7528\u6237\u5904\uff0c\u5728\u4e0b\u62c9\u663e\u793a\u6846\u4e2d\u9009\u62e9\u201c\u4e0b\u8f7d\u7528\u6237\u51ed\u636e\u201d\uff0c\u9009\u62e9\u96c6\u7fa4\u5e76\u786e\u8ba4\u4e0b\u8f7d\uff0c\u4e0b\u8f7d\u89e3\u538b\u540e\u5305\u62eckrb5.conf\u548cuser.keytab\u4e24\u4e2a\u6587\u4ef6\u3002 \u91cd\u547d\u540dkrb5.conf\u6587\u4ef6\u4e3akrb5.ini\uff0c\u5e76\u62f7\u8d1d\u5230\u201cC:\\ProgramData\\MIT\\Kerberos5\u201d\u76ee\u5f55\u4e0b\u3002\u201cC:\\ProgramData\u201d\u76ee\u5f55\u4e00\u822c\u662f\u9690\u85cf\u7684\uff0c\u9700\u8981\u8bbe\u7f6e\u663e\u793a\u9690\u85cf\u6587\u4ef6\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6\u3002\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982\u201cC:\\temp\u201d\u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5CCNAME\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\temp\\krb5cache\u201d\u3002\u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a\u201cKRB5_CONFIG\u201d\uff0c\u53d8\u91cf\u503c\u4e3a\u201cC:\\ProgramData\\MIT\\Kerberos5\\krb5.ini\u201d,\u91cd\u542f\u670d\u52a1\u5668\u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1\u3002\u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb\u201cget Ticket\u201d\uff0c\u5728\u5f39\u51fa\u7684\u201cMIT Kerberos: Get Ticket\u201d\u7a97\u53e3\u4e2d\uff0c\u201cPricipal\u201d\u8f93\u5165\u7528\u6237\u540d\uff0c\u201cPassword\u201d\u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb\u201cOK\u201d\u3002","title":"\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Analysis/SSAS/#spark-odbc-dsn","text":"\u6253\u5f00Windows ODBC\u914d\u7f6e\u5de5\u5177\uff0c\u5728 System DSN \u4e2d\uff0c\u5206\u522b\u914d\u7f6e Sample Microsoft Hive DSN \u548c Sample Microsoft Spark DSN \uff0c\u76f8\u5173\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff0c\u6839\u636e\u5b9e\u9645\u73af\u5883\u66ff\u6362HOST\u5730\u5740\u3002 Data Source Name: Sample Microsoft Spark DSN Spark Serve Type: SparkThriftServer(Spark1.1 and later) Host(s): 172.16.11.22\uff0cSpark2x\u7684JDBCServer2x\u4e3b\u8282\u70b9 Port\uff1a22550 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1aspark2x Realm\uff1a\u7559\u7a7a Thrift Transport: SASL \u53c2\u8003\u914d\u7f6e\uff1a \u5728 SSL OPTIONS \u4e2d\u53d6\u6d88\u52fe\u9009SSL\u6821\u9a8c\uff0c\u8bbe\u7f6e\u5982\u4e0b\u56fe\uff1a","title":"\u914d\u7f6eSpark ODBC\u3000DSN"},{"location":"Data_Analysis/SSAS/#hive-odbc-dsn","text":"HIVE DSN \u914d\u7f6e\u53c2\u8003\u9009\u9879\u5982\u4e0b\uff0c\u6839\u636e\u5b9e\u9645\u73af\u5883\u66ff\u6362HOST\u5730\u5740 Data Source Name: Sample Microsoft Hive DSN Host(s): 172.16.11.21\uff0cHive Service\u4e3b\u8282\u70b9 Port\uff1a21066\uff0cHive Service\u7aef\u53e3 Database: default Mechanism\uff1aKerberos Host FQDN\uff1ahadoop.hadoop.com Service Name\uff1ahive Realm\uff1a\u7559\u7a7a Thrift Transport: SASL SSL Options: \u53d6\u6d88\u52fe\u9009\u201cEnable SSL\u201d \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a","title":"\u914d\u7f6eHIVE ODBC DSN"},{"location":"Data_Analysis/SSAS/#_6","text":"\u9a8c\u8bc1\u4f7f\u7528Hive\u6570\u636e\u6e90\u521b\u5efa\u4e00\u4e2aSSAS\u8868\u683c\u6a21\u578b\u3002","title":"\u529f\u80fd\u9a8c\u8bc1\u4e00\uff1a\u521b\u5efa\u8868\u683c\u6a21\u578b"},{"location":"Data_Analysis/SSAS/#_7","text":"\u5728beeline\u6a21\u5f0f\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efa\u6d4b\u8bd5\u8868\u53ca\u76f8\u5173\u6570\u636e\u3002 create database ssas; create table city(ct_id string,ct_value string); insert into city values('sz', 'shenzhen'); insert into city values('bj', 'beijing'); insert into city values('sh', 'shanghai'); create table customer(cs_id string, cs_name string, cs_age int, cs_city string); insert into customer values('1','tk', 32, 'sz'); insert into customer values('2','ht', 18, 'bj'); insert into customer values('3','zhh', 21, 'sh'); insert into customer values('4','hhx', 22, 'sz'); create table store(st_id string, st_city string); insert into store values('store1', 'sz'); insert into store values('store2', 'bj'); insert into store values('store3', 'sh'); create table goods(gd_id string, gd_name string, gd_price int); insert into goods values('bd','bread',10); insert into goods values('mk','milk',6); create table date_dim(date_id string,dt_date date,year int,month int,day int); insert into date_dim values('2010','20101001',2010,10,1); insert into date_dim values('2011','20110501',2011,5,1); create table orders(od_cs_id string,od_st_id string, od_dd_id string,od_gd_id string, gd_price int,quantity int, income int); insert into orders values('1','store1','2010','bd',10,30,300); insert into orders values('2','store2','2011','mk',6,5,30); insert into orders values('3','store3','2010','mk',6,10,60); insert into orders values('3','store3','2011','bd',10,3,30); insert into orders values('2','store2','2010','mk',6,30,180); insert into orders values('1','store1','2011','bd',10,20,200);","title":"\u51c6\u5907\u6d4b\u8bd5\u6570\u636e\u3002"},{"location":"Data_Analysis/SSAS/#project","text":"\u6253\u5f00visual studio \u5de5\u5177\uff0c File -> new -> Project -> Analysis services tabular project \uff0c\u70b9\u51fb next ,\u8bbe\u7f6e\u9879\u76ee\u540d\u79f0\uff0c\u9009\u62e9\u6846\u67b6\u4e3a .NET Framework 4.7.2 \uff0c\u70b9\u51fb create , \u9009\u62e9 Integrated workspace\" \uff0c\u517c\u5bb9\u6a21\u5f0f\u8bbe\u7f6e\u4e3a SQL Server 2017/ Azure Analysis Services(1400) , \u70b9\u51fb OK \u5b8c\u6210\u9879\u76ee\u521b\u5efa\u3002","title":"\u521b\u5efaProject"},{"location":"Data_Analysis/SSAS/#_8","text":"\u5728\u53f3\u4fa7Tabular Model Explorer \u4e2d\uff0c\u53f3\u952e DATA SOURCE -> New DataSource -> ODBC -> \u70b9\u51fb Connect . \u65b0\u7684\u5f39\u51fa\u7a97\u53e3\u4e2d\uff0c\u9009\u62e9DSN\uff0c\u6bd4\u5982\u914d\u7f6e\u7684 Sample Microsoft Hive DSN \uff0c \u70b9\u51fb Ok \u3002 \u9009\u62e9 windows -> impersonation mode \u4e3a impersonate service account \uff0c\u70b9\u51fb Connect \u3002","title":"\u6dfb\u52a0\u6570\u636e"},{"location":"Data_Analysis/SSAS/#_9","text":"\u53f3\u952e SQL\u6570\u636e\u6e90\uff0c\u9009\u62e9 import new tables \uff0c\u52fe\u9009\u521b\u5efa\u7684\u6240\u6709\u89c6\u56fe\uff0c\u70b9\u51fb load \u52a0\u8f7d\u6570\u636e\uff0c\u6267\u884c\u6210\u529f\u540e\u5982\u4e0b\u56fe\uff1a","title":"\u5bfc\u5165\u8868\u683c"},{"location":"Data_Analysis/SSAS/#_10","text":"\u521b\u5efa\u6a21\u578b\u7684\u5b9e\u4f53\u5173\u8054\u5173\u7cfb\uff0c\u5982\u4e0b\u56fe\uff1a \u70b9\u51fb\u5de5\u5177\u680f \u542f\u52a8 \uff0c\u5b8c\u6210\u6a21\u578b\u521b\u5efa\u548c\u90e8\u7f72\uff0c\u6a21\u578b\u521b\u5efa\u5b8c\u6210\u540e\u5982\u4e0b\u56fe\uff1a","title":"\u521b\u5efa\u6a21\u578b"},{"location":"Data_Analysis/SSAS/#_11","text":"\u4f7f\u7528Power BI Desktop \u52a0\u8f7dssas\u6a21\u578b\u5e76\u8fdb\u884c\u62a5\u8868\u5c55\u793a \u6253\u5f00Power BI Desktop\u8f6f\u4ef6\uff0c\u9009\u62e9 Get Data \uff0c\u9009\u62e9 SQL Server Analysis Service Database \uff0c\u70b9\u51fb Connect , \u8f93\u5165 SSAS database server\u4fe1\u606f\uff0cDatabase\u53ef\u4ee5\u4e0d\u586b\uff0c\u8fde\u63a5\u65b9\u5f0f\u4f7f\u7528 Connect Live \u65b9\u5f0f\uff0c\u70b9\u51fb OK \u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u9009\u62e9\u5bf9\u5e94\u7684\u6a21\u578b\uff0c\u5e76\u70b9\u51fb OK \u8fdb\u5165\u4e0b\u4e00\u6b65\uff0c\u6a21\u578b\u5c06\u88ab\u5bfc\u5165\uff0c\u6a21\u578b\u4e2d\u7684\u8868\u53ca\u5b57\u6bb5\u4fe1\u606f\u5c06\u5728\u53f3\u4fa7Fields\u6a21\u5757\u4e2d\u663e\u793a\u3002 \u8bbe\u8ba1\u62a5\u8868\uff0c\u672c\u6848\u4f8b\u4e2d\u8bbe\u8ba1\u4e00\u4e2a\u76f4\u65b9\u56fe\uff0c\u5c55\u793a\u5404\u57ce\u5e02\u9762\u5305\u548c\u725b\u5976\u7684\u9500\u552e\u60c5\u51b5\uff1a \u53f3\u952e\u56fe\u8868\uff0c\u9009\u62e9 show data \uff0c\u53ef\u4ee5\u67e5\u770b\u8be6\u7ec6\u6570\u636e\uff0c\u5982\u4e0b\u56fe\uff1a Power BI Desktop \u80fd\u591f\u6b63\u786e\u8bfb\u53d6SSAS\u6a21\u578b\u6570\u636e\u5e76\u62a5\u8868\u5448\u73b0\uff0c\u9a8c\u8bc1\u5b8c\u6210\u3002","title":"\u62a5\u8868\u5206\u6790"},{"location":"Data_Analysis/SSAS/#_12","text":"\u7531\u4e8eMicrosoft Visual Studo\u591a\u7ef4\u6a21\u578b\u6570\u636e\u6e90\u4ee5\u652f\u6301Microsoft OLE DB\u4e3a\u4e3b\uff0c\u5e76\u4e0d\u652f\u6301ODBC\uff0c\u6240\u4ee5\u5bf9\u63a5Hive\u6570\u636e\u6e90\u65f6\uff0c\u76f4\u63a5\u4f7f\u7528Microsoft Hive/Spark ODBC\u662f\u65e0\u6cd5\u5bf9\u63a5\u4f7f\u7528\u7684\u3002\u9700\u8981\u5728SQL Server \u670d\u52a1\u5668\u4e2d\uff0c\u5c06Hive/Spark \u6dfb\u52a0\u4e3a\u5916\u90e8\u94fe\u63a5\u670d\u52a1\u5668\uff0c\u9a71\u52a8\u9009\u7528 Microsoft OLEDB Provider for ODBC Driver\uff0c\u540c\u65f6\u5728SQL Server\u4e2d\u521b\u5efa\u89c6\u56fe\u6765\u8c03\u7528Hive\u8868\u3002\u8fd9\u79cd\u65b9\u5f0f\u95f4\u63a5\u7684\u901a\u8fc7SQL Server\u505a\u4e00\u4e2a\u8f6c\u6362\uff0c\u5b9e\u73b0\u5bf9Hive\u6570\u636e\u6e90\u7684\u8bbf\u95ee \u3002 \u4e0b\u56fe\u4e3a\u591a\u7ef4\u6570\u636e\u6a21\u578b\u652f\u6301\u7684\u6570\u636e\u6e90\u9a71\u52a8\u5217\u8868\uff0c\u4e0d\u652f\u6301ODBC\u7c7b\u578b\u9a71\u52a8\u3002 \u591a\u7ef4\u6a21\u578b\u5efa\u6a21\u6d89\u53ca\u7684\u5de5\u5177\u53ca\u6d41\u7a0b\u53c2\u8003\u5982\u4e0b\u56fe\uff1a Microsoft\u5b98\u65b9SSAS\u5bf9\u63a5Hive/Spark\u6700\u4f73\u5b9e\u8df5: \u70b9\u51fb\u67e5\u770b","title":"\u529f\u80fd\u9a8c\u8bc1\u4e8c\uff1a\u521b\u5efa\u591a\u7ef4\u6a21\u578b\u6570\u636e\u6e90\u89c6\u56fe"},{"location":"Data_Analysis/SSAS/#_13","text":"\u5728beeline\u6a21\u5f0f\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efa\u6d4b\u8bd5\u8868\u53ca\u76f8\u5173\u6570\u636e\u3002 create database ssas; create table city(ct_id string,ct_value string); insert into city values('sz', 'shenzhen'); insert into city values('bj', 'beijing'); insert into city values('sh', 'shanghai'); create table customer(cs_id string, cs_name string, cs_age int, cs_city string); insert into customer values('1','tk', 32, 'sz'); insert into customer values('2','ht', 18, 'bj'); insert into customer values('3','zhh', 21, 'sh'); insert into customer values('4','hhx', 22, 'sz'); create table store(st_id string, st_city string); insert into store values('store1', 'sz'); insert into store values('store2', 'bj'); insert into store values('store3', 'sh'); create table goods(gd_id string, gd_name string, gd_price int); insert into goods values('bd','bread',10); insert into goods values('mk','milk',6); create table date_dim(date_id string,dt_date date,year int,month int,day int); insert into date_dim values('2010','20101001',2010,10,1); insert into date_dim values('2011','20110501',2011,5,1); create table orders(od_cs_id string,od_st_id string, od_dd_id string,od_gd_id string, gd_price int,quantity int, income int); insert into orders values('1','store1','2010','bd',10,30,300); insert into orders values('2','store2','2011','mk',6,5,30); insert into orders values('3','store3','2010','mk',6,10,60); insert into orders values('3','store3','2011','bd',10,3,30); insert into orders values('2','store2','2010','mk',6,30,180); insert into orders values('1','store1','2011','bd',10,20,200);","title":"\u51c6\u5907\u6d4b\u8bd5\u6570\u636e\u3002"},{"location":"Data_Analysis/SSAS/#linked-server","text":"\u5728SQL SERVER \u6570\u636e\u5e93\u4e2d\u94fe\u63a5\u6570\u636e\u5e93 \u6253\u5f00SQL Server Management Studio, \u9009\u62e9 file -> object Explorer \uff0c\u9009\u62e9 database engine ,\u8fde\u63a5\u672c\u673a\u5df2\u7ecf\u5b89\u88c5\u5b8c\u6210\u7684SQL Server\u3002 \u5de6\u4fa7\u8d44\u6e90\u680f\u4e2d\uff0c\u53f3\u952e server objects -> Linked Servers \uff0c\u9009\u62e9 New Linked Server \uff0c\u8f93\u5165\u94fe\u63a5\u670d\u52a1\u5668\u7684\u8fde\u63a5\u4fe1\u606f\uff0c\u70b9\u51fb OK \u5b8c\u6210\u914d\u7f6e\u3002 Linked Server: HIVELINK ,\u53ef\u81ea\u5b9a\u4e49 Provider: Microsoft OLEDB Provider for ODBC Driver Data Source: Sample Microsoft Hive DSN #\u5982\u679c\u662fSpark,\u5219\u6b64\u5904\u66ff\u6362\u4e3aSpark\u7684DSN,\u5982Sample Microsoft Spark DSN \u5176\u5b83\u53ef\u7559\u7a7a \u914d\u7f6e\u53c2\u8003\u5982\u4e0b\u56fe\uff1a","title":"\u521b\u5efa linked server"},{"location":"Data_Analysis/SSAS/#_14","text":"\u5728SSMS\u4e2d\uff0c\u6253\u5f00\u4e00\u4e2aSQL\u67e5\u8be2\u7a97\u53e3\uff0c\u5206\u522b\u6267\u884c\u4ee5\u4e0b\u6bcf\u6761\u8bed\u53e5\uff0c\u521b\u5efaHive\u8868\u5bf9\u5e94\u7684\u89c6\u56fe\uff1a create view vw_ssas_city AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.city;') create view vw_ssas_customer AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.customer;') create view vw_ssas_date_dim AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.date_dim;') create view vw_ssas_goods AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.goods;') create view vw_ssas_orders AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.orders;') create view vw_ssas_store AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.store;') create view vw_ssas_store AS SELECT * FROM OPENQUERY(HIVELINK,'SELECT * FROM ssas.store;')","title":"\u521b\u5efa\u89c6\u56fe"},{"location":"Data_Analysis/SSAS/#project_1","text":"\u6253\u5f00visual studio \u5de5\u5177\uff0c File -> new -> Project -> Analysis Services Multidimensional and Data Mining Project \uff0c\u70b9\u51fb next ,\u8bbe\u7f6e\u9879\u76ee\u540d\u79f0\uff0c\u70b9\u51fb create , \u5b8c\u6210\u9879\u76ee\u521b\u5efa\u3002","title":"\u521b\u5efaProject"},{"location":"Data_Analysis/SSAS/#_15","text":"\u5728\u53f3\u4fa7 Solution Explorer \u4e2d\uff0c\u53f3\u952e Data Sources -> New Data Source \uff0c\u70b9\u51fb NEXT , \u9009\u62e9 Create a data source based on an existing or a new connection ,\u70b9\u51fb new , \u8bbe\u7f6e\u6570\u636e\u5e93\u8fde\u63a5\u3002 Connection Manager \u7a97\u53e3\u4e2d\uff0c\u9009\u62e9\u6216\u8005\u8f93\u5165 Server Name \u7684IP\u6216\u8005\u57df\u540d\uff0cDatabase \u9009\u62e9 master(\u521b\u5efa\u7684\u89c6\u56fe\u6240\u5728\u7684DB) \uff0c\u70b9\u51fb next \u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 Impersonate Information \u9875\u9762\u4e2d\uff0c\u53ef\u4ee5\u9009\u62e9 use service as an account , \u8bbe\u7f6edatabase \u540d\u79f0\uff0c\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 \u53f3\u4fa7 Solution Explorer\u4e2d\uff0c\u53f3\u952e Data Source Views \uff0c\u9009\u62e9 New Data Source View , \u70b9\u51fb Next , \u4e0b\u4e00\u9875\u9762\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb Next \u3002 Select Tables and Views \u9875\u9762\u4e2d\uff0c\u9009\u62e9\u9700\u8981\u5305\u542b\u7684\u89c6\u56fe\uff0c\u6dfb\u52a0\u5230\u53f3\u4fa7 included Objects \u4e2d\uff0c\u70b9\u51fb next \u8fdb\u5165\u4e0b\u4e00\u6b65\uff0c\u70b9\u51fb Finish \u5b8c\u6210\u8bbe\u7f6e\u3002 \u5982\u4e0b\u56fe\u6240\u793a\uff0c\u6b64\u65f6\u6570\u636e\u6e90\u89c6\u56fe\u5df2\u7ecf\u6210\u529f\u83b7\u53d6Hive\u6570\u636e\u6e90\u4e2d\u7684\u76f8\u5173\u8868\u683c\u6570\u636e\u4fe1\u606f\u3002","title":"\u5bfc\u5165\u6570\u636e\u7528\u4e8e\u5efa\u6a21"},{"location":"Data_Analysis/Splunk/","text":"Splunk\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Splunk 7.2.4 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Splunk 7.2.4 \u2194 FusionInsight HD 6.5 (HDFS) Splunk 7.2.4 \u2194 FusionInsight MRS 8.0 (HDFS) \u5b89\u88c5\u4e0e\u542f\u52a8Splunk,\u83b7\u53d6\u914d\u7f6e\u6587\u4ef6 \u00b6 \u5173\u95ed\u4e3b\u673a\u9632\u706b\u5899 systemctl stop firewalld \u5b89\u88c5Splunk 7.2.4,\u5728\u7f51\u5740 https://www.splunk.com/en_us/download/splunk-enterprise.html \u4e0b\u8f7dLinux\u5e73\u53f0\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf splunk-7.2.4-8a94541dcfac-Linux-x86_64.tgz \u89e3\u538b\u51fasplunk\u76ee\u5f55\u3002 Splunk \u5bf9\u63a5Hadoop\u96c6\u7fa4\u9700\u8981\u4f7f\u7528Splunk Analytics for Hadoop \u7ec4\u4ef6\uff0c\u8be5\u7ec4\u4ef6\u4e0d\u652f\u6301Windows\u7248\u672c\u7684Splunk Enterprise\uff0c\u9700\u4e0b\u8f7dLinux\u7248\u672cSplunk \u542f\u52a8\u548c\u505c\u6b62splunk,\u8fdb\u5165splunk\u76ee\u5f55\u6267\u884c ./bin/splunk start ./bin/splunk stop \u7b2c\u4e00\u6b21\u542f\u52a8\u4f1a\u663e\u793aLicence Agreement\u9875\u9762\uff0c\u8f93\u5165 y ,\u7136\u540e\u8f93\u5165\u7528\u6237\u540d\u548c\u5bc6\u7801 \u542f\u52a8\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b \u5728\u6d4f\u89c8\u5668\u8f93\u5165 http://ip:8080 \uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801\u5373\u53ef\u8fdb\u5165splunk\u9875\u9762\u3002 - \u5728\u96c6\u7fa4\u670d\u52a1\u7aef\uff0c\u83b7\u53d6mapred\u7528\u6237\u7684keytab\u6587\u4ef6\u4ee5\u53ca\u96c6\u7fa4\u7684krb5.conf\u6587,\u4e0a\u4f20\u81f3splunk\u4e3b\u673a\u4e2d,\u4f8b\u5982 /opt/splunk/ \u76ee\u5f55\u4e0b \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f \u00b6 \u8fdb\u5165splunk\u4e3b\u754c\u9762\uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u8bbe\u7f6e\uff0c\u9009\u62e9\u865a\u62df\u7d22\u5f15 \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u586b\u5199\u76f8\u5173\u4fe1\u606f \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u63d0\u4f9b\u7a0b\u5e8f\u5e8f\u5217\uff1ahadoop Java\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfJAVA_HOME\u7684\u503c Hadoop\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfHADOOP_HOME\u7684\u503c \u586b\u5199Hadoop\u96c6\u7fa4\u4fe1\u606f Hadoop\u7248\u672c\uff1aHadoop2.x(Yarn) \u6587\u4ef6\u7cfb\u7edf\uff1ahdfs://hacluster \u52fe\u9009\u542f\u7528\u901a\u8fc7\u8eab\u4efd\u9a8c\u8bc1 \u8d44\u6e90\u7ba1\u7406\u5668\u5730\u5740:resourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip\u6216\u4e3b\u673a\u540d,\u7aef\u53e3\u4e3a26004,\u5728\u96c6\u7fa4manager\u754c\u9762,\u9009\u62e9\u670d\u52a1\u7ba1\u7406->yarn->resourcemanager\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip,\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\uff0c\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u7aef\u53e3 \u8d44\u6e90\u8ba1\u5212\u7a0b\u5e8f\u5730\u5740:\u8282\u70b9\u540cresourcemanager,\u7aef\u53e3\u53ef\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\u67e5\u770b HDFS\u5de5\u4f5c\u76ee\u5f55\uff1a\u81ea\u884c\u5236\u5b9a \u586b\u5199\u5b89\u5168\u8bbe\u7f6e\u4fe1\u606f \u52fe\u9009\u6dfb\u52a0\u5b89\u5168\u96c6\u7fa4\uff0c\u5b89\u5168\u6a21\u5f0f\u9009\u62e9kerberos Kerberos\u670d\u52a1\u5668\u914d\u7f6e\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u8def\u5f84,\u586b\u5199\u8def\u5f84 kerberos\u4e3b\u4f53\u540d\u79f0:mapred/hadoop. hadoop.com@HADOOP.com kerberos\u5bc6\u94a5\u5373\u4e3akeytab\u6587\u4ef6 HDFS\u4e3b\u4f53:hdfa/hadoop. hadoop.com@HADOOP.com MapreReduce\u4e3b\u4f53\u4e3a:mapred/hadoop. hadoop.com@HADOOP.com \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u4e0e\u8282\u70b9\u7ba1\u7406\u5668\u4e3b\u4f53\u53ef\u4e0d\u586b \u5176\u4ed6\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb\u201c\u4fdd\u5b58\u201d\u3002 \u65b0\u5efa\u865a\u62df\u7d22\u5f15 \u00b6 \u5728\u65b0\u5efa\u7d22\u5f15\u754c\u9762\uff0c\u81ea\u5b9a\u4e49\u7d22\u5f15\u540d\u79f0\uff0c\u63d0\u4f9b\u7a0b\u5e8f\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\uff0cHDFS \u4e2d\u6570\u636e\u7684\u8def\u5f84\u6839\u636e\u9700\u8981\u641c\u7d22\u7684\u8def\u5f84\u8fdb\u884c\u586b\u5199\uff0c\u52fe\u9009\u9012\u5f52\u5904\u7406\u76ee\u5f55\uff0c\u70b9\u51fb\u4fdd\u5b58\u3002 \u4f7f\u7528\u641c\u7d22\u7a0b\u5e8f \u00b6 \u5728splunk\u4e3b\u9875\u9762\uff0c\u70b9\u51fb\u6d4f\u89c8\u6570\u636e \u9009\u62e9\u5df2\u521b\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\u548c\u865a\u62df\u7d22\u5f15 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u9009\u62e9\u8981\u641c\u7d22\u7684\u6587\u4ef6 \u5728\u6570\u636e\u9884\u89c8\u4e2d\uff0c\u9009\u62e9\u6570\u636e\u6765\u6e90\u7c7b\u578b\uff0c\u6839\u636e\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u9009\u62e9 \u5728\u4e0a\u4e0b\u6587\u914d\u7f6e\u4e2d\u9009\u62e9\u5e94\u7528\u7a0b\u5e8f\u7684\u4e0a\u4e0b\u6587\uff0c\u70b9\u51fb\u4e0b\u4e00\u6b65 \u70b9\u51fb\u5b8c\u6210 \u70b9\u51fb\u641c\u7d22\u53ef\u4ee5\u8fdb\u5165\u5bf9\u6b64\u6587\u4ef6\u7684\u641c\u7d22\u9875\u9762 \u53ef\u4ee5\u6839\u636e\u67e5\u8be2\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u53ef\u89c6\u5316\u5c55\u793a \u8bfb\u53d6Hive\u8868 \u00b6 \u901a\u8fc7Splunk\u8bfb\u53d6Hive\u8868\uff0c\u9700\u8981\u5728\u63d0\u4f9b\u7a0b\u5e8f\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u914d\u7f6e\uff1a vix.splunk.search.splitter = HiveSplitGenerator vix.splunk.search.splitter.hive.serde = org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe \u5728\u865a\u62df\u7d22\u5f15\u4e2d\u914d\u7f6e\u8981\u641c\u7d22\u7684\u8868\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u6570\u636e\u5e93\u540d\u79f0\uff0c\u8868\u540d\uff0c\u8868\u5934\uff0c\u5b57\u6bb5\u7c7b\u578b\uff0c\u6587\u4ef6\u7c7b\u578b\uff0c\u5206\u9694\u7b26\uff0c\u6362\u884c\u7b26 \u76ee\u524d\u4ec5\u80fd\u6b63\u786e\u8bfb\u53d6rcfile\u683c\u5f0f\u7684\u6587\u4ef6 \u7136\u540e\u5728\u865a\u62df\u7d22\u5f15\u5904\u70b9\u51fb \u641c\u7d22 \uff0c\u8fdb\u5165\u641c\u7d22\u9875\u9762\uff0c\u5e76\u5728\u641c\u7d22\u6846\u524d\u9009\u62e9 \u6240\u6709\u65f6\u95f4 \uff0c\u5373\u53ef\u770b\u5230\u8868\u4e2d\u6570\u636e","title":"7.2.4 <--> 8.0"},{"location":"Data_Analysis/Splunk/#splunkfusioninsight-hd","text":"","title":"Splunk\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Analysis/Splunk/#_1","text":"Splunk 7.2.4 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Splunk 7.2.4 \u2194 FusionInsight HD 6.5 (HDFS) Splunk 7.2.4 \u2194 FusionInsight MRS 8.0 (HDFS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/Splunk/#splunk","text":"\u5173\u95ed\u4e3b\u673a\u9632\u706b\u5899 systemctl stop firewalld \u5b89\u88c5Splunk 7.2.4,\u5728\u7f51\u5740 https://www.splunk.com/en_us/download/splunk-enterprise.html \u4e0b\u8f7dLinux\u5e73\u53f0\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf splunk-7.2.4-8a94541dcfac-Linux-x86_64.tgz \u89e3\u538b\u51fasplunk\u76ee\u5f55\u3002 Splunk \u5bf9\u63a5Hadoop\u96c6\u7fa4\u9700\u8981\u4f7f\u7528Splunk Analytics for Hadoop \u7ec4\u4ef6\uff0c\u8be5\u7ec4\u4ef6\u4e0d\u652f\u6301Windows\u7248\u672c\u7684Splunk Enterprise\uff0c\u9700\u4e0b\u8f7dLinux\u7248\u672cSplunk \u542f\u52a8\u548c\u505c\u6b62splunk,\u8fdb\u5165splunk\u76ee\u5f55\u6267\u884c ./bin/splunk start ./bin/splunk stop \u7b2c\u4e00\u6b21\u542f\u52a8\u4f1a\u663e\u793aLicence Agreement\u9875\u9762\uff0c\u8f93\u5165 y ,\u7136\u540e\u8f93\u5165\u7528\u6237\u540d\u548c\u5bc6\u7801 \u542f\u52a8\u6210\u529f\u540e\u663e\u793a\u5982\u4e0b \u5728\u6d4f\u89c8\u5668\u8f93\u5165 http://ip:8080 \uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801\u5373\u53ef\u8fdb\u5165splunk\u9875\u9762\u3002 - \u5728\u96c6\u7fa4\u670d\u52a1\u7aef\uff0c\u83b7\u53d6mapred\u7528\u6237\u7684keytab\u6587\u4ef6\u4ee5\u53ca\u96c6\u7fa4\u7684krb5.conf\u6587,\u4e0a\u4f20\u81f3splunk\u4e3b\u673a\u4e2d,\u4f8b\u5982 /opt/splunk/ \u76ee\u5f55\u4e0b","title":"\u5b89\u88c5\u4e0e\u542f\u52a8Splunk,\u83b7\u53d6\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Analysis/Splunk/#_2","text":"\u8fdb\u5165splunk\u4e3b\u754c\u9762\uff0c\u70b9\u51fb\u53f3\u4e0a\u89d2\u8bbe\u7f6e\uff0c\u9009\u62e9\u865a\u62df\u7d22\u5f15 \u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f\uff0c\u586b\u5199\u76f8\u5173\u4fe1\u606f \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u63d0\u4f9b\u7a0b\u5e8f\u5e8f\u5217\uff1ahadoop Java\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfJAVA_HOME\u7684\u503c Hadoop\u4e3b\u9875\uff1a\u96c6\u7fa4\u4e2d\u73af\u5883\u53d8\u91cfHADOOP_HOME\u7684\u503c \u586b\u5199Hadoop\u96c6\u7fa4\u4fe1\u606f Hadoop\u7248\u672c\uff1aHadoop2.x(Yarn) \u6587\u4ef6\u7cfb\u7edf\uff1ahdfs://hacluster \u52fe\u9009\u542f\u7528\u901a\u8fc7\u8eab\u4efd\u9a8c\u8bc1 \u8d44\u6e90\u7ba1\u7406\u5668\u5730\u5740:resourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip\u6216\u4e3b\u673a\u540d,\u7aef\u53e3\u4e3a26004,\u5728\u96c6\u7fa4manager\u754c\u9762,\u9009\u62e9\u670d\u52a1\u7ba1\u7406->yarn->resourcemanager\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u6240\u5728\u8282\u70b9ip,\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\uff0c\u53ef\u67e5\u770bresourcemanager\u670d\u52a1\u7aef\u53e3 \u8d44\u6e90\u8ba1\u5212\u7a0b\u5e8f\u5730\u5740:\u8282\u70b9\u540cresourcemanager,\u7aef\u53e3\u53ef\u5728\u670d\u52a1\u914d\u7f6e\u4e2d\u67e5\u770b HDFS\u5de5\u4f5c\u76ee\u5f55\uff1a\u81ea\u884c\u5236\u5b9a \u586b\u5199\u5b89\u5168\u8bbe\u7f6e\u4fe1\u606f \u52fe\u9009\u6dfb\u52a0\u5b89\u5168\u96c6\u7fa4\uff0c\u5b89\u5168\u6a21\u5f0f\u9009\u62e9kerberos Kerberos\u670d\u52a1\u5668\u914d\u7f6e\u9009\u62e9\u914d\u7f6e\u6587\u4ef6\u8def\u5f84,\u586b\u5199\u8def\u5f84 kerberos\u4e3b\u4f53\u540d\u79f0:mapred/hadoop. hadoop.com@HADOOP.com kerberos\u5bc6\u94a5\u5373\u4e3akeytab\u6587\u4ef6 HDFS\u4e3b\u4f53:hdfa/hadoop. hadoop.com@HADOOP.com MapreReduce\u4e3b\u4f53\u4e3a:mapred/hadoop. hadoop.com@HADOOP.com \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u4e0e\u8282\u70b9\u7ba1\u7406\u5668\u4e3b\u4f53\u53ef\u4e0d\u586b \u5176\u4ed6\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb\u201c\u4fdd\u5b58\u201d\u3002","title":"\u65b0\u5efa\u63d0\u4f9b\u7a0b\u5e8f"},{"location":"Data_Analysis/Splunk/#_3","text":"\u5728\u65b0\u5efa\u7d22\u5f15\u754c\u9762\uff0c\u81ea\u5b9a\u4e49\u7d22\u5f15\u540d\u79f0\uff0c\u63d0\u4f9b\u7a0b\u5e8f\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\uff0cHDFS \u4e2d\u6570\u636e\u7684\u8def\u5f84\u6839\u636e\u9700\u8981\u641c\u7d22\u7684\u8def\u5f84\u8fdb\u884c\u586b\u5199\uff0c\u52fe\u9009\u9012\u5f52\u5904\u7406\u76ee\u5f55\uff0c\u70b9\u51fb\u4fdd\u5b58\u3002","title":"\u65b0\u5efa\u865a\u62df\u7d22\u5f15"},{"location":"Data_Analysis/Splunk/#_4","text":"\u5728splunk\u4e3b\u9875\u9762\uff0c\u70b9\u51fb\u6d4f\u89c8\u6570\u636e \u9009\u62e9\u5df2\u521b\u5efa\u7684\u63d0\u4f9b\u7a0b\u5e8f\u548c\u865a\u62df\u7d22\u5f15 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u9009\u62e9\u8981\u641c\u7d22\u7684\u6587\u4ef6 \u5728\u6570\u636e\u9884\u89c8\u4e2d\uff0c\u9009\u62e9\u6570\u636e\u6765\u6e90\u7c7b\u578b\uff0c\u6839\u636e\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u9009\u62e9 \u5728\u4e0a\u4e0b\u6587\u914d\u7f6e\u4e2d\u9009\u62e9\u5e94\u7528\u7a0b\u5e8f\u7684\u4e0a\u4e0b\u6587\uff0c\u70b9\u51fb\u4e0b\u4e00\u6b65 \u70b9\u51fb\u5b8c\u6210 \u70b9\u51fb\u641c\u7d22\u53ef\u4ee5\u8fdb\u5165\u5bf9\u6b64\u6587\u4ef6\u7684\u641c\u7d22\u9875\u9762 \u53ef\u4ee5\u6839\u636e\u67e5\u8be2\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u53ef\u89c6\u5316\u5c55\u793a","title":"\u4f7f\u7528\u641c\u7d22\u7a0b\u5e8f"},{"location":"Data_Analysis/Splunk/#hive","text":"\u901a\u8fc7Splunk\u8bfb\u53d6Hive\u8868\uff0c\u9700\u8981\u5728\u63d0\u4f9b\u7a0b\u5e8f\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u914d\u7f6e\uff1a vix.splunk.search.splitter = HiveSplitGenerator vix.splunk.search.splitter.hive.serde = org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe \u5728\u865a\u62df\u7d22\u5f15\u4e2d\u914d\u7f6e\u8981\u641c\u7d22\u7684\u8868\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u6570\u636e\u5e93\u540d\u79f0\uff0c\u8868\u540d\uff0c\u8868\u5934\uff0c\u5b57\u6bb5\u7c7b\u578b\uff0c\u6587\u4ef6\u7c7b\u578b\uff0c\u5206\u9694\u7b26\uff0c\u6362\u884c\u7b26 \u76ee\u524d\u4ec5\u80fd\u6b63\u786e\u8bfb\u53d6rcfile\u683c\u5f0f\u7684\u6587\u4ef6 \u7136\u540e\u5728\u865a\u62df\u7d22\u5f15\u5904\u70b9\u51fb \u641c\u7d22 \uff0c\u8fdb\u5165\u641c\u7d22\u9875\u9762\uff0c\u5e76\u5728\u641c\u7d22\u6846\u524d\u9009\u62e9 \u6240\u6709\u65f6\u95f4 \uff0c\u5373\u53ef\u770b\u5230\u8868\u4e2d\u6570\u636e","title":"\u8bfb\u53d6Hive\u8868"},{"location":"Data_Analysis/%E6%B0%B8%E6%B4%AA%E4%B8%80%E7%AB%99%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/","text":"\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 8.7 \u2194 FusionInsight MRS 8.0 (Hive) \u8bf4\u660e\uff1a \u6c38\u6d2aBI\u672c\u6587\u4f7f\u7528windows\u90e8\u7f72\u6a21\u5f0f\uff0c\u5982\u679c\u662f\u4f7f\u7528Linux\u90e8\u7f72\u6a21\u5f0f\u9700\u8981\u5b98\u65b9License\uff0c\u6ca1\u6709\u6d4b\u8bd5 \u5bf9\u63a5\u53c2\u6570\u914d\u7f6e \u00b6 jdbc\u8fde\u63a5url\u4e3a\uff1a jdbc:hive2://node131:24002,node132:24002,node133:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=E:/ecotesting_mrs/Fiber/conf/user.keytab \u5982\u679c\u662fmrs 8.0\u7248\u672c\uff0c\u5219\u9700\u8981\u7684\u9a71\u52a8jar\u5305\u4e3a \u5ba2\u6237\u7aef\u8def\u5f84\\Hive\\jdbc\u4e0b\u6240\u6709jar\u5305\u52a0\u4e0a\u5982\u4e0b\u989d\u5916\u4e09\u4e2ajar\u5305\uff08\u5982\u679c\u7f3a\u5c11\u7684\u8bdd\uff09 commons-lang-2.6.jar zookeeper-jute-3.5.6-hw-ei-302002.jar commons-collections-3.2.2.jar \u8fde\u63a5\u6d4b\u8bd5\uff1a HIVE\u67e5\u8be2\u7ed3\u679c \u00b6","title":"8.7 <--> 8.0"},{"location":"Data_Analysis/%E6%B0%B8%E6%B4%AA%E4%B8%80%E7%AB%99%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/#fusioninsight","text":"","title":"\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0\u5bf9\u63a5FusionInsight"},{"location":"Data_Analysis/%E6%B0%B8%E6%B4%AA%E4%B8%80%E7%AB%99%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/#_1","text":"\u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 7.1 \u2194 FusionInsight HD V100R002C60U20 (Hive/SparkSQL) \u6c38\u6d2a\u4e00\u7ad9\u5f0f\u5927\u6570\u636e\u5206\u6790\u5e73\u53f0 8.7 \u2194 FusionInsight MRS 8.0 (Hive) \u8bf4\u660e\uff1a \u6c38\u6d2aBI\u672c\u6587\u4f7f\u7528windows\u90e8\u7f72\u6a21\u5f0f\uff0c\u5982\u679c\u662f\u4f7f\u7528Linux\u90e8\u7f72\u6a21\u5f0f\u9700\u8981\u5b98\u65b9License\uff0c\u6ca1\u6709\u6d4b\u8bd5","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Analysis/%E6%B0%B8%E6%B4%AA%E4%B8%80%E7%AB%99%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/#_2","text":"jdbc\u8fde\u63a5url\u4e3a\uff1a jdbc:hive2://node131:24002,node132:24002,node133:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=E:/ecotesting_mrs/Fiber/conf/user.keytab \u5982\u679c\u662fmrs 8.0\u7248\u672c\uff0c\u5219\u9700\u8981\u7684\u9a71\u52a8jar\u5305\u4e3a \u5ba2\u6237\u7aef\u8def\u5f84\\Hive\\jdbc\u4e0b\u6240\u6709jar\u5305\u52a0\u4e0a\u5982\u4e0b\u989d\u5916\u4e09\u4e2ajar\u5305\uff08\u5982\u679c\u7f3a\u5c11\u7684\u8bdd\uff09 commons-lang-2.6.jar zookeeper-jute-3.5.6-hw-ei-302002.jar commons-collections-3.2.2.jar \u8fde\u63a5\u6d4b\u8bd5\uff1a","title":"\u5bf9\u63a5\u53c2\u6570\u914d\u7f6e"},{"location":"Data_Analysis/%E6%B0%B8%E6%B4%AA%E4%B8%80%E7%AB%99%E5%BC%8F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0/#hive","text":"","title":"HIVE\u67e5\u8be2\u7ed3\u679c"},{"location":"Data_Integration/","text":"\u6570\u636e\u96c6\u6210 \u00b6 Apache NiFi 1.7.1 \u2194 C80 1.9.2 \u2194 6.5 1.12.0 \u2194 8.0 Confluent 4.1.0 \u2194 C80 4.1.0 \u2194 6.5 5.5.0 \u2194 8.0 DataX 0.1 \u2194 6.5 0.1 \u2194 8.0 Denodo Platform 7.0 \u2194 C80 7.0 \u2194 6.5 7.0 \u2194 8.0 H2O.ai 3.24.0.2 \u2194 6.5 3.24.0.2 \u2194 8.0 IBM InfoSphere CDC 11.3.3.1 \u2194 C50 IBM InfoSphere DataStage 11.3.1.0 \u2194 C50 11.5.0.2 \u2194 C60 Informatica PowerCenter 10.2.0 \u2194 C80 10.2.0 \u2194 6.5 Informatica PowerexChange CDC 10.2.0 \u2194 C80 Informatica 10.0.0 \u2194 C50 10.0.0 \u2194 C60 10.0.0 \u2194 C80 10.0.0 \u2194 C70 10.2.2 \u2194 6.5 10.2.2 \u2194 8.0 Kafka Manager 1.3.3.21 \u2194 C80 1.3.3.21 \u2194 6.5 Kettle 6.1 \u2194 C60 6.1 \u2194 C70 6.1 \u2194 C80 6.1 \u2194 6.5 6.1 \u2194 8.0 Knime 3.6.1 \u2194 C80 4.1.0 \u2194 6.5 4.1.0 \u2194 8.0 OceanSource 1.0 \u2194 C80 Oracle GoldenGate 12.2 \u2194 C60 12.2 \u2194 6.5 12.2 \u2194 8.0 12.3 \u2194 C70 12.3 \u2194 C80 Pentaho EE 7.1 \u2194 C70 8.0 \u2194 C60 SharePlex 9.2.1 \u2194 C80 9.2.1 \u2194 6.5 Streamsets 3.16.1 \u2194 6.5 3.16.1 \u2194 8.0 Talend 6.4.1 \u2194 C80 6.4.1 \u2194 8.0 7.0.1 \u2194 C80 7.2.1 \u2194 6.5 Tibco BW 5.13 \u2194 6.5 debezium 1.0.0 \u2194 6.5 1.2.2 \u2194 8.0 \u676d\u5dde\u5408\u4f17UTL 5.1 \u2194 C50","title":"Index"},{"location":"Data_Integration/#_1","text":"Apache NiFi 1.7.1 \u2194 C80 1.9.2 \u2194 6.5 1.12.0 \u2194 8.0 Confluent 4.1.0 \u2194 C80 4.1.0 \u2194 6.5 5.5.0 \u2194 8.0 DataX 0.1 \u2194 6.5 0.1 \u2194 8.0 Denodo Platform 7.0 \u2194 C80 7.0 \u2194 6.5 7.0 \u2194 8.0 H2O.ai 3.24.0.2 \u2194 6.5 3.24.0.2 \u2194 8.0 IBM InfoSphere CDC 11.3.3.1 \u2194 C50 IBM InfoSphere DataStage 11.3.1.0 \u2194 C50 11.5.0.2 \u2194 C60 Informatica PowerCenter 10.2.0 \u2194 C80 10.2.0 \u2194 6.5 Informatica PowerexChange CDC 10.2.0 \u2194 C80 Informatica 10.0.0 \u2194 C50 10.0.0 \u2194 C60 10.0.0 \u2194 C80 10.0.0 \u2194 C70 10.2.2 \u2194 6.5 10.2.2 \u2194 8.0 Kafka Manager 1.3.3.21 \u2194 C80 1.3.3.21 \u2194 6.5 Kettle 6.1 \u2194 C60 6.1 \u2194 C70 6.1 \u2194 C80 6.1 \u2194 6.5 6.1 \u2194 8.0 Knime 3.6.1 \u2194 C80 4.1.0 \u2194 6.5 4.1.0 \u2194 8.0 OceanSource 1.0 \u2194 C80 Oracle GoldenGate 12.2 \u2194 C60 12.2 \u2194 6.5 12.2 \u2194 8.0 12.3 \u2194 C70 12.3 \u2194 C80 Pentaho EE 7.1 \u2194 C70 8.0 \u2194 C60 SharePlex 9.2.1 \u2194 C80 9.2.1 \u2194 6.5 Streamsets 3.16.1 \u2194 6.5 3.16.1 \u2194 8.0 Talend 6.4.1 \u2194 C80 6.4.1 \u2194 8.0 7.0.1 \u2194 C80 7.2.1 \u2194 6.5 Tibco BW 5.13 \u2194 6.5 debezium 1.0.0 \u2194 6.5 1.2.2 \u2194 8.0 \u676d\u5dde\u5408\u4f17UTL 5.1 \u2194 C50","title":"\u6570\u636e\u96c6\u6210"},{"location":"Data_Integration/ApacheNifi-1.9.2/","text":"Apache NiFi\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache NiFi 1.9.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive/Spark/Kafka) Apache NiFi 1.12.0 \u2194 FusionInsight MRS 8.0 (HDFS/HBase/Hive/Spark/Kafka) MRS 8.0 \u5bf9\u63a5\u8bf4\u660e \u00b6 \u8bf4\u660e\uff1a\u5bf9\u63a5FusionInsight MRS 8.0\u9700\u66ff\u6362jar\u5305\u5982\u4e0b hive\uff1a \u66ff\u6362\u8def\u5f84\uff1a/opt/nifi/nifi-1.12.0/work/nar/extensions/nifi-hive-nar-1.12.0.nar-unpacked/NAR-INF/bundled-dependencies \u9700\u8981\u628azookeeper-3.5.6-hw-ei-302002.jar\uff0czookeeper-jute-3.5.6-hw-ei-302002.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\uff0c\u628a\u539f\u6765\u7684zookeeper-3.4.6.jar\u6ce8\u91ca\u6389 hbase: \u66ff\u6362\u8def\u5f84\uff1a/opt/nifi/nifi-1.12.0/work/nar/extensions/nifi-hbase_2-client-service-nar-1.12.0.nar-unpacked/NAR-INF/bundled-dependencies \u9700\u8981\u628azookeeper-3.5.6-hw-ei-302002.jar\uff0czookeeper-jute-3.5.6-hw-ei-302002.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\uff0c\u628a\u539f\u6765\u7684zookeeper-3.4.10.jar\u6ce8\u91ca\u6389 \u914d\u7f6ehbase\u9700\u8981\u9009\u7528HBase_2_ClientService kafka\uff1a \u66ff\u6362\u8def\u5f84:/opt/nifi/nifi-1.12.0/work/nar/extensions/nifi-kafka-2-0-nar-1.12.0.nar-unpacked/NAR-INF/bundled-dependencies \u9700\u8981\u628akafka-clients-2.4.0-hw-ei-302002.jar\u5bfc\u5165\u5230\u8be5\u8def\u5f84\u4e0b\uff0c\u5e76\u628a\u539f\u6765\u7684kafka-clients-2.0.0.jar\u6ce8\u91ca\u6389 \u9700\u8981\u628azookeeper-3.5.6-hw-ei-302002.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84 \u5b89\u88c5Apache NiFi \u00b6 \u73af\u5883\uff1a172.16.2.121 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Apache NiFi 1.9.2 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u5b89\u88c5NiFi\uff0c\u5728\u7f51\u5740 https://nifi.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u547d\u4ee4 unzip nifi-1.9.2-bin.zip \u89e3\u538b\u5b89\u88c5\u751f\u6210nifi-1.9.2\u76ee\u5f55\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/nifi/nifi-1.9.2 \u6267\u884c vi /opt/nifi/nifi-1.9.2/conf/nifi.properties \u914d\u7f6eNiFi\u670d\u52a1\u5668ip\u548c\u7aef\u53e3\u5982\u4e0b\uff1a nifi.web.http.host=172.16.2.121 nifi.web.http.port=8085 \u542f\u52a8\u548c\u505c\u6b62NiFi cd /opt/nifi/nifi-1.9.2 bin/nifi.sh start bin/nifi.sh stop \u8fd0\u884cNiFi bin/nifi.sh start NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u914d\u7f6e\u5e76\u4fdd\u5b58Kerberos\u8ba4\u8bc1\u4fe1\u606f\uff0c\u4f9b\u4ee5\u540e\u4f7f\u7528 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Nifi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u5e76\u521b\u5efa\u6d4b\u8bd5\u7528\u6237developuser (\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1) \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728FusionInsight HD Manager\u4e0a\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6 user.keytab \uff0c krb5.conf \uff0c\u5e76\u4e00\u8d77\u5b58\u5165\u8def\u5f84 /opt/developuser \u6267\u884c\u547d\u4ee4 vi /opt/nifi/nifi-1.9.2/conf/nifi.properties \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5177\u4f53\u914d\u7f6e\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab \u91cd\u542fNiFi \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 KeytabCredentialsService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58KeytabCredentialsService \u5b8c\u6210 NiFi\u8fde\u63a5HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eHDFS\u76f8\u5173\u5904\u7406\u5668\uff0c\u5bf9\u63a5HDFS \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e PutHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHDFS\u7684\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \uff0c core-site.xml \u5bfc\u5165\u8def\u5f84 /opt/nifi/nifi-1.9.2/conf \u4fee\u6539 hdfs-site.xml \u5185\u5bb9\uff0c\u5220\u9664\u5982\u4e0b\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider</value> </property> \u4fee\u6539 core-site.xml \u5185\u5bb9\uff0c\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\u9879\u4e2dhacluster\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip\u52a0\u7aef\u53e3\u53f7 <property> <name>fs.defaultFS</name> <value>hdfs://172.16.4.123:25000</value> </property> \u5c06FI\u5ba2\u6237\u7aef\u7684 hadoop-plugins-1.0.jar \u62f7\u8d1d\u5230nifi hadoop\u76f8\u5173\u4f9d\u8d56\u8def\u5f84 cp /opt/125_651hdclient/hadoopclient/HDFS/hadoop/share/hadoop/common/lib/hadoop-plugins-1.0.jar /opt/nifi/nifi-1.9.2/work/nar/extensions/nifi-hadoop-nar-1.9.2.nar-unpacked/NAR-INF/bundled-dependencies \u8bf4\u660e\uff1a\u5426\u5219\u4f1a\u62a5\u4f9d\u8d56\u9519\u8bef \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset - \u5904\u7406\u5668PutHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /opt/nifi/nifi-1.9.2/conf/hdfs-site.xml,/opt/nifi/nifi-1.9.2/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest \u4e24\u4e2a\u5904\u7406\u5668\u7684\u8fde\u63a5\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5165\u8def\u5f84 /home/dataset \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u6d4b\u8bd5\u540e \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv GetHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /opt/nifi/nifi-1.9.2/conf/hdfs-site.xml,/opt/nifi/nifi-1.9.2/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest/HDFS - \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset/HDFS \u6d4b\u8bd5\u540e \u767b\u5f55\u5b89\u88c5 FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS \u67e5\u770b\u7ed3\u679c NiFi\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e HiveConnectionPool \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HiveConnectionPool \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u4e3a 1: jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService - \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HiveConnectionPool \u5b8c\u6210 \u5728\u8def\u5f84 /opt/nifi/nifi-1.9.2/conf \u4e0b\u521b\u5efa jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6267\u884c\u547d\u4ee4 vi /opt/nifi/nifi-1.9.2/conf/bootstrap.conf \u914d\u7f6e bootstrap.conf \u6587\u4ef6\u5982\u4e0b: java.arg.17=-Djava.security.auth.login.config=/opt/nifi/nifi-1.9.2/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=false \u6267\u884c\u547d\u4ee4 vi /opt/nifi/nifi-1.9.2/conf/nifi.properties \u914d\u7f6e nifi.properties \u6587\u4ef6\u5982\u4e0b\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true \u6267\u884c\u547d\u4ee4 cd /opt/nifi/nifi-1.9.2/work/nar/extensions/nifi-hive-nar-1.9.2.nar-unpacked/NAR-INF/bundled-dependencies \u5230NiFi Hive\u7c7b\u5e93\u4e2d\uff0c\u5c06\u539f\u6709\u7684 zookeeper-3.4.6.jar \u66ff\u6362\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684 zookeeper-3.5.1.jar SelectHiveQL \u8bfb\u53d6Hive\u8868 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668SelectHiveQL\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: HiveConnectionPool 2: select * from default.student 3. CSV \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u8fd0\u884c\u524d\u767b\u5f55\u96c6\u7fa4\u67e5\u770bhive\u8868student: \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/HIVE \u67e5\u770b\u7ed3\u679c\uff1a PutHiveQL \u6574\u8868\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2: iris.txt \u6570\u636e\u6587\u4ef6 iris.txt \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5904\u7406\u5668PutHDFS\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /opt/nifi/nifi-1.9.2/conf/hdfs-site.xml,/opt/nifi/nifi-1.9.2/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c: NiFi\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e HBase_1_1_2_ClientService \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHBase\u7684\u914d\u7f6e\u6587\u4ef6 hbase-site.xml \u5bfc\u5165\u8def\u5f84 /opt/nifi/nifi-1.9.2/conf \u66f4\u6362\u8def\u5f84 /opt/nifi/nifi-1.9.2/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.9.2.nar-unpacked/NAR-INF/bundled-dependencies \u4e0b\u9762\u7684 zookeeper-3.4.6.jar \u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u81ea\u5e26\u7684 zookeeper-3.5.1.jar \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HBase_1_1_2_ClientService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /opt/nifi/nifi-1.9.2/conf/hbase-site.xml,/opt/nifi/nifi-1.9.2/conf/core-site.xml 2\uff1a KeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HBase_1_1_2_ClientService \u5b8c\u6210 PutHBaseJSON \u5411HBase\u5bfc\u5165\u8868 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u6570\u636e\u6587\u4ef6 hbase_test.csv \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica \u5904\u7406\u5668InverAvroSchema\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data \u5904\u7406\u5668ConvertCSVToAvro\u914d\u7f6e\u5982\u4e0b: ${inferred.avro.schema} \u5904\u7406\u5668ConvertAvroToJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668SplitJson\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668PutHBaseJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data \u6d4b\u8bd5\u524d\u9700\u8981\u5c06\u6570\u636e\u6587\u4ef6 hbase_test.csv \u5bfc\u5165\u8def\u5f84 /home/dataset/HBASE \u5e76\u4e14\u9700\u8981\u5728\u96c6\u7fa4\u91cc\u9762\u5efa\u4e00\u4e2ahbase\u8868\uff0c\u6267\u884c\u547d\u4ee4 hbase shell create 'HBase_test','data' \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u96c6\u7fa4\u67e5\u770b\u7ed3\u679c\uff1a GetHbase \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHBase\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u9a71\u52a8\u5668PutFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u540e \u767b\u5f55\u5230\u8def\u5f84 /home/dataset/GetHBase_test \u67e5\u770b\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e \u5df2\u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \uff08Livy\u53ef\u5b89\u88c5\u5728FI HD\u5ba2\u6237\u7aef\u4e3b\u673a\uff0c\u4e5f\u53ef\u4ee5\u5b89\u88c5\u5728\u5176\u4ed6\u4e3b\u673a\u4f46\u662f\u9700\u8981\u4fdd\u8bc1\u5b89\u88c5Livy\u4e3b\u673a\u80fd\u591f\u548cFI HD\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a\uff09 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5 \u914d\u7f6eLivySessionController\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: spark 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_PySpark \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: pysaprk 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_SparkR \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: sparkr 4\uff1aKeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u9009\u62e9 Service and referencing components \u751f\u6548\u5e76\u4fdd\u5b58 LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR \u5b8c\u6210 \u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code1.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code1.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a 1+2 \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code1 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController 2: ${code1} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code1.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy\uff1a \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c \u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code2.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code2.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code2 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController_PySpark 2: ${code2} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code2.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c \u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u679c\u4e0eSpark\uff0cPySpark\u6837\u4f8b\u4e0d\u5b8c\u5168\u4e00\u6837 \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code3.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt\u91cc\u7684\u4ee3\u7801\u5185\u5bb9 \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code3.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c NiFi\u8fde\u63a5Kafka\u666e\u901a\u6a21\u5f0f \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21005\u7aef\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e GetHTTP & PutKafka \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PutKafka\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi \u6d4b\u8bd5\u524d\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u521b\u5efaTopic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u67e5\u770b\u7ed3\u679c\uff1a cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 2: SASL_PLAINTEXT 3: wikipedia21005 6: Guarantee Replicated Delivery \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41: \u767b\u9646FI HD kafka\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a ConsumeKafka_0_11 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ConsumeKafka_0_11\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff1a \u7528eclipse\u6253\u5f00\u5ba2\u6237\u7aef\u81ea\u5e26\u7684kafka\u6837\u4f8b\u4ee3\u7801 kafka-examples \uff0c\u8c03\u8bd5\u4f7f\u5f97\u6837\u4f8b\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u8fd0\u884c NewProducer.java \u7531\u4e8eConsumeKafka\u662f\u5b9e\u65f6\u83b7\u53d6\u65e5\u5fd7\u4fe1\u606f\u7684\uff0c\u6240\u4ee5\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c NewProducer.java \u5f80Kafka\u4e0a\u4f20\u65e5\u5fd7\u6587\u4ef6\uff0c\u518d\u540c\u65f6\u5f00\u542fnifi\u7684\u9a71\u52a8\u5668ConsumeKafka_0_11\u8fdb\u884c\u8bfb\u53d6\u65e5\u5fd7\u7684\u6d4b\u8bd5 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/Kafka \u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Kafka\u5b89\u5168\u6a21\u5f0f \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21007\u7aef\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e nifi\u4e3b\u673aip: 172.16.2.121, FI HD\u4e09\u8282\u70b9ip: 172.16.4.121-123 \u8ba4\u8bc1\u76f8\u5173\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728nifi\u4e3b\u673a/opt/nifi/nifi-1.9.2/conf/jaas.conf\u6587\u4ef6\u4e0b\u65b0\u589e\u5185\u5bb9\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/developuser/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u4f7f\u7528\u547d\u4ee4bin/nifi.sh stop\u505c\u6b62nifi \u5728FI HD\u7684kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982/opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar \u5c06nifi\u4e3b\u673a\u4e0b /opt/nifi/nifi-1.9.2/work/nar/extensions/nifi-kafka-1-0-nar-1.9.2.nar-unpacked/NAR-INF/bundled-dependencies \u8def\u5f84\u4e2d\u539f\u6765\u7684kafka client jar\u5305kafka-clients-1.0.2.jar \u4f7f\u7528\u91cd\u547d\u540d\u547d\u4ee4\u547d\u540d\u4e3a kafka-clients-1.0.2.jar.org \u5e76\u4e14\u628a\u4e0a\u4e00\u6b65\u5728 FI HD kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u7684kafka-clients-1.1.0.jar\u590d\u5236\u5230\u6b64\u8def\u5f84\u4e0b\uff1a \u767b\u9646nifi\u4e3b\u673a\uff0c\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 /opt/nifi/nifi-1.9.2/conf/bootstrap.conf \u6dfb\u52a0\u65b0\u7684jvm\u53c2\u6570\uff0c\u4fdd\u5b58\uff1a \u4f7f\u7528\u547d\u4ee4bin/nifi.sh start\u542f\u52a8nifi: PublishKafka_1_0\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/KAFKA 2: test.txt test.txt\u6587\u4ef6\u5185\u5bb9 id,name,class_id 1,\"Tom\",1 2,\"Sandy\",2 3,\"Benny\",3 4,\"Tina\",1 2,\"Sandy\",2 3,\"Benny\",3 4,\"Tina\",1 \u9a71\u52a8\u5668PublishKafka_1_0\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 2: SASL_PLAINTEXT 3: kafka 4: KeytabCredentialsService 5: testtopic_01 6: Guarantee Replicated Delivery \u8fd0\u884c\u6574\u4e2a\u5de5\u4f5c\u6d41\uff1a \u53bb\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\u68c0\u67e5\uff1a ConsumeKafka_1_0\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668ConsumeKafka_1_0\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 2: SASL_PLAINTEXT 3: kafka 4: KeytabCredentialsService 5: testtopic01 6: Demo \u9a71\u52a8\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41 \u4f7f\u7528FI HD producer\u4f20\u6570\u636e\uff1a \u767b\u9646druid\u4e3b\u673a\u68c0\u67e5\u7ed3\u679c FAQ \u00b6 \u95ee\u9898\uff1a \u8fdehive\u7684\u65f6\u5019\u9047\u5230\u95ee\u9898\uff1a 2020-07-06 16:09:39,819 ERROR [Timer-Driven Process Thread-3] o.a.nifi.dbcp.hive.HiveConnectionPool HiveConnectionPool[id=13ec476a-0173-1000-e692-3b30a2f32700] Error getting Hive connection: org.apache.commons.dbcp.SQLNestedException: Cannot create JDBC driver of class 'org.apache.hive.jdbc.HiveDriver' for connect URL ' jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM' org.apache.commons.dbcp.SQLNestedException: Cannot create JDBC driver of class 'org.apache.hive.jdbc.HiveDriver' for connect URL ' jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM' at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1452) at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1371) at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044) at org.apache.nifi.dbcp.hive.HiveConnectionPool.lambda$getConnection$0(HiveConnectionPool.java:369) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656) at org.apache.nifi.dbcp.hive.HiveConnectionPool.getConnection(HiveConnectionPool.java:369) at org.apache.nifi.dbcp.DBCPService.getConnection(DBCPService.java:49) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.nifi.controller.service.StandardControllerServiceInvocationHandler.invoke(StandardControllerServiceInvocationHandler.java:87) at com.sun.proxy.$Proxy153.getConnection(Unknown Source) at org.apache.nifi.processors.hive.SelectHiveQL.onTrigger(SelectHiveQL.java:339) at org.apache.nifi.processors.hive.SelectHiveQL.lambda$onTrigger$0(SelectHiveQL.java:285) at org.apache.nifi.processor.util.pattern.PartialFunctions.onTrigger(PartialFunctions.java:114) at org.apache.nifi.processor.util.pattern.PartialFunctions.onTrigger(PartialFunctions.java:106) at org.apache.nifi.processors.hive.SelectHiveQL.onTrigger(SelectHiveQL.java:285) at org.apache.nifi.controller.StandardProcessorNode.onTrigger(StandardProcessorNode.java:1162) at org.apache.nifi.controller.tasks.ConnectableTask.invoke(ConnectableTask.java:209) at org.apache.nifi.controller.scheduling.TimerDrivenSchedulingAgent$1.run(TimerDrivenSchedulingAgent.java:117) at org.apache.nifi.engine.FlowEngine$2.run(FlowEngine.java:110) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1437) ... 30 common frames omitted \u4ed4\u7ec6\u67e5\u770b\u62a5\u9519 \u53d1\u73b0\u8fde\u63a5\u7684url\u524d\u9762\u6709\u4e00\u4e2a\u7a7a\u683c\uff0c\u5728\u754c\u9762\u4e0a\u8fde\u63a5\u4e32\u524d\u9762\u591a\u4e86\u4e2a\u7a7a\u683c\uff0c\u53bb\u6389\u95ee\u9898\u89e3\u51b3","title":"1.12.0 <--> 8.0"},{"location":"Data_Integration/ApacheNifi-1.9.2/#apache-nififusioninsight","text":"","title":"Apache NiFi\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_1","text":"Apache NiFi 1.9.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive/Spark/Kafka) Apache NiFi 1.12.0 \u2194 FusionInsight MRS 8.0 (HDFS/HBase/Hive/Spark/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#mrs-80","text":"\u8bf4\u660e\uff1a\u5bf9\u63a5FusionInsight MRS 8.0\u9700\u66ff\u6362jar\u5305\u5982\u4e0b hive\uff1a \u66ff\u6362\u8def\u5f84\uff1a/opt/nifi/nifi-1.12.0/work/nar/extensions/nifi-hive-nar-1.12.0.nar-unpacked/NAR-INF/bundled-dependencies \u9700\u8981\u628azookeeper-3.5.6-hw-ei-302002.jar\uff0czookeeper-jute-3.5.6-hw-ei-302002.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\uff0c\u628a\u539f\u6765\u7684zookeeper-3.4.6.jar\u6ce8\u91ca\u6389 hbase: \u66ff\u6362\u8def\u5f84\uff1a/opt/nifi/nifi-1.12.0/work/nar/extensions/nifi-hbase_2-client-service-nar-1.12.0.nar-unpacked/NAR-INF/bundled-dependencies \u9700\u8981\u628azookeeper-3.5.6-hw-ei-302002.jar\uff0czookeeper-jute-3.5.6-hw-ei-302002.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\uff0c\u628a\u539f\u6765\u7684zookeeper-3.4.10.jar\u6ce8\u91ca\u6389 \u914d\u7f6ehbase\u9700\u8981\u9009\u7528HBase_2_ClientService kafka\uff1a \u66ff\u6362\u8def\u5f84:/opt/nifi/nifi-1.12.0/work/nar/extensions/nifi-kafka-2-0-nar-1.12.0.nar-unpacked/NAR-INF/bundled-dependencies \u9700\u8981\u628akafka-clients-2.4.0-hw-ei-302002.jar\u5bfc\u5165\u5230\u8be5\u8def\u5f84\u4e0b\uff0c\u5e76\u628a\u539f\u6765\u7684kafka-clients-2.0.0.jar\u6ce8\u91ca\u6389 \u9700\u8981\u628azookeeper-3.5.6-hw-ei-302002.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84","title":"MRS 8.0 \u5bf9\u63a5\u8bf4\u660e"},{"location":"Data_Integration/ApacheNifi-1.9.2/#apache-nifi","text":"\u73af\u5883\uff1a172.16.2.121","title":"\u5b89\u88c5Apache NiFi"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_2","text":"\u5b89\u88c5Apache NiFi 1.9.2","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_4","text":"\u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u5b89\u88c5NiFi\uff0c\u5728\u7f51\u5740 https://nifi.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u547d\u4ee4 unzip nifi-1.9.2-bin.zip \u89e3\u538b\u5b89\u88c5\u751f\u6210nifi-1.9.2\u76ee\u5f55\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/nifi/nifi-1.9.2 \u6267\u884c vi /opt/nifi/nifi-1.9.2/conf/nifi.properties \u914d\u7f6eNiFi\u670d\u52a1\u5668ip\u548c\u7aef\u53e3\u5982\u4e0b\uff1a nifi.web.http.host=172.16.2.121 nifi.web.http.port=8085 \u542f\u52a8\u548c\u505c\u6b62NiFi cd /opt/nifi/nifi-1.9.2 bin/nifi.sh start bin/nifi.sh stop \u8fd0\u884cNiFi bin/nifi.sh start","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#nifikerberos","text":"","title":"NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_5","text":"NiFi\u914d\u7f6e\u5e76\u4fdd\u5b58Kerberos\u8ba4\u8bc1\u4fe1\u606f\uff0c\u4f9b\u4ee5\u540e\u4f7f\u7528","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Nifi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u5e76\u521b\u5efa\u6d4b\u8bd5\u7528\u6237developuser (\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1)","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_7","text":"\u5728FusionInsight HD Manager\u4e0a\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6 user.keytab \uff0c krb5.conf \uff0c\u5e76\u4e00\u8d77\u5b58\u5165\u8def\u5f84 /opt/developuser \u6267\u884c\u547d\u4ee4 vi /opt/nifi/nifi-1.9.2/conf/nifi.properties \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5177\u4f53\u914d\u7f6e\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab \u91cd\u542fNiFi \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 KeytabCredentialsService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58KeytabCredentialsService \u5b8c\u6210","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#nifihdfs","text":"","title":"NiFi\u8fde\u63a5HDFS"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_8","text":"NiFi\u4e2d\u914d\u7f6eHDFS\u76f8\u5173\u5904\u7406\u5668\uff0c\u5bf9\u63a5HDFS","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_9","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/ApacheNifi-1.9.2/#puthdfs","text":"\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHDFS\u7684\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \uff0c core-site.xml \u5bfc\u5165\u8def\u5f84 /opt/nifi/nifi-1.9.2/conf \u4fee\u6539 hdfs-site.xml \u5185\u5bb9\uff0c\u5220\u9664\u5982\u4e0b\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider</value> </property> \u4fee\u6539 core-site.xml \u5185\u5bb9\uff0c\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\u9879\u4e2dhacluster\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip\u52a0\u7aef\u53e3\u53f7 <property> <name>fs.defaultFS</name> <value>hdfs://172.16.4.123:25000</value> </property> \u5c06FI\u5ba2\u6237\u7aef\u7684 hadoop-plugins-1.0.jar \u62f7\u8d1d\u5230nifi hadoop\u76f8\u5173\u4f9d\u8d56\u8def\u5f84 cp /opt/125_651hdclient/hadoopclient/HDFS/hadoop/share/hadoop/common/lib/hadoop-plugins-1.0.jar /opt/nifi/nifi-1.9.2/work/nar/extensions/nifi-hadoop-nar-1.9.2.nar-unpacked/NAR-INF/bundled-dependencies \u8bf4\u660e\uff1a\u5426\u5219\u4f1a\u62a5\u4f9d\u8d56\u9519\u8bef \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset - \u5904\u7406\u5668PutHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /opt/nifi/nifi-1.9.2/conf/hdfs-site.xml,/opt/nifi/nifi-1.9.2/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest \u4e24\u4e2a\u5904\u7406\u5668\u7684\u8fde\u63a5\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5165\u8def\u5f84 /home/dataset \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u6d4b\u8bd5\u540e \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv","title":"PutHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#gethdfs","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /opt/nifi/nifi-1.9.2/conf/hdfs-site.xml,/opt/nifi/nifi-1.9.2/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest/HDFS - \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset/HDFS \u6d4b\u8bd5\u540e \u767b\u5f55\u5b89\u88c5 FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS \u67e5\u770b\u7ed3\u679c","title":"GetHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#nifihive","text":"","title":"NiFi\u8fde\u63a5Hive"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_10","text":"NiFi\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_11","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/ApacheNifi-1.9.2/#hiveconnectionpool","text":"\u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HiveConnectionPool \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u4e3a 1: jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService - \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HiveConnectionPool \u5b8c\u6210 \u5728\u8def\u5f84 /opt/nifi/nifi-1.9.2/conf \u4e0b\u521b\u5efa jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6267\u884c\u547d\u4ee4 vi /opt/nifi/nifi-1.9.2/conf/bootstrap.conf \u914d\u7f6e bootstrap.conf \u6587\u4ef6\u5982\u4e0b: java.arg.17=-Djava.security.auth.login.config=/opt/nifi/nifi-1.9.2/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=false \u6267\u884c\u547d\u4ee4 vi /opt/nifi/nifi-1.9.2/conf/nifi.properties \u914d\u7f6e nifi.properties \u6587\u4ef6\u5982\u4e0b\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true \u6267\u884c\u547d\u4ee4 cd /opt/nifi/nifi-1.9.2/work/nar/extensions/nifi-hive-nar-1.9.2.nar-unpacked/NAR-INF/bundled-dependencies \u5230NiFi Hive\u7c7b\u5e93\u4e2d\uff0c\u5c06\u539f\u6709\u7684 zookeeper-3.4.6.jar \u66ff\u6362\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684 zookeeper-3.5.1.jar","title":"HiveConnectionPool \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#selecthiveql-hive","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668SelectHiveQL\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: HiveConnectionPool 2: select * from default.student 3. CSV \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u8fd0\u884c\u524d\u767b\u5f55\u96c6\u7fa4\u67e5\u770bhive\u8868student: \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/HIVE \u67e5\u770b\u7ed3\u679c\uff1a","title":"SelectHiveQL \u8bfb\u53d6Hive\u8868 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#puthiveql","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2: iris.txt \u6570\u636e\u6587\u4ef6 iris.txt \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5904\u7406\u5668PutHDFS\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /opt/nifi/nifi-1.9.2/conf/hdfs-site.xml,/opt/nifi/nifi-1.9.2/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c:","title":"PutHiveQL \u6574\u8868\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#nifihbase","text":"","title":"NiFi\u8fde\u63a5HBase"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_12","text":"NiFi\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_13","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/ApacheNifi-1.9.2/#hbase_1_1_2_clientservice","text":"\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHBase\u7684\u914d\u7f6e\u6587\u4ef6 hbase-site.xml \u5bfc\u5165\u8def\u5f84 /opt/nifi/nifi-1.9.2/conf \u66f4\u6362\u8def\u5f84 /opt/nifi/nifi-1.9.2/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.9.2.nar-unpacked/NAR-INF/bundled-dependencies \u4e0b\u9762\u7684 zookeeper-3.4.6.jar \u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u81ea\u5e26\u7684 zookeeper-3.5.1.jar \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HBase_1_1_2_ClientService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /opt/nifi/nifi-1.9.2/conf/hbase-site.xml,/opt/nifi/nifi-1.9.2/conf/core-site.xml 2\uff1a KeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HBase_1_1_2_ClientService \u5b8c\u6210","title":"HBase_1_1_2_ClientService \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#puthbasejson-hbase","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u6570\u636e\u6587\u4ef6 hbase_test.csv \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica \u5904\u7406\u5668InverAvroSchema\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data \u5904\u7406\u5668ConvertCSVToAvro\u914d\u7f6e\u5982\u4e0b: ${inferred.avro.schema} \u5904\u7406\u5668ConvertAvroToJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668SplitJson\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668PutHBaseJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data \u6d4b\u8bd5\u524d\u9700\u8981\u5c06\u6570\u636e\u6587\u4ef6 hbase_test.csv \u5bfc\u5165\u8def\u5f84 /home/dataset/HBASE \u5e76\u4e14\u9700\u8981\u5728\u96c6\u7fa4\u91cc\u9762\u5efa\u4e00\u4e2ahbase\u8868\uff0c\u6267\u884c\u547d\u4ee4 hbase shell create 'HBase_test','data' \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u96c6\u7fa4\u67e5\u770b\u7ed3\u679c\uff1a","title":"PutHBaseJSON \u5411HBase\u5bfc\u5165\u8868"},{"location":"Data_Integration/ApacheNifi-1.9.2/#gethbase","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHBase\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u9a71\u52a8\u5668PutFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u540e \u767b\u5f55\u5230\u8def\u5f84 /home/dataset/GetHBase_test \u67e5\u770b\u7ed3\u679c\uff1a","title":"GetHbase \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#nifispark","text":"","title":"NiFi\u8fde\u63a5Spark"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_14","text":"NiFi\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_15","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e \u5df2\u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \uff08Livy\u53ef\u5b89\u88c5\u5728FI HD\u5ba2\u6237\u7aef\u4e3b\u673a\uff0c\u4e5f\u53ef\u4ee5\u5b89\u88c5\u5728\u5176\u4ed6\u4e3b\u673a\u4f46\u662f\u9700\u8981\u4fdd\u8bc1\u5b89\u88c5Livy\u4e3b\u673a\u80fd\u591f\u548cFI HD\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a\uff09 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/ApacheNifi-1.9.2/#livysessioncontroller","text":"\u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: spark 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_PySpark \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: pysaprk 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_SparkR \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: sparkr 4\uff1aKeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u9009\u62e9 Service and referencing components \u751f\u6548\u5e76\u4fdd\u5b58 LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR \u5b8c\u6210","title":"\u914d\u7f6eLivySessionController\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#spark","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code1.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code1.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a 1+2 \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code1 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController 2: ${code1} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code1.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy\uff1a \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#pyspark","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code2.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code2.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code2 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController_PySpark 2: ${code2} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code2.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#sparkr","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u679c\u4e0eSpark\uff0cPySpark\u6837\u4f8b\u4e0d\u5b8c\u5168\u4e00\u6837 \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code3.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt\u91cc\u7684\u4ee3\u7801\u5185\u5bb9 \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code3.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#nifikafka","text":"","title":"NiFi\u8fde\u63a5Kafka\u666e\u901a\u6a21\u5f0f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_16","text":"NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21005\u7aef\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_17","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/ApacheNifi-1.9.2/#gethttp-putkafka","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PutKafka\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi \u6d4b\u8bd5\u524d\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u521b\u5efaTopic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u67e5\u770b\u7ed3\u679c\uff1a cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning","title":"GetHTTP &amp; PutKafka \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#publishkafka_0_11","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 2: SASL_PLAINTEXT 3: wikipedia21005 6: Guarantee Replicated Delivery \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41: \u767b\u9646FI HD kafka\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a","title":"PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#consumekafka_0_11","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ConsumeKafka_0_11\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff1a \u7528eclipse\u6253\u5f00\u5ba2\u6237\u7aef\u81ea\u5e26\u7684kafka\u6837\u4f8b\u4ee3\u7801 kafka-examples \uff0c\u8c03\u8bd5\u4f7f\u5f97\u6837\u4f8b\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u8fd0\u884c NewProducer.java \u7531\u4e8eConsumeKafka\u662f\u5b9e\u65f6\u83b7\u53d6\u65e5\u5fd7\u4fe1\u606f\u7684\uff0c\u6240\u4ee5\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c NewProducer.java \u5f80Kafka\u4e0a\u4f20\u65e5\u5fd7\u6587\u4ef6\uff0c\u518d\u540c\u65f6\u5f00\u542fnifi\u7684\u9a71\u52a8\u5668ConsumeKafka_0_11\u8fdb\u884c\u8bfb\u53d6\u65e5\u5fd7\u7684\u6d4b\u8bd5 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/Kafka \u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"ConsumeKafka_0_11 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#nifikafka_1","text":"","title":"NiFi\u8fde\u63a5Kafka\u5b89\u5168\u6a21\u5f0f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_18","text":"NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21007\u7aef\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_19","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.9.2\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e nifi\u4e3b\u673aip: 172.16.2.121, FI HD\u4e09\u8282\u70b9ip: 172.16.4.121-123","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/ApacheNifi-1.9.2/#_20","text":"\u5728nifi\u4e3b\u673a/opt/nifi/nifi-1.9.2/conf/jaas.conf\u6587\u4ef6\u4e0b\u65b0\u589e\u5185\u5bb9\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/developuser/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u4f7f\u7528\u547d\u4ee4bin/nifi.sh stop\u505c\u6b62nifi \u5728FI HD\u7684kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982/opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar \u5c06nifi\u4e3b\u673a\u4e0b /opt/nifi/nifi-1.9.2/work/nar/extensions/nifi-kafka-1-0-nar-1.9.2.nar-unpacked/NAR-INF/bundled-dependencies \u8def\u5f84\u4e2d\u539f\u6765\u7684kafka client jar\u5305kafka-clients-1.0.2.jar \u4f7f\u7528\u91cd\u547d\u540d\u547d\u4ee4\u547d\u540d\u4e3a kafka-clients-1.0.2.jar.org \u5e76\u4e14\u628a\u4e0a\u4e00\u6b65\u5728 FI HD kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u7684kafka-clients-1.1.0.jar\u590d\u5236\u5230\u6b64\u8def\u5f84\u4e0b\uff1a \u767b\u9646nifi\u4e3b\u673a\uff0c\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 /opt/nifi/nifi-1.9.2/conf/bootstrap.conf \u6dfb\u52a0\u65b0\u7684jvm\u53c2\u6570\uff0c\u4fdd\u5b58\uff1a \u4f7f\u7528\u547d\u4ee4bin/nifi.sh start\u542f\u52a8nifi:","title":"\u8ba4\u8bc1\u76f8\u5173\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#publishkafka_1_0","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/KAFKA 2: test.txt test.txt\u6587\u4ef6\u5185\u5bb9 id,name,class_id 1,\"Tom\",1 2,\"Sandy\",2 3,\"Benny\",3 4,\"Tina\",1 2,\"Sandy\",2 3,\"Benny\",3 4,\"Tina\",1 \u9a71\u52a8\u5668PublishKafka_1_0\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 2: SASL_PLAINTEXT 3: kafka 4: KeytabCredentialsService 5: testtopic_01 6: Guarantee Replicated Delivery \u8fd0\u884c\u6574\u4e2a\u5de5\u4f5c\u6d41\uff1a \u53bb\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\u68c0\u67e5\uff1a","title":"PublishKafka_1_0\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#consumekafka_1_0","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668ConsumeKafka_1_0\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 2: SASL_PLAINTEXT 3: kafka 4: KeytabCredentialsService 5: testtopic01 6: Demo \u9a71\u52a8\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41 \u4f7f\u7528FI HD producer\u4f20\u6570\u636e\uff1a \u767b\u9646druid\u4e3b\u673a\u68c0\u67e5\u7ed3\u679c","title":"ConsumeKafka_1_0\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/ApacheNifi-1.9.2/#faq","text":"\u95ee\u9898\uff1a \u8fdehive\u7684\u65f6\u5019\u9047\u5230\u95ee\u9898\uff1a 2020-07-06 16:09:39,819 ERROR [Timer-Driven Process Thread-3] o.a.nifi.dbcp.hive.HiveConnectionPool HiveConnectionPool[id=13ec476a-0173-1000-e692-3b30a2f32700] Error getting Hive connection: org.apache.commons.dbcp.SQLNestedException: Cannot create JDBC driver of class 'org.apache.hive.jdbc.HiveDriver' for connect URL ' jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM' org.apache.commons.dbcp.SQLNestedException: Cannot create JDBC driver of class 'org.apache.hive.jdbc.HiveDriver' for connect URL ' jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM' at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1452) at org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1371) at org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044) at org.apache.nifi.dbcp.hive.HiveConnectionPool.lambda$getConnection$0(HiveConnectionPool.java:369) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656) at org.apache.nifi.dbcp.hive.HiveConnectionPool.getConnection(HiveConnectionPool.java:369) at org.apache.nifi.dbcp.DBCPService.getConnection(DBCPService.java:49) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.nifi.controller.service.StandardControllerServiceInvocationHandler.invoke(StandardControllerServiceInvocationHandler.java:87) at com.sun.proxy.$Proxy153.getConnection(Unknown Source) at org.apache.nifi.processors.hive.SelectHiveQL.onTrigger(SelectHiveQL.java:339) at org.apache.nifi.processors.hive.SelectHiveQL.lambda$onTrigger$0(SelectHiveQL.java:285) at org.apache.nifi.processor.util.pattern.PartialFunctions.onTrigger(PartialFunctions.java:114) at org.apache.nifi.processor.util.pattern.PartialFunctions.onTrigger(PartialFunctions.java:106) at org.apache.nifi.processors.hive.SelectHiveQL.onTrigger(SelectHiveQL.java:285) at org.apache.nifi.controller.StandardProcessorNode.onTrigger(StandardProcessorNode.java:1162) at org.apache.nifi.controller.tasks.ConnectableTask.invoke(ConnectableTask.java:209) at org.apache.nifi.controller.scheduling.TimerDrivenSchedulingAgent$1.run(TimerDrivenSchedulingAgent.java:117) at org.apache.nifi.engine.FlowEngine$2.run(FlowEngine.java:110) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.sql.SQLException: No suitable driver at java.sql.DriverManager.getDriver(DriverManager.java:315) at org.apache.commons.dbcp.BasicDataSource.createConnectionFactory(BasicDataSource.java:1437) ... 30 common frames omitted \u4ed4\u7ec6\u67e5\u770b\u62a5\u9519 \u53d1\u73b0\u8fde\u63a5\u7684url\u524d\u9762\u6709\u4e00\u4e2a\u7a7a\u683c\uff0c\u5728\u754c\u9762\u4e0a\u8fde\u63a5\u4e32\u524d\u9762\u591a\u4e86\u4e2a\u7a7a\u683c\uff0c\u53bb\u6389\u95ee\u9898\u89e3\u51b3","title":"FAQ"},{"location":"Data_Integration/Apache_NiFi/","text":"Apache NiFi\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive/Spark/Kafka/Solr) \u5b89\u88c5Apache NiFi \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Apache NiFi 1.7.1 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u5b89\u88c5NiFi\uff0c\u5728\u7f51\u5740 https://nifi.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u547d\u4ee4 unzip nifi-1.7.1-bin.zip \u89e3\u538b\u5b89\u88c5\u751f\u6210nifi-1.7.1\u76ee\u5f55\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a /usr/nifi/nifi-1.7.1 \u6267\u884c vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eNiFi\u670d\u52a1\u5668ip\u548c\u7aef\u53e3\u5982\u4e0b\uff1a nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 \u542f\u52a8\u548c\u505c\u6b62NiFi cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop \u8fd0\u884cNiFi bin/nifi.sh start NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u914d\u7f6e\u5e76\u4fdd\u5b58Kerberos\u8ba4\u8bc1\u4fe1\u606f\uff0c\u4f9b\u4ee5\u540e\u4f7f\u7528 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Nifi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u5e76\u521b\u5efa\u6d4b\u8bd5\u7528\u6237developuser (\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1) \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728FusionInsight HD Manager\u4e0a\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6 user.keytab \uff0c krb5.conf \uff0c\u5e76\u4e00\u8d77\u5b58\u5165\u8def\u5f84 /opt/developuser \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5177\u4f53\u914d\u7f6e\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 KeytabCredentialsService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58KeytabCredentialsService \u5b8c\u6210 NiFi\u8fde\u63a5HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eHDFS\u76f8\u5173\u5904\u7406\u5668\uff0c\u5bf9\u63a5HDFS \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e PutHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHDFS\u7684\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \uff0c core-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4fee\u6539 hdfs-site.xml \u5185\u5bb9\uff0c\u5220\u9664\u5982\u4e0b\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u4fee\u6539 core-site.xml \u5185\u5bb9\uff0c\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\u9879\u4e2dhacluster\u6539\u4e3a\u8282\u70b9ip\u52a0\u7aef\u53e3\u53f7 <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset \u5904\u7406\u5668PutHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest \u4e24\u4e2a\u5904\u7406\u5668\u7684\u8fde\u63a5\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5165\u8def\u5f84 /home/dataset \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u6d4b\u8bd5\u540e \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv GetHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest/HDFS - \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset/HDFS \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/nifitest/HDFS \u8def\u5f84\u4e0b \u6d4b\u8bd5\u540e \u767b\u5f55\u5b89\u88c5 FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS \u67e5\u770b\u7ed3\u679c ListHDFS & FetchHDFS \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ListHDFS\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest \u5904\u7406\u5668RouteOnAttribute\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u56fe\u6807\u589e\u52a0\u4e00\u6761\u914d\u7f6e\uff0c Property \u914d\u7f6e\u4e3a requiredfilenames \uff0c Value \u914d\u7f6e\u4e3a ${filename:matches('sanguo.*')} \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} - \u5904\u7406\u5668RouteOnAttribute\u548c\u4e0a\u3001\u4e0b\u5904\u7406\u5668FetchHDFS\u7684\u8fde\u63a5\u914d\u7f6e\u5206\u522b\u5bf9\u5e94\u4e3a requiredfilenames \u548c unmatched \uff0c\u5982\u56fe\uff1a \u4e24\u4e2a\u5904\u7406\u5668FetchHDFS\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService \u4e0a\u3001\u4e0b\u5904\u7406\u5668PutFile\u914d\u7f6e\u5206\u522b\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff0c\u6267\u884c\u547d\u4ee4 hdfs dfs -ls /tmp/nifitest \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf /tmp/nifitest \u67e5\u770b\u6587\u4ef6 \u6d4b\u8bd5\u540e \u767b\u5f55FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS/matchedFiles \u548c /home/dataset/HDFS/unmatchedFiles \u5206\u522b\u67e5\u770b\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e HiveConnectionPool \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HiveConnectionPool \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u4e3a 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService - \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HiveConnectionPool \u5b8c\u6210 \u5728\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4e0b\u521b\u5efa jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf \u914d\u7f6e bootstrap.conf \u6587\u4ef6\u5982\u4e0b: java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6e nifi.properties \u6587\u4ef6\u5982\u4e0b\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true - \u6267\u884c\u547d\u4ee4 cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u5230NiFi Hive\u7c7b\u5e93\u4e2d\uff0c\u5c06\u539f\u6709\u7684 zookeeper-3.4.6.jar \u66ff\u6362\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684 zookeeper-3.5.1.jar SelectHiveQL \u8bfb\u53d6Hive\u8868 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668SelectHiveQL\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u8fd0\u884c\u524d\u767b\u5f55\u96c6\u7fa4\u67e5\u770bhive\u8868t2: \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/HIVE \u67e5\u770b\u7ed3\u679c\uff1a PutHiveQL \u6574\u8868\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2: iris.txt \u6570\u636e\u6587\u4ef6 iris.txt \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5904\u7406\u5668PutHDFS\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c: PutHiveQL \u5355\u884c\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt \u6570\u636e\u6587\u4ef6 iris_add.txt \u7684\u5185\u5bb9\u5982\u4e0b: \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" \u5904\u7406\u5668SplitText\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668ExtractText\u914d\u7f6e\u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b: \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris_add.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a NiFi\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e HBase_1_1_2_ClientService \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHBase\u7684\u914d\u7f6e\u6587\u4ef6 hbase-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u66f4\u6362\u8def\u5f84 /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u4e0b\u9762\u7684 zookeeper-3.4.6.jar \u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u81ea\u5e26\u7684 zookeeper-3.5.1.jar \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HBase_1_1_2_ClientService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HBase_1_1_2_ClientService \u5b8c\u6210 PutHBaseJSON \u5411HBase\u5bfc\u5165\u8868 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u6570\u636e\u6587\u4ef6 hbase_test.csv \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica \u5904\u7406\u5668InverAvroSchema\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data \u5904\u7406\u5668ConvertCSVToAvro\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668ConvertAvroToJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668SplitJson\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668PutHBaseJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data \u6d4b\u8bd5\u524d\u9700\u8981\u5c06\u6570\u636e\u6587\u4ef6 hbase_test.csv \u5bfc\u5165\u8def\u5f84 /home/dataset/HBASE \u5e76\u4e14\u9700\u8981\u5728\u96c6\u7fa4\u91cc\u9762\u5efa\u4e00\u4e2ahbase\u8868\uff0c\u6267\u884c\u547d\u4ee4 hbase shell create 'HBase_test','data' \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u96c6\u7fa4\u67e5\u770b\u7ed3\u679c\uff1a GetHbase \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHBase\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u9a71\u52a8\u5668PutFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u540e \u767b\u5f55\u5230\u8def\u5f84 /home/dataset/GetHBase_test \u67e5\u770b\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e \u5df2\u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \uff08Livy\u53ef\u5b89\u88c5\u5728FI HD\u5ba2\u6237\u7aef\u4e3b\u673a\uff0c\u4e5f\u53ef\u4ee5\u5b89\u88c5\u5728\u5176\u4ed6\u4e3b\u673a\u4f46\u662f\u9700\u8981\u4fdd\u8bc1\u5b89\u88c5Livy\u4e3b\u673a\u80fd\u591f\u548cFI HD\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a\uff09 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5 \u914d\u7f6eLivySessionController\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: spark 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_PySpark \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: pysaprk 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_SparkR \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: sparkr 4\uff1aKeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u9009\u62e9 Service and referencing components \u751f\u6548\u5e76\u4fdd\u5b58 LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR \u5b8c\u6210 \u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code1.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code1.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a 1+2 \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code1 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController 2: ${code1} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code1.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy\uff1a \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c \u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code2.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code2.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code2 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController_PySpark 2: ${code2} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code2.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c \u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u679c\u4e0eSpark\uff0cPySpark\u6837\u4f8b\u4e0d\u5b8c\u5168\u4e00\u6837 \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code3.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt\u91cc\u7684\u4ee3\u7801\u5185\u5bb9 \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code3.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c NiFi\u8fde\u63a5Kafka\u666e\u901a\u6a21\u5f0f \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21005\u7aef\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e GetHTTP & PutKafka \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PutKafka\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi \u6d4b\u8bd5\u524d\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u521b\u5efaTopic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u67e5\u770b\u7ed3\u679c\uff1a cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 2: SASL_PLAINTEXT 3: wikipedia21005 6: Guarantee Replicated Delivery \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41: \u767b\u9646FI HD kafka\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a ConsumeKafka_0_11 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ConsumeKafka_0_11\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff1a \u7528eclipse\u6253\u5f00\u5ba2\u6237\u7aef\u81ea\u5e26\u7684kafka\u6837\u4f8b\u4ee3\u7801 kafka-examples \uff0c\u8c03\u8bd5\u4f7f\u5f97\u6837\u4f8b\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u8fd0\u884c NewProducer.java \u7531\u4e8eConsumeKafka\u662f\u5b9e\u65f6\u83b7\u53d6\u65e5\u5fd7\u4fe1\u606f\u7684\uff0c\u6240\u4ee5\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c NewProducer.java \u5f80Kafka\u4e0a\u4f20\u65e5\u5fd7\u6587\u4ef6\uff0c\u518d\u540c\u65f6\u5f00\u542fnifi\u7684\u9a71\u52a8\u5668ConsumeKafka_0_11\u8fdb\u884c\u8bfb\u53d6\u65e5\u5fd7\u7684\u6d4b\u8bd5 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/Kafka \u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a NiFi\u8fde\u63a5Kafka\u5b89\u5168\u6a21\u5f0f \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21007\u7aef\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e nifi\u4e3b\u673aip: 172.16.2.119, FI HD\u4e09\u8282\u70b9ip: 172.16.6.10-12 \u8ba4\u8bc1\u76f8\u5173\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728nifi\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u4e3a\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u4f7f\u7528\u547d\u4ee4bin/nifi.sh stop\u505c\u6b62nifi \u5728FI HD\u7684kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982/opt/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar \u5c06nifi\u4e3b\u673a\u4e0b /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-kafka-0-11-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u8def\u5f84\u4e2d\u539f\u6765\u7684kafka client jar\u5305kafka-clients-0.11.0.1.jar \u4f7f\u7528\u91cd\u547d\u540d\u547d\u4ee4\u547d\u540d\u4e3a kafka-clients-0.11.0.1.jar.org \u5e76\u4e14\u628a\u4e0a\u4e00\u6b65\u5728 FI HD kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u7684kafka-clients-1.1.0.jar\u590d\u5236\u5230\u6b64\u8def\u5f84\u4e0b\u3002 \u767b\u9646nifi\u4e3b\u673a\uff0c\u5148\u4f7f\u7528 source /opt/hadoopclient/bigdata_env \u52a0\u8f7d\u8fd0\u884c\u7684\u73af\u5883\u53d8\u91cf\uff0c\u7136\u540e\u518d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7djava\u8fd0\u884c\u7684jvm\u53c2\u6570: export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf\" \u5176\u4e2d/etc/krb5.conf\u4e3a\u5bf9\u5e94\u5bf9\u63a5\u96c6\u7fa4\u7684\u8ba4\u8bc1krb5.conf\u6587\u4ef6 \u5b8c\u6210\u4e0a\u8ff0\u6b65\u9aa4\u540e\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5jvm\u53c2\u6570\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u547d\u4ee4bin/nifi.sh start\u542f\u52a8nifi: PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 2: SASL_PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: testtopic_01 6: Guarantee Replicated Delivery \u8fd0\u884c\u6574\u4e2a\u5de5\u4f5c\u6d41\uff1a \u53bb\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\u68c0\u67e5\uff1a ConsumeKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668ConsumeKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 2: SASL_PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: testtopic_01 6: Demo \u9a71\u52a8\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41 \u4f7f\u7528FI HD\u6837\u4f8b\u4ee3\u7801\u4f7f\u7528producer\u4f20\u6570\u636e\uff1a \u767b\u9646nifi\u4e3b\u673a/opt/nifikafka21007\u8def\u5f84\u68c0\u67e5\u7ed3\u679c NiFi\u5bf9\u63a5Solr\u5b89\u5168\u6a21\u5f0f \u00b6 \u73af\u5883\u8bf4\u660e \u00b6 FI HD: 172.16.4.121-123 NIFI: 172.17.2.124 FI HD\u76f8\u5173\u914d\u7f6e: \u00b6 \u53c2\u8003\u4ea7\u54c1\u6587\u6863solr\u90e8\u5206\uff0c\u505a\u5982\u4e0b\u914d\u7f6e\uff08\u53ef\u4e0d\u505a\uff09\uff1a \u4f7f\u7528\u5982\u4e0bcurl\u547d\u4ee4\u5728\u5bf9\u63a5\u96c6\u7fa4solr\u521b\u5efa\u4e00\u4e2acollection\uff0c\u540d\u5b57\u53eb\u505anifi_test curl --negotiate -k -v -u : \"https://172.16.4.122:21101/solr/admin/collections?action=CREATE&name=nifi_test&collection.configName=confWithSchema&numShards=3&replicationFactor=1\" \u767b\u9646\u5ba2\u6237\u7aef\uff0c\u6309\u7167\u5982\u4e0b\u622a\u56fe\u65b9\u5f0f\u767b\u9646kadmin\uff0c\u589e\u52a03\u4e2a\u8282\u70b9\u7684HTTP\u670d\u52a1principal \u6ce8\u610f\uff1a\u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801 \u767b\u9646oracle\u5b98\u7f51\u83b7\u53d6jce\uff0c\u5e76\u9002\u914d\u5230\u5bf9\u63a5FI HD\u96c6\u7fa4\u4e2d \u7531\u4e8ekerberos\u6821\u9a8c\u548c\u52a0\u89e3\u5bc6\u7528\u5230\u5bc6\u94a5\u957f\u5ea6\u8fdc\u8d85\u51fajre\u9ed8\u8ba4\u7684\u5b89\u5168\u5b57\u7b26\u957f\u5ea6\uff0c\u6240\u4ee5\u9700\u8981\u5230java\u5b98\u7f51\u4e0a\u4e0b\u8f7dJava Cryptography Extension (JCE)\uff1a https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html \uff0c\u7136\u540e\u89e3\u538b\u5230 %JAVA_HOME%/jre/lib/security \u4e2d\u66ff\u6362\u76f8\u5e94\u7684\u6587\u4ef6\u3002 \u5177\u4f53\u5c06\u4e0b\u8f7d\u597djce\u89e3\u538b\uff0c\u5e76\u5c06\u89e3\u538b\u540e\u7684\u4e24\u4e2ajar\u5305 US_export_policy.jar \uff0c local_policy.jar \u62f7\u8d1d\u5230\u4e09\u53f0\u670d\u52a1\u5668172.16.4.121,122,123\u7684 /opt/huawei/Bigdata/common/runtime0/jdk-8u201/jre/lib/security/ \u8def\u5f84\u4e0b,\u5e76\u4e14\u91cd\u542fsolr\u670d\u52a1 NiFi kerberos\u76f8\u5173\u914d\u7f6e \u00b6 nifi.properties\u914d\u7f6e\u6587\u4ef6\uff1a web properties\u90e8\u5206\u505a\u5982\u4e0b\u66f4\u6539\uff1a kerberos\u90e8\u5206\uff1a \u589e\u52a0sasl\u914d\u7f6e\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true bootstrap.conf\u914d\u7f6e\u6587\u4ef6\uff1a \u589e\u52a0\u4e00\u4e2ajvm\u53c2\u6570\uff1a java.arg.17=-Djava.security.auth.login.config=/opt/jaas.conf \u56e0\u4e3a\u5bf9\u63a5solr\uff0c\u6240\u4ee5\u627e\u5230nifi\u5173\u4e8esolr\u7684\u4f9d\u8d56\u5305\u8def\u5f84\uff0c\u4ee5\u672c\u673a\u4e3a\u4f8b\uff1a /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-solr-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u5c06\u539f\u6709\u7684zookeeper-3.4.6.jar\u91cd\u547d\u540dzookeeper-3.4.6.jar.org\uff0c\u518d\u5c06FI HD\u7684\u5339\u914dzookeeper-3.5.1.jar\u62f7\u8d1d\u8fc7\u6765 SSL\u8bc1\u4e66\u76f8\u5173\u914d\u7f6e \u00b6 \u8bf4\u660e\uff1a\u56e0\u4e3a\u5bf9\u63a5solr\u90e8\u7f72\u5728\u5b89\u5168\u6a21\u5f0f\u4e0a\uff0c\u4f7f\u7528rest\u63a5\u53e3\u8fdb\u884c\u4ea4\u4e92\u7684\u65f6\u5019\u662f\u8981\u901a\u8fc7ssl\u5c42\u7684\u8ba4\u8bc1\uff0c\u9700\u8981\u521b\u5efa\u5bf9\u5e94\u7684\u8bc1\u4e66\uff08truststore\uff09\uff0c\u5b8c\u6210\u540e\u8fd8\u8981\u4f7f\u7528spnego\u540c\u96c6\u7fa4solr\u8fdb\u884c\u4ea4\u4e92\u3002\u4e0b\u9762\u5c06\u4ecb\u7ecd\u4e24\u79cd\u8ba4\u8bc1\u8bc1\u4e66\u83b7\u53d6\u7684\u65b9\u5f0f\uff0c\u5206\u522b\u5bf9\u5e94solr\u4e24\u79cd\u5bf9\u63a5\u65b9\u6cd5CLOUD\u548cHTTPS huawei-huawei\u8bc1\u4e66 \u767b\u9646linux\u540e\u53f0\uff08\u9700\u8981\u5b89\u88c5openssl\uff09\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4 openssl s_client -host 172.16.4.122 -port 21101 -prexit -showcerts\\ \u4f1a\u6709\u4e09\u6bb5\u8bc1\u4e66\uff0c\u5206\u522b\u4e3ahuawei-huawei, huawei-FusionInsight, FusionInsight-172.16.4.122 \u5c06huawei-huawei\u90e8\u5206\u5185\u5bb9\u590d\u5236\uff0c\u62f7\u8d1d\u5230\u4e00\u4e2a\u6587\u4ef6 /opt/ssltest/huawei-huawei.pem \u6587\u4ef6\u4e2d \u4f7f\u7528\u547d\u4ee4 keytool -import -alias gca -file /opt/ssltest/huawei-huawei.pem -keystore /opt/ssltest/truststore \u5c06\u4e0a\u4e00\u6b65\u751f\u6210\u7684 huawei-huawei.pem \u8bc1\u4e66\u5185\u5bb9\u52a0\u5165 /opt/ssltest/truststore \u6587\u4ef6\u4e2d\uff0c\u8fc7\u7a0b\u4e2d\u8f93\u5165\u7684\u5bc6\u7801\u4e3a changeit \uff0c\u5b8c\u6210\u540e\u8f93\u5165yes \u8bc1\u4e66\u94fe\uff0c\u56e0\u4e3a\u7531\u4e0a\u5df2\u7ecf\u77e5\u9053\u4e86\u4f1a\u751f\u6210\u4e00\u4e2a\u8bc1\u4e66\u94fe\uff0c\u5305\u542b\u4e09\u6bb5\uff0c\u6b64\u6b65\u9aa4\u5c31\u662f\u5c06\u6574\u4e2a\u8bc1\u4e66\u94fe\u751f\u6210\u4e00\u4e2a\u65b0\u7684truststore_huawei\u6587\u4ef6 \u4f7f\u7528\u547d\u4ee4 echo \"\" | openssl s_client -host 172.16.4.122 -port 21101 -showcerts | awk '/BEGIN CERT/ {p=1} ; p==1; /END CERT/ {p=0}' > /opt/ssltest/allcerts122.pem \u5c06\u6574\u4e2a\u8bc1\u4e66\u94fe\u91cd\u5b9a\u5411\u5230 /opt/ssltest/allcerts122.pem \u6587\u4ef6\u4e2d \u4f7f\u7528\u547d\u4ee4 keytool -import -alias gca -file /opt/ssltest/allcerts122.pem -keystore /opt/ssltest/truststore_chain \u5c06\u4e0a\u4e00\u6b65\u751f\u6210\u7684 allcerts122.pem \u8bc1\u4e66\u5185\u5bb9\u52a0\u5165 /opt/ssltest/truststore_chain \u6587\u4ef6\u4e2d\uff0c\u8fc7\u7a0b\u4e2d\u8f93\u5165\u7684\u5bc6\u7801\u4e3a changeit \uff0c\u5b8c\u6210\u540e\u8f93\u5165yes NiFi PutSolrContentStream STANDARD\u6a21\u5f0f\u5de5\u4f5c\u6d41\u76f8\u5173\u914d\u7f6e \u00b6 \u8bf4\u660e\uff1aStandard\u6a21\u5f0f\u76f4\u63a5\u901a\u8fc7HTTPS\u65b9\u5f0f\u8fde\u63a5solr\u670d\u52a1\uff0cSSL\u9700\u8981\u8bc1\u4e66\u94fetruststore_chain \u914d\u7f6e KeytabCredentialsService \u914d\u7f6e StandardRestrictedSSLContextService \u5c06\u8fd9\u4e2acontoller\u6539\u540d\u4e3aCHAINStandardRestrictedSSLContextService\u4fbf\u4e8e\u533a\u5206 1. /opt/ssltest/truststore_chain 2. changeit 3. JKS 4. TLS \u6574\u4e2aPutSolrContentStream\u5de5\u4f5c\u6d41\u5982\u56fe\uff1a GenerateFlowFile\u914d\u7f6e\u5982\u4e0b: { \"id\":\"${UUID()}\", \"message\":\"The time is ${now()}\" } PutSolrContentStream\u914d\u7f6e\u5982\u4e0b\uff1a 1. Standard 2. https://172.16.4.122:21101/solr/nifi_test 3. nifi_test 4. KeytabCredentialsService 5. CHAINStandardRestrictedSSLContextService \u542f\u52a8\u5de5\u4f5c\u6d41 \u767b\u9646\u96c6\u7fa4Manager\u754c\u9762\uff0c\u767b\u9646solradmin webUI\u67e5\u770b\u7ed3\u679c\uff1a NiFi PutSolrContentStream CLOUD\u6a21\u5f0f\u5de5\u4f5c\u6d41\u76f8\u5173\u914d\u7f6e \u00b6 \u8bf4\u660e\uff1aCloud\u6a21\u5f0f\u5148\u8fde\u63a5\u96c6\u7fa4zookeeper\u670d\u52a1\u7136\u540e\u518d\u8bfb\u53d6solr\u8fde\u63a5\u4fe1\u606f\u8fde\u63a5solr\u670d\u52a1\uff0cSSL\u53ea\u9700\u8981huawei-huawei\u8bc1\u4e66truststore\u5373\u53ef\u3002 \u914d\u7f6e KeytabCredentialsService \u914d\u7f6e StandardRestrictedSSLContextService 1. /opt/ssltest/truststore 2. changeit 3. JKS 4. TLS \u6574\u4e2aPutSolrContentStream\u5de5\u4f5c\u6d41\u5982\u56fe\uff1a GenerateFlowFile\u914d\u7f6e\u5982\u4e0b: { \"id\":\"${UUID()}\", \"message\":\"The time is ${now()}\" } PutSolrContentStream\u914d\u7f6e\u5982\u4e0b\uff1a 1. Cloud 2. 172.16.4.122:24002/solr 3. nifi_test 4. KeytabCredentialsService 5. StandardRestrictedSSLContextService \u542f\u52a8\u5de5\u4f5c\u6d41 \u767b\u9646\u96c6\u7fa4Manager\u754c\u9762\uff0c\u767b\u9646solradmin webUI\u67e5\u770b\u7ed3\u679c\uff1a NIFI QuerySolr \u5de5\u4f5c\u6d41\u76f8\u5173\u914d\u7f6e \u00b6 \u8bf4\u660e\uff1aQuerySolr\u8fde\u63a5solr\u65b9\u5f0f\u4e5f\u53ef\u4ee5\u9009\u62e9Standard\u6216\u8005Cloud\u6a21\u5f0f\uff0c\u533a\u522b\u5c31\u662f\u8bf7\u6c42\u7684\u94fe\u63a5\uff0c\u4ee5\u53caSSL\u8bc1\u4e66\u914d\u7f6e\u4e0d\u4e00\u6837\uff0c\u5176\u4ed6\u4e00\u6837\uff0c\u94fe\u63a5\u4fe1\u606f\u53c2\u8003\u4e0a\u9762\u7684\u914d\u7f6e \u6574\u4e2a\u5de5\u4f5c\u6d41\u5982\u4e0b\uff1a QuerySolr Standard\u914d\u7f6e\u5982\u4e0b\uff1a 1. Standard 2. https://172.16.4.122:21101/solr/nifi_test 3. nifi_test 4. KeytabCredentialsService 5. CHAINStandardRestrictedSSLContextService QuerySolr Cloud\u914d\u7f6e\u5982\u4e0b\uff1a 1. Cloud 2. 172.16.4.122:24002/solr 3. nifi_test 4. KeytabCredentialsService 5. StandardRestrictedSSLContextService PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u542f\u52a8\u5de5\u4f5c\u6d41\uff1a \u767b\u9646\u540e\u53f0\u67e5\u770b\u7ed3\u679c\uff1a","title":"1.7.1 <--> C80"},{"location":"Data_Integration/Apache_NiFi/#apache-nififusioninsight","text":"","title":"Apache NiFi\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Apache_NiFi/#_1","text":"Apache NiFi 1.7.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive/Spark/Kafka/Solr)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#apache-nifi","text":"","title":"\u5b89\u88c5Apache NiFi"},{"location":"Data_Integration/Apache_NiFi/#_2","text":"\u5b89\u88c5Apache NiFi 1.7.1","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#_4","text":"\u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u5b89\u88c5NiFi\uff0c\u5728\u7f51\u5740 https://nifi.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u547d\u4ee4 unzip nifi-1.7.1-bin.zip \u89e3\u538b\u5b89\u88c5\u751f\u6210nifi-1.7.1\u76ee\u5f55\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a /usr/nifi/nifi-1.7.1 \u6267\u884c vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eNiFi\u670d\u52a1\u5668ip\u548c\u7aef\u53e3\u5982\u4e0b\uff1a nifi.web.http.host=172.16.52.190 nifi.web.http.port=8085 \u542f\u52a8\u548c\u505c\u6b62NiFi cd /usr/nifi/nifi-1.7.1 bin/nifi.sh start bin/nifi.sh stop \u8fd0\u884cNiFi bin/nifi.sh start","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifikerberos","text":"","title":"NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/Apache_NiFi/#_5","text":"NiFi\u914d\u7f6e\u5e76\u4fdd\u5b58Kerberos\u8ba4\u8bc1\u4fe1\u606f\uff0c\u4f9b\u4ee5\u540e\u4f7f\u7528","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Nifi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u5e76\u521b\u5efa\u6d4b\u8bd5\u7528\u6237developuser (\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1)","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#_7","text":"\u5728FusionInsight HD Manager\u4e0a\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6 user.keytab \uff0c krb5.conf \uff0c\u5e76\u4e00\u8d77\u5b58\u5165\u8def\u5f84 /opt/developuser \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5177\u4f53\u914d\u7f6e\uff1a nifi.kerberos.krb5.file=/opt/developuser/krb5.conf nifi.kerberos.service.principal=developuser nifi.kerberos.service.keytab.location=/opt/developuser/user.keytab \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 KeytabCredentialsService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58KeytabCredentialsService \u5b8c\u6210","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihdfs","text":"","title":"NiFi\u8fde\u63a5HDFS"},{"location":"Data_Integration/Apache_NiFi/#_8","text":"NiFi\u4e2d\u914d\u7f6eHDFS\u76f8\u5173\u5904\u7406\u5668\uff0c\u5bf9\u63a5HDFS","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_9","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#puthdfs","text":"\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHDFS\u7684\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \uff0c core-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4fee\u6539 hdfs-site.xml \u5185\u5bb9\uff0c\u5220\u9664\u5982\u4e0b\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u4fee\u6539 core-site.xml \u5185\u5bb9\uff0c\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\u9879\u4e2dhacluster\u6539\u4e3a\u8282\u70b9ip\u52a0\u7aef\u53e3\u53f7 <property> <name>fs.defaultFS</name> <value>hdfs://172.21.3.102:25000</value> </property> \u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset \u5904\u7406\u5668PutHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest \u4e24\u4e2a\u5904\u7406\u5668\u7684\u8fde\u63a5\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5165\u8def\u5f84 /home/dataset \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u6d4b\u8bd5\u540e \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c hdfs dfs -cat /tmp/nifitest/nifiHDFS.csv","title":"PutHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#gethdfs","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetHDFS\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2: \u9009\u62e9NiFi\u914d\u7f6eKerberos\u8ba4\u8bc1\u8fd9\u4e00\u8282\u4e2d\u521b\u5efa\u7684 KeytabCredentialsService 3: /tmp/nifitest/HDFS - \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\uff1a 1: /home/dataset/HDFS \u6d4b\u8bd5\u524d\u5c06\u6d4b\u8bd5\u6587\u4ef6 nifiHDFS.csv \u653e\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/nifitest/HDFS \u8def\u5f84\u4e0b \u6d4b\u8bd5\u540e \u767b\u5f55\u5b89\u88c5 FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS \u67e5\u770b\u7ed3\u679c","title":"GetHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#listhdfs-fetchhdfs","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ListHDFS\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService 3. /tmp/nifitest \u5904\u7406\u5668RouteOnAttribute\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u56fe\u6807\u589e\u52a0\u4e00\u6761\u914d\u7f6e\uff0c Property \u914d\u7f6e\u4e3a requiredfilenames \uff0c Value \u914d\u7f6e\u4e3a ${filename:matches('sanguo.*')} \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. Route to Property name 2. requiredfilenames 3. ${filename:matches('sanguo.*')} - \u5904\u7406\u5668RouteOnAttribute\u548c\u4e0a\u3001\u4e0b\u5904\u7406\u5668FetchHDFS\u7684\u8fde\u63a5\u914d\u7f6e\u5206\u522b\u5bf9\u5e94\u4e3a requiredfilenames \u548c unmatched \uff0c\u5982\u56fe\uff1a \u4e24\u4e2a\u5904\u7406\u5668FetchHDFS\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u4e3a\uff1a 1. /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2. KeytabCredentialsService \u4e0a\u3001\u4e0b\u5904\u7406\u5668PutFile\u914d\u7f6e\u5206\u522b\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff0c\u6267\u884c\u547d\u4ee4 hdfs dfs -ls /tmp/nifitest \u767b\u5f55\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf /tmp/nifitest \u67e5\u770b\u6587\u4ef6 \u6d4b\u8bd5\u540e \u767b\u5f55FusionInsight HD\u5ba2\u6237\u7aef\u4e3b\u673a\u8def\u5f84 /home/dataset/HDFS/matchedFiles \u548c /home/dataset/HDFS/unmatchedFiles \u5206\u522b\u67e5\u770b\u7ed3\u679c\uff1a","title":"ListHDFS &amp; FetchHDFS \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihive","text":"","title":"NiFi\u8fde\u63a5Hive"},{"location":"Data_Integration/Apache_NiFi/#_10","text":"NiFi\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_11","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#hiveconnectionpool","text":"\u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HiveConnectionPool \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u4e3a 1: jdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM 2: KeytabCredentialsService - \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HiveConnectionPool \u5b8c\u6210 \u5728\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u4e0b\u521b\u5efa jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/bootstrap.conf \u914d\u7f6e bootstrap.conf \u6587\u4ef6\u5982\u4e0b: java.arg.17=-Djava.security.auth.login.config=/usr/nifi/nifi-1.7.1/conf/jaas.conf java.arg.18=-Dsun.security.krb5.debug=true \u6267\u884c\u547d\u4ee4 vi /usr/nifi/nifi-1.7.1/conf/nifi.properties \u914d\u7f6e nifi.properties \u6587\u4ef6\u5982\u4e0b\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true - \u6267\u884c\u547d\u4ee4 cd /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hive-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u5230NiFi Hive\u7c7b\u5e93\u4e2d\uff0c\u5c06\u539f\u6709\u7684 zookeeper-3.4.6.jar \u66ff\u6362\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684 zookeeper-3.5.1.jar","title":"HiveConnectionPool \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#selecthiveql-hive","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668SelectHiveQL\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: HiveConnectionPool 2: select * from default.t2 3. CSV \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u8fd0\u884c\u524d\u767b\u5f55\u96c6\u7fa4\u67e5\u770bhive\u8868t2: \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/HIVE \u67e5\u770b\u7ed3\u679c\uff1a","title":"SelectHiveQL \u8bfb\u53d6Hive\u8868 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthiveql","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2: iris.txt \u6570\u636e\u6587\u4ef6 iris.txt \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5904\u7406\u5668PutHDFS\u7684\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hdfs-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService 3: /tmp/nifitest/loadhive \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: CREATE TABLE IF NOT EXISTS iris_createdBy_NiFi ( ID string, sepallength FLOAT, sepalwidth FLOAT, petallength FLOAT, petalwidth FLOAT, species string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;LOAD DATA INPATH \"hdfs:///tmp/nifitest/loadhive/iris.txt\" into table iris_createdBy_NiFi; \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c:","title":"PutHiveQL \u6574\u8868\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthiveql_1","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /home/dataset/ 2\uff1a iris_add.txt \u6570\u636e\u6587\u4ef6 iris_add.txt \u7684\u5185\u5bb9\u5982\u4e0b: \"11\",5.8,2.8,5.1,2.4,\"virginica\" \"12\",6.4,3.2,5.3,2.3,\"virginica\" \"13\",6.5,3,5.5,1.8,\"virginica\" \"14\",5.7,3,4.2,1.2,\"versicolor\" \"15\",5.7,2.9,4.2,1.3,\"versicolor\" \u5904\u7406\u5668SplitText\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668ExtractText\u914d\u7f6e\u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e \u5904\u7406\u5668ReplaceText\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668PutHiveQL\u914d\u7f6e\u5982\u4e0b: \u8fd0\u884c\u524d\u5c06\u6570\u636e\u6587\u4ef6 iris_add.txt \u5bfc\u5165\u8def\u5f84 /home/dataset/ \u8fd0\u884c\u540e\uff1a \u767b\u5f55HIVE\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"PutHiveQL \u5355\u884c\u5bfc\u5165 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifihbase","text":"","title":"NiFi\u8fde\u63a5HBase"},{"location":"Data_Integration/Apache_NiFi/#_12","text":"NiFi\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_13","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#hbase_1_1_2_clientservice","text":"\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u5173\u4e8eHBase\u7684\u914d\u7f6e\u6587\u4ef6 hbase-site.xml \u5bfc\u5165\u8def\u5f84 /usr/nifi/nifi-1.7.1/conf \u66f4\u6362\u8def\u5f84 /usr/nifi/nifi-1.7.1/work/nar/extensions/nifi-hbase_1_1_2-client-service-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u4e0b\u9762\u7684 zookeeper-3.4.6.jar \u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u81ea\u5e26\u7684 zookeeper-3.5.1.jar \u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 HBase_1_1_2_ClientService \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a /usr/nifi/nifi-1.7.1/conf/hbase-site.xml,/usr/nifi/nifi-1.7.1/conf/core-site.xml 2\uff1a KeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u70b9\u51fb\u95ea\u7535\u56fe\u6807\u751f\u6548\u5e76\u4fdd\u5b58 HBase_1_1_2_ClientService \u5b8c\u6210","title":"HBase_1_1_2_ClientService \u914d\u7f6e\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#puthbasejson-hbase","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u7684\u914d\u7f6e\u5982\u4e0b: \u6570\u636e\u6587\u4ef6 hbase_test.csv \u7684\u5185\u5bb9\u5982\u4e0b: 1,5.1,3.5,setosa 2,6.1,3.6,versicolor 3,7.1,3.7,virginica \u5904\u7406\u5668InverAvroSchema\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: flowfile-attribute 2: csv 3: false 4: hbase_test_data \u5904\u7406\u5668ConvertCSVToAvro\u914d\u7f6e\u5982\u4e0b: \u5904\u7406\u5668ConvertAvroToJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668SplitJson\u914d\u7f6e\u5982\u4e0b\uff1a \u5904\u7406\u5668PutHBaseJSON\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b: 1: HBase_1_1_2_ClientService 2: hbase_test 3: ${UUID()} 4: data \u6d4b\u8bd5\u524d\u9700\u8981\u5c06\u6570\u636e\u6587\u4ef6 hbase_test.csv \u5bfc\u5165\u8def\u5f84 /home/dataset/HBASE \u5e76\u4e14\u9700\u8981\u5728\u96c6\u7fa4\u91cc\u9762\u5efa\u4e00\u4e2ahbase\u8868\uff0c\u6267\u884c\u547d\u4ee4 hbase shell create 'HBase_test','data' \u8fd0\u884c\u540e\uff1a \u767b\u5f55\u96c6\u7fa4\u67e5\u770b\u7ed3\u679c\uff1a","title":"PutHBaseJSON \u5411HBase\u5bfc\u5165\u8868"},{"location":"Data_Integration/Apache_NiFi/#gethbase","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHBase\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u9a71\u52a8\u5668PutFile\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u540e \u767b\u5f55\u5230\u8def\u5f84 /home/dataset/GetHBase_test \u67e5\u770b\u7ed3\u679c\uff1a","title":"GetHbase \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifispark","text":"","title":"NiFi\u8fde\u63a5Spark"},{"location":"Data_Integration/Apache_NiFi/#_14","text":"NiFi\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_15","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e \u5df2\u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \uff08Livy\u53ef\u5b89\u88c5\u5728FI HD\u5ba2\u6237\u7aef\u4e3b\u673a\uff0c\u4e5f\u53ef\u4ee5\u5b89\u88c5\u5728\u5176\u4ed6\u4e3b\u673a\u4f46\u662f\u9700\u8981\u4fdd\u8bc1\u5b89\u88c5Livy\u4e3b\u673a\u80fd\u591f\u548cFI HD\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a\uff09 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#livysessioncontroller","text":"\u767b\u5f55NiFi\u7f51\u9875\u754c\u9762\uff0c\u53f3\u952e\u9009\u62e9**Configure** \u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: spark 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_PySpark \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: pysaprk 4\uff1aKeytabCredentialsService \u7ee7\u7eed\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\u6dfb\u52a0\u670d\u52a1 \u9009\u62e9 LivySessionController \uff0c\u70b9\u51fb**ADD**\u6dfb\u52a0 \u70b9\u51fb**\u9f7f\u8f6e**\u56fe\u6807\u8fdb\u884c\u914d\u7f6e \u66f4\u6539Controller\u540d\u5b57\u4e3a LivySessionController_SparkR \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.43 (\u5df2\u5b89\u88c5Apache Livy\u7684\u4e3b\u673aip) 2: 8998 (Livy\u9ed8\u8ba4\u7aef\u53e3\uff0c\u53ef\u66f4\u6539) 3: sparkr 4\uff1aKeytabCredentialsService \u70b9\u51fb**\u95ea\u7535**\u56fe\u6807\u9009\u62e9 Service and referencing components \u751f\u6548\u5e76\u4fdd\u5b58 LivySessionController , LivySessionController_PySpark , LivySessionController_SparkR \u5b8c\u6210","title":"\u914d\u7f6eLivySessionController\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#spark","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code1.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code1.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a 1+2 \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code1 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController 2: ${code1} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code1.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy\uff1a \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#pyspark","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code2.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code2.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5904\u7406\u5668ExtractText\u914d\u7f6e\u5982\u4e0b\uff1a \u9700\u8981\u70b9\u51fb**\u52a0\u53f7**\u6309\u94ae\uff0cProperty\u9879\u547d\u540d\u4e3a code2 \uff0cValue\u9879\u8d4b\u503c\u4e3a $ \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: LivySessionController_PySpark 2: ${code2} \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code2.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#sparkr","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u6ce8\u610f\uff1a\u5728\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u5982\u679c\u4e0eSpark\uff0cPySpark\u6837\u4f8b\u4e0d\u5b8c\u5168\u4e00\u6837 \u5904\u7406\u5668GetFile\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt \u4ee3\u7801\u5185\u5bb9\u6587\u4ef6 code3.txt \u7684\u5185\u5bb9\u5982\u4e0b\uff1a piR <- function(N) { x <- runif(N) y <- runif(N) d <- sqrt(x^2 + y^2) return(4 * sum(d < 1.0) / N) } set.seed(5) cat(\"Pi is roughly \",piR(1000000) ) \u5904\u7406\u5668ExecuteSparkInteractive\u914d\u7f6e\u4e3a\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: /home/dataset/sparkTest 2: code3.txt\u91cc\u7684\u4ee3\u7801\u5185\u5bb9 \u6d4b\u8bd5\u524d\u5c06\u4ee3\u7801\u6587\u4ef6 code3.txt \u4e0a\u4f20\u81f3\u5b89\u88c5nifi\u4e3b\u673a\u7684\u8def\u5f84 /home/dataset/sparkTest \u4e0b\uff1a \u5728\u5df2\u5b89\u88c5Livy\u7684\u4e3b\u673a\u4e0a\u542f\u52a8Livy \u6d4b\u8bd5\u540e \u767b\u5f55Livy sever\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"\u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifikafka","text":"","title":"NiFi\u8fde\u63a5Kafka\u666e\u901a\u6a21\u5f0f"},{"location":"Data_Integration/Apache_NiFi/#_16","text":"NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21005\u7aef\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_17","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#gethttp-putkafka","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PutKafka\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1\uff1a 172.21.3.102:21005,172.21.3.101:21005,172.21.3.103:21005 2\uff1a nifi-kafka-test-demo 3\uff1a nifi \u6d4b\u8bd5\u524d\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u521b\u5efaTopic nifi-kafka-test-demo cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --topic nifi-kafka-test-demo --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --partitions 1 --replication-factor 1 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55FI\u5ba2\u6237\u7aefkafak\u7ec4\u4ef6\uff0c\u67e5\u770b\u7ed3\u679c\uff1a cd /opt/hadoopclient/Kafka/kafka/bin kafka-console-consumer.sh --zookeeper 172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/kafka --topic nifi-kafka-test-demo --from-beginning","title":"GetHTTP &amp; PutKafka \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#publishkafka_0_11","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 2: SASL_PLAINTEXT 3: wikipedia21005 6: Guarantee Replicated Delivery \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41: \u767b\u9646FI HD kafka\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a","title":"PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#consumekafka_0_11","text":"\u6574\u4e2a\u8fc7\u7a0b\u7684\u6d41\u7a0b\u5982\u56fe\u6240\u793a\uff1a \u5904\u7406\u5668ConsumeKafka_0_11\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.21.3.101:21005,172.21.3.102:21005,172.21.3.103:21005 2: PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: example-metric1 6: DemoConsumer \u5904\u7406\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u6d4b\u8bd5\u524d\uff1a \u7528eclipse\u6253\u5f00\u5ba2\u6237\u7aef\u81ea\u5e26\u7684kafka\u6837\u4f8b\u4ee3\u7801 kafka-examples \uff0c\u8c03\u8bd5\u4f7f\u5f97\u6837\u4f8b\u4ee3\u7801\u80fd\u591f\u6b63\u5e38\u8fd0\u884c NewProducer.java \u7531\u4e8eConsumeKafka\u662f\u5b9e\u65f6\u83b7\u53d6\u65e5\u5fd7\u4fe1\u606f\u7684\uff0c\u6240\u4ee5\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c NewProducer.java \u5f80Kafka\u4e0a\u4f20\u65e5\u5fd7\u6587\u4ef6\uff0c\u518d\u540c\u65f6\u5f00\u542fnifi\u7684\u9a71\u52a8\u5668ConsumeKafka_0_11\u8fdb\u884c\u8bfb\u53d6\u65e5\u5fd7\u7684\u6d4b\u8bd5 \u6d4b\u8bd5\u540e\uff1a \u767b\u5f55\u8def\u5f84 /home/dataset/Kafka \u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"ConsumeKafka_0_11 \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifikafka_1","text":"","title":"NiFi\u8fde\u63a5Kafka\u5b89\u5168\u6a21\u5f0f"},{"location":"Data_Integration/Apache_NiFi/#_18","text":"NiFi\u4e2d\u914d\u7f6ekafka\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD kafka 21007\u7aef\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Apache_NiFi/#_19","text":"\u5df2\u7ecf\u5b8c\u6210NiFi 1.7.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bkafka\u7ec4\u4ef6 \u5df2\u5b8c\u6210 NiFi Kerberos\u8ba4\u8bc1\u914d\u7f6e nifi\u4e3b\u673aip: 172.16.2.119, FI HD\u4e09\u8282\u70b9ip: 172.16.6.10-12","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Apache_NiFi/#_20","text":"\u5728nifi\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u4e3a\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u4f7f\u7528\u547d\u4ee4bin/nifi.sh stop\u505c\u6b62nifi \u5728FI HD\u7684kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982/opt/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar \u5c06nifi\u4e3b\u673a\u4e0b /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-kafka-0-11-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u8def\u5f84\u4e2d\u539f\u6765\u7684kafka client jar\u5305kafka-clients-0.11.0.1.jar \u4f7f\u7528\u91cd\u547d\u540d\u547d\u4ee4\u547d\u540d\u4e3a kafka-clients-0.11.0.1.jar.org \u5e76\u4e14\u628a\u4e0a\u4e00\u6b65\u5728 FI HD kafka\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u7684kafka-clients-1.1.0.jar\u590d\u5236\u5230\u6b64\u8def\u5f84\u4e0b\u3002 \u767b\u9646nifi\u4e3b\u673a\uff0c\u5148\u4f7f\u7528 source /opt/hadoopclient/bigdata_env \u52a0\u8f7d\u8fd0\u884c\u7684\u73af\u5883\u53d8\u91cf\uff0c\u7136\u540e\u518d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7djava\u8fd0\u884c\u7684jvm\u53c2\u6570: export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf\" \u5176\u4e2d/etc/krb5.conf\u4e3a\u5bf9\u5e94\u5bf9\u63a5\u96c6\u7fa4\u7684\u8ba4\u8bc1krb5.conf\u6587\u4ef6 \u5b8c\u6210\u4e0a\u8ff0\u6b65\u9aa4\u540e\u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5jvm\u53c2\u6570\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u547d\u4ee4bin/nifi.sh start\u542f\u52a8nifi:","title":"\u8ba4\u8bc1\u76f8\u5173\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#publishkafka_0_12","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668GetHTTP\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5177\u4f53\u914d\u7f6e\u5982\u4e0b\uff1a 1: http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv 2: iris.csv - \u9a71\u52a8\u5668PublishKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 2: SASL_PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: testtopic_01 6: Guarantee Replicated Delivery \u8fd0\u884c\u6574\u4e2a\u5de5\u4f5c\u6d41\uff1a \u53bb\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\u68c0\u67e5\uff1a","title":"PublishKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#consumekafka_0_12","text":"\u6574\u4e2a\u5de5\u4f5c\u6d41\u4e3a\uff1a \u9a71\u52a8\u5668ConsumeKafka_0_11\u7684\u914d\u7f6e\u5982\u4e0b\uff1a 1: 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 2: SASL_PLAINTEXT 3: KeytabCredentialsService 4: Kafka 5: testtopic_01 6: Demo \u9a71\u52a8\u5668PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u542f\u52a8\u6574\u4e2a\u5de5\u4f5c\u6d41 \u4f7f\u7528FI HD\u6837\u4f8b\u4ee3\u7801\u4f7f\u7528producer\u4f20\u6570\u636e\uff1a \u767b\u9646nifi\u4e3b\u673a/opt/nifikafka21007\u8def\u5f84\u68c0\u67e5\u7ed3\u679c","title":"ConsumeKafka_0_11\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Apache_NiFi/#nifisolr","text":"","title":"NiFi\u5bf9\u63a5Solr\u5b89\u5168\u6a21\u5f0f"},{"location":"Data_Integration/Apache_NiFi/#_21","text":"FI HD: 172.16.4.121-123 NIFI: 172.17.2.124","title":"\u73af\u5883\u8bf4\u660e"},{"location":"Data_Integration/Apache_NiFi/#fi-hd","text":"\u53c2\u8003\u4ea7\u54c1\u6587\u6863solr\u90e8\u5206\uff0c\u505a\u5982\u4e0b\u914d\u7f6e\uff08\u53ef\u4e0d\u505a\uff09\uff1a \u4f7f\u7528\u5982\u4e0bcurl\u547d\u4ee4\u5728\u5bf9\u63a5\u96c6\u7fa4solr\u521b\u5efa\u4e00\u4e2acollection\uff0c\u540d\u5b57\u53eb\u505anifi_test curl --negotiate -k -v -u : \"https://172.16.4.122:21101/solr/admin/collections?action=CREATE&name=nifi_test&collection.configName=confWithSchema&numShards=3&replicationFactor=1\" \u767b\u9646\u5ba2\u6237\u7aef\uff0c\u6309\u7167\u5982\u4e0b\u622a\u56fe\u65b9\u5f0f\u767b\u9646kadmin\uff0c\u589e\u52a03\u4e2a\u8282\u70b9\u7684HTTP\u670d\u52a1principal \u6ce8\u610f\uff1a\u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801 \u767b\u9646oracle\u5b98\u7f51\u83b7\u53d6jce\uff0c\u5e76\u9002\u914d\u5230\u5bf9\u63a5FI HD\u96c6\u7fa4\u4e2d \u7531\u4e8ekerberos\u6821\u9a8c\u548c\u52a0\u89e3\u5bc6\u7528\u5230\u5bc6\u94a5\u957f\u5ea6\u8fdc\u8d85\u51fajre\u9ed8\u8ba4\u7684\u5b89\u5168\u5b57\u7b26\u957f\u5ea6\uff0c\u6240\u4ee5\u9700\u8981\u5230java\u5b98\u7f51\u4e0a\u4e0b\u8f7dJava Cryptography Extension (JCE)\uff1a https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html \uff0c\u7136\u540e\u89e3\u538b\u5230 %JAVA_HOME%/jre/lib/security \u4e2d\u66ff\u6362\u76f8\u5e94\u7684\u6587\u4ef6\u3002 \u5177\u4f53\u5c06\u4e0b\u8f7d\u597djce\u89e3\u538b\uff0c\u5e76\u5c06\u89e3\u538b\u540e\u7684\u4e24\u4e2ajar\u5305 US_export_policy.jar \uff0c local_policy.jar \u62f7\u8d1d\u5230\u4e09\u53f0\u670d\u52a1\u5668172.16.4.121,122,123\u7684 /opt/huawei/Bigdata/common/runtime0/jdk-8u201/jre/lib/security/ \u8def\u5f84\u4e0b,\u5e76\u4e14\u91cd\u542fsolr\u670d\u52a1","title":"FI HD\u76f8\u5173\u914d\u7f6e:"},{"location":"Data_Integration/Apache_NiFi/#nifi-kerberos","text":"nifi.properties\u914d\u7f6e\u6587\u4ef6\uff1a web properties\u90e8\u5206\u505a\u5982\u4e0b\u66f4\u6539\uff1a kerberos\u90e8\u5206\uff1a \u589e\u52a0sasl\u914d\u7f6e\uff1a nifi.zookeeper.auth.type=sasl nifi.zookeeper.kerberos.removeHostFromPrincipal=true nifi.zookeeper.kerberos.removeRealmFromPrincipal=true bootstrap.conf\u914d\u7f6e\u6587\u4ef6\uff1a \u589e\u52a0\u4e00\u4e2ajvm\u53c2\u6570\uff1a java.arg.17=-Djava.security.auth.login.config=/opt/jaas.conf \u56e0\u4e3a\u5bf9\u63a5solr\uff0c\u6240\u4ee5\u627e\u5230nifi\u5173\u4e8esolr\u7684\u4f9d\u8d56\u5305\u8def\u5f84\uff0c\u4ee5\u672c\u673a\u4e3a\u4f8b\uff1a /opt/nifi/nifi-1.7.1/work/nar/extensions/nifi-solr-nar-1.7.1.nar-unpacked/META-INF/bundled-dependencies \u5c06\u539f\u6709\u7684zookeeper-3.4.6.jar\u91cd\u547d\u540dzookeeper-3.4.6.jar.org\uff0c\u518d\u5c06FI HD\u7684\u5339\u914dzookeeper-3.5.1.jar\u62f7\u8d1d\u8fc7\u6765","title":"NiFi kerberos\u76f8\u5173\u914d\u7f6e"},{"location":"Data_Integration/Apache_NiFi/#ssl","text":"\u8bf4\u660e\uff1a\u56e0\u4e3a\u5bf9\u63a5solr\u90e8\u7f72\u5728\u5b89\u5168\u6a21\u5f0f\u4e0a\uff0c\u4f7f\u7528rest\u63a5\u53e3\u8fdb\u884c\u4ea4\u4e92\u7684\u65f6\u5019\u662f\u8981\u901a\u8fc7ssl\u5c42\u7684\u8ba4\u8bc1\uff0c\u9700\u8981\u521b\u5efa\u5bf9\u5e94\u7684\u8bc1\u4e66\uff08truststore\uff09\uff0c\u5b8c\u6210\u540e\u8fd8\u8981\u4f7f\u7528spnego\u540c\u96c6\u7fa4solr\u8fdb\u884c\u4ea4\u4e92\u3002\u4e0b\u9762\u5c06\u4ecb\u7ecd\u4e24\u79cd\u8ba4\u8bc1\u8bc1\u4e66\u83b7\u53d6\u7684\u65b9\u5f0f\uff0c\u5206\u522b\u5bf9\u5e94solr\u4e24\u79cd\u5bf9\u63a5\u65b9\u6cd5CLOUD\u548cHTTPS huawei-huawei\u8bc1\u4e66 \u767b\u9646linux\u540e\u53f0\uff08\u9700\u8981\u5b89\u88c5openssl\uff09\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4 openssl s_client -host 172.16.4.122 -port 21101 -prexit -showcerts\\ \u4f1a\u6709\u4e09\u6bb5\u8bc1\u4e66\uff0c\u5206\u522b\u4e3ahuawei-huawei, huawei-FusionInsight, FusionInsight-172.16.4.122 \u5c06huawei-huawei\u90e8\u5206\u5185\u5bb9\u590d\u5236\uff0c\u62f7\u8d1d\u5230\u4e00\u4e2a\u6587\u4ef6 /opt/ssltest/huawei-huawei.pem \u6587\u4ef6\u4e2d \u4f7f\u7528\u547d\u4ee4 keytool -import -alias gca -file /opt/ssltest/huawei-huawei.pem -keystore /opt/ssltest/truststore \u5c06\u4e0a\u4e00\u6b65\u751f\u6210\u7684 huawei-huawei.pem \u8bc1\u4e66\u5185\u5bb9\u52a0\u5165 /opt/ssltest/truststore \u6587\u4ef6\u4e2d\uff0c\u8fc7\u7a0b\u4e2d\u8f93\u5165\u7684\u5bc6\u7801\u4e3a changeit \uff0c\u5b8c\u6210\u540e\u8f93\u5165yes \u8bc1\u4e66\u94fe\uff0c\u56e0\u4e3a\u7531\u4e0a\u5df2\u7ecf\u77e5\u9053\u4e86\u4f1a\u751f\u6210\u4e00\u4e2a\u8bc1\u4e66\u94fe\uff0c\u5305\u542b\u4e09\u6bb5\uff0c\u6b64\u6b65\u9aa4\u5c31\u662f\u5c06\u6574\u4e2a\u8bc1\u4e66\u94fe\u751f\u6210\u4e00\u4e2a\u65b0\u7684truststore_huawei\u6587\u4ef6 \u4f7f\u7528\u547d\u4ee4 echo \"\" | openssl s_client -host 172.16.4.122 -port 21101 -showcerts | awk '/BEGIN CERT/ {p=1} ; p==1; /END CERT/ {p=0}' > /opt/ssltest/allcerts122.pem \u5c06\u6574\u4e2a\u8bc1\u4e66\u94fe\u91cd\u5b9a\u5411\u5230 /opt/ssltest/allcerts122.pem \u6587\u4ef6\u4e2d \u4f7f\u7528\u547d\u4ee4 keytool -import -alias gca -file /opt/ssltest/allcerts122.pem -keystore /opt/ssltest/truststore_chain \u5c06\u4e0a\u4e00\u6b65\u751f\u6210\u7684 allcerts122.pem \u8bc1\u4e66\u5185\u5bb9\u52a0\u5165 /opt/ssltest/truststore_chain \u6587\u4ef6\u4e2d\uff0c\u8fc7\u7a0b\u4e2d\u8f93\u5165\u7684\u5bc6\u7801\u4e3a changeit \uff0c\u5b8c\u6210\u540e\u8f93\u5165yes","title":"SSL\u8bc1\u4e66\u76f8\u5173\u914d\u7f6e"},{"location":"Data_Integration/Apache_NiFi/#nifi-putsolrcontentstream-standard","text":"\u8bf4\u660e\uff1aStandard\u6a21\u5f0f\u76f4\u63a5\u901a\u8fc7HTTPS\u65b9\u5f0f\u8fde\u63a5solr\u670d\u52a1\uff0cSSL\u9700\u8981\u8bc1\u4e66\u94fetruststore_chain \u914d\u7f6e KeytabCredentialsService \u914d\u7f6e StandardRestrictedSSLContextService \u5c06\u8fd9\u4e2acontoller\u6539\u540d\u4e3aCHAINStandardRestrictedSSLContextService\u4fbf\u4e8e\u533a\u5206 1. /opt/ssltest/truststore_chain 2. changeit 3. JKS 4. TLS \u6574\u4e2aPutSolrContentStream\u5de5\u4f5c\u6d41\u5982\u56fe\uff1a GenerateFlowFile\u914d\u7f6e\u5982\u4e0b: { \"id\":\"${UUID()}\", \"message\":\"The time is ${now()}\" } PutSolrContentStream\u914d\u7f6e\u5982\u4e0b\uff1a 1. Standard 2. https://172.16.4.122:21101/solr/nifi_test 3. nifi_test 4. KeytabCredentialsService 5. CHAINStandardRestrictedSSLContextService \u542f\u52a8\u5de5\u4f5c\u6d41 \u767b\u9646\u96c6\u7fa4Manager\u754c\u9762\uff0c\u767b\u9646solradmin webUI\u67e5\u770b\u7ed3\u679c\uff1a","title":"NiFi PutSolrContentStream STANDARD\u6a21\u5f0f\u5de5\u4f5c\u6d41\u76f8\u5173\u914d\u7f6e"},{"location":"Data_Integration/Apache_NiFi/#nifi-putsolrcontentstream-cloud","text":"\u8bf4\u660e\uff1aCloud\u6a21\u5f0f\u5148\u8fde\u63a5\u96c6\u7fa4zookeeper\u670d\u52a1\u7136\u540e\u518d\u8bfb\u53d6solr\u8fde\u63a5\u4fe1\u606f\u8fde\u63a5solr\u670d\u52a1\uff0cSSL\u53ea\u9700\u8981huawei-huawei\u8bc1\u4e66truststore\u5373\u53ef\u3002 \u914d\u7f6e KeytabCredentialsService \u914d\u7f6e StandardRestrictedSSLContextService 1. /opt/ssltest/truststore 2. changeit 3. JKS 4. TLS \u6574\u4e2aPutSolrContentStream\u5de5\u4f5c\u6d41\u5982\u56fe\uff1a GenerateFlowFile\u914d\u7f6e\u5982\u4e0b: { \"id\":\"${UUID()}\", \"message\":\"The time is ${now()}\" } PutSolrContentStream\u914d\u7f6e\u5982\u4e0b\uff1a 1. Cloud 2. 172.16.4.122:24002/solr 3. nifi_test 4. KeytabCredentialsService 5. StandardRestrictedSSLContextService \u542f\u52a8\u5de5\u4f5c\u6d41 \u767b\u9646\u96c6\u7fa4Manager\u754c\u9762\uff0c\u767b\u9646solradmin webUI\u67e5\u770b\u7ed3\u679c\uff1a","title":"NiFi PutSolrContentStream CLOUD\u6a21\u5f0f\u5de5\u4f5c\u6d41\u76f8\u5173\u914d\u7f6e"},{"location":"Data_Integration/Apache_NiFi/#nifi-querysolr","text":"\u8bf4\u660e\uff1aQuerySolr\u8fde\u63a5solr\u65b9\u5f0f\u4e5f\u53ef\u4ee5\u9009\u62e9Standard\u6216\u8005Cloud\u6a21\u5f0f\uff0c\u533a\u522b\u5c31\u662f\u8bf7\u6c42\u7684\u94fe\u63a5\uff0c\u4ee5\u53caSSL\u8bc1\u4e66\u914d\u7f6e\u4e0d\u4e00\u6837\uff0c\u5176\u4ed6\u4e00\u6837\uff0c\u94fe\u63a5\u4fe1\u606f\u53c2\u8003\u4e0a\u9762\u7684\u914d\u7f6e \u6574\u4e2a\u5de5\u4f5c\u6d41\u5982\u4e0b\uff1a QuerySolr Standard\u914d\u7f6e\u5982\u4e0b\uff1a 1. Standard 2. https://172.16.4.122:21101/solr/nifi_test 3. nifi_test 4. KeytabCredentialsService 5. CHAINStandardRestrictedSSLContextService QuerySolr Cloud\u914d\u7f6e\u5982\u4e0b\uff1a 1. Cloud 2. 172.16.4.122:24002/solr 3. nifi_test 4. KeytabCredentialsService 5. StandardRestrictedSSLContextService PutFile\u914d\u7f6e\u5982\u4e0b\uff1a \u542f\u52a8\u5de5\u4f5c\u6d41\uff1a \u767b\u9646\u540e\u53f0\u67e5\u770b\u7ed3\u679c\uff1a","title":"NIFI QuerySolr \u5de5\u4f5c\u6d41\u76f8\u5173\u914d\u7f6e"},{"location":"Data_Integration/Confluent_4_1_0/","text":"Confluent\u5bf9\u63a5FusionInsight \u00b6 \u4f7f\u7528\u573a\u666f \u00b6 Confluent 4.1.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka) Confluent 4.1.0 \u2194 FusionInsight HD 6.5 (HDFS/Kafka) Confluent 5.5.0 \u2194 FusionInsight MRS 8.0 (HDFS/Kafka) MRS 8.0 \u5bf9\u63a5\u8bf4\u660e \u00b6 \u8bf4\u660e\uff1a Confluent 5.5.0\u5bf9\u63a5FusionInsight MRS 8.0\u65f6KSQL\u529f\u80fd\u4e0d\u652f\u6301 \u5b89\u88c5Confluent \u00b6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55Confluent\u5b98\u65b9\u7f51\u7ad9\u4e0b\u8f7d\u9875\u9762\uff1a https://www.confluent.io/previous-versions/?_ga=2.102961223.611794173.1561088831-1783953529.1561088831 \u627e\u5230\u76f8\u5e94\u7684\u7248\u672c\u4e0b\u8f7d \u5c06\u4e0b\u8f7d\u7684\u5f00\u6e90\u538b\u7f29\u5305\u4f7f\u7528WinSCP\u5de5\u5177\u4e0a\u4f20\u81f3linux\u4e3b\u673a\uff0c\u4f7f\u7528 tar -xvf confluent-oss-4.1.0-2.11.tar \u89e3\u538b \u589e\u52a0confluent\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \uff0c\u589e\u52a0confluent bin\u76ee\u5f55\u5230PATH\u73af\u5883\u53d8\u91cf\u4e2d\uff0c\u5b8c\u6210\u540e\u4f7f\u7528 source ~/.bashrc \u4f7f\u4e4b\u751f\u6548 \u914d\u7f6eConfluent \u00b6 \u8bf4\u660e\uff1aConfluent\u542f\u52a8\u65f6\u4f1a\u8d77\u81ea\u5df1\u7684zookeeper\u548ckafka\u670d\u52a1\uff0c\u8fd9\u91cc\u4e0d\u505a\u4fee\u6539\u3002\u9700\u8981\u66f4\u6539\u7684\u662fconnect\uff0c schema-registry\u4ee5\u53caksql\u670d\u52a1\u914d\u7f6e\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u670d\u52a1\u76f4\u63a5\u5bf9\u63a5FusionInsight HD\u5b89\u5168\u6a21\u5f0f\u7684zookeeper\u548ckafka\u670d\u52a1 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728confluent\u5b89\u88c5\u76ee\u5f55\\share\\java\u4e0b\u65b0\u5efa\u8def\u5f84\uff0c\u540d\u4e3ahuawei \u5230FusionInsight 6.5.1\u7684kafka\u5ba2\u6237\u7aef\u4e0b\u83b7\u53d6 kafka-clients-1.1.0.jar\uff0c \u6ce8\u610f\u5373\u4f7f\u5bf9\u63a5FI HD 2.8\u7248\u7248\u672c\u4e5f\u9700\u8981\u5bf9\u5e94\u4e8eFI HD 6.5.1\u7684 kafka-clients-1.1.0.jar \u5305\uff0c\u5426\u5219\u4f1a\u62a5kafka\u7248\u672c\u5339\u914d\u76f8\u5173\u95ee\u9898 \u5c06kafka-clients-1.1.0.jar\u62f7\u8d1d\u81f3\u7b2c\u4e00\u6b65\u65b0\u5efa\u7684huawei\u8def\u5f84\u4e0b \u5728\u8def\u5f84/opt/confluent/confluent-4.1.0/bin\u4e0b\u627e\u5230connect-distributed\u6587\u4ef6\uff0c\u8fdb\u884c\u5982\u4e0b\u7f16\u8f91\uff1a \u5728\u9002\u5f53\u4f4d\u7f6e\u6dfb\u52a0KAFKA_OPTS\u7684\u542f\u52a8JVM\u53c2\u6570 \u5177\u4f53\u5185\u5bb9\u4e3a\uff1a export KAFKA_OPTS=\"-Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/user_keytabs/101keytab/krb5.conf -Dkerberos.domain.name=hadoop.hadoop.com\" \u5176\u4e2d-Djava.security.krb5.conf=/opt/user_keytabs/101keytab/krb5.conf\u4e3a\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u7684krb5.conf\u6587\u4ef6\uff0c\u53ef\u5728\u96c6\u7fa4 anager\u9875\u9762\u4e0a\u83b7\u53d6 \u53e6\u5916\u8fd8\u53ef\u4ee5\u6dfb\u52a0 -Dsun.security.krb5.debug=true \u6253\u5f00kerberos\u8ba4\u8bc1\u65e5\u5fd7\u5f00\u5173\uff0c\u8fdb\u884c\u9519\u8bef\u5b9a\u4f4d\u3001\u6392\u67e5 \u5728\u524d\u9762\u4f4d\u7f6e\u5c06huawei\u8def\u5f84\u5f15\u8fdb\u53bb \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-distributed.properties\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/schema-registry/connect-avro-distributed.properties\u914d\u7f6e\u6587\u4ef6\uff0c\u4e0e\u4e0a\u4e00\u6b65\u7c7b\u4f3c\uff1a \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka producer.kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka consumer.kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539/opt/confluent/confluent-4.1.0/bin/ksql-run-class\u6587\u4ef6\u5982\u4e0b\uff0c\u76ee\u7684\u662f\u5728\u8d77ksql-server\u670d\u52a1\u7684\u65f6\u5019\u80fd\u591f\u5728classpath\u91cc\u52a0\u8f7d\u5230\u4e4b\u524d\u5bfc\u5165\u7684\u534e\u4e3akafka jar\u5305\uff1a \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/ksql/ksql-server.properties\u914d\u7f6e\u6587\u4ef6 #bootstrap.servers=localhost:9092 security.protocol = SASL_PLAINTEXT bootstrap.servers=172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 kerberos.domain.name = hadoop.hadoop.com listeners=http://localhost:8088 ksql.server.ui.enabled=true sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4f7f\u7528\u547d\u4ee4confluent start\u542f\u52a8 \u4f7f\u7528Confluent KSQL\u670d\u52a1\u67e5\u8be2FI HD\u96c6\u7fa4Kafka\u7684Topic \u00b6 \u8bf4\u660e\uff1a \u5bf9\u63a5FusionInsight MRS 8.0\u7248\u672cKSQL\u529f\u80fd\u4e0d\u652f\u6301 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210 FI HD Kafka\u6837\u4f8b\u4ee3\u7801\u8c03\u8bd5\uff0c\u5177\u4f53\u53c2\u8003 https://support-it.huawei.com/solution-fid-gw/#/Intelligent_Data_Developer_Platform \u4e0b\u8f7d\uff0c\u8c03\u8bd5kafka\u6837\u4f8b\u4ee3\u7801 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8KSQL CLI LOG_DIR=/opt/confluent/confluent-4.1.0/ksql_logs /opt/confluent/confluent-4.1.0/bin/ksql http://localhost:8088 \u5728KSQL CLI\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2aSTREAM CREATE STREAM TEST_01 (id BIGINT) \\ WITH (KAFKA_TOPIC='testtopic_01', VALUE_FORMAT='DELIMITED', KEY = 'id'); \u914d\u7f6eKAFKA\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684producer\u4ee3\u7801,\u4f7f\u752821007\u7aef\u53e3\u7684\u5b89\u5168\u6a21\u5f0f\uff1a \u540c\u65f6\u5728KSQL CLI\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u8be2\uff0c\u68c0\u67e5\u7ed3\u679c\uff1a select ID from TEST_01; \u4f7f\u7528Confluent connect\u670d\u52a1\u540c\u6b65\u672c\u5730\u6587\u4ef6\u4fe1\u606f \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Confluent\u5b89\u88c5\uff0c\u914d\u7f6e \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-source.properties\u6587\u4ef6\uff1a \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-sink.properties\u6587\u4ef6\uff1a \u5728\u8def\u5f84/opt/confluent/confluent-4.1.0\u4e0b\u65b0\u5efa\u7a7a\u6587\u4ef6test.txt \u4f7f\u7528confluent start\u547d\u4ee4\u542f\u52a8confluent \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u6dfb\u52a0file-source, file-sink\uff0c \u5b8c\u6210\u540e\u67e5\u770bconnector\u72b6\u6001 confluent load file-source confluent load file-sink confluent status connectors \u5728test.txt\u7a7a\u6587\u4ef6\u4e2d\u8f93\u5165\u4fe1\u606f\uff0c\u4fdd\u5b58\uff0c\u5728\u751f\u6210\u7684test.sink.txt\u6587\u4ef6\u4e2d\u67e5\u770b\u4fe1\u606f\u540c\u6b65\u60c5\u51b5 \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u547d\u4ee4 bin/kafka-console-consumer.sh --topic connect-test --bootstrap-server 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --consumer.config config/consumer.properties -from-beginning \u67e5\u770b\u540c\u6b65kafka\u7684topic \u4f7f\u7528Confluent connect\u670d\u52a1\u540c\u6b65\u672c\u5730\u6587\u4ef6\u5230FusionInsight HDFS \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Confluent\u5b89\u88c5\uff0c\u914d\u7f6e \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55\u5bf9\u63a5FusionInsight HD\u96c6\u7fa4\uff0c\u67e5\u770bhdfs\u914d\u7f6e\uff1a \u5b89\u88c5kafka-connect-hdfs connector, \u53c2\u8003\uff1a https://docs.confluent.io/current/connect/managing/install.html \u5728Confluent Hub\u4e0a\u4e0b\u8f7d\u7248\u672c\u5339\u914d\u7684plugin\u5305\uff1a \u5c06\u4e0b\u8f7d\u597d\u7684confluentinc-kafka-connect-hdfs-4.1.0.zip\u538b\u7f29\u5305\u672c\u5730\u89e3\u538b\uff0c\u5e76\u4f7f\u7528WinSCP\u4e0a\u4f20\u5230/opt/confluent/confluent-4.1.0/share/java\u8def\u5f84\u4e0b \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-distributed.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684plugin.path \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/schema-registry/connect-avro-distributed.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684plugin.path \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka-connect-hdfs/quickstart-hdfs.properties \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-source.properties\u6587\u4ef6\u540c\u6b65\u7684topic\u4e3atest_hdfs: \u5728/opt/confluent/confluent-4.1.0\u8def\u5f84\u4e0b\u65b0\u5efatest_hdfs.txt\u7684\u7a7a\u6587\u4ef6 \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684hdfs\u4e0a\u67e5\u770b/tmp\u8def\u5f84\u662f\u5426\u5b58\u5728,\u6ca1\u6709\u9700\u8981\u521b\u5efa \u91cd\u542fconfluent, \u542f\u52a8\u4e4b\u540e\u68c0\u67e5 file-source \u662f\u5426\u4e3a\u8ddf\u65b0\u540e\u7684\u914d\u7f6e\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u4f7f\u7528 confluent unload file-source \u5378\u8f7d\u91cd\u65b0 confluent load file-source \u52a0\u8f7d \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dhdfs-sink confluent load hdfs-sink -d /opt/confluent/confluent-4.1.0/etc/kafka-connect-hdfs/quickstart-hdfs.properties \u6253\u5f00/opt/confluent/confluent-4.1.0/test_hdfs.txt\u6587\u4ef6,\u8f93\u5165\u4ee5\u4e0b\u4fe1\u606f \u5230\u5bf9\u63a5\u96c6\u7fa4kafka\u7aef\u6d88\u8d39\u7528\u4e8e\u4f20\u8f93\u7684topic test_hdfs \u767b\u5f55\u5230\u5bf9\u63a5\u96c6\u7fa4\u7684hdfs\u4e0a/tmp\u8def\u5f84\u4e0b\u67e5\u770b\u7ed3\u679c","title":"5.5.0 <--> 8.0"},{"location":"Data_Integration/Confluent_4_1_0/#confluentfusioninsight","text":"","title":"Confluent\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Confluent_4_1_0/#_1","text":"Confluent 4.1.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka) Confluent 4.1.0 \u2194 FusionInsight HD 6.5 (HDFS/Kafka) Confluent 5.5.0 \u2194 FusionInsight MRS 8.0 (HDFS/Kafka)","title":"\u4f7f\u7528\u573a\u666f"},{"location":"Data_Integration/Confluent_4_1_0/#mrs-80","text":"\u8bf4\u660e\uff1a Confluent 5.5.0\u5bf9\u63a5FusionInsight MRS 8.0\u65f6KSQL\u529f\u80fd\u4e0d\u652f\u6301","title":"MRS 8.0 \u5bf9\u63a5\u8bf4\u660e"},{"location":"Data_Integration/Confluent_4_1_0/#confluent","text":"","title":"\u5b89\u88c5Confluent"},{"location":"Data_Integration/Confluent_4_1_0/#_2","text":"\u767b\u5f55Confluent\u5b98\u65b9\u7f51\u7ad9\u4e0b\u8f7d\u9875\u9762\uff1a https://www.confluent.io/previous-versions/?_ga=2.102961223.611794173.1561088831-1783953529.1561088831 \u627e\u5230\u76f8\u5e94\u7684\u7248\u672c\u4e0b\u8f7d \u5c06\u4e0b\u8f7d\u7684\u5f00\u6e90\u538b\u7f29\u5305\u4f7f\u7528WinSCP\u5de5\u5177\u4e0a\u4f20\u81f3linux\u4e3b\u673a\uff0c\u4f7f\u7528 tar -xvf confluent-oss-4.1.0-2.11.tar \u89e3\u538b \u589e\u52a0confluent\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \uff0c\u589e\u52a0confluent bin\u76ee\u5f55\u5230PATH\u73af\u5883\u53d8\u91cf\u4e2d\uff0c\u5b8c\u6210\u540e\u4f7f\u7528 source ~/.bashrc \u4f7f\u4e4b\u751f\u6548","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/#confluent_1","text":"\u8bf4\u660e\uff1aConfluent\u542f\u52a8\u65f6\u4f1a\u8d77\u81ea\u5df1\u7684zookeeper\u548ckafka\u670d\u52a1\uff0c\u8fd9\u91cc\u4e0d\u505a\u4fee\u6539\u3002\u9700\u8981\u66f4\u6539\u7684\u662fconnect\uff0c schema-registry\u4ee5\u53caksql\u670d\u52a1\u914d\u7f6e\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u670d\u52a1\u76f4\u63a5\u5bf9\u63a5FusionInsight HD\u5b89\u5168\u6a21\u5f0f\u7684zookeeper\u548ckafka\u670d\u52a1","title":"\u914d\u7f6eConfluent"},{"location":"Data_Integration/Confluent_4_1_0/#_3","text":"\u5728confluent\u5b89\u88c5\u76ee\u5f55\\share\\java\u4e0b\u65b0\u5efa\u8def\u5f84\uff0c\u540d\u4e3ahuawei \u5230FusionInsight 6.5.1\u7684kafka\u5ba2\u6237\u7aef\u4e0b\u83b7\u53d6 kafka-clients-1.1.0.jar\uff0c \u6ce8\u610f\u5373\u4f7f\u5bf9\u63a5FI HD 2.8\u7248\u7248\u672c\u4e5f\u9700\u8981\u5bf9\u5e94\u4e8eFI HD 6.5.1\u7684 kafka-clients-1.1.0.jar \u5305\uff0c\u5426\u5219\u4f1a\u62a5kafka\u7248\u672c\u5339\u914d\u76f8\u5173\u95ee\u9898 \u5c06kafka-clients-1.1.0.jar\u62f7\u8d1d\u81f3\u7b2c\u4e00\u6b65\u65b0\u5efa\u7684huawei\u8def\u5f84\u4e0b \u5728\u8def\u5f84/opt/confluent/confluent-4.1.0/bin\u4e0b\u627e\u5230connect-distributed\u6587\u4ef6\uff0c\u8fdb\u884c\u5982\u4e0b\u7f16\u8f91\uff1a \u5728\u9002\u5f53\u4f4d\u7f6e\u6dfb\u52a0KAFKA_OPTS\u7684\u542f\u52a8JVM\u53c2\u6570 \u5177\u4f53\u5185\u5bb9\u4e3a\uff1a export KAFKA_OPTS=\"-Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/user_keytabs/101keytab/krb5.conf -Dkerberos.domain.name=hadoop.hadoop.com\" \u5176\u4e2d-Djava.security.krb5.conf=/opt/user_keytabs/101keytab/krb5.conf\u4e3a\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u7684krb5.conf\u6587\u4ef6\uff0c\u53ef\u5728\u96c6\u7fa4 anager\u9875\u9762\u4e0a\u83b7\u53d6 \u53e6\u5916\u8fd8\u53ef\u4ee5\u6dfb\u52a0 -Dsun.security.krb5.debug=true \u6253\u5f00kerberos\u8ba4\u8bc1\u65e5\u5fd7\u5f00\u5173\uff0c\u8fdb\u884c\u9519\u8bef\u5b9a\u4f4d\u3001\u6392\u67e5 \u5728\u524d\u9762\u4f4d\u7f6e\u5c06huawei\u8def\u5f84\u5f15\u8fdb\u53bb \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-distributed.properties\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/schema-registry/connect-avro-distributed.properties\u914d\u7f6e\u6587\u4ef6\uff0c\u4e0e\u4e0a\u4e00\u6b65\u7c7b\u4f3c\uff1a \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka producer.kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka consumer.kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539/opt/confluent/confluent-4.1.0/bin/ksql-run-class\u6587\u4ef6\u5982\u4e0b\uff0c\u76ee\u7684\u662f\u5728\u8d77ksql-server\u670d\u52a1\u7684\u65f6\u5019\u80fd\u591f\u5728classpath\u91cc\u52a0\u8f7d\u5230\u4e4b\u524d\u5bfc\u5165\u7684\u534e\u4e3akafka jar\u5305\uff1a \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/ksql/ksql-server.properties\u914d\u7f6e\u6587\u4ef6 #bootstrap.servers=localhost:9092 security.protocol = SASL_PLAINTEXT bootstrap.servers=172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 kerberos.domain.name = hadoop.hadoop.com listeners=http://localhost:8088 ksql.server.ui.enabled=true sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ debug=true \\ keyTab=\"/opt/user_keytabs/101keytab/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4f7f\u7528\u547d\u4ee4confluent start\u542f\u52a8","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/#confluent-ksqlfi-hdkafkatopic","text":"\u8bf4\u660e\uff1a \u5bf9\u63a5FusionInsight MRS 8.0\u7248\u672cKSQL\u529f\u80fd\u4e0d\u652f\u6301","title":"\u4f7f\u7528Confluent KSQL\u670d\u52a1\u67e5\u8be2FI HD\u96c6\u7fa4Kafka\u7684Topic"},{"location":"Data_Integration/Confluent_4_1_0/#_4","text":"\u5b8c\u6210 FI HD Kafka\u6837\u4f8b\u4ee3\u7801\u8c03\u8bd5\uff0c\u5177\u4f53\u53c2\u8003 https://support-it.huawei.com/solution-fid-gw/#/Intelligent_Data_Developer_Platform \u4e0b\u8f7d\uff0c\u8c03\u8bd5kafka\u6837\u4f8b\u4ee3\u7801","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Confluent_4_1_0/#_5","text":"\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8KSQL CLI LOG_DIR=/opt/confluent/confluent-4.1.0/ksql_logs /opt/confluent/confluent-4.1.0/bin/ksql http://localhost:8088 \u5728KSQL CLI\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2aSTREAM CREATE STREAM TEST_01 (id BIGINT) \\ WITH (KAFKA_TOPIC='testtopic_01', VALUE_FORMAT='DELIMITED', KEY = 'id'); \u914d\u7f6eKAFKA\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684producer\u4ee3\u7801,\u4f7f\u752821007\u7aef\u53e3\u7684\u5b89\u5168\u6a21\u5f0f\uff1a \u540c\u65f6\u5728KSQL CLI\u4e2d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u8be2\uff0c\u68c0\u67e5\u7ed3\u679c\uff1a select ID from TEST_01;","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/#confluent-connect","text":"","title":"\u4f7f\u7528Confluent connect\u670d\u52a1\u540c\u6b65\u672c\u5730\u6587\u4ef6\u4fe1\u606f"},{"location":"Data_Integration/Confluent_4_1_0/#_6","text":"\u5b8c\u6210Confluent\u5b89\u88c5\uff0c\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Confluent_4_1_0/#_7","text":"\u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-source.properties\u6587\u4ef6\uff1a \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-sink.properties\u6587\u4ef6\uff1a \u5728\u8def\u5f84/opt/confluent/confluent-4.1.0\u4e0b\u65b0\u5efa\u7a7a\u6587\u4ef6test.txt \u4f7f\u7528confluent start\u547d\u4ee4\u542f\u52a8confluent \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u6dfb\u52a0file-source, file-sink\uff0c \u5b8c\u6210\u540e\u67e5\u770bconnector\u72b6\u6001 confluent load file-source confluent load file-sink confluent status connectors \u5728test.txt\u7a7a\u6587\u4ef6\u4e2d\u8f93\u5165\u4fe1\u606f\uff0c\u4fdd\u5b58\uff0c\u5728\u751f\u6210\u7684test.sink.txt\u6587\u4ef6\u4e2d\u67e5\u770b\u4fe1\u606f\u540c\u6b65\u60c5\u51b5 \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u547d\u4ee4 bin/kafka-console-consumer.sh --topic connect-test --bootstrap-server 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --consumer.config config/consumer.properties -from-beginning \u67e5\u770b\u540c\u6b65kafka\u7684topic","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Confluent_4_1_0/#confluent-connectfusioninsight-hdfs","text":"","title":"\u4f7f\u7528Confluent connect\u670d\u52a1\u540c\u6b65\u672c\u5730\u6587\u4ef6\u5230FusionInsight HDFS"},{"location":"Data_Integration/Confluent_4_1_0/#_8","text":"\u5b8c\u6210Confluent\u5b89\u88c5\uff0c\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Confluent_4_1_0/#_9","text":"\u767b\u5f55\u5bf9\u63a5FusionInsight HD\u96c6\u7fa4\uff0c\u67e5\u770bhdfs\u914d\u7f6e\uff1a \u5b89\u88c5kafka-connect-hdfs connector, \u53c2\u8003\uff1a https://docs.confluent.io/current/connect/managing/install.html \u5728Confluent Hub\u4e0a\u4e0b\u8f7d\u7248\u672c\u5339\u914d\u7684plugin\u5305\uff1a \u5c06\u4e0b\u8f7d\u597d\u7684confluentinc-kafka-connect-hdfs-4.1.0.zip\u538b\u7f29\u5305\u672c\u5730\u89e3\u538b\uff0c\u5e76\u4f7f\u7528WinSCP\u4e0a\u4f20\u5230/opt/confluent/confluent-4.1.0/share/java\u8def\u5f84\u4e0b \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-distributed.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684plugin.path \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/schema-registry/connect-avro-distributed.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684plugin.path \u914d\u7f6e/opt/confluent/confluent-4.1.0/etc/kafka-connect-hdfs/quickstart-hdfs.properties \u4fee\u6539/opt/confluent/confluent-4.1.0/etc/kafka/connect-file-source.properties\u6587\u4ef6\u540c\u6b65\u7684topic\u4e3atest_hdfs: \u5728/opt/confluent/confluent-4.1.0\u8def\u5f84\u4e0b\u65b0\u5efatest_hdfs.txt\u7684\u7a7a\u6587\u4ef6 \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684hdfs\u4e0a\u67e5\u770b/tmp\u8def\u5f84\u662f\u5426\u5b58\u5728,\u6ca1\u6709\u9700\u8981\u521b\u5efa \u91cd\u542fconfluent, \u542f\u52a8\u4e4b\u540e\u68c0\u67e5 file-source \u662f\u5426\u4e3a\u8ddf\u65b0\u540e\u7684\u914d\u7f6e\uff0c\u5982\u679c\u4e0d\u662f\uff0c\u4f7f\u7528 confluent unload file-source \u5378\u8f7d\u91cd\u65b0 confluent load file-source \u52a0\u8f7d \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dhdfs-sink confluent load hdfs-sink -d /opt/confluent/confluent-4.1.0/etc/kafka-connect-hdfs/quickstart-hdfs.properties \u6253\u5f00/opt/confluent/confluent-4.1.0/test_hdfs.txt\u6587\u4ef6,\u8f93\u5165\u4ee5\u4e0b\u4fe1\u606f \u5230\u5bf9\u63a5\u96c6\u7fa4kafka\u7aef\u6d88\u8d39\u7528\u4e8e\u4f20\u8f93\u7684topic test_hdfs \u767b\u5f55\u5230\u5bf9\u63a5\u96c6\u7fa4\u7684hdfs\u4e0a/tmp\u8def\u5f84\u4e0b\u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/DEBEZIUM/","text":"debezium connector\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 debezium 1.0.0 \u2194 FusionInsight HD 6.5 (Kafka) debezium 1.2.2 \u2194 FusionInsight MRS 8.0 (Kafka) \u6d4b\u8bd5\u73af\u5883\u8bf4\u660e\uff1a FI HD: 6.5.1 (kafka\u5b89\u5168\u6a21\u5f0f) confluent: 5.3.1 debezium: 1.0.0 \u573a\u666f\u8bf4\u660e\uff1aconfluent\u6700\u65b0\u7248\u672c5.3.1\u80fd\u591f\u76f4\u63a5\u4e0b\u8f7ddebezium 1.0.0\u6700\u65b0\u7248\u672c\u7684 database connector\uff0c\u8fd9\u91cc\u4ee5mysql\u4e3a\u4f8b\u4f5c\u4e3a\u6e90\u7aef\uff0c\u540c\u6b65\u540e\u7684\u6570\u636e\u4f20\u5165huawei kafka\u4e2d\uff08\u5b89\u5168\u6a21\u5f0f\uff09 \u5b89\u88c5confluent \u00b6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55Confluent\u5b98\u65b9\u7f51\u7ad9\u4e0b\u8f7d\u9875\u9762\uff1a https://www.confluent.io/previous-versions/?_ga=2.102961223.611794173.1561088831-1783953529.1561088831 \u4e0b\u8f7d\u6700\u65b0\u7248\u672c5.3.1 \u5c06\u4e0b\u8f7d\u7684\u5f00\u6e90\u538b\u7f29\u5305\u4f7f\u7528WinSCP\u5de5\u5177\u4e0a\u4f20\u81f3linux\u4e3b\u673a\uff0c\u4f7f\u7528 tar -xvf confluent-5.3.1-2.12.tar.gz \u89e3\u538b \u589e\u52a0confluent\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \uff0c\u589e\u52a0confluent bin\u76ee\u5f55\u5230PATH\u73af\u5883\u53d8\u91cf\u4e2d\uff0c\u5b8c\u6210\u540e\u4f7f\u7528 source ~/.bashrc \u4f7f\u4e4b\u751f\u6548 \u914d\u7f6eConfluent \u00b6 \u8bf4\u660e\uff1aConfluent\u542f\u52a8\u65f6\u4f1a\u8d77\u81ea\u5df1\u7684zookeeper\u548ckafka\u670d\u52a1\uff0c\u8fd9\u91cc\u4e0d\u505a\u4fee\u6539\u3002\u9700\u8981\u66f4\u6539\u7684\u662fconnect\uff0c schema-registry\u670d\u52a1\u914d\u7f6e\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u670d\u52a1\u76f4\u63a5\u5bf9\u63a5FusionInsight HD\u5b89\u5168\u6a21\u5f0f\u7684zookeeper\u548ckafka\u670d\u52a1 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728 confluent\u5b89\u88c5\u76ee\u5f55\\share\\java \u4e0b\u65b0\u5efa\u8def\u5f84\uff0c\u540d\u4e3ahuawei \u5230FusionInsight 6.5.1\u7684kafka\u5ba2\u6237\u7aef\u4e0b\u6240\u6709kafka\u76f8\u5173jar\u5305\u62f7\u8d1d\u5230huawei\u8def\u5f84\u4e0b cp /opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/*.jar /opt/confluent/confluent-5.3.1/share/java/huawei/ \u5728\u8def\u5f84 /opt/confluent/confluent-5.3.1/bin \u4e0b\u627e\u5230connect-distributed\u6587\u4ef6\uff0c\u8fdb\u884c\u5982\u4e0b\u7f16\u8f91\uff1a \u5728\u9002\u5f53\u4f4d\u7f6e\u6dfb\u52a0KAFKA_OPTS\u7684\u542f\u52a8JVM\u53c2\u6570 \u5177\u4f53\u5185\u5bb9\u4e3a\uff1a export KAFKA_OPTS=\"-Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/krb5.conf -Dkerberos.domain.name=hadoop.hadoop.com\" \u5176\u4e2d-Djava.security.krb5.conf=/opt/krb5.conf\u4e3a\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u7684krb5.conf\u6587\u4ef6\uff0c\u53ef\u5728\u96c6\u7fa4Manager\u9875\u9762\u4e0a\u83b7\u53d6 \u53e6\u5916\u8fd8\u53ef\u4ee5\u6dfb\u52a0 -Dsun.security.krb5.debug=true \u6253\u5f00kerberos\u8ba4\u8bc1\u65e5\u5fd7\u5f00\u5173\uff0c\u8fdb\u884c\u9519\u8bef\u5b9a\u4f4d\u3001\u6392\u67e5 \u5728\u524d\u9762\u4f4d\u7f6e\u5c06huawei\u8def\u5f84\u5f15\u8fdb\u53bb \u4fee\u6539 /opt/confluent/confluent-5.3.1/etc/kafka/connect-distributed.properties \u914d\u7f6e\u6587\u4ef6 \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539 /opt/confluent/confluent-5.3.1/etc/schema-registry/connect-avro-distributed.properties \u914d\u7f6e\u6587\u4ef6\uff0c\u4e0e\u4e0a\u4e00\u6b65\u7c7b\u4f3c\uff1a \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor producer.confluent.monitoring.interceptor.sasl.mechanism=GSSAPI producer.confluent.monitoring.interceptor.security.protocol=SASL_PLAINTEXT producer.confluent.monitoring.interceptor.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor consumer.confluent.monitoring.interceptor.sasl.mechanism=GSSAPI consumer.confluent.monitoring.interceptor.security.protocol=SASL_PLAINTEXT consumer.confluent.monitoring.interceptor.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u53c2\u8003confluent\u5b98\u65b9\u6587\u6863\uff1a https://docs.confluent.io/current/quickstart/ce-quickstart.html#ce-quickstart \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728confluent\u7684bin\u76ee\u5f55\u4e0b\u5f15\u5165confluent\u6267\u884c\u6587\u4ef6 curl -L https://cnfl.io/cli | sh -s -- -b /opt/confluent/confluent-5.3.1/bin \u4f7f\u7528\u547d\u4ee4 bin/confluent local start start\u547d\u4ee4\u542f\u52a8confluent \u6ce8\u610f\uff1a\u540c\u4e4b\u524d\u7684confluent 4.1.0\u7248\u672c\u76f8\u6bd4 \u4e4b\u524d\u7684 confluent start \u547d\u4ee4\u6539\u4e3a\u4e86 conflueng local start \u573a\u666f1. \u6e90\u7aefmysql\u6570\u636e\u5b9e\u65f6\u540c\u6b65\u5230Kafka \u00b6 \u914d\u7f6e\u6e90\u7aefmysql \u00b6 \u5177\u4f53\u5b89\u88c5\u8fc7\u7a0b\u53c2\u8003mysql\u5b98\u65b9\u6587\u6863 \u8bf4\u660e\uff1a\u505a\u5b9e\u65f6\u6570\u636e\u5e93\u540c\u6b65\u7684\u65f6\u5019\u9700\u8981mysql\u6253\u5f00bin\u65e5\u5fd7\uff08binary log\uff09 \u53c2\u8003confluent\u5b98\u65b9\u6587\u6863\uff1a https://docs.confluent.io/current/connect/debezium-connect-mysql/index.html \u6838\u5fc3\u914d\u7f6e\u5982\u4e0b\uff0c\u5728Mysql\u4e3b\u673a\u7684/etc/my.cnf\u7684mysqld\u4e0b\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e,\u6253\u5f00mysql bin\u65e5\u5fd7 [mysqld] server-id = 223344 log_bin = mysql-bin binlog_format = row binlog_row_image = full expire_logs_days = 10 \u7136\u540e\uff0c\u91cd\u542f\u4e00\u4e0bMysql\u4ee5\u4f7f\u5f97binlog\u751f\u6548\u3002 systemctl start mysqld.service \u4f7f\u7528\u547d\u4ee4 mysql -u root -p \u767b\u9646mysql\uff0c\u9700\u8981\u8f93\u5165\u767b\u9646\u5bc6\u7801 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadatabase\u540d\u5b57\u53eb\u505a test CREATE DATABASE test; \u4f7f\u7528\u5982\u4e0b\u5efa\u8868\u8bed\u53e5\uff0c\u521b\u5efa\u8868tb_dept create table tb_dept( Id int primary key, Name varchar(18) ); \u914d\u7f6edebezium mysql connector \u00b6 \u540c\u6837\u53c2\u8003confluent\u5b98\u65b9\u6587\u6863\uff1a https://docs.confluent.io/current/connect/debezium-connect-mysql/index.html \u5b89\u88c5\u6700\u65b0\u7248\u672c\u7684debezium mysql connector: \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff1a bin/confluent-hub install debezium/debezium-connector-mysql:1.0.0 \u91cd\u542fconfluent\u4f7f\u7528\u547d\u4ee4\u67e5\u770bdebezium mysql connector\u662f\u5426\u5b89\u88c5\u6210\u529f\uff1a curl -sS localhost:8083/connector-plugins | grep mysql confluent 5.3.1\u7248\u672c\u52a0\u8f7d\u65b0\u7684connector\u90fd\u662f\u4ee5curl\u547d\u4ee4\u901a\u8fc7rest\u7684\u65b9\u5f0f\u8fdb\u884c\u7684\uff0c\u9996\u5148\u73b0\u5728confluent\u672c\u673a\u7684/opt\u8def\u5f84\u4e0b\u521b\u5efadebezium mysql connector\u7684\u914d\u7f6ejson\u6587\u4ef6\uff0c\u540d\u5b57\u53ebregister-mysql-huawei2.json\uff1a \u5185\u5bb9\u5982\u4e0b\uff1a { \"name\": \"inventory-connector2\", \"config\": { \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"key.converter.schema.registry.url\": \"http://172-16-2-124:8081\", \"value.converter.schema.registry.url\": \"http://172-16-2-124:8081\", \"database.hostname\": \"172-16-2-124\", \"database.port\": \"3306\", \"database.user\": \"root\", \"database.password\": \"Huawei@123\", \"database.server.id\": \"223344\", \"database.server.name\": \"dbserver1\", \"database.whitelist\": \"test\", \"table.whitlelist\" : \"tb_dept\", \"database.history.producer.security.protocol\": \"SASL_PLAINTEXT\", \"database.history.consumer.security.protocol\":\"SASL_PLAINTEXT\", \"database.history.kafka.bootstrap.servers\": \"172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007\", \"database.history.kafka.topic\": \"schema-changes.test\", \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\", \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\", \"value.converter.schemas.enable\": \"true\", \"transforms\": \"unwrap\", \"transforms.unwrap.type\": \"io.debezium.transforms.UnwrapFromEnvelope\", \"transforms\": \"route\", \"transforms.route.type\": \"org.apache.kafka.connect.transforms.RegexRouter\", \"transforms.route.regex\": \"([^.]+)\\\\.([^.]+)\\\\.([^.]+)\", \"transforms.route.replacement\": \"$3\" } } \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5c06\u4e0a\u8ff0\u914d\u7f6e\u7684debezium mysql connector\u88c5\u8f7d\u5230confluent \u4e0a\u53bb curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" http://localhost:8083/connectors/ -d @/opt/register-mysql-huawei2.json \u767b\u9646confluent /tmp\u76ee\u5f55\u4e0b\uff0c\u5728\u5bf9\u5e94\u7684confluent\u65e5\u5fd7\u8def\u5f84\u4e0b\u67e5\u770bconnect.stout\u65e5\u5fd7\u68c0\u67e5\u662f\u5426\u88c5\u8f7d\u6210\u529f\uff1a \u5728mysql\u6e90\u7aef\u4f7f\u7528 insert into tb_dept values(1,'aaa'); \u63d2\u5165\u4e00\u6761\u6570\u636e\uff0c\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39\u5bf9\u5e94\u540c\u6b65topic\u540d\u5b57\u53eb\u505atb_dept: \u5728mysql\u6e90\u7aef\u4f7f\u7528 UPDATE test.tb_dept SET Name='Anne Marie' WHERE id=1; \u5bf9\u7b2c\u4e00\u6761\u6570\u636e\u505aupdate\u64cd\u4f5c\uff0c\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39\u5bf9\u5e94\u540c\u6b65topic\u540d\u5b57\u53eb\u505atb_dept: \u5728mysql\u6e90\u7aef\u4f7f\u7528 delete from tb_dept where Id=1; \u5bf9\u7b2c\u4e00\u6761\u6570\u636e\u505adelete\u64cd\u4f5c\uff0c\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39\u5bf9\u5e94\u540c\u6b65topic\u540d\u5b57\u53eb\u505atb_dept: \u8bc1\u660e\u4f7f\u7528debezium mysql connector\u53ef\u4ee5\u5c06Mysql\u6e90\u7aef\u6570\u636e\u5e93\u589e\u3001\u5220\u3001\u6539\u64cd\u4f5c\u8bb0\u5f55\u4e0b\u6765\u5e76\u53d1\u5230\u5bf9\u5e94\u7684FI hd\u7684kafka\u4e2d \u4f7f\u7528\u547d\u4ee4 bin/confluent local status connectors \u67e5\u770b\u88c5\u8f7d\u6210\u529f\u7684connector\u72b6\u6001 \u4f7f\u7528\u547d\u4ee4 bin/confluent local unload inventory-connector2 \u5c06\u6682\u65f6\u4e0d\u9700\u8981\u7684connector\u79fb\u9664","title":"1.2.2 <--> 8.0"},{"location":"Data_Integration/DEBEZIUM/#debezium-connectorfusioninsight","text":"","title":"debezium connector\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/DEBEZIUM/#_1","text":"debezium 1.0.0 \u2194 FusionInsight HD 6.5 (Kafka) debezium 1.2.2 \u2194 FusionInsight MRS 8.0 (Kafka) \u6d4b\u8bd5\u73af\u5883\u8bf4\u660e\uff1a FI HD: 6.5.1 (kafka\u5b89\u5168\u6a21\u5f0f) confluent: 5.3.1 debezium: 1.0.0 \u573a\u666f\u8bf4\u660e\uff1aconfluent\u6700\u65b0\u7248\u672c5.3.1\u80fd\u591f\u76f4\u63a5\u4e0b\u8f7ddebezium 1.0.0\u6700\u65b0\u7248\u672c\u7684 database connector\uff0c\u8fd9\u91cc\u4ee5mysql\u4e3a\u4f8b\u4f5c\u4e3a\u6e90\u7aef\uff0c\u540c\u6b65\u540e\u7684\u6570\u636e\u4f20\u5165huawei kafka\u4e2d\uff08\u5b89\u5168\u6a21\u5f0f\uff09","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/DEBEZIUM/#confluent","text":"","title":"\u5b89\u88c5confluent"},{"location":"Data_Integration/DEBEZIUM/#_2","text":"\u767b\u5f55Confluent\u5b98\u65b9\u7f51\u7ad9\u4e0b\u8f7d\u9875\u9762\uff1a https://www.confluent.io/previous-versions/?_ga=2.102961223.611794173.1561088831-1783953529.1561088831 \u4e0b\u8f7d\u6700\u65b0\u7248\u672c5.3.1 \u5c06\u4e0b\u8f7d\u7684\u5f00\u6e90\u538b\u7f29\u5305\u4f7f\u7528WinSCP\u5de5\u5177\u4e0a\u4f20\u81f3linux\u4e3b\u673a\uff0c\u4f7f\u7528 tar -xvf confluent-5.3.1-2.12.tar.gz \u89e3\u538b \u589e\u52a0confluent\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \uff0c\u589e\u52a0confluent bin\u76ee\u5f55\u5230PATH\u73af\u5883\u53d8\u91cf\u4e2d\uff0c\u5b8c\u6210\u540e\u4f7f\u7528 source ~/.bashrc \u4f7f\u4e4b\u751f\u6548","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/DEBEZIUM/#confluent_1","text":"\u8bf4\u660e\uff1aConfluent\u542f\u52a8\u65f6\u4f1a\u8d77\u81ea\u5df1\u7684zookeeper\u548ckafka\u670d\u52a1\uff0c\u8fd9\u91cc\u4e0d\u505a\u4fee\u6539\u3002\u9700\u8981\u66f4\u6539\u7684\u662fconnect\uff0c schema-registry\u670d\u52a1\u914d\u7f6e\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u670d\u52a1\u76f4\u63a5\u5bf9\u63a5FusionInsight HD\u5b89\u5168\u6a21\u5f0f\u7684zookeeper\u548ckafka\u670d\u52a1","title":"\u914d\u7f6eConfluent"},{"location":"Data_Integration/DEBEZIUM/#_3","text":"\u5728 confluent\u5b89\u88c5\u76ee\u5f55\\share\\java \u4e0b\u65b0\u5efa\u8def\u5f84\uff0c\u540d\u4e3ahuawei \u5230FusionInsight 6.5.1\u7684kafka\u5ba2\u6237\u7aef\u4e0b\u6240\u6709kafka\u76f8\u5173jar\u5305\u62f7\u8d1d\u5230huawei\u8def\u5f84\u4e0b cp /opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/*.jar /opt/confluent/confluent-5.3.1/share/java/huawei/ \u5728\u8def\u5f84 /opt/confluent/confluent-5.3.1/bin \u4e0b\u627e\u5230connect-distributed\u6587\u4ef6\uff0c\u8fdb\u884c\u5982\u4e0b\u7f16\u8f91\uff1a \u5728\u9002\u5f53\u4f4d\u7f6e\u6dfb\u52a0KAFKA_OPTS\u7684\u542f\u52a8JVM\u53c2\u6570 \u5177\u4f53\u5185\u5bb9\u4e3a\uff1a export KAFKA_OPTS=\"-Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/opt/krb5.conf -Dkerberos.domain.name=hadoop.hadoop.com\" \u5176\u4e2d-Djava.security.krb5.conf=/opt/krb5.conf\u4e3a\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u7684krb5.conf\u6587\u4ef6\uff0c\u53ef\u5728\u96c6\u7fa4Manager\u9875\u9762\u4e0a\u83b7\u53d6 \u53e6\u5916\u8fd8\u53ef\u4ee5\u6dfb\u52a0 -Dsun.security.krb5.debug=true \u6253\u5f00kerberos\u8ba4\u8bc1\u65e5\u5fd7\u5f00\u5173\uff0c\u8fdb\u884c\u9519\u8bef\u5b9a\u4f4d\u3001\u6392\u67e5 \u5728\u524d\u9762\u4f4d\u7f6e\u5c06huawei\u8def\u5f84\u5f15\u8fdb\u53bb \u4fee\u6539 /opt/confluent/confluent-5.3.1/etc/kafka/connect-distributed.properties \u914d\u7f6e\u6587\u4ef6 \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u4fee\u6539 /opt/confluent/confluent-5.3.1/etc/schema-registry/connect-avro-distributed.properties \u914d\u7f6e\u6587\u4ef6\uff0c\u4e0e\u4e0a\u4e00\u6b65\u7c7b\u4f3c\uff1a \u4fee\u6539bootstrap.servers\u4e3a\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5730\u5740 \u5728\u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\u5185\u5bb9\u5982\u4e0b,\u914d\u7f6eKerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a sasl.mechanism=GSSAPI sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com security.protocol=SASL_PLAINTEXT sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.sasl.mechanism=GSSAPI producer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT producer.security.protocol=SASL_PLAINTEXT producer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.sasl.mechanism=GSSAPI consumer.sasl.kerberos.service.name=kafka kerberos.domain.name=hadoop.hadoop.com # Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT consumer.security.protocol=SASL_PLAINTEXT consumer.sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor producer.confluent.monitoring.interceptor.sasl.mechanism=GSSAPI producer.confluent.monitoring.interceptor.security.protocol=SASL_PLAINTEXT producer.confluent.monitoring.interceptor.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor consumer.confluent.monitoring.interceptor.sasl.mechanism=GSSAPI consumer.confluent.monitoring.interceptor.security.protocol=SASL_PLAINTEXT consumer.confluent.monitoring.interceptor.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=\"/opt/user.keytab\" \\ principal=\"developuser@HADOOP.COM\"; \u53c2\u8003confluent\u5b98\u65b9\u6587\u6863\uff1a https://docs.confluent.io/current/quickstart/ce-quickstart.html#ce-quickstart \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728confluent\u7684bin\u76ee\u5f55\u4e0b\u5f15\u5165confluent\u6267\u884c\u6587\u4ef6 curl -L https://cnfl.io/cli | sh -s -- -b /opt/confluent/confluent-5.3.1/bin \u4f7f\u7528\u547d\u4ee4 bin/confluent local start start\u547d\u4ee4\u542f\u52a8confluent \u6ce8\u610f\uff1a\u540c\u4e4b\u524d\u7684confluent 4.1.0\u7248\u672c\u76f8\u6bd4 \u4e4b\u524d\u7684 confluent start \u547d\u4ee4\u6539\u4e3a\u4e86 conflueng local start","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/DEBEZIUM/#1-mysqlkafka","text":"","title":"\u573a\u666f1. \u6e90\u7aefmysql\u6570\u636e\u5b9e\u65f6\u540c\u6b65\u5230Kafka"},{"location":"Data_Integration/DEBEZIUM/#mysql","text":"\u5177\u4f53\u5b89\u88c5\u8fc7\u7a0b\u53c2\u8003mysql\u5b98\u65b9\u6587\u6863 \u8bf4\u660e\uff1a\u505a\u5b9e\u65f6\u6570\u636e\u5e93\u540c\u6b65\u7684\u65f6\u5019\u9700\u8981mysql\u6253\u5f00bin\u65e5\u5fd7\uff08binary log\uff09 \u53c2\u8003confluent\u5b98\u65b9\u6587\u6863\uff1a https://docs.confluent.io/current/connect/debezium-connect-mysql/index.html \u6838\u5fc3\u914d\u7f6e\u5982\u4e0b\uff0c\u5728Mysql\u4e3b\u673a\u7684/etc/my.cnf\u7684mysqld\u4e0b\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e,\u6253\u5f00mysql bin\u65e5\u5fd7 [mysqld] server-id = 223344 log_bin = mysql-bin binlog_format = row binlog_row_image = full expire_logs_days = 10 \u7136\u540e\uff0c\u91cd\u542f\u4e00\u4e0bMysql\u4ee5\u4f7f\u5f97binlog\u751f\u6548\u3002 systemctl start mysqld.service \u4f7f\u7528\u547d\u4ee4 mysql -u root -p \u767b\u9646mysql\uff0c\u9700\u8981\u8f93\u5165\u767b\u9646\u5bc6\u7801 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadatabase\u540d\u5b57\u53eb\u505a test CREATE DATABASE test; \u4f7f\u7528\u5982\u4e0b\u5efa\u8868\u8bed\u53e5\uff0c\u521b\u5efa\u8868tb_dept create table tb_dept( Id int primary key, Name varchar(18) );","title":"\u914d\u7f6e\u6e90\u7aefmysql"},{"location":"Data_Integration/DEBEZIUM/#debezium-mysql-connector","text":"\u540c\u6837\u53c2\u8003confluent\u5b98\u65b9\u6587\u6863\uff1a https://docs.confluent.io/current/connect/debezium-connect-mysql/index.html \u5b89\u88c5\u6700\u65b0\u7248\u672c\u7684debezium mysql connector: \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\uff1a bin/confluent-hub install debezium/debezium-connector-mysql:1.0.0 \u91cd\u542fconfluent\u4f7f\u7528\u547d\u4ee4\u67e5\u770bdebezium mysql connector\u662f\u5426\u5b89\u88c5\u6210\u529f\uff1a curl -sS localhost:8083/connector-plugins | grep mysql confluent 5.3.1\u7248\u672c\u52a0\u8f7d\u65b0\u7684connector\u90fd\u662f\u4ee5curl\u547d\u4ee4\u901a\u8fc7rest\u7684\u65b9\u5f0f\u8fdb\u884c\u7684\uff0c\u9996\u5148\u73b0\u5728confluent\u672c\u673a\u7684/opt\u8def\u5f84\u4e0b\u521b\u5efadebezium mysql connector\u7684\u914d\u7f6ejson\u6587\u4ef6\uff0c\u540d\u5b57\u53ebregister-mysql-huawei2.json\uff1a \u5185\u5bb9\u5982\u4e0b\uff1a { \"name\": \"inventory-connector2\", \"config\": { \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\", \"tasks.max\": \"1\", \"key.converter.schema.registry.url\": \"http://172-16-2-124:8081\", \"value.converter.schema.registry.url\": \"http://172-16-2-124:8081\", \"database.hostname\": \"172-16-2-124\", \"database.port\": \"3306\", \"database.user\": \"root\", \"database.password\": \"Huawei@123\", \"database.server.id\": \"223344\", \"database.server.name\": \"dbserver1\", \"database.whitelist\": \"test\", \"table.whitlelist\" : \"tb_dept\", \"database.history.producer.security.protocol\": \"SASL_PLAINTEXT\", \"database.history.consumer.security.protocol\":\"SASL_PLAINTEXT\", \"database.history.kafka.bootstrap.servers\": \"172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007\", \"database.history.kafka.topic\": \"schema-changes.test\", \"key.converter\": \"org.apache.kafka.connect.json.JsonConverter\", \"value.converter\": \"org.apache.kafka.connect.json.JsonConverter\", \"value.converter.schemas.enable\": \"true\", \"transforms\": \"unwrap\", \"transforms.unwrap.type\": \"io.debezium.transforms.UnwrapFromEnvelope\", \"transforms\": \"route\", \"transforms.route.type\": \"org.apache.kafka.connect.transforms.RegexRouter\", \"transforms.route.regex\": \"([^.]+)\\\\.([^.]+)\\\\.([^.]+)\", \"transforms.route.replacement\": \"$3\" } } \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5c06\u4e0a\u8ff0\u914d\u7f6e\u7684debezium mysql connector\u88c5\u8f7d\u5230confluent \u4e0a\u53bb curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" http://localhost:8083/connectors/ -d @/opt/register-mysql-huawei2.json \u767b\u9646confluent /tmp\u76ee\u5f55\u4e0b\uff0c\u5728\u5bf9\u5e94\u7684confluent\u65e5\u5fd7\u8def\u5f84\u4e0b\u67e5\u770bconnect.stout\u65e5\u5fd7\u68c0\u67e5\u662f\u5426\u88c5\u8f7d\u6210\u529f\uff1a \u5728mysql\u6e90\u7aef\u4f7f\u7528 insert into tb_dept values(1,'aaa'); \u63d2\u5165\u4e00\u6761\u6570\u636e\uff0c\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39\u5bf9\u5e94\u540c\u6b65topic\u540d\u5b57\u53eb\u505atb_dept: \u5728mysql\u6e90\u7aef\u4f7f\u7528 UPDATE test.tb_dept SET Name='Anne Marie' WHERE id=1; \u5bf9\u7b2c\u4e00\u6761\u6570\u636e\u505aupdate\u64cd\u4f5c\uff0c\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39\u5bf9\u5e94\u540c\u6b65topic\u540d\u5b57\u53eb\u505atb_dept: \u5728mysql\u6e90\u7aef\u4f7f\u7528 delete from tb_dept where Id=1; \u5bf9\u7b2c\u4e00\u6761\u6570\u636e\u505adelete\u64cd\u4f5c\uff0c\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39\u5bf9\u5e94\u540c\u6b65topic\u540d\u5b57\u53eb\u505atb_dept: \u8bc1\u660e\u4f7f\u7528debezium mysql connector\u53ef\u4ee5\u5c06Mysql\u6e90\u7aef\u6570\u636e\u5e93\u589e\u3001\u5220\u3001\u6539\u64cd\u4f5c\u8bb0\u5f55\u4e0b\u6765\u5e76\u53d1\u5230\u5bf9\u5e94\u7684FI hd\u7684kafka\u4e2d \u4f7f\u7528\u547d\u4ee4 bin/confluent local status connectors \u67e5\u770b\u88c5\u8f7d\u6210\u529f\u7684connector\u72b6\u6001 \u4f7f\u7528\u547d\u4ee4 bin/confluent local unload inventory-connector2 \u5c06\u6682\u65f6\u4e0d\u9700\u8981\u7684connector\u79fb\u9664","title":"\u914d\u7f6edebezium mysql connector"},{"location":"Data_Integration/DataX/","text":"DataX \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DataX 0.1 \u2194 FusionInsight HD 6.5 (HDFS) DataX 0.1 \u2194 FusionInsight MRS 8.0 (HDFS) \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7d\u89e3\u538b\u5b89\u88c5DataX wget http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz tar -zxvf datax.tar.gz \u5728FusionInsight\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u4fe1\u606f\uff0c\u5c06krb5.conf\u653e\u5165/etc\u4e0b \u5728FusionInsight\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u4fe1\u606f\uff0c\u5c06user.keytab\u653e\u5165/opt\u4e0b \u8bfb\u53d6HDFS \u00b6 \u65b0\u589ehdfsread.json\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"job\" : { \"setting\" : { \"speed\" : { \"channel\" : 3 } }, \"content\" : [ { \"reader\" : { \"name\" : \"hdfsreader\" , \"parameter\" : { \"path\" : \"/user/developuser/datax/*\" , \"defaultFS\" : \"hdfs://hacluster\" , \"column\" : [ { \"index\" : 0 , \"type\" : \"string\" }, { \"index\" : 1 , \"type\" : \"string\" }, { \"type\" : \"string\" , \"value\" : \"hello\" }, { \"index\" : 2 , \"type\" : \"string\" } ], \"fileType\" : \"csv\" , \"encoding\" : \"UTF-8\" , \"fieldDelimiter\" : \",\" , \"haveKerberos\" : \"true\" , \"kerberosKeytabFilePath\" : \"/opt/user.keytab\" , \"kerberosPrincipal\" : \"developuser@HADOOP.COM\" , \"hadoopConfig\" :{ \"dfs.nameservices\" : \"hacluster\" , \"dfs.ha.namenodes.hacluster\" : \"15,16\" , \"dfs.namenode.rpc-address.hacluster.15\" : \"172.16.10.132:25000\" , \"dfs.namenode.rpc-address.hacluster.16\" : \"172.16.10.133:25000\" , \"dfs.client.failover.proxy.provider.hacluster\" : \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\" , \"hadoop.security.authentication\" : \"Kerberos\" , \"hadoop.rpc.protection\" : \"privacy\" } } }, \"writer\" : { \"name\" : \"streamwriter\" , \"parameter\" : { \"print\" : true } } } ] } } \u5176\u4e2d\u51e0\u4e2a\u91cd\u8981\u7684\u53c2\u8003\u914d\u7f6e\u65b9\u5f0f\u5982\u4e0b\uff1a \u53c2\u6570\u540d\u79f0 \u8bf4\u660e defaultFS FusionInsight\u96c6\u7fa4\u9ed8\u8ba4\u4e3ahacluster haveKerberos true kerberosKeytabFilePath keytab\u6587\u4ef6\u8def\u5f84\uff0c\u5982\uff1a/opt/user.keytab kerberosPrincipal kerberos\u7528\u6237\uff0c\u5982\uff1a developuser@HADOOP.COM hadoop.security.authentication \u53c2\u8003\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5982\uff1aKerberos hadoop.rpc.protection \u53c2\u8003\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5982\uff1aprivacy dfs.nameservices \u53c2\u8003\u96c6\u7fa4\u5ba2\u6237\u7aef\u7684hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u5982hacluster dfs.ha.namenodes.hacluster \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.namenode.rpc-address.hacluster.15 \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.namenode.rpc-address.hacluster.16 \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.client.failover.proxy.provider.hacluster \u9ed8\u8ba4\u586b\u5199org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u6267\u884cDataX\u7684\u4efb\u52a1 python datax.py ../hdfs_read.json \u80fd\u591f\u8bfb\u53d6\u51faHDFS\u4e2d\u6587\u4ef6\u7684\u5185\u5bb9 \u5199\u5165HDFS \u00b6 \u65b0\u589ehdfswrite.json\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"setting\" : {}, \"job\" : { \"setting\" : { \"speed\" : { \"channel\" : 2 } }, \"content\" : [ { \"reader\" : { \"name\" : \"txtfilereader\" , \"parameter\" : { \"path\" : [ \"/opt/data.csv\" ], \"encoding\" : \"UTF-8\" , \"column\" : [ { \"index\" : 0 , \"type\" : \"long\" }, { \"index\" : 1 , \"type\" : \"DOUBLE\" }, { \"index\" : 2 , \"type\" : \"STRING\" }, { \"index\" : 3 , \"type\" : \"BOOLEAN\" }, { \"index\" : 4 , \"type\" : \"date\" } ], \"fieldDelimiter\" : \",\" } }, \"writer\" : { \"name\" : \"hdfswriter\" , \"parameter\" : { \"defaultFS\" : \"hdfs://hacluster\" , \"fileType\" : \"orc\" , \"path\" : \"/user/developuser/hdfswrite\" , \"fileName\" : \"hdfsdata.orc\" , \"column\" : [ { \"name\" : \"col1\" , \"type\" : \"TINYINT\" }, { \"name\" : \"col2\" , \"type\" : \"DOUBLE\" }, { \"name\" : \"col3\" , \"type\" : \"CHAR\" }, { \"name\" : \"col4\" , \"type\" : \"BOOLEAN\" }, { \"name\" : \"col5\" , \"type\" : \"DATE\" } ], \"writeMode\" : \"append\" , \"fieldDelimiter\" : \",\" , \"compress\" : \"NONE\" , \"haveKerberos\" : \"true\" , \"kerberosKeytabFilePath\" : \"/opt/user.keytab\" , \"kerberosPrincipal\" : \"developuser@HADOOP.COM\" , \"hadoopConfig\" :{ \"dfs.nameservices\" : \"hacluster\" , \"dfs.ha.namenodes.hacluster\" : \"15,16\" , \"dfs.namenode.rpc-address.hacluster.15\" : \"172.16.10.132:25000\" , \"dfs.namenode.rpc-address.hacluster.16\" : \"172.16.10.133:25000\" , \"dfs.client.failover.proxy.provider.hacluster\" : \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\" , \"hadoop.security.authentication\" : \"Kerberos\" , \"hadoop.rpc.protection\" : \"privacy\" } } } } ] } } \u5176\u4e2d\u51e0\u4e2a\u91cd\u8981\u7684\u53c2\u8003\u914d\u7f6e\u65b9\u5f0f\u5982\u4e0b\uff1a \u53c2\u6570\u540d\u79f0 \u8bf4\u660e defaultFS FusionInsight\u96c6\u7fa4\u9ed8\u8ba4\u4e3ahacluster haveKerberos true kerberosKeytabFilePath keytab\u6587\u4ef6\u8def\u5f84\uff0c\u5982\uff1a/opt/user.keytab kerberosPrincipal kerberos\u7528\u6237\uff0c\u5982\uff1a developuser@HADOOP.COM hadoop.security.authentication \u53c2\u8003\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5982\uff1aKerberos hadoop.rpc.protection \u53c2\u8003\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5982\uff1aprivacy dfs.nameservices \u53c2\u8003\u96c6\u7fa4\u5ba2\u6237\u7aef\u7684hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u5982hacluster dfs.ha.namenodes.hacluster \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.namenode.rpc-address.hacluster.15 \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.namenode.rpc-address.hacluster.16 \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.client.failover.proxy.provider.hacluster \u9ed8\u8ba4\u586b\u5199org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u6267\u884cDataX\u7684\u4efb\u52a1 python datax.py ../hdfs_write.json \u6570\u636e\u80fd\u591f\u6b63\u5e38\u5199\u5165HDFS","title":"0.1 <--> 8.0"},{"location":"Data_Integration/DataX/#datax-fusioninsight","text":"","title":"DataX \u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/DataX/#_1","text":"DataX 0.1 \u2194 FusionInsight HD 6.5 (HDFS) DataX 0.1 \u2194 FusionInsight MRS 8.0 (HDFS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/DataX/#_2","text":"\u4e0b\u8f7d\u89e3\u538b\u5b89\u88c5DataX wget http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz tar -zxvf datax.tar.gz \u5728FusionInsight\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u4fe1\u606f\uff0c\u5c06krb5.conf\u653e\u5165/etc\u4e0b \u5728FusionInsight\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u4fe1\u606f\uff0c\u5c06user.keytab\u653e\u5165/opt\u4e0b","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/DataX/#hdfs","text":"\u65b0\u589ehdfsread.json\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"job\" : { \"setting\" : { \"speed\" : { \"channel\" : 3 } }, \"content\" : [ { \"reader\" : { \"name\" : \"hdfsreader\" , \"parameter\" : { \"path\" : \"/user/developuser/datax/*\" , \"defaultFS\" : \"hdfs://hacluster\" , \"column\" : [ { \"index\" : 0 , \"type\" : \"string\" }, { \"index\" : 1 , \"type\" : \"string\" }, { \"type\" : \"string\" , \"value\" : \"hello\" }, { \"index\" : 2 , \"type\" : \"string\" } ], \"fileType\" : \"csv\" , \"encoding\" : \"UTF-8\" , \"fieldDelimiter\" : \",\" , \"haveKerberos\" : \"true\" , \"kerberosKeytabFilePath\" : \"/opt/user.keytab\" , \"kerberosPrincipal\" : \"developuser@HADOOP.COM\" , \"hadoopConfig\" :{ \"dfs.nameservices\" : \"hacluster\" , \"dfs.ha.namenodes.hacluster\" : \"15,16\" , \"dfs.namenode.rpc-address.hacluster.15\" : \"172.16.10.132:25000\" , \"dfs.namenode.rpc-address.hacluster.16\" : \"172.16.10.133:25000\" , \"dfs.client.failover.proxy.provider.hacluster\" : \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\" , \"hadoop.security.authentication\" : \"Kerberos\" , \"hadoop.rpc.protection\" : \"privacy\" } } }, \"writer\" : { \"name\" : \"streamwriter\" , \"parameter\" : { \"print\" : true } } } ] } } \u5176\u4e2d\u51e0\u4e2a\u91cd\u8981\u7684\u53c2\u8003\u914d\u7f6e\u65b9\u5f0f\u5982\u4e0b\uff1a \u53c2\u6570\u540d\u79f0 \u8bf4\u660e defaultFS FusionInsight\u96c6\u7fa4\u9ed8\u8ba4\u4e3ahacluster haveKerberos true kerberosKeytabFilePath keytab\u6587\u4ef6\u8def\u5f84\uff0c\u5982\uff1a/opt/user.keytab kerberosPrincipal kerberos\u7528\u6237\uff0c\u5982\uff1a developuser@HADOOP.COM hadoop.security.authentication \u53c2\u8003\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5982\uff1aKerberos hadoop.rpc.protection \u53c2\u8003\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5982\uff1aprivacy dfs.nameservices \u53c2\u8003\u96c6\u7fa4\u5ba2\u6237\u7aef\u7684hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u5982hacluster dfs.ha.namenodes.hacluster \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.namenode.rpc-address.hacluster.15 \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.namenode.rpc-address.hacluster.16 \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.client.failover.proxy.provider.hacluster \u9ed8\u8ba4\u586b\u5199org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u6267\u884cDataX\u7684\u4efb\u52a1 python datax.py ../hdfs_read.json \u80fd\u591f\u8bfb\u53d6\u51faHDFS\u4e2d\u6587\u4ef6\u7684\u5185\u5bb9","title":"\u8bfb\u53d6HDFS"},{"location":"Data_Integration/DataX/#hdfs_1","text":"\u65b0\u589ehdfswrite.json\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"setting\" : {}, \"job\" : { \"setting\" : { \"speed\" : { \"channel\" : 2 } }, \"content\" : [ { \"reader\" : { \"name\" : \"txtfilereader\" , \"parameter\" : { \"path\" : [ \"/opt/data.csv\" ], \"encoding\" : \"UTF-8\" , \"column\" : [ { \"index\" : 0 , \"type\" : \"long\" }, { \"index\" : 1 , \"type\" : \"DOUBLE\" }, { \"index\" : 2 , \"type\" : \"STRING\" }, { \"index\" : 3 , \"type\" : \"BOOLEAN\" }, { \"index\" : 4 , \"type\" : \"date\" } ], \"fieldDelimiter\" : \",\" } }, \"writer\" : { \"name\" : \"hdfswriter\" , \"parameter\" : { \"defaultFS\" : \"hdfs://hacluster\" , \"fileType\" : \"orc\" , \"path\" : \"/user/developuser/hdfswrite\" , \"fileName\" : \"hdfsdata.orc\" , \"column\" : [ { \"name\" : \"col1\" , \"type\" : \"TINYINT\" }, { \"name\" : \"col2\" , \"type\" : \"DOUBLE\" }, { \"name\" : \"col3\" , \"type\" : \"CHAR\" }, { \"name\" : \"col4\" , \"type\" : \"BOOLEAN\" }, { \"name\" : \"col5\" , \"type\" : \"DATE\" } ], \"writeMode\" : \"append\" , \"fieldDelimiter\" : \",\" , \"compress\" : \"NONE\" , \"haveKerberos\" : \"true\" , \"kerberosKeytabFilePath\" : \"/opt/user.keytab\" , \"kerberosPrincipal\" : \"developuser@HADOOP.COM\" , \"hadoopConfig\" :{ \"dfs.nameservices\" : \"hacluster\" , \"dfs.ha.namenodes.hacluster\" : \"15,16\" , \"dfs.namenode.rpc-address.hacluster.15\" : \"172.16.10.132:25000\" , \"dfs.namenode.rpc-address.hacluster.16\" : \"172.16.10.133:25000\" , \"dfs.client.failover.proxy.provider.hacluster\" : \"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\" , \"hadoop.security.authentication\" : \"Kerberos\" , \"hadoop.rpc.protection\" : \"privacy\" } } } } ] } } \u5176\u4e2d\u51e0\u4e2a\u91cd\u8981\u7684\u53c2\u8003\u914d\u7f6e\u65b9\u5f0f\u5982\u4e0b\uff1a \u53c2\u6570\u540d\u79f0 \u8bf4\u660e defaultFS FusionInsight\u96c6\u7fa4\u9ed8\u8ba4\u4e3ahacluster haveKerberos true kerberosKeytabFilePath keytab\u6587\u4ef6\u8def\u5f84\uff0c\u5982\uff1a/opt/user.keytab kerberosPrincipal kerberos\u7528\u6237\uff0c\u5982\uff1a developuser@HADOOP.COM hadoop.security.authentication \u53c2\u8003\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5982\uff1aKerberos hadoop.rpc.protection \u53c2\u8003\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u7684\u53c2\u6570\u914d\u7f6e\uff0c\u5982\uff1aprivacy dfs.nameservices \u53c2\u8003\u96c6\u7fa4\u5ba2\u6237\u7aef\u7684hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u5982hacluster dfs.ha.namenodes.hacluster \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.namenode.rpc-address.hacluster.15 \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.namenode.rpc-address.hacluster.16 \u53c2\u6570\u540d\u79f0\u548c\u503c\u90fd\u8981\u53c2\u8003hdfs-site.xml\u8fdb\u884c\u914d\u7f6e\uff0c\u6bcf\u5957\u96c6\u7fa4\u8fd9\u91cc\u7684\u503c\u90fd\u4e0d\u4e00\u6837 dfs.client.failover.proxy.provider.hacluster \u9ed8\u8ba4\u586b\u5199org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u6267\u884cDataX\u7684\u4efb\u52a1 python datax.py ../hdfs_write.json \u6570\u636e\u80fd\u591f\u6b63\u5e38\u5199\u5165HDFS","title":"\u5199\u5165HDFS"},{"location":"Data_Integration/Denodo/","text":"Denodo\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Denodo Platform 7.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive) Denodo Platform 7.0 \u2194 FusionInsight HD 6.5 (Hive) Denodo Platform 7.0 \u2194 FusionInsight MRS 8.0 (Hive) \u51c6\u5907\u5de5\u4f5c \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Denodo Platform 7.0 Denodo\u662f\u4e00\u4e2a\u6570\u636e\u865a\u62df\u5316\u7cfb\u7edf\uff0c\u5141\u8bb8\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u7684\u6570\u636e\uff0c\u5e76\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u7edf\u4e00\u7684\u8bbf\u95ee\u63a5\u53e3\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u6e90\u5b9e\u65f6\u5730\u8bbf\u95ee\u548c\u96c6\u6210\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u4ece\u6570\u636e\u6e90\u590d\u5236\u6216\u79fb\u52a8\u6570\u636e\u3002\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u5728\u865a\u62df\u5c42\u4e2d\u5b9a\u4e49\u7684\u8bed\u4e49\u7ec4\u4ef6\uff0c\u72ec\u7acb\u4e8e\u5b58\u50a8\u6570\u636e\u7684\u7269\u7406\u6e90\u3002 \u4ece https://community.denodo.com/express/download \u4e0b\u8f7dDenodo Platform 7.0\u7684\u201cDenodo Express Installer\u201d\u548c\u201cDenodo Express License\u201d\u3002\u4e0b\u8f7d\u9009\u62e9\u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672c\u662fWindows 64 bits\u3002 \u4e0b\u8f7d\u5b8c\u6210\u540e\u5b89\u88c5\u4e8e\u672c\u5730 C:\\Denodo\\ \u3002 FusionInsight HD\u76f8\u5173\u914d\u7f6e\uff08\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff09 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88Hive\u7684\u6240\u6709\u8bbf\u95ee\u6743\u9650\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06user.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u5c06krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55FusionInsight Manager \u4e3b\u673a->\u66f4\u591a->\u4e0b\u8f7d\u5ba2\u6237\u7aef \uff0c\u4e0b\u8f7dFusionInsight HD\u5ba2\u6237\u7aef\u5230\u672c\u5730\u3002 \u5bf9\u63a5Hive\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Hive\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive\\ \uff0c\u5982\u679chive\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u51c6\u5907\u6570\u636e Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); INSERT INTO student VALUES (5,'Vina',2); INSERT INTO student VALUES (6,'Manson',3); INSERT INTO student VALUES (7,'Summy',1); INSERT INTO student VALUES (8,'Peter',2); INSERT INTO student VALUES (9,'Wendy',3); INSERT INTO student VALUES (10,'Andy',1); INSERT INTO student VALUES (11,'Miki',2); INSERT INTO student VALUES (12,'Aurora',3); INSERT INTO student VALUES (13,'Carina',1); INSERT INTO student VALUES (14,'Hely',1); INSERT INTO student VALUES (15,'Tracy',2); \u521b\u5efa\u4e0estudent.class_id\u76f8\u5173\u7684\u6570\u636e\u5b58\u653e\u4e8eexcel\u8868\u4e2d\u3002\u4f8b\u5982\u521b\u5efa C:\\developuser\\Class.xlsx \uff0csheet\u547d\u540d\u4e3a\u201cClass\u201d\uff0c\u5305\u542b\u4e24\u5217\uff0c\u5206\u522b\u662fid\u548cname\uff0cid\u5217\u7684\u53d6\u503c\u5fc5\u987b\u5b58\u5728\u4e8estudent.class_id\u4e2d\u3002 JDBC\u8fde\u63a5\u9700\u8981\u67e5\u8be2Zookeeper\uff0cZookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8\u5e76\u914d\u7f6eDenodo \u00b6 \u70b9\u51fb \u5f00\u59cb->Denodo Platform->Denodo Platform 7.0 \u542f\u52a8Denodo Platform Control Center\u3002 \u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server\u3002 \u70b9\u51fb Virtual DataPort->Configure \u3002 \u70b9\u51fb JVM Options \u3002 Virtual DataPort Server\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \uff0c\u4e24\u4e2aOptions\u4e4b\u95f4\u7528\u7a7a\u683c\u9694\u5f00\u3002\u70b9\u51fb Ok \u3002 \u70b9\u51fb Virtual DataPort \u8fd4\u56de\u4e3b\u754c\u9762\uff0c\u70b9\u51fb Start \u542f\u52a8Virtual DataPort Server\u3002 \u542f\u52a8Virtual DataPort Administration Tool\u3002 Virtual DataPort Server\u542f\u52a8\u6210\u529f\u540e\u72b6\u6001\u663e\u793a\u4e3aRunning\uff0c\u70b9\u51fb LAUNCH \u542f\u52a8Virtual DataPort Administration Tool\u3002 \u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Connect \u767b\u5f55\u3002 \u6210\u529f\u767b\u5f55Virtual DataPort Administration Tool\u3002 \u5bf9\u63a5Hive \u00b6 \u521b\u5efaJDBC\u8fde\u63a5\u7684Data source \u00b6 \u53f3\u952e admin->Big Data \u9009\u62e9 New->Data source->JDBC \u3002 \u914d\u7f6e\u8fde\u63a5\u4fe1\u606f\uff1a Name\uff1a\u81ea\u547d\u540d\u7684\u65b0\u5efa\u7684Data Source\u540d\u79f0\u3002 Driver class path\uff1aHive\u7684Jar\u5305\u6240\u5728\u7684\u4f4d\u7f6e\u3002\u5177\u4f53\u914d\u7f6e\u8def\u5f84\u53c2\u8003\u672c\u6587 \u51c6\u5907\u5de5\u4f5c->\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5 \u7ae0\u8282\u3002 Database URI\uff1aHive\u8fde\u63a5\u7684URL\u3002 Authentication\uff1a\u9009\u62e9Kerberos\u8ba4\u8bc1\u3002 \u5bf9\u63a5Hive\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: hive_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u70b9\u51fb Test connection \uff0c\u8fd4\u56de JDBC connection tested successfully \u3002\u5982\u679c\u8fd4\u56de\u5931\u8d25\uff0c\u53ef\u5728 C:\\Denodo\\DenodoPlatform7.0\\logs\\vdp\\vdp.log \u67e5\u770b\u8be6\u7ec6\u7684\u5931\u8d25\u65e5\u5fd7\u3002\u70b9\u51fb Ok \u5173\u95ed\u6210\u529f\u63d0\u793a\u3002 \u70b9\u51fb Save \u4fdd\u5b58hive_ds\u3002 \u4fdd\u5b58\u6210\u529f\u540e\uff0c\u5de6\u8fb9\u663e\u793a\u7684 admin->Big Data->hive_ds \u5373\u4e3a\u65b0\u589e\u7684Data Source\u3002 \u521b\u5efaHive\u6570\u636e\u6e90 \u00b6 \u4e3a\u4e86\u66f4\u597d\u89c2\u5bdf\uff0c\u53f3\u952eBig Data\u6587\u4ef6\u5939 New->Folder \u65b0\u5efa\u4e09\u4e2a\u6587\u4ef6\u5939\u5206\u522b\u5b58\u5728Data source(01_data source)\u3001base views(02_base views)\u3001\u96c6\u6210\u6570\u636e(03_reports)\uff0c\u5e76\u628a\u5df2\u521b\u5efa\u7684data sources\u79fb\u5165\u6587\u4ef6\u593901_data source\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684Data Source hive_ds \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Create base view \u3002\u9009\u62e9employee\u8868 default->Tables->student \uff0c\u518d\u70b9\u51fb Create selected \u3002 View name\u547d\u540d\u4e3a student \uff0c\u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View student \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56de\u7684student\u8868\u7684\u6570\u636e\u3002 \u521b\u5efaExcel\u8868\u6570\u636e\u6e90 \u00b6 \u53f3\u952e\u6587\u4ef6\u593901_data source\uff0c\u9009\u62e9 New->Data source->Excel \u3002 \u9009\u62e9\u5bfc\u5165\u5df2\u51c6\u5907\u597d\u5b58\u653e\u4e8e C:\\developuser\\ \u7684 Class.xlsx \u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u5177\u4f53\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Name: class Type of file: \u6839\u636e\u51c6\u5907\u7684Excel\u8868\u7684\u7248\u672c\u9009\u62e9 File location: \u4e0b\u62c9\u6846\u9009\u62e9Local\u540e\uff0c\u518d\u70b9\u51fbConfigure\u9009\u62e9C:\\developuser\\Class.xlsx Worksheets: \u8f93\u5165\u51c6\u5907\u6570\u636e\u5bf9\u5e94\u7684Sheet\u540d\u79f0Class Start cell: \u51c6\u5907\u6570\u636e\u5f00\u59cb\u7684\u5355\u5143\u683c End cell: \u51c6\u5907\u6570\u636e\u7ed3\u675f\u7684\u5355\u5143\u683c Has headers: \u52fe\u9009 Stream tuples: \u52fe\u9009 \u9009\u62e9Data source class \uff0c\u70b9\u51fb Create base view \u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View class \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deExcel\u7684Class\u7684\u6570\u636e\u3002 \u5c06View class \u548c student \u79fb\u5165\u6587\u4ef6\u593902_base views\u3002 \u7ec4\u5408Hive\u548cExcel\u7684\u6570\u636e \u00b6 \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New->Join \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06View name\u8bbe\u7f6e\u4e3a\u201cstudent_class\u201d\uff0c\u5c06student\u7684class_id\u548cclass\u7684id\u5220\u9664\u3002 \u91cd\u547d\u540dclass\u7684name\u4e3aclass_name\u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u9009\u62e9 student_class \uff0c\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deHive\u548cExcel\u7ec4\u5408\u540e\u7684\u6570\u636e\u3002 \u4f7f\u7528DbVisualizer\u67e5\u770bDenodo Views\u7684\u6570\u636e \u00b6 \u4ece https://www.dbvis.com/download/ \u4e0b\u8f7dDbVisualizer\u5e76\u5b89\u88c5\u4e8e\u672c\u5730\u3002 \u6253\u5f00DbVisualizer\uff0c\u9009\u62e9 Tools->Driver Manager \u3002 \u9009\u62e9 Drive->Create Driver \u3002 \u8f93\u5165\u4ee5\u4e0b\u914d\u7f6e\u4fe1\u606f\u540e\u5173\u95ed\u8be5\u754c\u9762\uff1a Name: Denodo 7.0 URL Format: jdbc:vdb://host:port/database Drive Class: \u70b9\u51fb\u6587\u4ef6\u5939\u5bfc\u5165Denodo\u81ea\u5e26\u7684JDBC jar\u5305\uff0c\u4f8b\u5982C:\\Denodo\\DenodoPlatform7.0\\tools\\client-drivers\\jdbc\\denodo-vdp-jdbcdriver.jar\uff0c\u518d\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9com.denodo.vdp.jdbc.Driver \u8fd4\u56de\u4e3b\u754c\u9762\u540e\uff0c\u9009\u62e9 Database->Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u9009\u62e9 Denodo 7.0 \uff0c\u70b9\u51fb Next \u3002 \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u540e\u70b9\u51fb Finish \u3002 \u8fde\u63a5\u4fe1\u606f\u5982\u4e0b\uff1a Database URL: jdbc:vdb://localhost:9999/admin Database Userid: admin Database Password: admin \u8fde\u63a5Denodo\u9ed8\u8ba4\u7684admin\u6570\u636e\u5e93\u6210\u529f\u3002 \u53cc\u51fb VIEW->student_class \uff0c\u9009\u62e9 Open Object \u3002 \u70b9\u51fb Data \u67e5\u8be2\u8fd4\u56de\u6570\u636e\u6b63\u786e\u3002 \u767b\u5f55RESTful Web service\u67e5\u770bAssociations \u00b6 \u521b\u5efastudent\u548cclass\u7684Association \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New Association \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u5e76\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06\u201cAssociation name\u201d\u8bbe\u7f6e\u4e3a student_class \uff0c\u201cEnd point 'student'\u201d\u4e3a Principal \u4e14\u201cRole name\u201d\u4e3a class \uff0c\u201cEnd point 'class'\u201d\u4e3a Dependent \u4e14\u201cRole name\u201d\u4e3a belongs_to_student \u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u4fdd\u5b58\u540e\u53ef\u4ee5\u5728\u6587\u4ef6\u593903_reports\u4e0b\u9762\u770b\u5230Association student_class\u3002 \u767b\u5f55Denodo\u7684RESTful Web service\u67e5\u770bAssociation student_class \u767b\u5f55 http://localhost:9090/denodo-restfulws/admin/ \uff0c\u7528\u6237\u540d\u4e3a admin \uff0c\u5bc6\u7801\u4e3a admin \u3002 \u70b9\u51fb class \uff0c\u8fd4\u56de\u8be5view\u7684\u76f8\u5173\u4fe1\u606f\u3002 \u70b9\u51fb belongs_to_student \uff0c\u8fd4\u56de\u5c5e\u4e8e\u8be5class\u7684\u6240\u6709student\u3002 \u767b\u5f55Data Catalog\u67e5\u770bViews \u00b6 \u5728\u4e3b\u754c\u9762\u70b9\u51fb Denodo Platform Control Center->Virtual DataPort->Start \u542f\u52a8Data Catalog\u670d\u52a1\u3002 \u72b6\u6001\u663e\u793a\u4e3aRunning\uff0cData Catalog\u670d\u52a1\u542f\u52a8\u6210\u529f\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:9090/denodo-data-catalog \uff0c\u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Sign In \u767b\u5f55\u3002 \u9009\u62e9 Browser->DB/Folders \u3002 \u9009\u62e9 admin->Big Data->02_base views->student->Query \u67e5\u8be2\u89c6\u56festudent\u7684\u6570\u636e\u3002 \u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u8f93\u51fa\u5217\u3002 \u8f93\u5165Name= id \uff0cExpression= id \uff0c\u70b9\u51fb Save \u3002 id\u5217\u6210\u529f\u4fdd\u5b58\u5728Output columns\u3002 \u91c7\u7528\u540c\u6837\u7684\u64cd\u4f5c\uff0c\u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u5176\u4ed6\u8f93\u51fa\u5217\uff0c\u4f8b\u5982name\u3002 \u70b9\u51fb Run \u67e5\u8be2\u6210\u529f\u8fd4\u56destudent\u8868\u7684id\u3001name\u3001\u4e24\u5217\u7684\u503c\u3002 FAQ \u00b6 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: KrbException: Cannot locate default realm \u89e3\u51b3\u529e\u6cd5\uff1a \u5c06developuser\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u8bc1krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u540e\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: Clock skew too great (37) - PREAUTH_FAILED \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u5ba2\u6237\u7aef\u673a\u5668\uff08\u672c\u5730\uff09\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u7684\u65f6\u95f4\u5dee\u662f\u5426\u5c0f\u4e8e5\u5206\u949f\u3002\u5982\u679c\u4e0d\u662f\uff0c\u5efa\u8bae\u4fee\u6539\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4fdd\u6301\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002","title":"7.0 <--> 8.0"},{"location":"Data_Integration/Denodo/#denodofusioninsight","text":"","title":"Denodo\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Denodo/#_1","text":"Denodo Platform 7.0 \u2194 FusionInsight HD V100R002C80SPC100 (Hive) Denodo Platform 7.0 \u2194 FusionInsight HD 6.5 (Hive) Denodo Platform 7.0 \u2194 FusionInsight MRS 8.0 (Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Denodo/#_2","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Denodo Platform 7.0 Denodo\u662f\u4e00\u4e2a\u6570\u636e\u865a\u62df\u5316\u7cfb\u7edf\uff0c\u5141\u8bb8\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u6765\u81ea\u591a\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u7684\u6570\u636e\uff0c\u5e76\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u7edf\u4e00\u7684\u8bbf\u95ee\u63a5\u53e3\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u636e\u6e90\u5b9e\u65f6\u5730\u8bbf\u95ee\u548c\u96c6\u6210\u6570\u636e\uff0c\u800c\u4e0d\u9700\u8981\u4ece\u6570\u636e\u6e90\u590d\u5236\u6216\u79fb\u52a8\u6570\u636e\u3002\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u5728\u865a\u62df\u5c42\u4e2d\u5b9a\u4e49\u7684\u8bed\u4e49\u7ec4\u4ef6\uff0c\u72ec\u7acb\u4e8e\u5b58\u50a8\u6570\u636e\u7684\u7269\u7406\u6e90\u3002 \u4ece https://community.denodo.com/express/download \u4e0b\u8f7dDenodo Platform 7.0\u7684\u201cDenodo Express Installer\u201d\u548c\u201cDenodo Express License\u201d\u3002\u4e0b\u8f7d\u9009\u62e9\u7248\u672c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u4f4d\u6570\u4fdd\u6301\u4e00\u81f4\uff0c\u672c\u6587\u7248\u672c\u662fWindows 64 bits\u3002 \u4e0b\u8f7d\u5b8c\u6210\u540e\u5b89\u88c5\u4e8e\u672c\u5730 C:\\Denodo\\ \u3002 FusionInsight HD\u76f8\u5173\u914d\u7f6e\uff08\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff09 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88Hive\u7684\u6240\u6709\u8bbf\u95ee\u6743\u9650\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06user.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u5c06krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55FusionInsight Manager \u4e3b\u673a->\u66f4\u591a->\u4e0b\u8f7d\u5ba2\u6237\u7aef \uff0c\u4e0b\u8f7dFusionInsight HD\u5ba2\u6237\u7aef\u5230\u672c\u5730\u3002 \u5bf9\u63a5Hive\u9700\u8981\u51c6\u5907\u7684jar\u5305 \u5c06\u89e3\u538b\u540e\u7684\u5ba2\u6237\u7aef ..\\FusionInsight_Services_Client\\FusionInsight_Services_ClientConfig\\Hive\\jdbc\\ \u76ee\u5f55\u4e0b\u6240\u6709jar\u5305\u62f7\u8d1d\u81f3 C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive\\ \uff0c\u5982\u679chive\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u3002 \u51c6\u5907\u6570\u636e Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); INSERT INTO student VALUES (5,'Vina',2); INSERT INTO student VALUES (6,'Manson',3); INSERT INTO student VALUES (7,'Summy',1); INSERT INTO student VALUES (8,'Peter',2); INSERT INTO student VALUES (9,'Wendy',3); INSERT INTO student VALUES (10,'Andy',1); INSERT INTO student VALUES (11,'Miki',2); INSERT INTO student VALUES (12,'Aurora',3); INSERT INTO student VALUES (13,'Carina',1); INSERT INTO student VALUES (14,'Hely',1); INSERT INTO student VALUES (15,'Tracy',2); \u521b\u5efa\u4e0estudent.class_id\u76f8\u5173\u7684\u6570\u636e\u5b58\u653e\u4e8eexcel\u8868\u4e2d\u3002\u4f8b\u5982\u521b\u5efa C:\\developuser\\Class.xlsx \uff0csheet\u547d\u540d\u4e3a\u201cClass\u201d\uff0c\u5305\u542b\u4e24\u5217\uff0c\u5206\u522b\u662fid\u548cname\uff0cid\u5217\u7684\u53d6\u503c\u5fc5\u987b\u5b58\u5728\u4e8estudent.class_id\u4e2d\u3002 JDBC\u8fde\u63a5\u9700\u8981\u67e5\u8be2Zookeeper\uff0cZookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5Zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; };","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Denodo/#denodo","text":"\u70b9\u51fb \u5f00\u59cb->Denodo Platform->Denodo Platform 7.0 \u542f\u52a8Denodo Platform Control Center\u3002 \u914d\u7f6e\u5e76\u542f\u52a8Virtual DataPort Server\u3002 \u70b9\u51fb Virtual DataPort->Configure \u3002 \u70b9\u51fb JVM Options \u3002 Virtual DataPort Server\u65b0\u589e -Djava.security.auth.login.config=c:/developuser/jaas.conf \uff0c\u4e24\u4e2aOptions\u4e4b\u95f4\u7528\u7a7a\u683c\u9694\u5f00\u3002\u70b9\u51fb Ok \u3002 \u70b9\u51fb Virtual DataPort \u8fd4\u56de\u4e3b\u754c\u9762\uff0c\u70b9\u51fb Start \u542f\u52a8Virtual DataPort Server\u3002 \u542f\u52a8Virtual DataPort Administration Tool\u3002 Virtual DataPort Server\u542f\u52a8\u6210\u529f\u540e\u72b6\u6001\u663e\u793a\u4e3aRunning\uff0c\u70b9\u51fb LAUNCH \u542f\u52a8Virtual DataPort Administration Tool\u3002 \u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Connect \u767b\u5f55\u3002 \u6210\u529f\u767b\u5f55Virtual DataPort Administration Tool\u3002","title":"\u542f\u52a8\u5e76\u914d\u7f6eDenodo"},{"location":"Data_Integration/Denodo/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/Denodo/#jdbcdata-source","text":"\u53f3\u952e admin->Big Data \u9009\u62e9 New->Data source->JDBC \u3002 \u914d\u7f6e\u8fde\u63a5\u4fe1\u606f\uff1a Name\uff1a\u81ea\u547d\u540d\u7684\u65b0\u5efa\u7684Data Source\u540d\u79f0\u3002 Driver class path\uff1aHive\u7684Jar\u5305\u6240\u5728\u7684\u4f4d\u7f6e\u3002\u5177\u4f53\u914d\u7f6e\u8def\u5f84\u53c2\u8003\u672c\u6587 \u51c6\u5907\u5de5\u4f5c->\u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5 \u7ae0\u8282\u3002 Database URI\uff1aHive\u8fde\u63a5\u7684URL\u3002 Authentication\uff1a\u9009\u62e9Kerberos\u8ba4\u8bc1\u3002 \u5bf9\u63a5Hive\u5177\u4f53\u914d\u7f6e\u4fe1\u606f\u5982\u4e0b\uff1a Name: hive_ds Database adapter: Hive 2.0.0(HiveServer2) Dirver class path: 'C:\\Denodo\\DenodoPlatform7.0\\extensions\\thirdparty\\lib\\hive' Dirver class: org.apache.hive.jdbc.HiveDriver Database URI: jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/developuser/user.keytab Transaction Isolation: Database default Authentication: Use Kerberos Kerberos login: deverlopuser \u9009\u62e9Use Key tab keytab file: C:/developuser/user.keytab \u70b9\u51fb Test connection \uff0c\u8fd4\u56de JDBC connection tested successfully \u3002\u5982\u679c\u8fd4\u56de\u5931\u8d25\uff0c\u53ef\u5728 C:\\Denodo\\DenodoPlatform7.0\\logs\\vdp\\vdp.log \u67e5\u770b\u8be6\u7ec6\u7684\u5931\u8d25\u65e5\u5fd7\u3002\u70b9\u51fb Ok \u5173\u95ed\u6210\u529f\u63d0\u793a\u3002 \u70b9\u51fb Save \u4fdd\u5b58hive_ds\u3002 \u4fdd\u5b58\u6210\u529f\u540e\uff0c\u5de6\u8fb9\u663e\u793a\u7684 admin->Big Data->hive_ds \u5373\u4e3a\u65b0\u589e\u7684Data Source\u3002","title":"\u521b\u5efaJDBC\u8fde\u63a5\u7684Data source"},{"location":"Data_Integration/Denodo/#hive_1","text":"\u4e3a\u4e86\u66f4\u597d\u89c2\u5bdf\uff0c\u53f3\u952eBig Data\u6587\u4ef6\u5939 New->Folder \u65b0\u5efa\u4e09\u4e2a\u6587\u4ef6\u5939\u5206\u522b\u5b58\u5728Data source(01_data source)\u3001base views(02_base views)\u3001\u96c6\u6210\u6570\u636e(03_reports)\uff0c\u5e76\u628a\u5df2\u521b\u5efa\u7684data sources\u79fb\u5165\u6587\u4ef6\u593901_data source\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684Data Source hive_ds \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Create base view \u3002\u9009\u62e9employee\u8868 default->Tables->student \uff0c\u518d\u70b9\u51fb Create selected \u3002 View name\u547d\u540d\u4e3a student \uff0c\u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View student \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56de\u7684student\u8868\u7684\u6570\u636e\u3002","title":"\u521b\u5efaHive\u6570\u636e\u6e90"},{"location":"Data_Integration/Denodo/#excel","text":"\u53f3\u952e\u6587\u4ef6\u593901_data source\uff0c\u9009\u62e9 New->Data source->Excel \u3002 \u9009\u62e9\u5bfc\u5165\u5df2\u51c6\u5907\u597d\u5b58\u653e\u4e8e C:\\developuser\\ \u7684 Class.xlsx \u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u5177\u4f53\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Name: class Type of file: \u6839\u636e\u51c6\u5907\u7684Excel\u8868\u7684\u7248\u672c\u9009\u62e9 File location: \u4e0b\u62c9\u6846\u9009\u62e9Local\u540e\uff0c\u518d\u70b9\u51fbConfigure\u9009\u62e9C:\\developuser\\Class.xlsx Worksheets: \u8f93\u5165\u51c6\u5907\u6570\u636e\u5bf9\u5e94\u7684Sheet\u540d\u79f0Class Start cell: \u51c6\u5907\u6570\u636e\u5f00\u59cb\u7684\u5355\u5143\u683c End cell: \u51c6\u5907\u6570\u636e\u7ed3\u675f\u7684\u5355\u5143\u683c Has headers: \u52fe\u9009 Stream tuples: \u52fe\u9009 \u9009\u62e9Data source class \uff0c\u70b9\u51fb Create base view \u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u5728\u5de6\u8fb9\u5217\u8868\u70b9\u51fb\u9009\u62e9\u4fdd\u5b58\u540e\u7684View class \uff0c\u53f3\u8fb9\u6846\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deExcel\u7684Class\u7684\u6570\u636e\u3002 \u5c06View class \u548c student \u79fb\u5165\u6587\u4ef6\u593902_base views\u3002","title":"\u521b\u5efaExcel\u8868\u6570\u636e\u6e90"},{"location":"Data_Integration/Denodo/#hiveexcel","text":"\u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New->Join \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06View name\u8bbe\u7f6e\u4e3a\u201cstudent_class\u201d\uff0c\u5c06student\u7684class_id\u548cclass\u7684id\u5220\u9664\u3002 \u91cd\u547d\u540dclass\u7684name\u4e3aclass_name\u3002 \u70b9\u51fb \u4fdd\u5b58\u3002 \u9009\u62e9 student_class \uff0c\u70b9\u51fb Execution panel->Execute \u3002 \u7b49\u5f85\u8fd4\u56de\u67e5\u8be2\u7ed3\u679c\u540e\uff0c Query Results->Results \uff0c\u53ef\u67e5\u770b\u8fd4\u56deHive\u548cExcel\u7ec4\u5408\u540e\u7684\u6570\u636e\u3002","title":"\u7ec4\u5408Hive\u548cExcel\u7684\u6570\u636e"},{"location":"Data_Integration/Denodo/#dbvisualizerdenodo-views","text":"\u4ece https://www.dbvis.com/download/ \u4e0b\u8f7dDbVisualizer\u5e76\u5b89\u88c5\u4e8e\u672c\u5730\u3002 \u6253\u5f00DbVisualizer\uff0c\u9009\u62e9 Tools->Driver Manager \u3002 \u9009\u62e9 Drive->Create Driver \u3002 \u8f93\u5165\u4ee5\u4e0b\u914d\u7f6e\u4fe1\u606f\u540e\u5173\u95ed\u8be5\u754c\u9762\uff1a Name: Denodo 7.0 URL Format: jdbc:vdb://host:port/database Drive Class: \u70b9\u51fb\u6587\u4ef6\u5939\u5bfc\u5165Denodo\u81ea\u5e26\u7684JDBC jar\u5305\uff0c\u4f8b\u5982C:\\Denodo\\DenodoPlatform7.0\\tools\\client-drivers\\jdbc\\denodo-vdp-jdbcdriver.jar\uff0c\u518d\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9com.denodo.vdp.jdbc.Driver \u8fd4\u56de\u4e3b\u754c\u9762\u540e\uff0c\u9009\u62e9 Database->Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u9009\u62e9 Denodo 7.0 \uff0c\u70b9\u51fb Next \u3002 \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u540e\u70b9\u51fb Finish \u3002 \u8fde\u63a5\u4fe1\u606f\u5982\u4e0b\uff1a Database URL: jdbc:vdb://localhost:9999/admin Database Userid: admin Database Password: admin \u8fde\u63a5Denodo\u9ed8\u8ba4\u7684admin\u6570\u636e\u5e93\u6210\u529f\u3002 \u53cc\u51fb VIEW->student_class \uff0c\u9009\u62e9 Open Object \u3002 \u70b9\u51fb Data \u67e5\u8be2\u8fd4\u56de\u6570\u636e\u6b63\u786e\u3002","title":"\u4f7f\u7528DbVisualizer\u67e5\u770bDenodo Views\u7684\u6570\u636e"},{"location":"Data_Integration/Denodo/#restful-web-serviceassociations","text":"\u521b\u5efastudent\u548cclass\u7684Association \u53f3\u952e\u6587\u4ef6\u593903_reports\uff0c\u9009\u62e9 New Association \u3002 \u5206\u522b\u5c06\u6587\u4ef6\u593902_base views\u4e0b\u7684student\u3001class\u62d6\u81f3\u53f3\u8fb9\u7f16\u8f91\u6846\uff0c\u5e76\u8fde\u63a5student.class_id\u548cclass.id\u3002 \u79fb\u81f3 Output \u5c06\u201cAssociation name\u201d\u8bbe\u7f6e\u4e3a student_class \uff0c\u201cEnd point 'student'\u201d\u4e3a Principal \u4e14\u201cRole name\u201d\u4e3a class \uff0c\u201cEnd point 'class'\u201d\u4e3a Dependent \u4e14\u201cRole name\u201d\u4e3a belongs_to_student \u3002\u70b9\u51fb \u4fdd\u5b58\u3002 \u4fdd\u5b58\u540e\u53ef\u4ee5\u5728\u6587\u4ef6\u593903_reports\u4e0b\u9762\u770b\u5230Association student_class\u3002 \u767b\u5f55Denodo\u7684RESTful Web service\u67e5\u770bAssociation student_class \u767b\u5f55 http://localhost:9090/denodo-restfulws/admin/ \uff0c\u7528\u6237\u540d\u4e3a admin \uff0c\u5bc6\u7801\u4e3a admin \u3002 \u70b9\u51fb class \uff0c\u8fd4\u56de\u8be5view\u7684\u76f8\u5173\u4fe1\u606f\u3002 \u70b9\u51fb belongs_to_student \uff0c\u8fd4\u56de\u5c5e\u4e8e\u8be5class\u7684\u6240\u6709student\u3002","title":"\u767b\u5f55RESTful Web service\u67e5\u770bAssociations"},{"location":"Data_Integration/Denodo/#data-catalogviews","text":"\u5728\u4e3b\u754c\u9762\u70b9\u51fb Denodo Platform Control Center->Virtual DataPort->Start \u542f\u52a8Data Catalog\u670d\u52a1\u3002 \u72b6\u6001\u663e\u793a\u4e3aRunning\uff0cData Catalog\u670d\u52a1\u542f\u52a8\u6210\u529f\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u8bbf\u95ee http://127.0.0.1:9090/denodo-data-catalog \uff0c\u8f93\u5165\u9ed8\u8ba4\u7684\u7528\u6237\u540d admin \u548c\u5bc6\u7801 admin \uff0c\u70b9\u51fb Sign In \u767b\u5f55\u3002 \u9009\u62e9 Browser->DB/Folders \u3002 \u9009\u62e9 admin->Big Data->02_base views->student->Query \u67e5\u8be2\u89c6\u56festudent\u7684\u6570\u636e\u3002 \u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u8f93\u51fa\u5217\u3002 \u8f93\u5165Name= id \uff0cExpression= id \uff0c\u70b9\u51fb Save \u3002 id\u5217\u6210\u529f\u4fdd\u5b58\u5728Output columns\u3002 \u91c7\u7528\u540c\u6837\u7684\u64cd\u4f5c\uff0c\u70b9\u51fbOutput columns\u7684 Add->New Field \u6dfb\u52a0\u5176\u4ed6\u8f93\u51fa\u5217\uff0c\u4f8b\u5982name\u3002 \u70b9\u51fb Run \u67e5\u8be2\u6210\u529f\u8fd4\u56destudent\u8868\u7684id\u3001name\u3001\u4e24\u5217\u7684\u503c\u3002","title":"\u767b\u5f55Data Catalog\u67e5\u770bViews"},{"location":"Data_Integration/Denodo/#faq","text":"\u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: KrbException: Cannot locate default realm \u89e3\u51b3\u529e\u6cd5\uff1a \u5c06developuser\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u8bc1krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3akrb5.ini\u5e76\u653e\u5728 C:\\Windows\\ \u76ee\u5f55\u540e\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002 \u70b9\u51fb Test connection \u8fd4\u56de\u9519\u8befUnable to establish connection: javax.security.auth.login.LoginException: Clock skew too great (37) - PREAUTH_FAILED \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5\u5ba2\u6237\u7aef\u673a\u5668\uff08\u672c\u5730\uff09\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u7684\u65f6\u95f4\u5dee\u662f\u5426\u5c0f\u4e8e5\u5206\u949f\u3002\u5982\u679c\u4e0d\u662f\uff0c\u5efa\u8bae\u4fee\u6539\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4fdd\u6301\u4e0eFusionInsight HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f\uff0c\u518d\u70b9\u51fb Test connection \u91cd\u8bd5\u3002","title":"FAQ"},{"location":"Data_Integration/H2O.ai/","text":"H2O.ai \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 H2O.ai 3.24.0.2 \u2194 FusionInsight HD 6.5 (HDFS/GaussDB) H2O.ai 3.24.0.2 \u2194 FusionInsight MRS 8.0 (HDFS) \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7dH2O.ai\u5b89\u88c5\u5305 \u4e0b\u8f7d\u5730\u5740\u4e3a http://h2o-release.s3.amazonaws.com/h2o/rel-yates/2/h2o-3.24.0.2-cdh6.0.zip \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55h2o-3.24.0.2-cdh6.0 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient h2o\u8981\u653e\u5230\u96c6\u7fa4\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u4e0a H2o\u4f7f\u7528 \u00b6 \u542f\u52a8H2O cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -Dmapreduce.map.log.level=DEBUG -JJ \"-Djava.security.krb5.conf=/opt/huawei/Bigdata/common/runtime/krb5.conf\" -nodes 1 -mapperXmx 8g -network 172.16.4.131/24 > -nodes \u6307\u5b9aH2o\u96c6\u7fa4\u4e2d\u8282\u70b9\u6570\u91cf > -mapperXmx \u6307\u5b9aH2O\u96c6\u7fa4\u4f7f\u7528\u5185\u5b58\u5927\u5c0f > -network \u6307\u5b9aH2Oweb\u754c\u9762\u8bbf\u95ee\u7684IP\u5730\u5740\u8303\u56f4 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165http://172.16.4.21:54321\uff0c\u5373\u53ef\u8bbf\u95eeH2O \u8fde\u63a5HDFS \u00b6 \u5728H2O\u7684web\u754c\u9762\u4e0a\uff0c\u4f7f\u7528 Import Files \uff0c\u586b\u5165HDFS\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u70b9\u51fb import \u5373\u53ef \u5728\u4e0b\u9762\u53ef\u4ee5\u770b\u5230\u6267\u884c\u7ed3\u679c * \u53ef\u4ee5\u5bf9\u6587\u4ef6\u8fdb\u884c\u4e00\u4e9b\u8f6c\u6362\uff0c\u9884\u5904\u7406 \u8fde\u63a5GaussDB \u00b6 \u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u4e0a\u4f20\u81f3\u8282\u70b9\uff0c\u4f8b\u5982 /opt/h2o-3.24.0.2-cdh6.0 \u76ee\u5f55\u4e0b \u8fde\u63a5GaussDB \u9700\u8981\u52a0\u8f7dJDBC\u9a71\u52a8\u5305\uff0c\u9700\u5728\u542f\u52a8H2O\u96c6\u7fa4\u65f6\u6307\u5b9a,\u4f7f\u7528\u4ee5\u4e0b\u8bed\u53e5\u542f\u52a8H2O\u96c6\u7fa4 cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -libjars gsjdbc4.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 * \u5728H2O\u7684web\u754c\u9762\uff0c\u4f7f\u7528 import SQL Table \uff0c\u586b\u5165\u4ee5\u4e0b\u4fe1\u606f,\u70b9\u51fb import \u70b9\u51fb view Data \uff0c\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"3.24.0.2 <--> 8.0"},{"location":"Data_Integration/H2O.ai/#h2oai-fusioninsight","text":"","title":"H2O.ai \u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/H2O.ai/#_1","text":"H2O.ai 3.24.0.2 \u2194 FusionInsight HD 6.5 (HDFS/GaussDB) H2O.ai 3.24.0.2 \u2194 FusionInsight MRS 8.0 (HDFS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/H2O.ai/#_2","text":"\u4e0b\u8f7dH2O.ai\u5b89\u88c5\u5305 \u4e0b\u8f7d\u5730\u5740\u4e3a http://h2o-release.s3.amazonaws.com/h2o/rel-yates/2/h2o-3.24.0.2-cdh6.0.zip \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55h2o-3.24.0.2-cdh6.0 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient h2o\u8981\u653e\u5230\u96c6\u7fa4\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u4e0a","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/H2O.ai/#h2o","text":"\u542f\u52a8H2O cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -Dmapreduce.map.log.level=DEBUG -JJ \"-Djava.security.krb5.conf=/opt/huawei/Bigdata/common/runtime/krb5.conf\" -nodes 1 -mapperXmx 8g -network 172.16.4.131/24 > -nodes \u6307\u5b9aH2o\u96c6\u7fa4\u4e2d\u8282\u70b9\u6570\u91cf > -mapperXmx \u6307\u5b9aH2O\u96c6\u7fa4\u4f7f\u7528\u5185\u5b58\u5927\u5c0f > -network \u6307\u5b9aH2Oweb\u754c\u9762\u8bbf\u95ee\u7684IP\u5730\u5740\u8303\u56f4 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165http://172.16.4.21:54321\uff0c\u5373\u53ef\u8bbf\u95eeH2O","title":"H2o\u4f7f\u7528"},{"location":"Data_Integration/H2O.ai/#hdfs","text":"\u5728H2O\u7684web\u754c\u9762\u4e0a\uff0c\u4f7f\u7528 Import Files \uff0c\u586b\u5165HDFS\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u70b9\u51fb import \u5373\u53ef \u5728\u4e0b\u9762\u53ef\u4ee5\u770b\u5230\u6267\u884c\u7ed3\u679c * \u53ef\u4ee5\u5bf9\u6587\u4ef6\u8fdb\u884c\u4e00\u4e9b\u8f6c\u6362\uff0c\u9884\u5904\u7406","title":"\u8fde\u63a5HDFS"},{"location":"Data_Integration/H2O.ai/#gaussdb","text":"\u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u4e0a\u4f20\u81f3\u8282\u70b9\uff0c\u4f8b\u5982 /opt/h2o-3.24.0.2-cdh6.0 \u76ee\u5f55\u4e0b \u8fde\u63a5GaussDB \u9700\u8981\u52a0\u8f7dJDBC\u9a71\u52a8\u5305\uff0c\u9700\u5728\u542f\u52a8H2O\u96c6\u7fa4\u65f6\u6307\u5b9a,\u4f7f\u7528\u4ee5\u4e0b\u8bed\u53e5\u542f\u52a8H2O\u96c6\u7fa4 cd /opt/h2o-3.24.0.2-cdh6.0 hadoop jar h2odriver.jar -libjars gsjdbc4.jar -nodes 1 -mapperXmx 2g -network 172.16.4.21/32 * \u5728H2O\u7684web\u754c\u9762\uff0c\u4f7f\u7528 import SQL Table \uff0c\u586b\u5165\u4ee5\u4e0b\u4fe1\u606f,\u70b9\u51fb import \u70b9\u51fb view Data \uff0c\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"\u8fde\u63a5GaussDB"},{"location":"Data_Integration/IBM_InfoSphere_CDC/","text":"IBM InfoSphere CDC\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM InfoSphere CDC 11.3.3.1 \u2194 FusionInsight HD V100R002C50 (HDFS)","title":"11.3.3.1 <--> C50"},{"location":"Data_Integration/IBM_InfoSphere_CDC/#ibm-infosphere-cdcfusioninsight","text":"","title":"IBM InfoSphere CDC\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/IBM_InfoSphere_CDC/#_1","text":"IBM InfoSphere CDC 11.3.3.1 \u2194 FusionInsight HD V100R002C50 (HDFS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/","text":"IBM InfoSphere DataStage\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM InfoSphere DataStage 11.3.1.0 \u2194 FusionInsight HD V100R002C50 (HDFS/Hive/SparkSQL) IBM InfoSphere DataStage 11.5.0.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Phoenix/SparkSQL/Kafka/GaussDB) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210IBM InfoSphere DataStage 11.5.0.2\u7684\u5b89\u88c5\u90e8\u7f72\uff08\u672c\u6587\u90e8\u7f72\u5728Centos7.2\u4e0a\uff09 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD V100R002C60U20 \u51c6\u5907\u5de5\u4f5c \u00b6 \u914d\u7f6e\u57df\u540d\u89e3\u6790 \u00b6 \u4f7f\u7528 vi /etc/hosts \u547d\u4ee4\u4fee\u6539DataStage Server\u548cClient\u7684hosts\u6587\u4ef6\uff0c\u6dfb\u52a0FI\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f\uff0c\u5982\uff1a 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3 \u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u5728FI\u7ba1\u7406\u754c\u9762\u521b\u5efaDataStage\u5bf9\u63a5\u7528\u6237\uff0c\u5e76\u8d4b\u4e88\u8be5\u7528\u6237\u6240\u9700\u6743\u9650\uff0c\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u4e0b\u8f7d\u7684tar\u6587\u4ef6\uff0c\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u7684keytab\u6587\u4ef6\u3002 \u4ee5root\u767b\u5f55DataStage Server\u8282\u70b9\uff0c\u5c06FI\u96c6\u7fa4\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u5230 /etc \u76ee\u5f55\u3002 \u5c06\u7528\u6237\u7684user.keytab\u6587\u4ef6\u4e0a\u4f20\u5230DataStage Server\u8282\u70b9\u7684\u4efb\u610f\u76ee\u5f55\uff0c\u5982 /home/dsadm \u3002 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u00b6 \u53c2\u8003FI\u4ea7\u54c1\u6587\u6863\uff0c\u5728FI\u670d\u52a1\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230DataStageServer\uff0c\u5b89\u88c5\u81f3\u81ea\u5b9a\u4e49\u76ee\u5f55\uff0c\u5982 /opt/ficlient \u3002 \u5bf9\u63a5HDFS \u00b6 \u5bfc\u5165FI\u96c6\u7fa4\u7684SSL\u8bc1\u4e66 \u00b6 \u6d4f\u89c8\u5668\u5bfc\u51faFI\u96c6\u7fa4\u7684\u6839\u8bc1\u4e66 \u6d4f\u89c8\u5668\u6253\u5f00FI\u7ba1\u7406\u754c\u9762\uff0c\u67e5\u770b\u8bc1\u4e66\uff0c\u70b9\u51fb\u201c\u8bc1\u4e66\u8def\u5f84\u201d\u9875\u7b7e\uff0c\u9009\u62e9\u6839\u8def\u5f84\uff0c\u67e5\u770b\u6839\u8bc1\u4e66\uff0c\u5728\u201c\u8be6\u7ec6\u4fe1\u606f\u201d\u9875\u7b7e\u4e0b\uff0c\u70b9\u51fb\u201c\u590d\u5236\u5230\u6587\u4ef6\u201d\uff0c\u5bfc\u51fa\u4e3acer\u683c\u5f0f \u8bc1\u4e66\u5bfc\u5165DataStage\u7684keystore\u6587\u4ef6 \u5c06\u5bfc\u51fa\u7684FI\u6839\u8bc1\u4e66fi-root-ca.cer\u4e0a\u4f20\u5230DataStage\u670d\u52a1\u7aef\uff0c\u5982 /home/dsadm \u8def\u5f84\u4e0b\uff0c\u5c06\u8bc1\u4e66\u5bfc\u5165\u5230keystore\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003\uff1a /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca.cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks \u751f\u6210\u5e76\u4fdd\u5b58\u52a0\u5bc6\u540e\u7684keystore\u5bc6\u7801 \u4f7f\u7528 vi /home/dsadm/authenticate.properties \u547d\u4ee4\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6\uff0c\u4fdd\u5b58\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5bc6\u6587\uff1a password={iisenc}SvtJ2f/uNTrvbuh26XDzag== \u6267\u884c chown dsadm:dstage /home/dsadm/ authenticate.properties \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u7684\u5c5e\u4e3b \u5bfc\u51fatruststore\u73af\u5883\u53d8\u91cf \u4f7f\u7528 vi /opt/IBM/InformationServer/Server/DSEngine/dsenv \u7f16\u8f91DSEngine\u7684\u73af\u5883\u53d8\u91cf\uff0c\u5728\u6700\u540e\u6dfb\u52a0 export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties \u91cd\u542fDSEngine\uff0c\u53c2\u8003\u547d\u4ee4 su - dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfs2sf \u6dfb\u52a0File_Connector\u7ec4\u4ef6\u548cSequential File\u7ec4\u4ef6\uff0c\u4ee5\u53caFile_Connector\u5230Sequential File\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u7f16\u8bd1\uff0c\u8fd0\u884c \u5728\u83dc\u5355 Tools -> Run Director \u4e2d\u6253\u5f00Director\u5ba2\u6237\u7aef\uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7 \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u5199\u5165HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfswrite \u6dfb\u52a0Row Generator\u7ec4\u4ef6\u548cFile Connector\u7ec4\u4ef6\uff0c\u4ee5\u53caRow Generator\u5230File Connector\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u5199\u5165\u6570\u636e \u5bf9\u63a5Hive \u00b6 \u4f7f\u7528Hive Connector \u00b6 \u8bf4\u660e\uff1aHive Connector\u5b98\u65b9\u8ba4\u8bc1\u8fc7\u7684Hive JDBC Driver\u53ea\u6709DataDirect Hive Driver(IShive.jar)\uff0c\u7528DataStage 11.5.0.2\u4e2d\u81ea\u5e26\u7684IShive.jar\u8fde\u63a5FusionInsight\u7684hive\u65f6\uff0c\u4f1a\u6709thrift protocol\u62a5\u9519\uff0c\u9700\u8981\u54a8\u8be2IBM\u6280\u672f\u652f\u6301\u63d0\u4f9b\u7684\u6700\u65b0\u7684IShive.jar \u8bbe\u7f6eJDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u5728$DSHOME\u8def\u5f84\u4e0b\u521b\u5efaisjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0DataDirect Hive Driver (IShive.jar)\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.ibm.isf.jdbc.hive.HiveDriver\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u5728isjdbc.config\u4e2d\u6dfb\u52a0\u5982\u4e0b\u4fe1\u606f: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver \u914d\u7f6eKerberos\u8ba4\u8bc1\u4fe1\u606f\uff1a \u5728IShive.jar\u6240\u5728\u76ee\u5f55\u4e0b\u521b\u5efaJDBCDriverLogin.conf cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\u5982\u4e0b\u8fdb\u884c\u914d\u7f6e\uff1a jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; \u5176\u4e2dJDBC_DRIVER_test_keytab\u4e3a\u4e0a\u4e00\u6b65\u6307\u5b9a\u7684\u9274\u6743\u4fe1\u606f \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u6570\u636e\u5199\u5165Hive\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff0c\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62\u201911\u201d \u67e5\u770bHive\u8868\u6570\u636e\uff1a Hive Connector\u5411Hive\u8868\u5199\u6570\u636e\u4f7f\u7528Insert\u8bed\u53e5\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002\u53ef\u4ee5\u5c06\u6570\u636e\u76f4\u63a5\u5199\u5165HDFS\u6587\u4ef6\u3002 \u4f7f\u7528JDBC Connector \u00b6 \u5982\u679c\u8981\u4f7f\u7528FusionInsight\u7684Hive JDBC\u9a71\u52a8\uff0c \u7528isjdbc.config\u6587\u4ef6CLASSPATH\u4e2d\u6dfb\u52a0jdbc\u9a71\u52a8\u548c\u4f9d\u8d56\u5305\u7684\u65b9\u5f0f\uff0c\u5728\u8fd0\u884c\u4f5c\u4e1a\u65f6\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff0c\u6b64\u65f6\u9700\u8981\u7528\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u7684\u65b9\u5f0f\u52a0\u8f7d \u800c\u4e14\u53ea\u80fd\u7528JDBC Connector\uff0c\u4e0d\u80fd\u7528Hive Connector\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 Hive jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eHive\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Hive/Beeline/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u8fd9\u4e9bjar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm vi $DSHOME/dsenv \u6587\u4ef6\u6700\u540e\u6dfb\u52a0\u76f8\u5173\u7684jar\u5305\uff08\u5177\u4f53\u8def\u5f84\u6839\u636e\u5b9e\u9645\u73af\u5883\u8c03\u6574\uff09 export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3.jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastore-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j-1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper-3.5.1.jar \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u5176\u4e2dURL\u4e3a\uff1a jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c \u6570\u636e\u5199\u5165Hive\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5199\u51655\u6761\u6570\u636e\uff0c\u7528\u65f61\u201949\u201d \u6570\u636e\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u5199\u5165\u6570\u636e hive\u8868\u6570\u636e\u589e\u91cf100 \u589e\u91cf\u6570\u636e\u5b9a\u671f\u81ea\u52a8\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6 \u00b6 \u589e\u91cf\u6570\u636e\u53ef\u4ee5\u65b0\u589eHDFS\u6587\u4ef6\u7684\u65b9\u5f0f\u5bfc\u5165hive\uff0c\u5982\u679c\u8981\u5b9a\u671f\u81ea\u52a8\u5316\u6267\u884c\uff0c\u5bfc\u5165\u7684\u6587\u4ef6\u540d\u4e2d\u9700\u8981\u5305\u542b\u53ef\u53d8\u53c2\u6570\u8fdb\u884c\u8bbe\u7f6e\u548c\u533a\u5206\uff0c\u7136\u540e\u4ee5\u547d\u4ee4\u6216\u811a\u672c\u65b9\u5f0f\u8fd0\u884c\u4f5c\u4e1a\uff0c\u7ed9\u8be5\u53c2\u6570\u8d4b\u503c\u3002 \u521b\u5efa\u4f5c\u4e1a \u8bbe\u7f6e\u4f5c\u4e1a\u53c2\u6570 \u70b9\u51fb\u201cjob properties\u201d\u6309\u94ae\uff0c\u8bbe\u7f6e\u53c2\u6570\u5982\u4e0b \u4fee\u6539\u914d\u7f6e File Connector\u914d\u7f6e\u5bfc\u51fa\u6587\u4ef6\u7684\u540d\u79f0\uff0c\u4ee5\u201c#\u201d\u5f15\u7528\u8bbe\u7f6e\u7684\u53c2\u6570 dsjob\u547d\u4ee4\u8fd0\u884c\u4f5c\u4e1a \u4fdd\u5b58\u7f16\u8bd1\u4f5c\u4e1a\uff0c\u5728DataStage Server\u4e0a\u6267\u884cdsjob -run\u547d\u4ee4\uff0c\u683c\u5f0f\u4e3a\uff1a dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME \u547d\u4ee4\u53c2\u8003: su - dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append \u67e5\u770bHDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u6570\u636e\u589e\u91cf\u4e3a200\u6761 \u5bf9\u63a5SparkSQL \u00b6 \u4e0e\u4f7f\u7528FI Hive JDBC\u9a71\u52a8\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u7528SparkSQL JDBC\u9a71\u52a8\u8fde\u63a5Hive\uff0c\u540c\u6837\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 SparkSQL jdbc\u4e0d\u652f\u6301insert into\u8bed\u53e5\uff0c\u53ea\u80fd\u7528\u6765\u8bfbhive\u6570\u636e\uff0c\u4e0d\u80fd\u63d2\u5165\u6570\u636e\u5230hive\u8868\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 SparkSQL jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eSpark\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Spark/spark/lib/ \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caspark\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08SparkSQL jdbc\u8fde\u63a5hive\u65f6\u9700\u8981\u8bfb\u53d6hive-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7.1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark/spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Spark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17.jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark/spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u8bfb\u53d6Hive\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c \u5bf9\u63a5Phoenix \u00b6 \u4f7f\u7528Phoenix\u4ee5JDBC\u65b9\u5f0f\u8bbf\u95eeHBase\u8868\uff0c\u4e5f\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf \u00b6 Phoenix\u76f8\u5173\u7684jar\u5305\u4f4d\u4e8eHBase\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/HBase/hbase/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caHBase\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08phoenix\u8fde\u63a5\u65f6\u9700\u8981\u8bfb\u53d6hbase-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons-collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30.jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-secondaryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib/httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6.jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt/ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start \u521b\u5efajaas\u914d\u7f6e\u6587\u4ef6 \u00b6 Phoenix\u8fde\u63a5\u9700\u8981\u67e5\u8be2zookeeper \uff0czookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6 su - admin vi /home/dsadm/jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u8bfb\u53d6Phoenix\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab \u914d\u7f6eJVM options\u4e3a -Djava.security.auth.login.config=/home/dsadm/jaas.conf \u7f16\u8bd1\u8fd0\u884c \u5199\u5165Phoenix\u8868\u6570\u636e \u00b6 Phoenix\u63d2\u5165\u8bed\u53e5\u662fupsert into\uff0c\u4e0d\u652f\u6301Insert into \u8bed\u53e5\uff0c\u6240\u4ee5\u4e0d\u80fd\u7528JDBC Connector\u5728\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210SQL\u8bed\u53e5\uff0c\u9700\u8981\u81ea\u5df1\u586b\u5199\uff0c\u5426\u5219\u4f1a\u62a5\u9519\uff1a main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 (42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5bf9\u63a5Fiber \u00b6 \u5bf9\u63a5Fiber\u9700\u8981\u5148\u5b89\u88c5FI\u5ba2\u6237\u7aef \u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0Fiber jdbc driver\u53ca\u4f9d\u8d56\u5305\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver; org.apache.phoenix.jdbc.PhoenixDriver \u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\u5982\u4e0b\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar;/opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10.jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org.apache.phoenix.jdbc.PhoenixDriver \u4fee\u6539Fiber\u914d\u7f6e\u6587\u4ef6 \u00b6 DataStage\u4f7f\u7528IBM jdk\uff0c\u9700\u8981\u65b0\u5efaFiber\u914d\u7f6e\u6587\u4ef6\u7ed9DataStage\u4f7f\u7528 cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml \u4fee\u6539fiber_ibm.xml\u4e2dphoenix,hive,spark\u5404driver\u7684\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570\uff1a java.security.auth.login.config \u4fee\u6539\u4e3a /home/dsadm/jaas.conf zookeeper.kinit \u4fee\u6539\u4e3a /opt/IBM/InformationServer/jdk/jre/bin/kinit \u6587\u4ef6/home/dsadm/jaas.conf\u7684\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u5176\u5b83\u914d\u7f6e\u9879\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863Fiber\u5ba2\u6237\u7aef\u914d\u7f6e\u6307\u5bfc\u4fee\u6539\u3002 \u4f7f\u7528Hive Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Hive Driver\u5199\u5165\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Spark Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark \u7f16\u8bd1\u8fd0\u884c \u4f7f\u7528Phoenix Driver\u8bfb\u53d6\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u76ee\u524d\u672a\u80fd\u8bfb\u53d6\u5230\u6570\u636e\uff0c\u201dThe connector could not determine the value for the fetch size.\u201d\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d \u4f7f\u7528Phoenix Driver\u5199\u5165\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u5199\u5165\u6570\u636e0\u884c\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d \u5bf9\u63a5Kafka \u00b6 \u8bf4\u660e\uff1akafka Connector\u4e0d\u652f\u6301\u53d1\u9001\u6216\u8005\u6d88\u8d39integer, float, double, numeric, decimal\u7b49\u6570\u503c\u7c7b\u578b\u7684\u5b57\u6bb5\uff0c\u9700\u8981\u8f6c\u6362\u6210char, varchar, longvarchar\u7b49\u7c7b\u578b\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff1a main_program: APT_PMsectionLeader(2, node2), player 2 - Unexpected termination by Unix signal 9(SIGKILL). \u5b89\u88c5kafka\u5ba2\u6237\u7aef \u00b6 kafka Connector\u9700\u8981\u914d\u7f6eKafka client Classpath\uff0c\u53ef\u4ee5\u5728DataStage\u8282\u70b9\u5b89\u88c5kafka\u5ba2\u6237\u7aef\u6765\u83b7\u53d6kafka-client jar\u5305\u3002\u5b89\u88c5\u6b65\u9aa4\u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u3002 Kafka Client Classpath \u9700\u8981\u63d0\u4f9bkafka-client, log4j, slf4j-api \u4e09\u4e2ajar\u5305\u7684\u8def\u5f84\uff0c\u5982\uff1a /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs/slf4j-api-1.7.21.jar \u53d1\u9001\u6d88\u606f\u5230kafka \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e RowGenerator \u751f\u6210\u6570\u636e transformer\u6570\u636e\u7c7b\u578b\u8f6c\u6362\uff1a Kafka\u914d\u7f6e\uff1a \u7f16\u8bd1\u8fd0\u884c \u8bfb\u53d6Kafka\u6d88\u606f \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e \u5bf9\u63a5MPPDB \u00b6 \u83b7\u53d6MPPDB JDBC Driver \u00b6 \u4eceMPPDB\u53d1\u5e03\u5305\u4e2d\u83b7\u53d6\uff0c\u5305\u540d\u4e3aGauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz \u89e3\u538b\u540e\u5f97\u5230gsjdbc4.jar\uff0c\u4e0a\u4f20\u5230DataStage Server \u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6 \u00b6 \u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0MPPDB Driver \u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0org.postgresql.Driver su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver; \u8bfb\u53d6MPPDB\u8868\u6570\u636e \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u6570\u636e\u5199\u5165MPPDB\u8868 \u00b6 \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u67e5\u770bMPPDB\u8868\u6570\u636e\uff1a","title":"11.5.0.2 <--> C60"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#ibm-infosphere-datastagefusioninsight","text":"","title":"IBM InfoSphere DataStage\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_1","text":"IBM InfoSphere DataStage 11.3.1.0 \u2194 FusionInsight HD V100R002C50 (HDFS/Hive/SparkSQL) IBM InfoSphere DataStage 11.5.0.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive/Phoenix/SparkSQL/Kafka/GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_2","text":"\u5df2\u5b8c\u6210IBM InfoSphere DataStage 11.5.0.2\u7684\u5b89\u88c5\u90e8\u7f72\uff08\u672c\u6587\u90e8\u7f72\u5728Centos7.2\u4e0a\uff09 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD V100R002C60U20","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_3","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#_4","text":"\u4f7f\u7528 vi /etc/hosts \u547d\u4ee4\u4fee\u6539DataStage Server\u548cClient\u7684hosts\u6587\u4ef6\uff0c\u6dfb\u52a0FI\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f\uff0c\u5982\uff1a 162.1.61.42 FusionInsight2 162.1.61.41 FusionInsight1 162.1.61.43 FusionInsight3","title":"\u914d\u7f6e\u57df\u540d\u89e3\u6790"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kerberos","text":"\u5728FI\u7ba1\u7406\u754c\u9762\u521b\u5efaDataStage\u5bf9\u63a5\u7528\u6237\uff0c\u5e76\u8d4b\u4e88\u8be5\u7528\u6237\u6240\u9700\u6743\u9650\uff0c\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u4e0b\u8f7d\u7684tar\u6587\u4ef6\uff0c\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u7684keytab\u6587\u4ef6\u3002 \u4ee5root\u767b\u5f55DataStage Server\u8282\u70b9\uff0c\u5c06FI\u96c6\u7fa4\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u5230 /etc \u76ee\u5f55\u3002 \u5c06\u7528\u6237\u7684user.keytab\u6587\u4ef6\u4e0a\u4f20\u5230DataStage Server\u8282\u70b9\u7684\u4efb\u610f\u76ee\u5f55\uff0c\u5982 /home/dsadm \u3002","title":"\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fusioninsight","text":"\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863\uff0c\u5728FI\u670d\u52a1\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230DataStageServer\uff0c\u5b89\u88c5\u81f3\u81ea\u5b9a\u4e49\u76ee\u5f55\uff0c\u5982 /opt/ficlient \u3002","title":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fissl","text":"\u6d4f\u89c8\u5668\u5bfc\u51faFI\u96c6\u7fa4\u7684\u6839\u8bc1\u4e66 \u6d4f\u89c8\u5668\u6253\u5f00FI\u7ba1\u7406\u754c\u9762\uff0c\u67e5\u770b\u8bc1\u4e66\uff0c\u70b9\u51fb\u201c\u8bc1\u4e66\u8def\u5f84\u201d\u9875\u7b7e\uff0c\u9009\u62e9\u6839\u8def\u5f84\uff0c\u67e5\u770b\u6839\u8bc1\u4e66\uff0c\u5728\u201c\u8be6\u7ec6\u4fe1\u606f\u201d\u9875\u7b7e\u4e0b\uff0c\u70b9\u51fb\u201c\u590d\u5236\u5230\u6587\u4ef6\u201d\uff0c\u5bfc\u51fa\u4e3acer\u683c\u5f0f \u8bc1\u4e66\u5bfc\u5165DataStage\u7684keystore\u6587\u4ef6 \u5c06\u5bfc\u51fa\u7684FI\u6839\u8bc1\u4e66fi-root-ca.cer\u4e0a\u4f20\u5230DataStage\u670d\u52a1\u7aef\uff0c\u5982 /home/dsadm \u8def\u5f84\u4e0b\uff0c\u5c06\u8bc1\u4e66\u5bfc\u5165\u5230keystore\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003\uff1a /opt/IBM/InformationServer/jdk/bin/keytool -importcert -file /home/dsadm/fi-root-ca.cer -keystore /home/dsadm/iis-ds-truststore_ssl.jks -alias fi-root-ca.cer -storepass Huawei@123 -trustcacerts -noprompt chown dsadm:dstage /home/dsadm/iis-ds-truststore_ssl.jks \u751f\u6210\u5e76\u4fdd\u5b58\u52a0\u5bc6\u540e\u7684keystore\u5bc6\u7801 \u4f7f\u7528 vi /home/dsadm/authenticate.properties \u547d\u4ee4\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6\uff0c\u4fdd\u5b58\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5bc6\u6587\uff1a password={iisenc}SvtJ2f/uNTrvbuh26XDzag== \u6267\u884c chown dsadm:dstage /home/dsadm/ authenticate.properties \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u7684\u5c5e\u4e3b \u5bfc\u51fatruststore\u73af\u5883\u53d8\u91cf \u4f7f\u7528 vi /opt/IBM/InformationServer/Server/DSEngine/dsenv \u7f16\u8f91DSEngine\u7684\u73af\u5883\u53d8\u91cf\uff0c\u5728\u6700\u540e\u6dfb\u52a0 export DS_TRUSTSTORE_LOCATION=/home/dsadm/iis-ds-truststore_ssl.jks export DS_TRUSTSTORE_PROPERTIES=/home/dsadm/authenticate.properties \u91cd\u542fDSEngine\uff0c\u53c2\u8003\u547d\u4ee4 su - dsadm cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u5bfc\u5165FI\u96c6\u7fa4\u7684SSL\u8bc1\u4e66"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs_1","text":"\u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfs2sf \u6dfb\u52a0File_Connector\u7ec4\u4ef6\u548cSequential File\u7ec4\u4ef6\uff0c\u4ee5\u53caFile_Connector\u5230Sequential File\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58\u914d\u7f6e\u540e\uff0c\u7f16\u8bd1\uff0c\u8fd0\u884c \u5728\u83dc\u5355 Tools -> Run Director \u4e2d\u6253\u5f00Director\u5ba2\u6237\u7aef\uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7 \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hdfs_2","text":"\u521b\u5efa\u4f5c\u4e1a \u65b0\u5efa\u5e76\u884c\u4f5c\u4e1a\uff0c\u4fdd\u5b58\u4e3ahdfswrite \u6dfb\u52a0Row Generator\u7ec4\u4ef6\u548cFile Connector\u7ec4\u4ef6\uff0c\u4ee5\u53caRow Generator\u5230File Connector\u94fe\u63a5 \u53c2\u8003\u4e0b\u56fe\u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u5199\u5165\u6570\u636e","title":"\u5199\u5165HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-connector","text":"\u8bf4\u660e\uff1aHive Connector\u5b98\u65b9\u8ba4\u8bc1\u8fc7\u7684Hive JDBC Driver\u53ea\u6709DataDirect Hive Driver(IShive.jar)\uff0c\u7528DataStage 11.5.0.2\u4e2d\u81ea\u5e26\u7684IShive.jar\u8fde\u63a5FusionInsight\u7684hive\u65f6\uff0c\u4f1a\u6709thrift protocol\u62a5\u9519\uff0c\u9700\u8981\u54a8\u8be2IBM\u6280\u672f\u652f\u6301\u63d0\u4f9b\u7684\u6700\u65b0\u7684IShive.jar","title":"\u4f7f\u7528Hive Connector"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver","text":"\u5728$DSHOME\u8def\u5f84\u4e0b\u521b\u5efaisjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0DataDirect Hive Driver (IShive.jar)\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.ibm.isf.jdbc.hive.HiveDriver\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u5728isjdbc.config\u4e2d\u6dfb\u52a0\u5982\u4e0b\u4fe1\u606f: CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver \u914d\u7f6eKerberos\u8ba4\u8bc1\u4fe1\u606f\uff1a \u5728IShive.jar\u6240\u5728\u76ee\u5f55\u4e0b\u521b\u5efaJDBCDriverLogin.conf cd /opt/IBM/InformationServer/ASBNode/lib/java/ vi JDBCDriverLogin.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a JDBC_DRIVER_test_cache{ com.ibm.security.auth.module.Krb5LoginModule required credsType=initiator principal=\"test@HADOOP.COM\" useCcache=\"FILE:/tmp/krb5cc_1004\"; }; JDBC_DRIVER_test_keytab{ com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; };","title":"\u8bbe\u7f6eJDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\u5982\u4e0b\u8fdb\u884c\u914d\u7f6e\uff1a jdbc:ibm:hive://162.1.61.41:21066;DataBaseName=default;AuthenticationMethod=kerberos;ServicePrincipalName=hive/hadoop.hadoop.com@HADOOP.COM;loginConfigName=JDBC_DRIVER_test_keytab; \u5176\u4e2dJDBC_DRIVER_test_keytab\u4e3a\u4e0a\u4e00\u6b65\u6307\u5b9a\u7684\u9274\u6743\u4fe1\u606f \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff1a \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u4fdd\u5b58 \u2014 \u7f16\u8bd1 \u2014 \u8fd0\u884c \uff0c\u67e5\u770b\u4f5c\u4e1a\u65e5\u5fd7\uff0c\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62\u201911\u201d \u67e5\u770bHive\u8868\u6570\u636e\uff1a Hive Connector\u5411Hive\u8868\u5199\u6570\u636e\u4f7f\u7528Insert\u8bed\u53e5\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u4f7f\u7528\u8fd9\u79cd\u65b9\u5f0f\u3002\u53ef\u4ee5\u5c06\u6570\u636e\u76f4\u63a5\u5199\u5165HDFS\u6587\u4ef6\u3002","title":"\u6570\u636e\u5199\u5165Hive\u8868"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-connector","text":"\u5982\u679c\u8981\u4f7f\u7528FusionInsight\u7684Hive JDBC\u9a71\u52a8\uff0c \u7528isjdbc.config\u6587\u4ef6CLASSPATH\u4e2d\u6dfb\u52a0jdbc\u9a71\u52a8\u548c\u4f9d\u8d56\u5305\u7684\u65b9\u5f0f\uff0c\u5728\u8fd0\u884c\u4f5c\u4e1a\u65f6\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff0c\u6b64\u65f6\u9700\u8981\u7528\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u7684\u65b9\u5f0f\u52a0\u8f7d \u800c\u4e14\u53ea\u80fd\u7528JDBC Connector\uff0c\u4e0d\u80fd\u7528Hive Connector\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519","title":"\u4f7f\u7528JDBC Connector"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath","text":"Hive jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eHive\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Hive/Beeline/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u8fd9\u4e9bjar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u53c2\u8003\u547d\u4ee4\uff1a su - dsadm vi $DSHOME/dsenv \u6587\u4ef6\u6700\u540e\u6dfb\u52a0\u76f8\u5173\u7684jar\u5305\uff08\u5177\u4f53\u8def\u5f84\u6839\u636e\u5b9e\u9645\u73af\u5883\u8c03\u6574\uff09 export CLASSPATH=/opt/ficlient/Hive/Beeline/lib/commons-cli-1.2.jar:/opt/ficlient/Hive/Beeline/lib/commons-collections-3.2.1.jar:/opt/ficlient/Hive/Beeline/lib/commons-configuration-1.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-lang-2.6.jar:/opt/ficlient/Hive/Beeline/lib/commons-logging-1.1.3.jar:/opt/ficlient/Hive/Beeline/lib/curator-client-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-framework-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/curator-recipes-2.7.1.jar:/opt/ficlient/Hive/Beeline/lib/guava-14.0.1.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Hive/Beeline/lib/hive-beeline-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-cli-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-exec-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-jdbc-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-metastore-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-serde-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-service-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-0.23-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/hive-shims-common-1.3.0.jar:/opt/ficlient/Hive/Beeline/lib/httpclient-4.5.2.jar:/opt/ficlient/Hive/Beeline/lib/httpcore-4.4.jar:/opt/ficlient/Hive/Beeline/lib/jline-2.12.jar:/opt/ficlient/Hive/Beeline/lib/libfb303-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/libthrift-0.9.3.jar:/opt/ficlient/Hive/Beeline/lib/log4j-1.2.17.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-api-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/slf4j-log4j12-1.7.5.jar:/opt/ficlient/Hive/Beeline/lib/super-csv-2.2.0.jar:/opt/ficlient/Hive/Beeline/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Hive/Beeline/lib/zookeeper-3.5.1.jar \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_3","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u5176\u4e2dURL\u4e3a\uff1a jdbc:hive2://162.1.61.41:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_4","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u5199\u51655\u6761\u6570\u636e\uff0c\u7528\u65f61\u201949\u201d","title":"\u6570\u636e\u5199\u5165Hive\u8868"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hivehdfs","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u5199\u5165\u6570\u636e hive\u8868\u6570\u636e\u589e\u91cf100","title":"\u6570\u636e\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hivehdfs_1","text":"\u589e\u91cf\u6570\u636e\u53ef\u4ee5\u65b0\u589eHDFS\u6587\u4ef6\u7684\u65b9\u5f0f\u5bfc\u5165hive\uff0c\u5982\u679c\u8981\u5b9a\u671f\u81ea\u52a8\u5316\u6267\u884c\uff0c\u5bfc\u5165\u7684\u6587\u4ef6\u540d\u4e2d\u9700\u8981\u5305\u542b\u53ef\u53d8\u53c2\u6570\u8fdb\u884c\u8bbe\u7f6e\u548c\u533a\u5206\uff0c\u7136\u540e\u4ee5\u547d\u4ee4\u6216\u811a\u672c\u65b9\u5f0f\u8fd0\u884c\u4f5c\u4e1a\uff0c\u7ed9\u8be5\u53c2\u6570\u8d4b\u503c\u3002 \u521b\u5efa\u4f5c\u4e1a \u8bbe\u7f6e\u4f5c\u4e1a\u53c2\u6570 \u70b9\u51fb\u201cjob properties\u201d\u6309\u94ae\uff0c\u8bbe\u7f6e\u53c2\u6570\u5982\u4e0b \u4fee\u6539\u914d\u7f6e File Connector\u914d\u7f6e\u5bfc\u51fa\u6587\u4ef6\u7684\u540d\u79f0\uff0c\u4ee5\u201c#\u201d\u5f15\u7528\u8bbe\u7f6e\u7684\u53c2\u6570 dsjob\u547d\u4ee4\u8fd0\u884c\u4f5c\u4e1a \u4fdd\u5b58\u7f16\u8bd1\u4f5c\u4e1a\uff0c\u5728DataStage Server\u4e0a\u6267\u884cdsjob -run\u547d\u4ee4\uff0c\u683c\u5f0f\u4e3a\uff1a dsjob -run [-mode ] -param = -jobstatus PROJECT_NAME JOB_NAME \u547d\u4ee4\u53c2\u8003: su - dsadm cd $DSHOME/bin ./dsjob -run -param jobruntime=`date +'%Y-%m-%d-%H-%M-%S'` -jobstatus dstage1 hive_append \u67e5\u770bHDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u6570\u636e\u589e\u91cf\u4e3a200\u6761","title":"\u589e\u91cf\u6570\u636e\u5b9a\u671f\u81ea\u52a8\u5bfc\u5165Hive\u8868\u7684HDFS\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#sparksql","text":"\u4e0e\u4f7f\u7528FI Hive JDBC\u9a71\u52a8\u7c7b\u4f3c\uff0c\u53ef\u4ee5\u7528SparkSQL JDBC\u9a71\u52a8\u8fde\u63a5Hive\uff0c\u540c\u6837\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002 SparkSQL jdbc\u4e0d\u652f\u6301insert into\u8bed\u53e5\uff0c\u53ea\u80fd\u7528\u6765\u8bfbhive\u6570\u636e\uff0c\u4e0d\u80fd\u63d2\u5165\u6570\u636e\u5230hive\u8868\u3002","title":"\u5bf9\u63a5SparkSQL"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath_1","text":"SparkSQL jdbc\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u4f4d\u4e8eSpark\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/Spark/spark/lib/ \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caspark\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08SparkSQL jdbc\u8fde\u63a5hive\u65f6\u9700\u8981\u8bfb\u53d6hive-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/Spark/spark/lib/commons-collections-3.2.2.jar:/opt/ficlient/Spark/spark/lib/commons-configuration-1.6.jar:/opt/ficlient/Spark/spark/lib/commons-lang-2.6.jar:/opt/ficlient/Spark/spark/lib/commons-logging-1.1.3.jar:/opt/ficlient/Spark/spark/lib/curator-client-2.7.1.jar:/opt/ficlient/Spark/spark/lib/curator-framework-2.7.1.jar:/opt/ficlient/Spark/spark/lib/guava-12.0.1.jar:/opt/ficlient/Spark/spark/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-common-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hadoop-mapreduce-client-core-2.7.2.jar:/opt/ficlient/Spark/spark/lib/hive-common-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-exec-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-jdbc-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-metastore-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/hive-service-1.2.1.spark.jar:/opt/ficlient/Spark/spark/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/Spark/spark/lib/httpclient-4.5.2.jar:/opt/ficlient/Spark/spark/lib/httpcore-4.4.4.jar:/opt/ficlient/Spark/spark/lib/libthrift-0.9.3.jar:/opt/ficlient/Spark/spark/lib/log4j-1.2.17.jar:/opt/ficlient/Spark/spark/lib/slf4j-api-1.7.10.jar:/opt/ficlient/Spark/spark/lib/slf4j-log4j12-1.7.10.jar:/opt/ficlient/Spark/spark/lib/xercesImpl-2.9.1.jar:/opt/ficlient/Spark/spark/lib/zookeeper-3.5.1.jar:/opt/ficlient/Spark/spark/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive_5","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:hive2://ha-cluster/default;user.principal=spark/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test@HADOOP.COM;user.keytab=/home/dsadm/user.keytab; \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Hive\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix","text":"\u4f7f\u7528Phoenix\u4ee5JDBC\u65b9\u5f0f\u8bbf\u95eeHBase\u8868\uff0c\u4e5f\u9700\u8981\u5bfc\u51faCLASSPATH\u73af\u5883\u53d8\u91cf\u6765\u52a0\u8f7d\u9a71\u52a8\u5305\u53ca\u4f9d\u8d56\u5305\u3002","title":"\u5bf9\u63a5Phoenix"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#classpath_2","text":"Phoenix\u76f8\u5173\u7684jar\u5305\u4f4d\u4e8eHBase\u5ba2\u6237\u7aeflib\u76ee\u5f55\u4e0b /opt/ficlient/HBase/hbase/lib \uff0c\u82e5\u672a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4e5f\u53ef\u5355\u72ec\u4e0a\u4f20\u6240\u9700jar\u5305\u5230\u4efb\u610f\u76ee\u5f55\u3002 \u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf\uff0c\u6dfb\u52a0\u4e0a\u8ff0jar\u5305\u7684\u5b8c\u6574\u8def\u5f84\uff0c\u4ee5\u53caHBase\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u8def\u5f84\uff08phoenix\u8fde\u63a5\u65f6\u9700\u8981\u8bfb\u53d6hbase-site.xml\u4e2d\u7684\u914d\u7f6e\uff09\uff1a su - dsadm vi $DSHOME/dsenv \u914d\u7f6e\u5982\u4e0b\u5185\u5bb9\uff1a export CLASSPATH= /opt/ficlient/HBase/hbase/lib/commons-cli-1.2.jar:/opt/ficlient/HBase/hbase/lib/commons-codec-1.9.jar:/opt/ficlient/HBase/hbase/lib/commons-collections-3.2.2.jar:/opt/ficlient/HBase/hbase/lib/commons-configuration-1.6.jar:/opt/ficlient/HBase/hbase/lib/commons-io-2.4.jar:/opt/ficlient/HBase/hbase/lib/commons-lang-2.6.jar:/opt/ficlient/HBase/hbase/lib/commons-logging-1.2.jar:/opt/ficlient/HBase/hbase/lib/dynalogger-V100R002C30.jar:/opt/ficlient/HBase/hbase/lib/gson-2.2.4.jar:/opt/ficlient/HBase/hbase/lib/guava-12.0.1.jar:/opt/ficlient/HBase/hbase/lib/hadoop-auth-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-common-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hadoop-hdfs-client-2.7.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-client-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-common-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbaseFileStream-1.0.jar:/opt/ficlient/HBase/hbase/lib/hbase-protocol-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-secondaryindex-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/hbase-server-1.0.2.jar:/opt/ficlient/HBase/hbase/lib/htrace-core-3.1.0-incubating.jar:/opt/ficlient/HBase/hbase/lib/httpclient-4.5.2.jar:/opt/ficlient/HBase/hbase/lib/httpcore-4.4.4.jar:/opt/ficlient/HBase/hbase/lib/httpmime-4.3.6.jar:/opt/ficlient/HBase/hbase/lib/jackson-core-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/jackson-mapper-asl-1.9.13.jar:/opt/ficlient/HBase/hbase/lib/log4j-1.2.17.jar:/opt/ficlient/HBase/hbase/lib/luna-0.1.jar:/opt/ficlient/HBase/hbase/lib/netty-3.2.4.Final.jar:/opt/ficlient/HBase/hbase/lib/netty-all-4.0.23.Final.jar:/opt/ficlient/HBase/hbase/lib/noggit-0.6.jar:/opt/ficlient/HBase/hbase/lib/phoenix-core-4.4.0-HBase-1.0.jar:/opt/ficlient/HBase/hbase/lib/protobuf-java-2.5.0.jar:/opt/ficlient/HBase/hbase/lib/slf4j-api-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/slf4j-log4j12-1.7.7.jar:/opt/ficlient/HBase/hbase/lib/solr-solrj-5.3.1.jar:/opt/ficlient/HBase/hbase/lib/zookeeper-3.5.1.jar:/opt/ficlient/HBase/hbase/conf \u5bfc\u5165\u73af\u5883\u53d8\u91cf source $DSHOME/dsenv \u91cd\u542fDSEngine cd $DSHOME bin/uv -admin -stop bin/uv -admin -start","title":"\u8bbe\u7f6eCLASSPATH\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jaas","text":"Phoenix\u8fde\u63a5\u9700\u8981\u67e5\u8be2zookeeper \uff0czookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6 su - admin vi /home/dsadm/jaas.conf \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; };","title":"\u521b\u5efajaas\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:phoenix:fusioninsight3,fusioninsight2,fusioninsight1:24002:/hbase:test@HADOOP.COM:/home/dsadm/user.keytab \u914d\u7f6eJVM options\u4e3a -Djava.security.auth.login.config=/home/dsadm/jaas.conf \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6Phoenix\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix_2","text":"Phoenix\u63d2\u5165\u8bed\u53e5\u662fupsert into\uff0c\u4e0d\u652f\u6301Insert into \u8bed\u53e5\uff0c\u6240\u4ee5\u4e0d\u80fd\u7528JDBC Connector\u5728\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210SQL\u8bed\u53e5\uff0c\u9700\u8981\u81ea\u5df1\u586b\u5199\uff0c\u5426\u5219\u4f1a\u62a5\u9519\uff1a main_program: Fatal Error: The connector failed to prepare the statement: INSERT INTO us_population (STATE, CITY, POPULATION) VALUES (?, ?, ?). The reported error is: org.apache.phoenix.exception.PhoenixParserException: ERROR 601 (42P00): Syntax error. Encountered \"INSERT\" at line 1, column 1.. \u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c","title":"\u5199\u5165Phoenix\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fiber","text":"\u5bf9\u63a5Fiber\u9700\u8981\u5148\u5b89\u88c5FI\u5ba2\u6237\u7aef","title":"\u5bf9\u63a5Fiber"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver_1","text":"\u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0Fiber jdbc driver\u53ca\u4f9d\u8d56\u5305\u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver; org.apache.phoenix.jdbc.PhoenixDriver \u53c2\u8003\u547d\u4ee4\uff1a su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\u5982\u4e0b\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar;/opt/Progress/DataDirect/JDBC\\_60/lib/mongodb.jar;/opt/ficlient/Fiber/lib/commons-cli-1.2.jar;/opt/ficlient/Fiber/lib/commons-logging-1.1.3.jar;/opt/ficlient/Fiber/lib/fiber-jdbc-1.0.jar;/opt/ficlient/Fiber/lib/hadoop-common-2.7.2.jar;/opt/ficlient/Fiber/lib/hive-beeline-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-common-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/hive-jdbc-1.2.1.spark.jar;/opt/ficlient/Fiber/lib/jline-2.12.jar;/opt/ficlient/Fiber/lib/log4j-1.2.17.jar;/opt/ficlient/Fiber/lib/slf4j-api-1.7.10.jar;/opt/ficlient/Fiber/lib/slf4j-log4j12-1.7.10.jar;/opt/ficlient/Fiber/lib/super-csv-2.2.0.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;com.ddtek.jdbc.mongodb.MongoDBDriver;com.huawei.fiber.FiberDriver;org.apache.hive.jdbc.HiveDriver;org.apache.phoenix.jdbc.PhoenixDriver","title":"\u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#fiber_1","text":"DataStage\u4f7f\u7528IBM jdk\uff0c\u9700\u8981\u65b0\u5efaFiber\u914d\u7f6e\u6587\u4ef6\u7ed9DataStage\u4f7f\u7528 cd /opt/ficlient/Fiber/conf cp fiber.xml fiber_ibm.xml \u4fee\u6539fiber_ibm.xml\u4e2dphoenix,hive,spark\u5404driver\u7684\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570\uff1a java.security.auth.login.config \u4fee\u6539\u4e3a /home/dsadm/jaas.conf zookeeper.kinit \u4fee\u6539\u4e3a /opt/IBM/InformationServer/jdk/jre/bin/kinit \u6587\u4ef6/home/dsadm/jaas.conf\u7684\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.ibm.security.auth.module.Krb5LoginModule required credsType=both principal=\"test@HADOOP.COM\" useKeytab=\"/home/dsadm/user.keytab\"; }; \u5176\u5b83\u914d\u7f6e\u9879\u53c2\u8003FI\u4ea7\u54c1\u6587\u6863Fiber\u5ba2\u6237\u7aef\u914d\u7f6e\u6307\u5bfc\u4fee\u6539\u3002","title":"\u4fee\u6539Fiber\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=hive \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Hive Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#hive-driver_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Hive Driver\u5199\u5165\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#spark-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=spark \u7f16\u8bd1\u8fd0\u884c","title":"\u4f7f\u7528Spark Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix-driver","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u76ee\u524d\u672a\u80fd\u8bfb\u53d6\u5230\u6570\u636e\uff0c\u201dThe connector could not determine the value for the fetch size.\u201d\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d","title":"\u4f7f\u7528Phoenix Driver\u8bfb\u53d6\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#phoenix-driver_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u53c2\u8003\uff1a jdbc:fiber://fiberconfig=/opt/ficlient/Fiber/conf/fiber_ibm.xml;defaultDriver=phoenix \u7f16\u8bd1\u8fd0\u884c \u5199\u5165\u6570\u636e0\u884c\uff0c\u95ee\u9898\u6b63\u5728\u786e\u8ba4\u4e2d","title":"\u4f7f\u7528Phoenix Driver\u5199\u5165\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka","text":"\u8bf4\u660e\uff1akafka Connector\u4e0d\u652f\u6301\u53d1\u9001\u6216\u8005\u6d88\u8d39integer, float, double, numeric, decimal\u7b49\u6570\u503c\u7c7b\u578b\u7684\u5b57\u6bb5\uff0c\u9700\u8981\u8f6c\u6362\u6210char, varchar, longvarchar\u7b49\u7c7b\u578b\uff0c\u5426\u5219\u4f1a\u6709\u5982\u4e0b\u62a5\u9519\uff1a main_program: APT_PMsectionLeader(2, node2), player 2 - Unexpected termination by Unix signal 9(SIGKILL).","title":"\u5bf9\u63a5Kafka"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_1","text":"kafka Connector\u9700\u8981\u914d\u7f6eKafka client Classpath\uff0c\u53ef\u4ee5\u5728DataStage\u8282\u70b9\u5b89\u88c5kafka\u5ba2\u6237\u7aef\u6765\u83b7\u53d6kafka-client jar\u5305\u3002\u5b89\u88c5\u6b65\u9aa4\u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u3002 Kafka Client Classpath \u9700\u8981\u63d0\u4f9bkafka-client, log4j, slf4j-api \u4e09\u4e2ajar\u5305\u7684\u8def\u5f84\uff0c\u5982\uff1a /opt/ficlient/Kafka/kafka/libs/kafka-clients-0.10.0.0.jar;/opt/ficlient/Kafka/kafka/libs/log4j-1.2.17.jar;/opt/ficlient/Kafka/kafka/libs/slf4j-api-1.7.21.jar","title":"\u5b89\u88c5kafka\u5ba2\u6237\u7aef"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e RowGenerator \u751f\u6210\u6570\u636e transformer\u6570\u636e\u7c7b\u578b\u8f6c\u6362\uff1a Kafka\u914d\u7f6e\uff1a \u7f16\u8bd1\u8fd0\u884c","title":"\u53d1\u9001\u6d88\u606f\u5230kafka"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#kafka_3","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e \u7f16\u8bd1\u8fd0\u884c \u67e5\u770b\u8bfb\u53d6\u7684\u6570\u636e","title":"\u8bfb\u53d6Kafka\u6d88\u606f"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb","text":"","title":"\u5bf9\u63a5MPPDB"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb-jdbc-driver","text":"\u4eceMPPDB\u53d1\u5e03\u5305\u4e2d\u83b7\u53d6\uff0c\u5305\u540d\u4e3aGauss200-OLAP-VxxxRxxxCxx-xxxx-64bit-Jdbc.tar.gz \u89e3\u538b\u540e\u5f97\u5230gsjdbc4.jar\uff0c\u4e0a\u4f20\u5230DataStage Server","title":"\u83b7\u53d6MPPDB JDBC Driver"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#jdbc-driver_2","text":"\u4fee\u6539$DSHOME\u8def\u5f84\u7684isjdbc.config\u6587\u4ef6\uff0cCLASSPATH\u53d8\u91cf\u4e2d\u6dfb\u52a0MPPDB Driver \u7684\u8def\u5f84\uff0cCLASS_NAMES\u53d8\u91cf\u4e2d\u6dfb\u52a0org.postgresql.Driver su - dsadm cd $DSHOME vi isjdbc.config \u914d\u7f6e\uff1a CLASSPATH=/opt/IBM/InformationServer/ASBNode/lib/java/IShive.jar;/opt/mppdb/jdbc/gsjdbc4.jar; CLASS_NAMES=com.ibm.isf.jdbc.hive.HiveDriver;org.postgresql.Driver;","title":"\u4fee\u6539JDBC Driver\u914d\u7f6e\u6587\u4ef6"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb_1","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c","title":"\u8bfb\u53d6MPPDB\u8868\u6570\u636e"},{"location":"Data_Integration/IBM_InfoSphere_DataStage/#mppdb_2","text":"\u521b\u5efa\u4f5c\u4e1a \u4fee\u6539\u914d\u7f6e URL\u683c\u5f0f\u4e3a\uff1a jdbc:postgresql://host:port/database \u7f16\u8bd1\u8fd0\u884c \u67e5\u770bMPPDB\u8868\u6570\u636e\uff1a","title":"\u6570\u636e\u5199\u5165MPPDB\u8868"},{"location":"Data_Integration/Informatica_BDM_10.2.2/","text":"Informatica BDM\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica 10.0.0 \u2194 FusionInsight HD V100R002C50 (HDFS/HBase/Hive) Informatica 10.0.0 \u2194 FusionInsight HD V100R002C60U20 (HDFS/HBase/Hive) Informatica 10.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Informatica 10.2.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive) Informatica 10.2.2 \u2194 FusionInsight MRS 8.0 (HDFS/Hive) \u6ce8\uff1a\u4ee5\u4e0a\u5bf9\u63a5\u6d4b\u8bd5Informatica BDM\u91c7\u7528\u7684\u662fNative Engine\u3002Informatica 10.2.2\u5bf9\u63a5FusionInsight HD 6.5 HBase\u7ec4\u4ef6\u65f6\uff0c\u9700\u8981Zookeeper\u7ec4\u4ef6\u914d\u7f6eenforce.auth.enabled=false\uff0c\u5426\u5219\u5bf9\u63a5\u5931\u8d25\u3002 \u7b80\u4ecb \u00b6 Informatica\u7528\u4e8e\u7ba1\u7406\u5927\u6570\u636e\u5de5\u7a0b\u7684\u5de5\u5177\u4e3b\u8981\u6709Informatica Administrator\u3001Infoormatica Analyst\u548cInformatica Developer\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0Linux\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5Informatica 10.2.2\u670d\u52a1\u7aef\uff08Informatica Administrator\uff09\u5e76\u4f7f\u7528Oracle\u6570\u636e\u5e93\u7ba1\u7406\u57df\u6570\u636e\u3001\u8fde\u63a5\u6570\u636e\u7b49\uff0c\u5728Window\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5Informatica\u5ba2\u6237\u7aefBig Data Developuser\uff08Informatica Developer\u5176\u4e2d\u4e00\u79cd\u5de5\u5177\uff09\u3002Informatica\u670d\u52a1\u7aef\u4e0eFusionInsight HD\u7684HDFS\u548cHive\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u901a\u8fc7Informatica\u7684Big Data Developer\u5ba2\u6237\u7aef\u5b9e\u73b0Oracle\u6570\u636e\u5e93\u3001HDFS\u3001Hive\u3001HBase\u4e4b\u95f4\u6570\u636e\u4e92\u4f20\u3002 \u672c\u6587\u6863\u7684\u63cf\u8ff0\u4f7f\u7528\u7684Informatic Server\u5b89\u88c5\u8282\u70b9\u7684IP\u4e3a172.16.6.120\uff0c\u4e3b\u673a\u540d\u4e3a172-16-6-120\u3002\u5bf9\u63a5\u7684FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u5206\u522b\u662f172.16.4.21/172.16.4.22/172.16.4.23. \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u3001HBASE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 C:\\ecotesting\\hadoopConfig \u76ee\u5f55\u4e0b\uff0c\u5e76\u538b\u7f29\u4e3a hadoopConfig.zip \u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\config \u7684hbase-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002 \u5b89\u88c5Infomatica\u670d\u52a1\u7aef \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728Linux\u4e0a\u5b89\u88c5Infomatica Server\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5b89\u88c5\u8282\u70b9\u4e0a\u5df2\u5b89\u88c5\u597dOracle\u6570\u636e\u5e93\u3002\u672c\u6307\u5bfc\u6587\u6863\u5b89\u88c5\u7248\u672c\u4e3a Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production \uff0c\u975e\u5bb9\u5668\u6570\u636e\u5e93\uff0c\u5b89\u88c5\u7528\u6237\u540d\u4e3a oracle \u5e76\u5c5e\u4e8e\u7fa4\u7ec4 oinstall \uff0c\u6570\u636e\u5e93SID\u4e3a orcl \u3002 \u5df2\u83b7\u53d6Informatica\u670d\u52a1\u7aef\u5b89\u88c5\u5305\uff0c\u4f8b\u5982\uff1ainformatica_1022_server_linux-x64.tar\uff0c\u5e76\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u5df2\u83b7\u53d6Informatica\u7684License\uff0c\u4f8b\u5982\uff1a infa1022.key \uff0c\u5e76\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5\u670d\u52a1\u7aef \u00b6 \u5b89\u88c5Informatica\u670d\u52a1\u7aef\u9700\u8981\u8fde\u63a5Domain\u548cModel_Repository_Service\u7684\u6570\u636e\u5e93\u7528\u6237\u3002\u767b\u5f55oracle\u6570\u636e\u5e93\uff0c\u521b\u5efa\u4e24\u4e2a\u7528\u6237\uff0c\u5206\u522b\u547d\u540d\u4e3a domain_user \u548c mdl_user \u3002 su - oracle sqlplus / as sysdba SQL> create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512m; SQL> create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp; SQL> create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp; SQL> grant dba to domain_user,mdl_user; SQL> exit \u521b\u5efa\u5b89\u88c5\u7528\u6237 infa \u5e76\u5f52\u5c5e\u4e8e\u7fa4\u7ec4 oinstall \u3002 su - root useradd -g oinstall -d /home/infa infa echo \"Huawei@123\" | passwd --stdin infa \u4f7f\u7528 root \u7528\u6237\u89e3\u538binformatica_1022_server_linux-x64.tar\u81f3 /opt/informatica \uff0c\u8bbe\u7f6e\u62e5\u6709\u8005\u4e3ainfa\u7528\u6237\u5e76\u8d4b\u4e88755\u7684\u64cd\u4f5c\u6743\u9650\u3002 su - root mkdir -p /opt/informatica tar -xvf /opt/informatica_1022_server_linux-x64.tar -C /opt/informatica chown -R infa:oinstall /opt/informatica chmod -R 755 /opt/informatica \u4fee\u6539 infa \u7528\u6237\u7684\u73af\u5883\u53d8\u91cf\u3002 su - infa vi ~/.bash_profile source ~/.bash_profile \u6dfb\u52a0\u73af\u5883\u53d8\u91cf\u5982\u4e0b\u6240\u793a\uff1a PATH=$PATH:/u01/app/oracle/product/12.2.1/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.2.1/db_1 export ORACLE_SID=orcl export NLS_LANG=AMERICAN_AMERICA.AL32UTF8 export INFA_CODEPAGENAME=\"UTF-8\" export PATH=/opt/informatica/10.2.2/server/bin:$PATH export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/informatica/10.2.2/server/bin \u8bf4\u660e\uff1a12.2.1\u4e3aoracle\u7248\u672c\u53f7\uff0c10.2.2\u4e3aInformatica\u7684\u7248\u672c\u53f7\u3002 \u4f7f\u7528infa\u7528\u6237\u767b\u5f55\u542f\u52a8\u56fe\u5f62\u5316\u7ec8\u7aef\u5f00\u59cb\u5b89\u88c5\u3002 su - root export display=:0.0 xhost + su - infa export display=:0.0 cd /opt/informatica ./install.sh \u8bf4\u660e\uff1a\u6267\u884c xhost + \u547d\u4ee4\u65f6\uff0c\u786e\u8ba4\u8fd4\u56de\u201caccess control disabled, clients can connect from any host\u201d\uff0c\u624d\u80fd\u7ee7\u7eed\u6267\u884c\u540e\u9762\u7684\u547d\u4ee4\u3002 \u8f93\u5165 y \u9009\u62e9\u7ee7\u7eed\u5b89\u88c5\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cInstall and configure Informatica Big Data suite products.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 3 \u9009\u62e9\u201cRun the installer.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u540c\u610f\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u7ee7\u7eed\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cInstall Informatica domain services.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u7ee7\u7eed\u5b89\u88c5\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cNo\u201d\u4e0d\u6fc0\u6d3bKerberos\u8ba4\u8bc1\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cNo\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165License\u7684\u8def\u5f84\uff0c\u4f8b\u5982\uff1a /opt/infa1022.key \uff0c\u6309 Enter \u952e\uff0c\u8f93\u5165\u5b89\u88c5\u8def\u5f84\uff08\u786e\u4fdd\u548c\u73af\u5883\u53d8\u91cf~/.bash_profile\u914d\u7f6e\u7684\u8def\u5f84\u4e00\u81f4\uff09\uff0c\u4f8b\u5982\uff1a /opt/informatica/10.2.2 \uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6309 Enter \u952e\u5f00\u59cb\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u8fdb\u5ea6100%\u540e\uff0c\u6b65\u9aa45\u7684\u9009\u62e9\u521b\u5efaDomain\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa46\u8f93\u5165Domain\u7684\u6570\u636e\u5e93\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa47\u8f93\u5165\u81ea\u5b9a\u4e49\u5bc6\u7801 Huawei@123 \uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6b65\u9aa48\u8f93\u5165Domain\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6b65\u9aa48B\u8f93\u5165Model Repository Service\u7684\u6570\u636e\u5e93\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa49\u8f93\u5165Data Integration Service\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u9009\u62e9\u4e0d\u521b\u5efaCCO\u8fde\u63a5\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6309 Enter \u952e\u7ed3\u675f\u5b89\u88c5\u3002 \u786e\u8ba4\u9632\u706b\u5899\u662f\u5173\u95ed\u72b6\u6001\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u6253\u5f00 http://172-16-6-120:6008 \u767b\u5f55Informatica Administrator\uff0c\u767b\u5f55\u7528\u6237\u540d\u4e3a Administrator \uff0c\u5bc6\u7801\u4e3a Huawei@123 \uff0c\u70b9\u51fb \u767b\u5f55 \u3002 \u767b\u5f55\u6210\u529f\u3002 \u914d\u7f6eInformatica Server \u00b6 \u914d\u7f6eKerberos\u8ba4\u8bc1 \u00b6 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06 krb5.conf \u4e0a\u4f20\u81f3Infomatica Server\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u5c06 krb5.conf \u590d\u5236\u5230 $INFA_HOME/java/jre/lib/security \u548c $INFA_HOME/services/shared/security \u3002\u547d\u4ee4\u6267\u884c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a chown infa:oinstall /opt/krb5.conf cp /opt/krb5.conf /opt/informatica/10.2.2/services/shared/security/ cp /opt/krb5.conf /opt/informatica/10.2.2/java/jre/lib/security/ \u914d\u7f6eData_Integration_Service \u00b6 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u670d\u52a1\u548c\u8282\u70b9->Data_Integration_Service \u3002\u70b9\u51fb\u201c\u6267\u884c\u9009\u9879\u201d\u7684\u7f16\u8f91\u6309\u94ae\uff0c\u8bbe\u7f6e\u201cHadoop Kerberos\u670d\u52a1\u4e3b\u4f53\u540d\u79f0\u201d\u4e3a developuser@HADOOP.COM \uff0c\u201cHadoop Kerberos Keytab\u201d\u4e3a /opt/user.keytab \uff0c\u70b9\u89e3 \u786e\u5b9a \uff0c\u9009\u62e9 \u662f\uff0c\u4fdd\u5b58\u66f4\u6539 \u3002 \u70b9\u51fb\u201cData_Integration_Service\u201d\u7684\u5e94\u7528\u670d\u52a1\u6309\u94ae \uff0c\u70b9\u51fb \u786e\u5b9a \u8ba9\u4fee\u6539\u751f\u6548\u3002 \u521b\u5efa\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1 \u00b6 \u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u662f\u4e00\u9879\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\uff0c\u5b83\u53ef\u8ba9 Developer tool \u8bbf\u95ee Hadoop \u8fde\u63a5\u4fe1\u606f\u4ee5\u5bfc\u5165\u548c\u9884\u89c8\u5143\u6570\u636e\u3002\u4eceHadoop\u96c6\u7fa4\u5bfc\u5165\u5bf9\u8c61\u65f6\uff0cHBase\u3001HDFS\u3001Hive\u8fde\u63a5\u4f1a\u4f7f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06 user.keytab \u4e0a\u4f20\u81f3Infomatica Server\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u767b\u5f55 http://172-16-6-120:6008 \uff0c\u767b\u5f55\u7528\u6237\u540d\u4e3a Administrator \uff0c\u5bc6\u7801\u4e3a Huawei@123 \u3002 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u670d\u52a1\u548c\u8282\u70b9 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120\u201d\uff0c\u9009\u62e9 \u65b0->\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a Metadata_Service \uff0c\u201c\u4f4d\u7f6e\u201d\u9ed8\u8ba4\u4e3a Domain_172-16-6-120 \uff0c\u201c\u8bb8\u53ef\u8bc1\u201d\u9009\u62e9\u6709\u6548\u7684License\uff0c\u201c\u8282\u70b9\u201d\u9009\u62e9 node01_172-16-6-120 \u3002 \u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201cHadoop Kerberos\u670d\u52a1\u4e3b\u4f53\u540d\u79f0\u201d\u8f93\u5165 developuser@HADOOP.COM \uff0c\u201cHadoop Kerberos Keytab\u201d\u8f93\u5165 /opt/user.keytab \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u70b9\u51fb\u201cMetadata_Service\u201d\u53f3\u4e0a\u89d2\u7684 \u6309\u94ae\u542f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u521b\u5efaInformatica\u7fa4\u96c6 \u00b6 \u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u662f\u4e00\u9879\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\uff0c\u5b83\u53ef\u8ba9 Developer tool \u8bbf\u95ee Hadoop \u8fde\u63a5\u4fe1\u606f\u4ee5\u5bfc\u5165\u548c\u9884\u89c8\u5143\u6570\u636e\u3002\u4eceHadoop\u96c6\u7fa4\u5bfc\u5165\u5bf9\u8c61\u65f6\uff0cHBase\u3001HDFS\u3001Hive\u8fde\u63a5\u4f1a\u4f7f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u8fde\u63a5 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120->ClusterConfigurations\u201d\uff0c\u9009\u62e9 \u65b0->\u7fa4\u96c6\u914d\u7f6e \u3002 \u201c\u7fa4\u96c6\u914d\u7f6e\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a FusionInsightHD \uff0c\u201c\u5206\u53d1\u7c7b\u578b\u201d\u9009\u62e9 Cloudera \uff0c\u201c\u5bfc\u5165\u7fa4\u96c6\u914d\u7f6e\u7684\u65b9\u6cd5\u201d\u9009\u62e9 \u4ece\u5b58\u6863\u6587\u4ef6\u4e2d\u5bfc\u5165 \uff0c\u4e0a\u8f7d\u914d\u7f6e\u5b58\u6863\u6587\u4ef6\u9009\u62e9 C:\\ecotesting\\hadoopConfig.zip \uff0c\u52fe\u9009 \u521b\u5efa\u8fde\u63a5 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u786e\u8ba4\u5b89\u88c5\u4fe1\u606f\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5b89\u88c5\u5b8c\u6210\u3002 \u4fee\u6539\u8fde\u63a5\u201cHIVE_fusionginsighthd\u201d\u4ee5\u4e0b\u5171\u540c\u5c5e\u6027\u3002 \u5143\u6570\u636e\u8fde\u63a5\u5b57\u7b26\u4e32\uff1ajdbc:hive2://172.16.4.21:21066/default;saslQop=auth-conf;principal=hive/hadoop.hadoop.com@HADOOP.COM \u6570\u636e\u8bbf\u95ee\u8fde\u63a5\u5b57\u7b26\u4e32\uff1ajdbc:hive2://172.16.4.21:21066/default;saslQop=auth-conf;principal=hive/hadoop.hadoop.com@HADOOP.COM HDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\uff1a/user/hive/warehouse Hive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\uff1adefault \u8bf4\u660e\uff1a\u5982\u679c\u9700\u8981\u5411Hive\u5199\u5165\u6570\u636e\uff0c\u5fc5\u987b\u914d\u7f6e\u201cHDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\u201d\u548c\u201cHive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\u201d\u3002 \u5982\u679c\u9700\u8981\u5411Hive\u5199\u5165\u6570\u636e\uff0c\u5fc5\u987b\u5c06\u96c6\u6210\u914d\u7f6e\u7684 hdfs_site_xml \u7684 dfs.client.failover.proxy.provider.hacluster \u7684\u503c\u4fee\u6539\u4e3a org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u3002\u5426\u5219Mapping\u65e5\u5fd7\u4f1a\u8fd4\u56de\u7c7b\u4f3c\u201cClass org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider not found\u201d\u7684\u9519\u8bef\u4e14\u5199\u5165\u5931\u8d25\u3002 \u9009\u62e9\u7fa4\u96c6\u201cFusionInsightHD\u201d\uff0c\u70b9\u51fb hdfs_site_xml \u7684\u7f16\u8f91\u6309\u94ae\uff0c\u9009\u4e2d dfs.client.failover.proxy.provider.hacluster \u540e\u70b9\u51fb \u7f16\u8f91 \uff0c\u201c\u8986\u76d6\u7684\u503c\u201d\u8f93\u5165 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \uff0c\u70b9\u51fb \u786e\u5b9a \u3002\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u4fee\u6539\u3002 \u521b\u5efaOracle\u8fde\u63a5 \u00b6 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u8fde\u63a5 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120\u201d\uff0c\u9009\u62e9 \u65b0->\u8fde\u63a5 \u3002 \u9009\u62e9 Oracle \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u201c\u540d\u79f0\u201d\u548c\u201cID\u201d\u81ea\u5b9a\u4e49\u4e3a ORACLE \uff0c\u201c\u7528\u6237\u540d\u201d\u548c\u201c\u5bc6\u7801\u201d\u90fd\u8f93\u5165 mdl_user \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u5143\u6570\u636e\u8bbf\u95ee\u5c5e\u6027\u201d\u7684\u201c\u8fde\u63a5\u5b57\u7b26\u4e32\u201d\u8f93\u5165 jdbc:informatica:oracle://172-16-6-120:1521;SID=orcl \uff0c\u201c\u6570\u636e\u8bbf\u95ee\u5c5e\u6027\u201d\u7684\u201c\u8fde\u63a5\u5b57\u7b26\u4e32\u201d\u4e3a orcl \u548c \u201c\u4ee3\u7801\u9875\u201d\u9009\u62e9 UTF-8 encoding of Unicode \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \u8fd4\u56de \u201c\u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u201d\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u91cd\u542fInformatica Server\u3002\u5173\u95edInformatica Server\u540e\uff0c\u9700\u8981\u6267\u884c ps -ef | grep informatica \u68c0\u67e5\u6240\u6709\u7684informatica\u8fdb\u7a0b\u90fd\u5173\u95ed\u540e\u518d\u542f\u52a8Informatica Server\u3002 su - infa cd /opt/informatica/10.2.2/tomcat/bin ./infaservice.sh shutdown ps -ef | grep informatica ./infaservice.sh startup \u8bf4\u660e:\u5982\u679c\u65b0\u5efaORACLE\u8fde\u63a5\u540e\u672a\u91cd\u542f\u8fc7Informatica Server\uff0c\u5728\u5ba2\u6237\u7aefBig Data Developer\u8fd0\u884cORACLE\u5173\u7cfb\u578b\u6570\u636e\u5bf9\u8c61\u65f6\uff0c\u8fd4\u56de\u7c7b\u4f3c\u4ee5\u4e0b\u7684\u9519\u8bef\uff1a [LDTMCMN_0029] \u7531\u4e8e\u4ee5\u4e0b\u9519\u8bef\uff0cLDTM \u65e0\u6cd5\u5b8c\u6210\u8bf7\u6c42: com.informatica.sdk.dtm.ExecutionException: [EdtmExec_00007] CMN_1022 Database driver error... CMN_1022 [Database driver event...Error occurred loading library [libclntsh.so.11.1: cannot open shared object file: No such file or directory]Database driver event...Error occurred loading library [libpmora8.so]] \u589e\u52a0\u7528\u6237Administrator\u6743\u9650 \u00b6 \u5bfc\u822a\u81f3 \u5b89\u5168->\u7528\u6237 \uff0c\u9009\u4e2d \u7528\u6237->Native->Administrator \uff0c\u70b9\u51fb \u6982\u89c8->\u7f16\u8f91->\u7ec4 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u5c06 Operator \u6dfb\u52a0\u81f3\u201c\u5206\u914d\u7684\u7ec4\u201d \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u8bf4\u660e\uff1a\u5982\u679c\u7528\u6237\u4e0d\u5c5e\u4e8eOperator\u7ec4\uff0c\u4f7f\u7528\u8be5\u7528\u6237\u6267\u884cmapping\u4ece\u8fde\u63a5\u4e2d\u83b7\u53d6\u6570\u636e\u65f6\uff0c\u4f1a\u8fd4\u56de\u7c7b\u4f3c\u4e8e\u201c\u6ca1\u6709\u9488\u5bf9\u8fde\u63a5 [ORACLE] (\u5728\u57df [Domain_172-16-6-120] \u4e2d)\u7684\u6267\u884c\u6743\u9650\u201d\u7684\u9519\u8bef\u3002 \u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728Windows\u4e0a\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5df2\u83b7\u53d6Informatica\u5ba2\u6237\u7aef\u5b89\u88c5\u5305\uff0c\u4f8b\u5982\uff1ainformatica_1022_client_winem-64t.zip\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5\u5ba2\u6237\u7aef \u00b6 \u89e3\u538binformatica_1022_client_winem-64t.zip\u540e\uff0c\u53cc\u51fb install.bat \u3002\u9009\u62e9 \u5b89\u88c5Informatica Developer\u7248\u672c10.2.2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u8f93\u5165\u5b89\u88c5\u8def\u5f84\uff0c\u9ed8\u8ba4\u5b89\u88c5\u8def\u5f84\u4e3a C:\\Informatica\\10.2.2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u70b9\u51fb \u5b89\u88c5 \u540e\u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\u3002 \u70b9\u51fb \u5b8c\u6210 \u3002 Informatica BDM\u5bf9\u63a5FusionInsight HD \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Informatica BDM\u5bf9\u63a5FusionInsight HD\u7684HDFS\u548cHive\u3002\u901a\u8fc7Informatica\u7684Big Data Developer\u5ba2\u6237\u7aef\u5b9e\u73b0Oracle\u6570\u636e\u5e93\u4e0eHDFS\u548cHive\u4e4b\u95f4\u4e92\u76f8\u4e0a\u4f20/\u4e0b\u8f7d\u6570\u636e\u3001HDFS\u4e0eHive\u4e92\u76f8\u4e0a\u4f20/\u4e0b\u8f7d\u6570\u636e\u3001HDFS/Hive\u4e0e\u672c\u5730\u4e4b\u95f4\u4e92\u76f8\u4e0a\u4f20\u4e0b\u8f7d\u6570\u636e\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b89\u88c5Informatica\u670d\u52a1\u7aef\u548cBig Data Developer\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06FusionInsight\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230Informatica Server\u5b89\u88c5\u8282\u70b9\u7684 /etc/hosts \u6587\u4ef6\u4e2d\u3002 \u5b89\u88c5Infomatica\u670d\u52a1\u7aef\u8282\u70b9\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u51c6\u5907\u6570\u636e\u3002 \u672c\u5730 \u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u5728 /tmp \u76ee\u5f55\u4e0b\u521b\u5efa\u6587\u4ef6 user_local_to_hdfs.csv \uff0c\u64cd\u4f5c\u547d\u4ee4\u5982\u4e0b\u3002\u5e76\u4e14\u5c06 user_local_to_hdfs.csv \u62f7\u8d1d\u81f3\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef\u7684window\u7cfb\u7edf\uff0c\u4f8b\u5982 C:\\ \u76ee\u5f55\u4e0b\u3002 su - infa cd /tmp vi user_local_to_hdfs.csv user_local_to_hdfs.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 10,Andy-in-local 11,Benny-in-local 12,Tom-in-local HDFS\u6587\u4ef6\u7cfb\u7edf \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp \u76ee\u5f55\u521b\u5efa\u4e24\u4e2a\u6587\u4ef6\u5206\u522b\u547d\u540d\u4e3a user_hdfs_to_oracle.csv \u548c user_hdfs_to_hive.csv \u3002 cd /opt vi user_hdfs_to_oracle.csv vi user_hdfs_to_hive.csv hdfs dfs -put user_hdfs_to_* /tmp user_hdfs_to_oracle.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 20,Andy-in-hdfs 21,Benny-in-hdfs 22,Tom-in-hdfs user_hdfs_to_hive.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 30,Andy-in-hdfs 31,Benny-in-hdfs 32,Tom-in-hdfs Hive\u6570\u636e\u5e93 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a user_hive_in \u548c user_hive_out \u3002 \u521b\u5efauser_hive_in\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS user_hive_in(id INT, name STRING); \u521b\u5efauser_hive_out\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS user_hive_out(id INT, name STRING); INSERT INTO user_hive_out VALUES (40,'Andy-in-hive'); INSERT INTO user_hive_out VALUES (41,'Benny-in-hive'); INSERT INTO user_hive_out VALUES (42,'Tom-in-hive'); Oracle\u6570\u636e\u5e93 \u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a user_oracle_in \u548c user_oracle_out \u3002 su - oracle sqlplus mdl_user/mdl_user \u521b\u5efauser_oracle_in\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE user_oracle_in(ID INTEGER PRIMARY KEY,NAME VARCHAR2(30)); \u521b\u5efauser_oracle_out\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE user_oracle_out(ID INTEGER PRIMARY KEY,NAME VARCHAR2(30)); INSERT INTO user_oracle_out VALUES (50,'Andy-in-oracle'); INSERT INTO user_oracle_out VALUES (51,'Benny-in-oracle'); INSERT INTO user_oracle_out VALUES (52,'Tom-in-oracle'); HBase \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a USER_HBASE_IN \u548c USER_HBASE_OUT \u3002 \u521b\u5efa\u547d\u540d\u7a7a\u95f4INFA\uff1a hbase shell create_namespace 'INFA' \u521b\u5efaUSER_HBASE_IN\u8868\u793a\u4f8b\u5982\u4e0b\uff1a create 'INFA.USER_HBASE_IN',{NAME=>'cf1'},{NAME=>'cf2'} \u521b\u5efaUSER_HBASE_OUT\u8868\u793a\u4f8b\u5982\u4e0b\uff1a create 'INFA.USER_HBASE_OUT',{NAME=>'cf1'},{NAME=>'cf2'} put 'INFA.USER_HBASE_OUT', '001','cf1:id','60' put 'INFA.USER_HBASE_OUT', '001','cf1:name','Andy-in-HBase' put 'INFA.USER_HBASE_OUT', '002','cf1:id','61' put 'INFA.USER_HBASE_OUT', '002','cf1:name','Benny-in-HBase' put 'INFA.USER_HBASE_OUT', '003','cf1:id','62' put 'INFA.USER_HBASE_OUT', '003','cf1:name','Tom-in-HBase' \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5efa\u7acb\u9879\u76ee \u00b6 \u6253\u5f00 Big Data Developer \uff0c\u70b9\u51fb \u6587\u4ef6->\u8fde\u63a5\u5230\u5b58\u50a8\u5e93 \u3002 \u70b9\u51fb \u914d\u7f6e\u57df \uff0c \u70b9\u51fb \u6dfb\u52a0 \uff0c\u201c\u57df\u540d\u201d\u8f93\u5165\u81ea\u5b9a\u4e49\u540d\u79f0\u4e3a Domain_172-16-6-120 \uff0c\u201c\u4e3b\u673a\u540d\u201d\u8f93\u5165Informatica Server\u5b89\u88c5\u8282\u70b9\u5bf9\u5e94\u7684\u4e3b\u673a\u540d 172-16-6-120 \uff0c\u201c\u7aef\u53e3\u53f7\u201d\u8f93\u5165\u5b89\u88c5Informatica Server\u65f6\u6307\u5b9a\u7684\u7aef\u53e3 6005 \uff0c\u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u201c\u8fde\u63a5\u6210\u529f\u201d\u5219\u8868\u793a\u4e3b\u673a\u540d\u4e3a172-16-6-120\u7684\u57df\u53ef\u7528\u3002\u70b9\u51fb \u786e\u5b9a \u5e76 \u5b8c\u6210 \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 Domain_172-16-6-120.Model_Repository_Service \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u8f93\u5165\u201c\u7528\u6237\u540d\u201d\u4e3a Administrator \uff0c\u201c\u5bc6\u7801\u201d\u4e3a Huawei@123 \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728\u201c\u5bf9\u8c61\u6d4f\u89c8\u5668\u201d\u4e0b\u663e\u793a\u8fde\u63a5\u6210\u529f\u7684\u5b58\u50a8\u5e93 Model_Repository_Service \uff08Administrator\uff09 \u3002 \u53f3\u952e Model_Repository_Service \uff08Administrator\uff09 \u9009\u62e9 \u65b0\u5efa->\u9879\u76ee \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u8f93\u5165\u81ea\u5b9a\u4e49\u540d\u79f0 fi_project \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u5b8c\u6210 \u3002 \u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \u00b6 \u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 - Oracle \u00b6 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 ORACLE \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u8d44\u6e90\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\u9009\u62e9\u8868 USER_ORACLE_IN \u548c USER_ORACLE_OUT \u3002\u70b9\u51fb \u5b8c\u6210 \u3002 \u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 - Hive \u00b6 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 HIVE_fusionginsighthd \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u8d44\u6e90\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\u9009\u62e9\u8868 user_hive_in \u548c user_hive_out \u3002\u70b9\u51fb \u5b8c\u6210 \u3002 \u521b\u5efaHBase\u6570\u636e\u5bf9\u8c61 \u00b6 \u767b\u5f55FusionInsight Manager\u5c06Zookeeper\u7ec4\u4ef6\u7684\u914d\u7f6e enforce.auth.enabled \u4fee\u6539\u4e3a false \u4fdd\u5b58\u540e\uff0c\u5e76 \u91cd\u542f Zookeeper\u4ee5\u53ca\u5176\u4e0a\u5c42\u670d\u52a1\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 HBase\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_IN \uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 HBASE_fusionginsighthd \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u9009\u5b9a\u8d44\u6e90\u201d\u7684 \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9\u8868 INFA.USER_HBASE_IN \u3002\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u52fe\u9009 cf1 \uff0c\u9009\u62e9 \u6dfb\u52a0\u5217 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u6dfb\u52a0\u4e24\u5217\u540d\u79f0\u5206\u522b\u4e3a id \u548c name \u3002\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u52fe\u9009 \u5305\u542b\u884cID \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u5b8c\u6210 \u3002 \u540c\u6837\u5730\u65b0\u589eHBase\u8868 INFA.USER_HBASE_OUT \u3002 Informatica BDM\u5bf9\u63a5FusionInsight HDFS \u00b6 HDFS from Local \u00b6 \u5c06\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_to_hdfs.csv \u4e0a\u4f20\u81f3FusionInsight HD\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5e76\u547d\u540d\u4e3auser_hdfs_from_local.csv\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u9009\u62e9 \u4ece\u73b0\u6709\u5e73\u9762\u6587\u4ef6\u521b\u5efa \u5e76\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 C:\\user_local_to_hdfs.csv \uff0c\u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_local \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u683c\u5f0f\u201d\u9009\u62e9 \u5e26\u5206\u9694\u7b26 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u5206\u9694\u7b26\u201d\u9009\u62e9 \u9017\u53f7 \uff0c\u52fe\u9009 \u5bfc\u5165\u7b2c\u4e00\u884c\u4e2d\u7684\u5217\u540d\u79f0 \uff0c\u5176\u4f59\u4fdd\u6301\u9ed8\u8ba4\u9009\u9879\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u9009\u62e9\u6570\u636e\u5bf9\u8c61 hdfs_from_local \uff0c\u8bbe\u7f6e \u9ad8\u7ea7 \u5c5e\u6027\u540e\u4fdd\u5b58\u3002 \u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\u3002\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_local_to_hdfs.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \u3002 \u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\u3002\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_local.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002\u201cCtrl+s\u201d\u4fdd\u5b58\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u62d6\u66f3\u81f3\u201c\u6620\u5c04\u201d hdfs_from_local_mapping \u7684 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u7c7b\u4f3c\u5730\u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u62d6\u66f3\u81f3\u201c\u6620\u5c04\u201d hdfs_from_local_mapping \u7684 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002\u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_local.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_local.csv\u201d\u3002 HDFS from Oracle \u00b6 \u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5e76\u547d\u540d\u4e3auser_hdfs_from_oracle.csv\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_oracle \u3002 hdfs_from_oracle**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_oracle.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002\u201cCtrl+s\u201d\u4fdd\u5b58\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_oracle \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201c\u5199\u5165_hdfs_from_oracle\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_oracle.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_oracle.csv\u201d\u3002 HDFS to Oracle \u00b6 \u5c06FusionInsight HD\u7684HDFS\u7cfb\u7edf\u6587\u4ef6user_hdfs_to_oracle.csv\u6570\u636e\u4e0a\u4f20\u81f3Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_IN\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_oracle \u3002 hdfs_to_oracle**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\uff1a\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_to_oracle.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_to_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_to_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_oracle \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_hdfs_to_oracle\u201d\u548c\u201c\u5199\u5165_USER_ORACLE_IN\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u67e5\u8be2\u8868\u201cUSER_ORACLE_IN\u201d\u6570\u636e\u3002 su - oracle sqlplus mdl_user/mdl_user select * from USER_ORACLE_IN; HDFS to Hive \u00b6 \u5c06FusionInsight HD\u7684HDFS\u7cfb\u7edf\u6587\u4ef6user_hdfs_to_hive.csv\u6570\u636e\u4e0a\u4f20\u81f3Hive\u6570\u636e\u5e93\u8868user_hive_in\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_hive \u3002 hdfs_to_hive**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\uff1a\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_to_hive.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_to_hive_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_to_hive_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_hive \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_in \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_hdfs_to_hive\u201d\u548c\u201c\u5199\u5165_user_hive_in\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 user_hive_in \u6570\u636e\u3002 beeline select * from user_hive_in; Informatica BDM\u5bf9\u63a5FusionInsight Hive \u00b6 Hive to Local \u00b6 \u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0b\u8f7d\u81f3\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_from_hive.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hive_to_local \u3002 hive_to_local**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_local_from_hive.csv \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_local_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hive_to_local \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_hive_to_local\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u67e5\u770b\u6587\u4ef6 /tmp/user_local_from_hive.csv \u3002 cat /tmp/user_local_from_hive.csv Hive to Oracle \u00b6 \u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0a\u4f20\u81f3Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_IN\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_USER_ORACLE_IN\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u67e5\u8be2\u8868\u201cUSER_ORACLE_IN\u201d\u6570\u636e\u3002 su - oracle sqlplus mdl_user/mdl_user select * from USER_ORACLE_IN; Hive from Oracle \u00b6 \u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_in\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_in \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201c\u5199\u5165_user_hive_in\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 user_hive_in \u6570\u636e\u3002 beeline select * from user_hive_in; Hive to HDFS \u00b6 \u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0b\u8f7d\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/user_hdfs_from_hive.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hive_to_hdfs \u3002 hive_to_hdfs**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_hive.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_hdfs_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_hdfs_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hive_to_hdfs \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_hive_to_hdfs\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_hive.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_hive.csv\u201d\u3002 cat /tmp/user_hdfs_from_hive.csv Informatica BDM\u5bf9\u63a5FusionInsight HBase \u00b6 HBase to Local \u00b6 \u5c06FusionInsight HD\u7684HBase\u8868USER_HBASE_OUT\u6570\u636e\u4e0b\u8f7d\u81f3\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_from_hbase.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hbase_to_local \u3002 hbase_from_local**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_local_from_hbase.csv \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hbase_to_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chbase_to_local_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HBASE_fusionginsighthd->USER_HBASE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \uff0c\u70b9\u51fb \u65b0\u5efa\u64cd\u4f5c \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_OUT_Read \uff0c\u201c\u529f\u80fd\u201d\u9009\u62e9 \u8bfb\u53d6 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9 INFA_USER_HBASE_OUT \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201c\u9009\u62e9\u64cd\u4f5c\u201d\u4e3a USER_HBASE_OUT_Read \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hbase_to_local \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201cUSER_HBASE_OUT_Read\u201d\u548c\u201c\u5199\u5165_hbase_to_local\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u67e5\u770b\u6587\u4ef6 /tmp/user_local_from_hbase.csv \u3002 cat /tmp/user_local_from_hbase.csv HBase from Oracle \u00b6 \u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684HBase\u8868USER_HBASE_IN\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hbase_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chbase_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HBASE_fusionginsighthd->USER_HBASE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \uff0c\u70b9\u51fb \u65b0\u5efa\u64cd\u4f5c \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_IN_Write \uff0c\u201c\u529f\u80fd\u201d\u9009\u62e9 \u5199\u5165 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9 INFA_USER_HBASE_IN \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201c\u9009\u62e9\u64cd\u4f5c\u201d\u4e3a USER_HBASE_IN_Write \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201cUSER_HBASE_IN_Write\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 INFA.USER_HBASE_IN \u6570\u636e\u3002 hbase shell scan 'INFA.USER_HBASE_IN' FAQ \u00b6 \u6267\u884c./infaservice.sh startup\u542f\u52a8Infomatica Server\u65f6\uff0c\u8fd4\u56deERROR: Node configuration file not accessible or invalid\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5728\u6267\u884c./infaservice.sh startup\u542f\u52a8infomatica server\u65f6\uff0c\u8fd4\u56deERROR: Node configuration file not accessible or invalid\u3002 \u6216\u8005\u5728 INFA_HOME/tomcat/bin\u76ee\u5f55\u6267\u884c./startup.sh\u542f\u52a8tomcat\u540e\uff0c\u6ca1\u67e5\u8be2\u5230java\u8fdb\u7a0b\uff0c\u5728 INFA_HOME/tomcat/bin\u76ee\u5f55\u6267\u884c./startup.sh\u542f\u52a8tomcat\u540e\uff0c\u6ca1\u67e5\u8be2\u5230java\u8fdb\u7a0b\uff0c\u5728 INFA_HOME/tomcat/logs/catalina.out\u8fd4\u56de\u9519\u8befjava.io.FileNotFoundException: null/isp/config/nodemeta.xml (No such file or directory)\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u95ee\u9898\u539f\u56e0\uff1a$INFA_HOME/isp/config/nodemeta.xml\u7684\u6240\u6709\u8005\u548c\u6240\u5c5e\u7ec4\u4e0d\u6b63\u786e\u3002infa\u7528\u6237\u65e0\u6cd5\u83b7\u53d6\u5230nodemeta.xml\u3002 root\u7528\u6237\u767b\u5f55\u5e76\u5207\u6362\u81f3 $INFA_HOME/isp/config/ \uff0c\u6267\u884c chown -R infa:oinstall nodemeta.xml \u4fee\u6539nodemeta.xml\u6240\u6709\u8005\u4e3ainfa\u548c\u6240\u5c5e\u7528\u6237\u7ec4\u4e3aoinstall\u3002\u5efa\u8bae\u6267\u884c chown -R infa:oinstall /opt/informatica/10.2.2/ \u4fee\u6539Informatica Server\u6240\u6709\u6587\u4ef6\u7684\u6240\u6709\u8005\u4e3ainfa\u548c\u6240\u5c5e\u7528\u6237\u7ec4\u4e3aoinstall\u3002 \u5411Hive\u6570\u636e\u5e93\u5199\u5165\u6570\u636e\u5931\u8d25 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u8fd0\u884cmapping\u5411Hive\u6570\u636e\u5e93\u7684\u67d0\u4e00\u5f20\u8868\u5199\u5165\u6570\u636e\uff0c\u67e5\u8be2\u76ee\u6807\u8868\u65f6\uff0c\u6ca1\u6709\u6570\u636e\u5199\u5165\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5982\u679c\u5411Hive\u8868\u5199\u5165\u6570\u636e\uff0c\u786e\u8ba4\u4ee5\u4e0b\u4e24\u70b9\u662f\u5426\u5df2\u914d\u7f6e\uff1a \u914d\u7f6eHIVE\u8fde\u63a5\u4ee5\u4e0b\u4e24\u4e2a\u5c5e\u6027\u7684\u503c\uff1a\u201cHDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\u201d\u8bbe\u7f6e\u4e3a /user/hive/warehouse \uff0c\u201cHive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\u201d\u8bbe\u7f6e\u4e3a default \u3002 \u5c06\u96c6\u6210\u914d\u7f6e\u7684 hdfs_site_xml \u7684 dfs.client.failover.proxy.provider.hacluster \u7684\u503c\u4fee\u6539\u4e3a org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u3002 \u9009\u62e9\u7fa4\u96c6\u201cFusionInsightHD\u201d\uff0c\u70b9\u51fb hdfs_site_xml \u7684\u7f16\u8f91\u6309\u94ae\uff0c\u9009\u4e2d dfs.client.failover.proxy.provider.hacluster \u540e\u70b9\u51fb \u7f16\u8f91 \uff0c\u201c\u8986\u76d6\u7684\u503c\u201d\u8f93\u5165 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \uff0c\u70b9\u51fb \u786e\u5b9a \u3002\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u4fee\u6539\u3002 Big Data Developer\u6dfb\u52a0Hbase\u6570\u636e\u5bf9\u8c61\u65f6\u8fd4\u56deKeeperErrorCode = ConnectionLoss for /hbase \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u65b0\u5efaHBase\u6570\u636e\u5bf9\u8c61\uff0c\u70b9\u51fb \u6dfb\u52a0 \u83b7\u53d6HBase\u8868\u65f6\uff0c\u8fd4\u56de\u9519\u8befSDK_APP_COM_20000\u3002 Java.lang.RuntionException:org.apache.hadoop.hbase.ZookeeperConnectionException: Can\u2019t connet to Zookeeper KeeperErrorCode = ConnectionLoss for /hbase FusionInsight HD\u7684zookeeper\u65e5\u5fd7\uff0c\u4f8b\u5982\uff1a /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-euleros-hd01.log \uff0c\u8fd4\u56de\u7c7b\u4f3c\u4ee5\u4e0b\u7684\u9519\u8bef\uff1a ERROR | NIOWorkerThread-41 | Authentication failed as scheme is not valid: ['ip,'172.16.6.120], expected scheme zookeeper.enforce.auth.scheme=sasl \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5c06Zookeeper\u7ec4\u4ef6\u7684\u914d\u7f6e enforce.auth.enabled \u4fee\u6539\u4e3a false \u4fdd\u5b58\u540e\uff0c\u5e76 \u91cd\u542f Zookeeper\u4ee5\u53ca\u5176\u4e0a\u5c42\u670d\u52a1\u3002 \u5982\u4f55\u67e5\u770bmapping\u7684\u65e5\u5fd7 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5982\u679cmapping\u8fd0\u884c\u5b8c\u6210\u4e4b\u540e\uff0c\u6ca1\u6709\u5199\u5165\u6570\u636e\u6216\u8005mapping\u8fd0\u884c\u5931\u8d25\u7b49\uff0c\u5982\u4f55\u83b7\u53d6mapping\u8fd0\u884c\u7684\u8be6\u7ec6\u65e5\u5fd7\uff1f \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 Mapping\u8fd0\u884c\u7684\u65e5\u5fd7\u5b58\u653e\u4e8eInformatica Server\u5b89\u88c5\u8282\u70b9\u7684 $INFA_HOME/logs/node01_172-16-6-120/services/DataIntegrationService/disLogs/ms \u76ee\u5f55\u4e0b\u3002","title":"10.2.2 <--> 8.0"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_1","text":"Informatica 10.0.0 \u2194 FusionInsight HD V100R002C50 (HDFS/HBase/Hive) Informatica 10.0.0 \u2194 FusionInsight HD V100R002C60U20 (HDFS/HBase/Hive) Informatica 10.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Informatica 10.2.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive) Informatica 10.2.2 \u2194 FusionInsight MRS 8.0 (HDFS/Hive) \u6ce8\uff1a\u4ee5\u4e0a\u5bf9\u63a5\u6d4b\u8bd5Informatica BDM\u91c7\u7528\u7684\u662fNative Engine\u3002Informatica 10.2.2\u5bf9\u63a5FusionInsight HD 6.5 HBase\u7ec4\u4ef6\u65f6\uff0c\u9700\u8981Zookeeper\u7ec4\u4ef6\u914d\u7f6eenforce.auth.enabled=false\uff0c\u5426\u5219\u5bf9\u63a5\u5931\u8d25\u3002","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_2","text":"Informatica\u7528\u4e8e\u7ba1\u7406\u5927\u6570\u636e\u5de5\u7a0b\u7684\u5de5\u5177\u4e3b\u8981\u6709Informatica Administrator\u3001Infoormatica Analyst\u548cInformatica Developer\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0Linux\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5Informatica 10.2.2\u670d\u52a1\u7aef\uff08Informatica Administrator\uff09\u5e76\u4f7f\u7528Oracle\u6570\u636e\u5e93\u7ba1\u7406\u57df\u6570\u636e\u3001\u8fde\u63a5\u6570\u636e\u7b49\uff0c\u5728Window\u64cd\u4f5c\u7cfb\u7edf\u5b89\u88c5Informatica\u5ba2\u6237\u7aefBig Data Developuser\uff08Informatica Developer\u5176\u4e2d\u4e00\u79cd\u5de5\u5177\uff09\u3002Informatica\u670d\u52a1\u7aef\u4e0eFusionInsight HD\u7684HDFS\u548cHive\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u901a\u8fc7Informatica\u7684Big Data Developer\u5ba2\u6237\u7aef\u5b9e\u73b0Oracle\u6570\u636e\u5e93\u3001HDFS\u3001Hive\u3001HBase\u4e4b\u95f4\u6570\u636e\u4e92\u4f20\u3002 \u672c\u6587\u6863\u7684\u63cf\u8ff0\u4f7f\u7528\u7684Informatic Server\u5b89\u88c5\u8282\u70b9\u7684IP\u4e3a172.16.6.120\uff0c\u4e3b\u673a\u540d\u4e3a172-16-6-120\u3002\u5bf9\u63a5\u7684FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u5206\u522b\u662f172.16.4.21/172.16.4.22/172.16.4.23.","title":"\u7b80\u4ecb"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u3001HBASE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 C:\\ecotesting\\hadoopConfig \u76ee\u5f55\u4e0b\uff0c\u5e76\u538b\u7f29\u4e3a hadoopConfig.zip \u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\config \u7684hbase-site.xml\u3002 ..\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#infomatica","text":"","title":"\u5b89\u88c5Infomatica\u670d\u52a1\u7aef"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_4","text":"\u5728Linux\u4e0a\u5b89\u88c5Infomatica Server\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5b89\u88c5\u8282\u70b9\u4e0a\u5df2\u5b89\u88c5\u597dOracle\u6570\u636e\u5e93\u3002\u672c\u6307\u5bfc\u6587\u6863\u5b89\u88c5\u7248\u672c\u4e3a Oracle Database 12c Enterprise Edition Release 12.2.0.1.0 - 64bit Production \uff0c\u975e\u5bb9\u5668\u6570\u636e\u5e93\uff0c\u5b89\u88c5\u7528\u6237\u540d\u4e3a oracle \u5e76\u5c5e\u4e8e\u7fa4\u7ec4 oinstall \uff0c\u6570\u636e\u5e93SID\u4e3a orcl \u3002 \u5df2\u83b7\u53d6Informatica\u670d\u52a1\u7aef\u5b89\u88c5\u5305\uff0c\u4f8b\u5982\uff1ainformatica_1022_server_linux-x64.tar\uff0c\u5e76\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u5df2\u83b7\u53d6Informatica\u7684License\uff0c\u4f8b\u5982\uff1a infa1022.key \uff0c\u5e76\u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_6","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_7","text":"\u5b89\u88c5Informatica\u670d\u52a1\u7aef\u9700\u8981\u8fde\u63a5Domain\u548cModel_Repository_Service\u7684\u6570\u636e\u5e93\u7528\u6237\u3002\u767b\u5f55oracle\u6570\u636e\u5e93\uff0c\u521b\u5efa\u4e24\u4e2a\u7528\u6237\uff0c\u5206\u522b\u547d\u540d\u4e3a domain_user \u548c mdl_user \u3002 su - oracle sqlplus / as sysdba SQL> create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512m; SQL> create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp; SQL> create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp; SQL> grant dba to domain_user,mdl_user; SQL> exit \u521b\u5efa\u5b89\u88c5\u7528\u6237 infa \u5e76\u5f52\u5c5e\u4e8e\u7fa4\u7ec4 oinstall \u3002 su - root useradd -g oinstall -d /home/infa infa echo \"Huawei@123\" | passwd --stdin infa \u4f7f\u7528 root \u7528\u6237\u89e3\u538binformatica_1022_server_linux-x64.tar\u81f3 /opt/informatica \uff0c\u8bbe\u7f6e\u62e5\u6709\u8005\u4e3ainfa\u7528\u6237\u5e76\u8d4b\u4e88755\u7684\u64cd\u4f5c\u6743\u9650\u3002 su - root mkdir -p /opt/informatica tar -xvf /opt/informatica_1022_server_linux-x64.tar -C /opt/informatica chown -R infa:oinstall /opt/informatica chmod -R 755 /opt/informatica \u4fee\u6539 infa \u7528\u6237\u7684\u73af\u5883\u53d8\u91cf\u3002 su - infa vi ~/.bash_profile source ~/.bash_profile \u6dfb\u52a0\u73af\u5883\u53d8\u91cf\u5982\u4e0b\u6240\u793a\uff1a PATH=$PATH:/u01/app/oracle/product/12.2.1/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.2.1/db_1 export ORACLE_SID=orcl export NLS_LANG=AMERICAN_AMERICA.AL32UTF8 export INFA_CODEPAGENAME=\"UTF-8\" export PATH=/opt/informatica/10.2.2/server/bin:$PATH export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/informatica/10.2.2/server/bin \u8bf4\u660e\uff1a12.2.1\u4e3aoracle\u7248\u672c\u53f7\uff0c10.2.2\u4e3aInformatica\u7684\u7248\u672c\u53f7\u3002 \u4f7f\u7528infa\u7528\u6237\u767b\u5f55\u542f\u52a8\u56fe\u5f62\u5316\u7ec8\u7aef\u5f00\u59cb\u5b89\u88c5\u3002 su - root export display=:0.0 xhost + su - infa export display=:0.0 cd /opt/informatica ./install.sh \u8bf4\u660e\uff1a\u6267\u884c xhost + \u547d\u4ee4\u65f6\uff0c\u786e\u8ba4\u8fd4\u56de\u201caccess control disabled, clients can connect from any host\u201d\uff0c\u624d\u80fd\u7ee7\u7eed\u6267\u884c\u540e\u9762\u7684\u547d\u4ee4\u3002 \u8f93\u5165 y \u9009\u62e9\u7ee7\u7eed\u5b89\u88c5\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cInstall and configure Informatica Big Data suite products.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 3 \u9009\u62e9\u201cRun the installer.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u540c\u610f\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u7ee7\u7eed\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cInstall Informatica domain services.\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 2 \u9009\u62e9\u201cYes\u201d\u7ee7\u7eed\u5b89\u88c5\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cNo\u201d\u4e0d\u6fc0\u6d3bKerberos\u8ba4\u8bc1\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165 1 \u9009\u62e9\u201cNo\u201d\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u8f93\u5165License\u7684\u8def\u5f84\uff0c\u4f8b\u5982\uff1a /opt/infa1022.key \uff0c\u6309 Enter \u952e\uff0c\u8f93\u5165\u5b89\u88c5\u8def\u5f84\uff08\u786e\u4fdd\u548c\u73af\u5883\u53d8\u91cf~/.bash_profile\u914d\u7f6e\u7684\u8def\u5f84\u4e00\u81f4\uff09\uff0c\u4f8b\u5982\uff1a /opt/informatica/10.2.2 \uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6309 Enter \u952e\u5f00\u59cb\u5b89\u88c5\u3002 \u7b49\u5f85\u5b89\u88c5\u8fdb\u5ea6100%\u540e\uff0c\u6b65\u9aa45\u7684\u9009\u62e9\u521b\u5efaDomain\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa46\u8f93\u5165Domain\u7684\u6570\u636e\u5e93\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa47\u8f93\u5165\u81ea\u5b9a\u4e49\u5bc6\u7801 Huawei@123 \uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6b65\u9aa48\u8f93\u5165Domain\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u6309 Enter \u952e\u8fdb\u5165\u4e0b\u4e00\u6b65\u3002 \u6b65\u9aa48B\u8f93\u5165Model Repository Service\u7684\u6570\u636e\u5e93\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6b65\u9aa49\u8f93\u5165Data Integration Service\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u9009\u62e9\u4e0d\u521b\u5efaCCO\u8fde\u63a5\uff0c\u5177\u4f53\u914d\u7f6e\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6309 Enter \u952e\u7ed3\u675f\u5b89\u88c5\u3002 \u786e\u8ba4\u9632\u706b\u5899\u662f\u5173\u95ed\u72b6\u6001\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u6253\u5f00 http://172-16-6-120:6008 \u767b\u5f55Informatica Administrator\uff0c\u767b\u5f55\u7528\u6237\u540d\u4e3a Administrator \uff0c\u5bc6\u7801\u4e3a Huawei@123 \uff0c\u70b9\u51fb \u767b\u5f55 \u3002 \u767b\u5f55\u6210\u529f\u3002","title":"\u5b89\u88c5\u670d\u52a1\u7aef"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-server","text":"","title":"\u914d\u7f6eInformatica Server"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#kerberos","text":"\u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06 krb5.conf \u4e0a\u4f20\u81f3Infomatica Server\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u5c06 krb5.conf \u590d\u5236\u5230 $INFA_HOME/java/jre/lib/security \u548c $INFA_HOME/services/shared/security \u3002\u547d\u4ee4\u6267\u884c\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a chown infa:oinstall /opt/krb5.conf cp /opt/krb5.conf /opt/informatica/10.2.2/services/shared/security/ cp /opt/krb5.conf /opt/informatica/10.2.2/java/jre/lib/security/","title":"\u914d\u7f6eKerberos\u8ba4\u8bc1"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#data_integration_service","text":"\u5bfc\u822a\u81f3 \u7ba1\u7406->\u670d\u52a1\u548c\u8282\u70b9->Data_Integration_Service \u3002\u70b9\u51fb\u201c\u6267\u884c\u9009\u9879\u201d\u7684\u7f16\u8f91\u6309\u94ae\uff0c\u8bbe\u7f6e\u201cHadoop Kerberos\u670d\u52a1\u4e3b\u4f53\u540d\u79f0\u201d\u4e3a developuser@HADOOP.COM \uff0c\u201cHadoop Kerberos Keytab\u201d\u4e3a /opt/user.keytab \uff0c\u70b9\u89e3 \u786e\u5b9a \uff0c\u9009\u62e9 \u662f\uff0c\u4fdd\u5b58\u66f4\u6539 \u3002 \u70b9\u51fb\u201cData_Integration_Service\u201d\u7684\u5e94\u7528\u670d\u52a1\u6309\u94ae \uff0c\u70b9\u51fb \u786e\u5b9a \u8ba9\u4fee\u6539\u751f\u6548\u3002","title":"\u914d\u7f6eData_Integration_Service"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_8","text":"\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u662f\u4e00\u9879\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\uff0c\u5b83\u53ef\u8ba9 Developer tool \u8bbf\u95ee Hadoop \u8fde\u63a5\u4fe1\u606f\u4ee5\u5bfc\u5165\u548c\u9884\u89c8\u5143\u6570\u636e\u3002\u4eceHadoop\u96c6\u7fa4\u5bfc\u5165\u5bf9\u8c61\u65f6\uff0cHBase\u3001HDFS\u3001Hive\u8fde\u63a5\u4f1a\u4f7f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06 user.keytab \u4e0a\u4f20\u81f3Infomatica Server\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\u3002 \u4f7f\u7528\u6d4f\u89c8\u5668\u767b\u5f55 http://172-16-6-120:6008 \uff0c\u767b\u5f55\u7528\u6237\u540d\u4e3a Administrator \uff0c\u5bc6\u7801\u4e3a Huawei@123 \u3002 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u670d\u52a1\u548c\u8282\u70b9 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120\u201d\uff0c\u9009\u62e9 \u65b0->\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a Metadata_Service \uff0c\u201c\u4f4d\u7f6e\u201d\u9ed8\u8ba4\u4e3a Domain_172-16-6-120 \uff0c\u201c\u8bb8\u53ef\u8bc1\u201d\u9009\u62e9\u6709\u6548\u7684License\uff0c\u201c\u8282\u70b9\u201d\u9009\u62e9 node01_172-16-6-120 \u3002 \u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201cHadoop Kerberos\u670d\u52a1\u4e3b\u4f53\u540d\u79f0\u201d\u8f93\u5165 developuser@HADOOP.COM \uff0c\u201cHadoop Kerberos Keytab\u201d\u8f93\u5165 /opt/user.keytab \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u70b9\u51fb\u201cMetadata_Service\u201d\u53f3\u4e0a\u89d2\u7684 \u6309\u94ae\u542f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002","title":"\u521b\u5efa\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica","text":"\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u662f\u4e00\u9879\u5e94\u7528\u7a0b\u5e8f\u670d\u52a1\uff0c\u5b83\u53ef\u8ba9 Developer tool \u8bbf\u95ee Hadoop \u8fde\u63a5\u4fe1\u606f\u4ee5\u5bfc\u5165\u548c\u9884\u89c8\u5143\u6570\u636e\u3002\u4eceHadoop\u96c6\u7fa4\u5bfc\u5165\u5bf9\u8c61\u65f6\uff0cHBase\u3001HDFS\u3001Hive\u8fde\u63a5\u4f1a\u4f7f\u7528\u5143\u6570\u636e\u8bbf\u95ee\u670d\u52a1\u3002 \u5bfc\u822a\u81f3 \u7ba1\u7406->\u8fde\u63a5 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120->ClusterConfigurations\u201d\uff0c\u9009\u62e9 \u65b0->\u7fa4\u96c6\u914d\u7f6e \u3002 \u201c\u7fa4\u96c6\u914d\u7f6e\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a FusionInsightHD \uff0c\u201c\u5206\u53d1\u7c7b\u578b\u201d\u9009\u62e9 Cloudera \uff0c\u201c\u5bfc\u5165\u7fa4\u96c6\u914d\u7f6e\u7684\u65b9\u6cd5\u201d\u9009\u62e9 \u4ece\u5b58\u6863\u6587\u4ef6\u4e2d\u5bfc\u5165 \uff0c\u4e0a\u8f7d\u914d\u7f6e\u5b58\u6863\u6587\u4ef6\u9009\u62e9 C:\\ecotesting\\hadoopConfig.zip \uff0c\u52fe\u9009 \u521b\u5efa\u8fde\u63a5 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u786e\u8ba4\u5b89\u88c5\u4fe1\u606f\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5b89\u88c5\u5b8c\u6210\u3002 \u4fee\u6539\u8fde\u63a5\u201cHIVE_fusionginsighthd\u201d\u4ee5\u4e0b\u5171\u540c\u5c5e\u6027\u3002 \u5143\u6570\u636e\u8fde\u63a5\u5b57\u7b26\u4e32\uff1ajdbc:hive2://172.16.4.21:21066/default;saslQop=auth-conf;principal=hive/hadoop.hadoop.com@HADOOP.COM \u6570\u636e\u8bbf\u95ee\u8fde\u63a5\u5b57\u7b26\u4e32\uff1ajdbc:hive2://172.16.4.21:21066/default;saslQop=auth-conf;principal=hive/hadoop.hadoop.com@HADOOP.COM HDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\uff1a/user/hive/warehouse Hive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\uff1adefault \u8bf4\u660e\uff1a\u5982\u679c\u9700\u8981\u5411Hive\u5199\u5165\u6570\u636e\uff0c\u5fc5\u987b\u914d\u7f6e\u201cHDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\u201d\u548c\u201cHive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\u201d\u3002 \u5982\u679c\u9700\u8981\u5411Hive\u5199\u5165\u6570\u636e\uff0c\u5fc5\u987b\u5c06\u96c6\u6210\u914d\u7f6e\u7684 hdfs_site_xml \u7684 dfs.client.failover.proxy.provider.hacluster \u7684\u503c\u4fee\u6539\u4e3a org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u3002\u5426\u5219Mapping\u65e5\u5fd7\u4f1a\u8fd4\u56de\u7c7b\u4f3c\u201cClass org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider not found\u201d\u7684\u9519\u8bef\u4e14\u5199\u5165\u5931\u8d25\u3002 \u9009\u62e9\u7fa4\u96c6\u201cFusionInsightHD\u201d\uff0c\u70b9\u51fb hdfs_site_xml \u7684\u7f16\u8f91\u6309\u94ae\uff0c\u9009\u4e2d dfs.client.failover.proxy.provider.hacluster \u540e\u70b9\u51fb \u7f16\u8f91 \uff0c\u201c\u8986\u76d6\u7684\u503c\u201d\u8f93\u5165 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \uff0c\u70b9\u51fb \u786e\u5b9a \u3002\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u4fee\u6539\u3002","title":"\u521b\u5efaInformatica\u7fa4\u96c6"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#oracle","text":"\u5bfc\u822a\u81f3 \u7ba1\u7406->\u8fde\u63a5 \uff0c\u53f3\u952e\u201cDomain_172-16-6-120\u201d\uff0c\u9009\u62e9 \u65b0->\u8fde\u63a5 \u3002 \u9009\u62e9 Oracle \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u201c\u540d\u79f0\u201d\u548c\u201cID\u201d\u81ea\u5b9a\u4e49\u4e3a ORACLE \uff0c\u201c\u7528\u6237\u540d\u201d\u548c\u201c\u5bc6\u7801\u201d\u90fd\u8f93\u5165 mdl_user \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u5143\u6570\u636e\u8bbf\u95ee\u5c5e\u6027\u201d\u7684\u201c\u8fde\u63a5\u5b57\u7b26\u4e32\u201d\u8f93\u5165 jdbc:informatica:oracle://172-16-6-120:1521;SID=orcl \uff0c\u201c\u6570\u636e\u8bbf\u95ee\u5c5e\u6027\u201d\u7684\u201c\u8fde\u63a5\u5b57\u7b26\u4e32\u201d\u4e3a orcl \u548c \u201c\u4ee3\u7801\u9875\u201d\u9009\u62e9 UTF-8 encoding of Unicode \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \u8fd4\u56de \u201c\u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u201d\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u91cd\u542fInformatica Server\u3002\u5173\u95edInformatica Server\u540e\uff0c\u9700\u8981\u6267\u884c ps -ef | grep informatica \u68c0\u67e5\u6240\u6709\u7684informatica\u8fdb\u7a0b\u90fd\u5173\u95ed\u540e\u518d\u542f\u52a8Informatica Server\u3002 su - infa cd /opt/informatica/10.2.2/tomcat/bin ./infaservice.sh shutdown ps -ef | grep informatica ./infaservice.sh startup \u8bf4\u660e:\u5982\u679c\u65b0\u5efaORACLE\u8fde\u63a5\u540e\u672a\u91cd\u542f\u8fc7Informatica Server\uff0c\u5728\u5ba2\u6237\u7aefBig Data Developer\u8fd0\u884cORACLE\u5173\u7cfb\u578b\u6570\u636e\u5bf9\u8c61\u65f6\uff0c\u8fd4\u56de\u7c7b\u4f3c\u4ee5\u4e0b\u7684\u9519\u8bef\uff1a [LDTMCMN_0029] \u7531\u4e8e\u4ee5\u4e0b\u9519\u8bef\uff0cLDTM \u65e0\u6cd5\u5b8c\u6210\u8bf7\u6c42: com.informatica.sdk.dtm.ExecutionException: [EdtmExec_00007] CMN_1022 Database driver error... CMN_1022 [Database driver event...Error occurred loading library [libclntsh.so.11.1: cannot open shared object file: No such file or directory]Database driver event...Error occurred loading library [libpmora8.so]]","title":"\u521b\u5efaOracle\u8fde\u63a5"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#administrator","text":"\u5bfc\u822a\u81f3 \u5b89\u5168->\u7528\u6237 \uff0c\u9009\u4e2d \u7528\u6237->Native->Administrator \uff0c\u70b9\u51fb \u6982\u89c8->\u7f16\u8f91->\u7ec4 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u5c06 Operator \u6dfb\u52a0\u81f3\u201c\u5206\u914d\u7684\u7ec4\u201d \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u8bf4\u660e\uff1a\u5982\u679c\u7528\u6237\u4e0d\u5c5e\u4e8eOperator\u7ec4\uff0c\u4f7f\u7528\u8be5\u7528\u6237\u6267\u884cmapping\u4ece\u8fde\u63a5\u4e2d\u83b7\u53d6\u6570\u636e\u65f6\uff0c\u4f1a\u8fd4\u56de\u7c7b\u4f3c\u4e8e\u201c\u6ca1\u6709\u9488\u5bf9\u8fde\u63a5 [ORACLE] (\u5728\u57df [Domain_172-16-6-120] \u4e2d)\u7684\u6267\u884c\u6743\u9650\u201d\u7684\u9519\u8bef\u3002","title":"\u589e\u52a0\u7528\u6237Administrator\u6743\u9650"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#big-data-developer","text":"","title":"\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_9","text":"\u5728Windows\u4e0a\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_10","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5df2\u83b7\u53d6Informatica\u5ba2\u6237\u7aef\u5b89\u88c5\u5305\uff0c\u4f8b\u5982\uff1ainformatica_1022_client_winem-64t.zip\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_11","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_12","text":"\u89e3\u538binformatica_1022_client_winem-64t.zip\u540e\uff0c\u53cc\u51fb install.bat \u3002\u9009\u62e9 \u5b89\u88c5Informatica Developer\u7248\u672c10.2.2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u8f93\u5165\u5b89\u88c5\u8def\u5f84\uff0c\u9ed8\u8ba4\u5b89\u88c5\u8def\u5f84\u4e3a C:\\Informatica\\10.2.2 \uff0c\u70b9\u51fb \u4e0b\u4e00\u9875 \u3002 \u70b9\u51fb \u5b89\u88c5 \u540e\u7b49\u5f85\u5b89\u88c5\u5b8c\u6210\u3002 \u70b9\u51fb \u5b8c\u6210 \u3002","title":"\u5b89\u88c5\u5ba2\u6237\u7aef"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight-hd","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_13","text":"Informatica BDM\u5bf9\u63a5FusionInsight HD\u7684HDFS\u548cHive\u3002\u901a\u8fc7Informatica\u7684Big Data Developer\u5ba2\u6237\u7aef\u5b9e\u73b0Oracle\u6570\u636e\u5e93\u4e0eHDFS\u548cHive\u4e4b\u95f4\u4e92\u76f8\u4e0a\u4f20/\u4e0b\u8f7d\u6570\u636e\u3001HDFS\u4e0eHive\u4e92\u76f8\u4e0a\u4f20/\u4e0b\u8f7d\u6570\u636e\u3001HDFS/Hive\u4e0e\u672c\u5730\u4e4b\u95f4\u4e92\u76f8\u4e0a\u4f20\u4e0b\u8f7d\u6570\u636e\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_14","text":"\u5df2\u5b89\u88c5Informatica\u670d\u52a1\u7aef\u548cBig Data Developer\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06FusionInsight\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230Informatica Server\u5b89\u88c5\u8282\u70b9\u7684 /etc/hosts \u6587\u4ef6\u4e2d\u3002 \u5b89\u88c5Infomatica\u670d\u52a1\u7aef\u8282\u70b9\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u51c6\u5907\u6570\u636e\u3002 \u672c\u5730 \u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u5728 /tmp \u76ee\u5f55\u4e0b\u521b\u5efa\u6587\u4ef6 user_local_to_hdfs.csv \uff0c\u64cd\u4f5c\u547d\u4ee4\u5982\u4e0b\u3002\u5e76\u4e14\u5c06 user_local_to_hdfs.csv \u62f7\u8d1d\u81f3\u5b89\u88c5Big Data Developer\u5ba2\u6237\u7aef\u7684window\u7cfb\u7edf\uff0c\u4f8b\u5982 C:\\ \u76ee\u5f55\u4e0b\u3002 su - infa cd /tmp vi user_local_to_hdfs.csv user_local_to_hdfs.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 10,Andy-in-local 11,Benny-in-local 12,Tom-in-local HDFS\u6587\u4ef6\u7cfb\u7edf \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5728HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp \u76ee\u5f55\u521b\u5efa\u4e24\u4e2a\u6587\u4ef6\u5206\u522b\u547d\u540d\u4e3a user_hdfs_to_oracle.csv \u548c user_hdfs_to_hive.csv \u3002 cd /opt vi user_hdfs_to_oracle.csv vi user_hdfs_to_hive.csv hdfs dfs -put user_hdfs_to_* /tmp user_hdfs_to_oracle.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 20,Andy-in-hdfs 21,Benny-in-hdfs 22,Tom-in-hdfs user_hdfs_to_hive.csv\u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a id,name 30,Andy-in-hdfs 31,Benny-in-hdfs 32,Tom-in-hdfs Hive\u6570\u636e\u5e93 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a user_hive_in \u548c user_hive_out \u3002 \u521b\u5efauser_hive_in\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS user_hive_in(id INT, name STRING); \u521b\u5efauser_hive_out\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS user_hive_out(id INT, name STRING); INSERT INTO user_hive_out VALUES (40,'Andy-in-hive'); INSERT INTO user_hive_out VALUES (41,'Benny-in-hive'); INSERT INTO user_hive_out VALUES (42,'Tom-in-hive'); Oracle\u6570\u636e\u5e93 \u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a user_oracle_in \u548c user_oracle_out \u3002 su - oracle sqlplus mdl_user/mdl_user \u521b\u5efauser_oracle_in\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE user_oracle_in(ID INTEGER PRIMARY KEY,NAME VARCHAR2(30)); \u521b\u5efauser_oracle_out\u8868\u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE user_oracle_out(ID INTEGER PRIMARY KEY,NAME VARCHAR2(30)); INSERT INTO user_oracle_out VALUES (50,'Andy-in-oracle'); INSERT INTO user_oracle_out VALUES (51,'Benny-in-oracle'); INSERT INTO user_oracle_out VALUES (52,'Tom-in-oracle'); HBase \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u521b\u5efa\u4e24\u4e2a\u8868\u5206\u522b\u547d\u540d\u4e3a USER_HBASE_IN \u548c USER_HBASE_OUT \u3002 \u521b\u5efa\u547d\u540d\u7a7a\u95f4INFA\uff1a hbase shell create_namespace 'INFA' \u521b\u5efaUSER_HBASE_IN\u8868\u793a\u4f8b\u5982\u4e0b\uff1a create 'INFA.USER_HBASE_IN',{NAME=>'cf1'},{NAME=>'cf2'} \u521b\u5efaUSER_HBASE_OUT\u8868\u793a\u4f8b\u5982\u4e0b\uff1a create 'INFA.USER_HBASE_OUT',{NAME=>'cf1'},{NAME=>'cf2'} put 'INFA.USER_HBASE_OUT', '001','cf1:id','60' put 'INFA.USER_HBASE_OUT', '001','cf1:name','Andy-in-HBase' put 'INFA.USER_HBASE_OUT', '002','cf1:id','61' put 'INFA.USER_HBASE_OUT', '002','cf1:name','Benny-in-HBase' put 'INFA.USER_HBASE_OUT', '003','cf1:id','62' put 'INFA.USER_HBASE_OUT', '003','cf1:name','Tom-in-HBase'","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_15","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_16","text":"\u6253\u5f00 Big Data Developer \uff0c\u70b9\u51fb \u6587\u4ef6->\u8fde\u63a5\u5230\u5b58\u50a8\u5e93 \u3002 \u70b9\u51fb \u914d\u7f6e\u57df \uff0c \u70b9\u51fb \u6dfb\u52a0 \uff0c\u201c\u57df\u540d\u201d\u8f93\u5165\u81ea\u5b9a\u4e49\u540d\u79f0\u4e3a Domain_172-16-6-120 \uff0c\u201c\u4e3b\u673a\u540d\u201d\u8f93\u5165Informatica Server\u5b89\u88c5\u8282\u70b9\u5bf9\u5e94\u7684\u4e3b\u673a\u540d 172-16-6-120 \uff0c\u201c\u7aef\u53e3\u53f7\u201d\u8f93\u5165\u5b89\u88c5Informatica Server\u65f6\u6307\u5b9a\u7684\u7aef\u53e3 6005 \uff0c\u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u201c\u8fde\u63a5\u6210\u529f\u201d\u5219\u8868\u793a\u4e3b\u673a\u540d\u4e3a172-16-6-120\u7684\u57df\u53ef\u7528\u3002\u70b9\u51fb \u786e\u5b9a \u5e76 \u5b8c\u6210 \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 Domain_172-16-6-120.Model_Repository_Service \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u8f93\u5165\u201c\u7528\u6237\u540d\u201d\u4e3a Administrator \uff0c\u201c\u5bc6\u7801\u201d\u4e3a Huawei@123 \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5728\u201c\u5bf9\u8c61\u6d4f\u89c8\u5668\u201d\u4e0b\u663e\u793a\u8fde\u63a5\u6210\u529f\u7684\u5b58\u50a8\u5e93 Model_Repository_Service \uff08Administrator\uff09 \u3002 \u53f3\u952e Model_Repository_Service \uff08Administrator\uff09 \u9009\u62e9 \u65b0\u5efa->\u9879\u76ee \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u8f93\u5165\u81ea\u5b9a\u4e49\u540d\u79f0 fi_project \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u5b8c\u6210 \u3002","title":"\u5efa\u7acb\u9879\u76ee"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#_17","text":"","title":"\u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#-oracle","text":"\u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 ORACLE \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u8d44\u6e90\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\u9009\u62e9\u8868 USER_ORACLE_IN \u548c USER_ORACLE_OUT \u3002\u70b9\u51fb \u5b8c\u6210 \u3002","title":"\u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 - Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#-hive","text":"\u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5173\u7cfb\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 HIVE_fusionginsighthd \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u8d44\u6e90\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\u9009\u62e9\u8868 user_hive_in \u548c user_hive_out \u3002\u70b9\u51fb \u5b8c\u6210 \u3002","title":"\u521b\u5efa\u5173\u7cfb\u6570\u636e\u5bf9\u8c61 - Hive"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hbase","text":"\u767b\u5f55FusionInsight Manager\u5c06Zookeeper\u7ec4\u4ef6\u7684\u914d\u7f6e enforce.auth.enabled \u4fee\u6539\u4e3a false \u4fdd\u5b58\u540e\uff0c\u5e76 \u91cd\u542f Zookeeper\u4ee5\u53ca\u5176\u4e0a\u5c42\u670d\u52a1\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 HBase\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_IN \uff0c\u70b9\u51fb\u201c\u8fde\u63a5\u201d\u7684 \u6d4f\u89c8 \u6309\u94ae\uff0c\u9009\u62e9\u8fde\u63a5 HBASE_fusionginsighthd \u3002\u9009\u62e9 \u4ece\u73b0\u6709\u8d44\u6e90\u521b\u5efa\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb\u201c\u9009\u5b9a\u8d44\u6e90\u201d\u7684 \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9\u8868 INFA.USER_HBASE_IN \u3002\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u52fe\u9009 cf1 \uff0c\u9009\u62e9 \u6dfb\u52a0\u5217 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u6dfb\u52a0\u4e24\u5217\u540d\u79f0\u5206\u522b\u4e3a id \u548c name \u3002\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u52fe\u9009 \u5305\u542b\u884cID \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u70b9\u51fb \u5b8c\u6210 \u3002 \u540c\u6837\u5730\u65b0\u589eHBase\u8868 INFA.USER_HBASE_OUT \u3002","title":"\u521b\u5efaHBase\u6570\u636e\u5bf9\u8c61"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight-hdfs","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight HDFS"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hdfs-from-local","text":"\u5c06\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_to_hdfs.csv \u4e0a\u4f20\u81f3FusionInsight HD\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5e76\u547d\u540d\u4e3auser_hdfs_from_local.csv\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6570\u636e\u5bf9\u8c61 \u3002 \u9009\u62e9 \u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u9009\u62e9 \u4ece\u73b0\u6709\u5e73\u9762\u6587\u4ef6\u521b\u5efa \u5e76\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 C:\\user_local_to_hdfs.csv \uff0c\u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_local \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u683c\u5f0f\u201d\u9009\u62e9 \u5e26\u5206\u9694\u7b26 \uff0c\u70b9\u51fb \u4e0b\u4e00\u6b65 \u3002 \u201c\u5206\u9694\u7b26\u201d\u9009\u62e9 \u9017\u53f7 \uff0c\u52fe\u9009 \u5bfc\u5165\u7b2c\u4e00\u884c\u4e2d\u7684\u5217\u540d\u79f0 \uff0c\u5176\u4f59\u4fdd\u6301\u9ed8\u8ba4\u9009\u9879\uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u9009\u62e9\u6570\u636e\u5bf9\u8c61 hdfs_from_local \uff0c\u8bbe\u7f6e \u9ad8\u7ea7 \u5c5e\u6027\u540e\u4fdd\u5b58\u3002 \u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\u3002\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_local_to_hdfs.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \u3002 \u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\u3002\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_local.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002\u201cCtrl+s\u201d\u4fdd\u5b58\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u62d6\u66f3\u81f3\u201c\u6620\u5c04\u201d hdfs_from_local_mapping \u7684 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u7c7b\u4f3c\u5730\u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u62d6\u66f3\u81f3\u201c\u6620\u5c04\u201d hdfs_from_local_mapping \u7684 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002\u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_local.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_local.csv\u201d\u3002","title":"HDFS from Local"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hdfs-from-oracle","text":"\u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u5e76\u547d\u540d\u4e3auser_hdfs_from_oracle.csv\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_oracle \u3002 hdfs_from_oracle**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_oracle.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002\u201cCtrl+s\u201d\u4fdd\u5b58\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_oracle \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201c\u5199\u5165_hdfs_from_oracle\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_oracle.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_oracle.csv\u201d\u3002","title":"HDFS from Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hdfs-to-oracle","text":"\u5c06FusionInsight HD\u7684HDFS\u7cfb\u7edf\u6587\u4ef6user_hdfs_to_oracle.csv\u6570\u636e\u4e0a\u4f20\u81f3Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_IN\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_oracle \u3002 hdfs_to_oracle**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\uff1a\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_to_oracle.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_to_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_to_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_oracle \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_hdfs_to_oracle\u201d\u548c\u201c\u5199\u5165_USER_ORACLE_IN\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u67e5\u8be2\u8868\u201cUSER_ORACLE_IN\u201d\u6570\u636e\u3002 su - oracle sqlplus mdl_user/mdl_user select * from USER_ORACLE_IN;","title":"HDFS to Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hdfs-to-hive","text":"\u5c06FusionInsight HD\u7684HDFS\u7cfb\u7edf\u6587\u4ef6user_hdfs_to_hive.csv\u6570\u636e\u4e0a\u4f20\u81f3Hive\u6570\u636e\u5e93\u8868user_hive_in\u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_hive \u3002 hdfs_to_hive**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u5c5e\u6027\uff1a\u201c\u6e90\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_to_hive.csv \uff0c\u201c\u6e90\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hdfs_to_hive_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chdfs_to_hive_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hdfs_to_hive \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_in \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_hdfs_to_hive\u201d\u548c\u201c\u5199\u5165_user_hive_in\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 user_hive_in \u6570\u636e\u3002 beeline select * from user_hive_in;","title":"HDFS to Hive"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight-hive","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight Hive"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hive-to-local","text":"\u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0b\u8f7d\u81f3\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_from_hive.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hive_to_local \u3002 hive_to_local**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_local_from_hive.csv \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_local_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hive_to_local \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_hive_to_local\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u67e5\u770b\u6587\u4ef6 /tmp/user_local_from_hive.csv \u3002 cat /tmp/user_local_from_hive.csv","title":"Hive to Local"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hive-to-oracle","text":"\u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0a\u4f20\u81f3Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_IN\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_USER_ORACLE_IN\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55\u5b89\u88c5Informatica Server\u8282\u70b9\uff0cOracle\u6570\u636e\u5e93\u7528\u6237mdl_user\u4f7f\u7528sqlplus\u5ba2\u6237\u7aef\u67e5\u8be2\u8868\u201cUSER_ORACLE_IN\u201d\u6570\u636e\u3002 su - oracle sqlplus mdl_user/mdl_user select * from USER_ORACLE_IN;","title":"Hive to Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hive-from-oracle","text":"\u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_in\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_in \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201c\u5199\u5165_user_hive_in\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 user_hive_in \u6570\u636e\u3002 beeline select * from user_hive_in;","title":"Hive from Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hive-to-hdfs","text":"\u5c06FusionInsight HD\u7684Hive\u6570\u636e\u5e93\u8868user_hive_out\u6570\u636e\u4e0b\u8f7d\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684 /tmp/user_hdfs_from_hive.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hive_to_hdfs \u3002 hive_to_hdfs**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_hdfs_from_hive.csv \uff0c\u201c\u8fde\u63a5\u7c7b\u578b\u201d\u9009\u62e9 Hadoop\u6587\u4ef6\u7cfb\u7edf \uff0c\u201c\u8fde\u63a5\u540d\u79f0\u201d\u70b9\u51fb \u6d4f\u89c8 \u9009\u62e9 HDFS_fusioninsighthd \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hive_to_hdfs_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chive_to_hdfs_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HIVE_fusionginsighthd->user_hive_out \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hive_to_hdfs \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201c\u8bfb\u53d6_user_hive_out\u201d\u548c\u201c\u5199\u5165_hive_to_hdfs\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -cat /tmp/user_hdfs_from_hive.csv \u67e5\u770bmapping\u4ea7\u751f\u7684\u6587\u4ef6\u201cuser_hdfs_from_hive.csv\u201d\u3002 cat /tmp/user_hdfs_from_hive.csv","title":"Hive to HDFS"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#informatica-bdmfusioninsight-hbase","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight HBase"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hbase-to-local","text":"\u5c06FusionInsight HD\u7684HBase\u8868USER_HBASE_OUT\u6570\u636e\u4e0b\u8f7d\u81f3\u5b89\u88c5Informatica Server\u8282\u70b9\u7684\u672c\u5730\u6587\u4ef6 /tmp/user_local_from_hbase.csv \u3002 \u53c2\u8003\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hdfs_from_local \u7684\u64cd\u4f5c\u6b65\u9aa4\uff0c\u521b\u5efa\u201c\u5e73\u9762\u6587\u4ef6\u6570\u636e\u5bf9\u8c61\u201d hbase_to_local \u3002 hbase_from_local**\u7684 **\u9ad8\u7ea7 \u5c5e\u6027\u4e2d\uff0c\u4e0d\u9700\u8981\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u8bfb\u53d6 \u7684\u76f8\u5173\u53c2\u6570\u3002\u8bbe\u7f6e \u8fd0\u884c\u65f6\uff1a\u5199\u5165 \u7684\u5c5e\u6027\uff1a\u201c\u8f93\u51fa\u6587\u4ef6\u76ee\u5f55\u201d \u4e3a /tmp/ \uff0c\u201c\u8f93\u51fa\u6587\u4ef6\u540d\u201d\u4e3a user_local_from_hbase.csv \u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hbase_to_local_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chbase_to_local_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HBASE_fusionginsighthd->USER_HBASE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \uff0c\u70b9\u51fb \u65b0\u5efa\u64cd\u4f5c \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_OUT_Read \uff0c\u201c\u529f\u80fd\u201d\u9009\u62e9 \u8bfb\u53d6 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9 INFA_USER_HBASE_OUT \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201c\u9009\u62e9\u64cd\u4f5c\u201d\u4e3a USER_HBASE_OUT_Read \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d hbase_to_local \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \u3002 \u5c06\u201cUSER_HBASE_OUT_Read\u201d\u548c\u201c\u5199\u5165_hbase_to_local\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55Informatica Server\u5b89\u88c5\u8282\u70b9\uff0c\u67e5\u770b\u6587\u4ef6 /tmp/user_local_from_hbase.csv \u3002 cat /tmp/user_local_from_hbase.csv","title":"HBase to Local"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#hbase-from-oracle","text":"\u83b7\u53d6Oracle\u6570\u636e\u5e93\u8868USER_ORACLE_OUT\u7684\u6570\u636e\u4e0a\u4f20\u81f3FusionInsight HD\u7684HBase\u8868USER_HBASE_IN\u3002 \u53f3\u952e fi_project \uff0c\u9009\u62e9 \u65b0\u5efa->\u6620\u5c04 \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a hbase_from_oracle_mapping \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201chbase_from_oracle_mapping\u201d\u7684\u914d\u7f6e\u5982\u4e0b\uff1a \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d ORACLE->USER_ORACLE_OUT \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u8bfb\u53d6 \u3002 \u5c06\u201c\u7269\u7406\u6570\u636e\u5bf9\u8c61\u201d HBASE_fusionginsighthd->USER_HBASE_IN \u62d6\u66f3\u81f3 \u9ed8\u8ba4\u89c6\u56fe \u4e2d\uff0c\u5e76\u9009\u62e9\u4e3a \u5199\u5165 \uff0c\u70b9\u51fb \u65b0\u5efa\u64cd\u4f5c \u3002 \u201c\u540d\u79f0\u201d\u81ea\u5b9a\u4e49\u4e3a USER_HBASE_IN_Write \uff0c\u201c\u529f\u80fd\u201d\u9009\u62e9 \u5199\u5165 \uff0c\u70b9\u51fb \u6dfb\u52a0 \u6309\u94ae\u9009\u62e9 INFA_USER_HBASE_IN \uff0c\u70b9\u51fb \u5b8c\u6210 \u3002 \u201c\u9009\u62e9\u64cd\u4f5c\u201d\u4e3a USER_HBASE_IN_Write \uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u5c06\u201c\u8bfb\u53d6_USER_ORACLE_OUT\u201d\u548c\u201cUSER_HBASE_IN_Write\u201d\u5bf9\u5e94\u7684\u5217\u8fde\u7ebf\u3002 \u70b9\u51fbmapping\u7684\u7a7a\u767d\u5904\u786e\u8ba4 \u5c5e\u6027->\u8fd0\u884c\u65f6->\u9a8c\u8bc1\u73af\u5883 \u4e3a \u672c\u5730 \u3002 \u53f3\u952emapping\u7684\u7a7a\u767d\u5904\uff0c\u9009\u62e9 \u8fd0\u884c\u6620\u5c04 \u3002 mapping\u8fd0\u884c\u6210\u529f\u4e4b\u540e\uff0c\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528hbase shell\u5ba2\u6237\u7aef\u67e5\u8be2\u8868 INFA.USER_HBASE_IN \u6570\u636e\u3002 hbase shell scan 'INFA.USER_HBASE_IN'","title":"HBase from Oracle"},{"location":"Data_Integration/Informatica_BDM_10.2.2/#faq","text":"\u6267\u884c./infaservice.sh startup\u542f\u52a8Infomatica Server\u65f6\uff0c\u8fd4\u56deERROR: Node configuration file not accessible or invalid\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5728\u6267\u884c./infaservice.sh startup\u542f\u52a8infomatica server\u65f6\uff0c\u8fd4\u56deERROR: Node configuration file not accessible or invalid\u3002 \u6216\u8005\u5728 INFA_HOME/tomcat/bin\u76ee\u5f55\u6267\u884c./startup.sh\u542f\u52a8tomcat\u540e\uff0c\u6ca1\u67e5\u8be2\u5230java\u8fdb\u7a0b\uff0c\u5728 INFA_HOME/tomcat/bin\u76ee\u5f55\u6267\u884c./startup.sh\u542f\u52a8tomcat\u540e\uff0c\u6ca1\u67e5\u8be2\u5230java\u8fdb\u7a0b\uff0c\u5728 INFA_HOME/tomcat/logs/catalina.out\u8fd4\u56de\u9519\u8befjava.io.FileNotFoundException: null/isp/config/nodemeta.xml (No such file or directory)\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u95ee\u9898\u539f\u56e0\uff1a$INFA_HOME/isp/config/nodemeta.xml\u7684\u6240\u6709\u8005\u548c\u6240\u5c5e\u7ec4\u4e0d\u6b63\u786e\u3002infa\u7528\u6237\u65e0\u6cd5\u83b7\u53d6\u5230nodemeta.xml\u3002 root\u7528\u6237\u767b\u5f55\u5e76\u5207\u6362\u81f3 $INFA_HOME/isp/config/ \uff0c\u6267\u884c chown -R infa:oinstall nodemeta.xml \u4fee\u6539nodemeta.xml\u6240\u6709\u8005\u4e3ainfa\u548c\u6240\u5c5e\u7528\u6237\u7ec4\u4e3aoinstall\u3002\u5efa\u8bae\u6267\u884c chown -R infa:oinstall /opt/informatica/10.2.2/ \u4fee\u6539Informatica Server\u6240\u6709\u6587\u4ef6\u7684\u6240\u6709\u8005\u4e3ainfa\u548c\u6240\u5c5e\u7528\u6237\u7ec4\u4e3aoinstall\u3002 \u5411Hive\u6570\u636e\u5e93\u5199\u5165\u6570\u636e\u5931\u8d25 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u8fd0\u884cmapping\u5411Hive\u6570\u636e\u5e93\u7684\u67d0\u4e00\u5f20\u8868\u5199\u5165\u6570\u636e\uff0c\u67e5\u8be2\u76ee\u6807\u8868\u65f6\uff0c\u6ca1\u6709\u6570\u636e\u5199\u5165\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5982\u679c\u5411Hive\u8868\u5199\u5165\u6570\u636e\uff0c\u786e\u8ba4\u4ee5\u4e0b\u4e24\u70b9\u662f\u5426\u5df2\u914d\u7f6e\uff1a \u914d\u7f6eHIVE\u8fde\u63a5\u4ee5\u4e0b\u4e24\u4e2a\u5c5e\u6027\u7684\u503c\uff1a\u201cHDFS\u4e0a\u7684Hive\u6682\u5b58\u76ee\u5f55\u201d\u8bbe\u7f6e\u4e3a /user/hive/warehouse \uff0c\u201cHive\u6682\u5b58\u6570\u636e\u5e93\u540d\u79f0\u201d\u8bbe\u7f6e\u4e3a default \u3002 \u5c06\u96c6\u6210\u914d\u7f6e\u7684 hdfs_site_xml \u7684 dfs.client.failover.proxy.provider.hacluster \u7684\u503c\u4fee\u6539\u4e3a org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u3002 \u9009\u62e9\u7fa4\u96c6\u201cFusionInsightHD\u201d\uff0c\u70b9\u51fb hdfs_site_xml \u7684\u7f16\u8f91\u6309\u94ae\uff0c\u9009\u4e2d dfs.client.failover.proxy.provider.hacluster \u540e\u70b9\u51fb \u7f16\u8f91 \uff0c\u201c\u8986\u76d6\u7684\u503c\u201d\u8f93\u5165 org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \uff0c\u70b9\u51fb \u786e\u5b9a \u3002\u70b9\u51fb \u786e\u5b9a \u5b8c\u6210\u4fee\u6539\u3002 Big Data Developer\u6dfb\u52a0Hbase\u6570\u636e\u5bf9\u8c61\u65f6\u8fd4\u56deKeeperErrorCode = ConnectionLoss for /hbase \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u65b0\u5efaHBase\u6570\u636e\u5bf9\u8c61\uff0c\u70b9\u51fb \u6dfb\u52a0 \u83b7\u53d6HBase\u8868\u65f6\uff0c\u8fd4\u56de\u9519\u8befSDK_APP_COM_20000\u3002 Java.lang.RuntionException:org.apache.hadoop.hbase.ZookeeperConnectionException: Can\u2019t connet to Zookeeper KeeperErrorCode = ConnectionLoss for /hbase FusionInsight HD\u7684zookeeper\u65e5\u5fd7\uff0c\u4f8b\u5982\uff1a /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-euleros-hd01.log \uff0c\u8fd4\u56de\u7c7b\u4f3c\u4ee5\u4e0b\u7684\u9519\u8bef\uff1a ERROR | NIOWorkerThread-41 | Authentication failed as scheme is not valid: ['ip,'172.16.6.120], expected scheme zookeeper.enforce.auth.scheme=sasl \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5c06Zookeeper\u7ec4\u4ef6\u7684\u914d\u7f6e enforce.auth.enabled \u4fee\u6539\u4e3a false \u4fdd\u5b58\u540e\uff0c\u5e76 \u91cd\u542f Zookeeper\u4ee5\u53ca\u5176\u4e0a\u5c42\u670d\u52a1\u3002 \u5982\u4f55\u67e5\u770bmapping\u7684\u65e5\u5fd7 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5982\u679cmapping\u8fd0\u884c\u5b8c\u6210\u4e4b\u540e\uff0c\u6ca1\u6709\u5199\u5165\u6570\u636e\u6216\u8005mapping\u8fd0\u884c\u5931\u8d25\u7b49\uff0c\u5982\u4f55\u83b7\u53d6mapping\u8fd0\u884c\u7684\u8be6\u7ec6\u65e5\u5fd7\uff1f \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 Mapping\u8fd0\u884c\u7684\u65e5\u5fd7\u5b58\u653e\u4e8eInformatica Server\u5b89\u88c5\u8282\u70b9\u7684 $INFA_HOME/logs/node01_172-16-6-120/services/DataIntegrationService/disLogs/ms \u76ee\u5f55\u4e0b\u3002","title":"FAQ"},{"location":"Data_Integration/Informatica_BDM_PushDown/","text":"Informatica BDM\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica 10.0.0 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Hive/Yarn)","title":"10.0.0 <--> C70"},{"location":"Data_Integration/Informatica_BDM_PushDown/#informatica-bdmfusioninsight","text":"","title":"Informatica BDM\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Informatica_BDM_PushDown/#_1","text":"Informatica 10.0.0 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Hive/Yarn)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_PWX_CDC/","text":"Informatica PowerExchange CDC\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) \u73af\u5883\u4fe1\u606f \u00b6 Informatica PowerExchange CDC 10.2.0 Linux & Windows\u7248\u672c Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka\u5ba2\u6237\u7aef \u90e8\u7f72\u65b9\u6848 \u00b6 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72oracle\u6570\u636e\u5e93\uff0c\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u540c\u65f6\u90e8\u7f72Informatica PWX CDC\uff0c\u5e76\u542f\u7528listener\u548clogger\u8fdb\u884c\u65e5\u5fd7\u76d1\u542c\uff0c\u518d\u5b89\u88c5PWX Publisher,\u5c06\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e\u4f20\u9001\u5230kafka\u7684topic\u4e2d\u3002 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u5b89\u88c5FusionInsight HD Kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39PWX Publisher\u4f20\u9001\u8fc7\u6765\u7684\u6570\u636e (\u53ef\u9009)\u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5PWX CDC\uff0c\u542f\u7528listener\uff0c\u542f\u52a8navigator\u56fe\u5f62\u5316\u754c\u9762\uff0c\u67e5\u770bPWX\u6355\u83b7\u5230\u7684\u6570\u636e. \u6570\u636e\u5e93\u914d\u7f6e \u00b6 >\u6b64\u90e8\u5206\u914d\u7f6e\u8bf7\u53c2\u8003Informatica PowerExchange CDC\u6307\u5bfc\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html \u5207\u6362\u81f3oracle\u7528\u6237,\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >\u5efa\u8bae\u5728\u4e24\u6b21SHUTDOWN\u64cd\u4f5c\u4e4b\u524d\u5907\u4efd\u6570\u636e\u5e93. \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; \u62f7\u8d1dOracle Catalog \u81f3\u5f52\u6863\u65e5\u5fd7\u4e2d EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u521b\u5efa\u666e\u901a\u7528\u6237C##PWX,\u8d4b\u4e88\u521b\u5efa\u8868\u7684\u6743\u9650\uff0c\u8fde\u63a5\u81f3\u6570\u636e\u5e93 \u521b\u5efa\u6d4b\u8bd5\u8868,\u5411\u8868\u4e2d\u63d2\u5165\u4e00\u4e9b\u6570\u636e. Informatica PWX CDC & PWX Publisher \u5b89\u88c5\u914d\u7f6e \u00b6 \u5728Linux\u4e0a\u5b89\u88c5Informatica PWX CDC \u00b6 \u83b7\u53d6\u5b89\u88c5\u5305 pwx1020_linux_em64t.tar . \u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8bbe\u7f6e\u5b89\u88c5\u8def\u5f84\u5373\u53ef,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/PowerExchange/10.2.0 . \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u6253\u5f00\u914d\u7f6e\u6587\u4ef6 vi ~/.bash_profile \u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * \u6267\u884c source ~/.bash_profile ,source\u73af\u5883\u53d8\u91cf * \u6267\u884c dtlinfo ,\u68c0\u67e5\u5b89\u88c5\u4ee5\u53ca\u914d\u7f6e\u662f\u5426\u6210\u529f \u914d\u7f6edbmover.cfg\u4e0epwxccl.cfg\u6587\u4ef6 \u00b6 \u4fee\u6539PWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6\u5982\u4e0b nodeln \u4e3a\u81ea\u5b9a\u4e49\u7684\u76d1\u542c\u8282\u70b9\u540d ORACLEID\u4e2d\u7684\u7b2c\u4e8c\u4e2aORCL\uff0c\u4e3a\u88ab\u76d1\u542c\u7684\u6570\u636e\u5e93\u540d\u79f0\uff0c\u6b64\u5904\u4e3a\u9ed8\u8ba4\u7684ORCL CAPT_PATH\u6307\u5b9a\u4e86CDC\u7684\u63a7\u5236\u6587\u4ef6\u8def\u5f84\uff0c\u9700\u63d0\u524d\u521b\u5efa\u597d\u76f8\u5e94\u76ee\u5f55 \u6307\u5b9aSVCNODE\u548cCMDNODE\u540d\u79f0 \u4fee\u6539pwxccl.cfg\u6587\u4ef6\u5982\u4e0b CONDENSENAME\u9700\u8981\u548cdbmover.cfg\u6587\u4ef6\u4e2dSVCNODE\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 DBID \u4e3a\u6570\u636e\u5e93\u540d\u79f0 CAPTURE_NODE \u4e3a\u8fdb\u884c\u6355\u83b7\u8282\u70b9\u540d\u79f0 CAPTURE_NODE_UID \u4e3a\u767b\u5f55\u6570\u636e\u5e93\u7684\u7528\u6237\u540d CAPTURE_NODE_PWD \u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 \u542f\u52a8listener\u4ee5\u53calogger PWX CDC \u6355\u83b7ORACLE\u65e5\u5fd7\u6570\u636e \u00b6 ### \u5728Windows\u4e0a\u5b89\u88c5Informatica PWX CDC Windows\u4e0a\u5b89\u88c5Informatica PWX CDC\u4e3b\u8981\u662f\u53ef\u4ee5\u4f7f\u7528Navigator\u754c\u9762,\u67e5\u770b\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e. \u83b7\u53d6\u5b89\u88c5\u5305\u4e4b\u540e\u53cc\u51fb\u8fdb\u884c\u5b89\u88c5,\u4fee\u6539\u7cfb\u7edf\u73af\u5883\u53d8\u91cfPATH,\u6dfb\u52a0PWX\u5b89\u88c5\u76ee\u5f55. * \u6dfb\u52a0\u73af\u5883\u53d8\u91cfPWX_CONFIG,\u8bbe\u7f6e\u4e3aPWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6 * \u914d\u7f6edbmover.cfg\u6587\u4ef6 - \u914d\u7f6elistener\u540d\u79f0,\u6dfb\u52a0\u670d\u52a1\u7aeflistener\u914d\u7f6e\u4fe1\u606f - \u6307\u5b9a\u76d1\u542c\u6570\u636e\u5e93\u540d\u79f0 - \u8bbe\u7f6e\u63a7\u5236\u6587\u4ef6\u8def\u5f84 * \u542f\u52a8listener * \u4ece\u5f00\u59cb\u83dc\u5355\u680f\u542f\u52a8Navigator * \u5728\u83dc\u5355\u680f\u8d44\u6e90->\u6570\u636e\u6355\u83b7->\u6ce8\u518c\u7ec4\uff0c\u53f3\u952e\u65b0\u5efa\u6ce8\u518c\u7ec4\uff0c\u586b\u5199\u4fe1\u606f\u5982\u4e0b - \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 - \u4f4d\u7f6e\uff1aLinux\u670d\u52a1\u7aef\u76d1\u542c\u8282\u70b9\u540d\u79f0 - \u7c7b\u578b\uff1aORACLE - \u7528\u6237ID\u548c\u5bc6\u7801\uff1a\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 - \u96c6\u5408\u6807\u5fd7\u7b26\uff1a\u6570\u636e\u5e93\u540dORCL \u70b9\u51fb\u4e0b\u4e00\u6b65 \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u67b6\u6784\uff1aschema\u540d\u79f0\uff0c\u5373\u7528\u6237\u540d \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4f1a\u770b\u5230\u521a\u624d\u521b\u5efa\u7684test\u8868\uff0c\u53cc\u51fb\u8868\u540d\uff0c\u88ab\u9009\u5165\u53f3\u4fa7\uff0c\u9009\u62e9\u6240\u6709\u5217 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4fee\u6539\u72b6\u6001\u4e3a \u6d3b\u52a8 \uff0c\u52fe\u9009 \u7acb\u5373\u6267\u884cDDL ,\u70b9\u51fb\u5b8c\u6210 \u5728\u63d0\u53d6\u7ec4\uff0c\u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684orcl12,\u8fdb\u5165\u63d0\u53d6\u7ec4\u754c\u9762\uff0c\u53f3\u952e\uff0c\u6dfb\u52a0\u63d0\u53d6\u81ea\u5b9a\u4e49\uff0c\u586b\u5199\u6620\u5c04\u540d\u79f0\u4ee5\u53ca\u8868\u540d\u79f0 * \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u521b\u5efa\u7684\u6ce8\u518c \u70b9\u51fb\u6dfb\u52a0\uff0c\u5b8c\u6210 \u70b9\u51fb\u83dc\u5355\u680f\u56fe\u8868\uff0c\u6267\u884c\u884c\u6d4b\u8bd5,\u53ef\u770b\u5230\u6355\u83b7\u5230\u7684\u6570\u636e\u5e93\u65e5\u5fd7\u8bb0\u5f55 \u4f7f\u7528PWX CDC publisher\u5bf9\u63a5Kafka \u00b6 ### \u4fee\u6539kafka\u914d\u7f6e\u6587\u4ef6 * \u4fee\u6539producer.properties\u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * \u4fee\u6539jaas.conf\u6587\u4ef6\u5982\u4e0b ![](assets/Informatica_PWX_CDC/14cae.png) \u521b\u5efa\u4e00\u4e2akafka topic, pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### \u5b89\u88c5\u914d\u7f6eInformatica PWX Publisher * \u83b7\u53d6\u5b89\u88c5\u5305 pwxcdcpub120_linux_x64.tar.gz ,\u4ee5root\u7528\u6237\u8eab\u4efd\u89e3\u538b\u81f3\u5b89\u88c5\u76ee\u5f55\u5373\u53ef \u4ee5root\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u5728~/.bash_profile\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0csource\u73af\u5883\u53d8\u91cf,\u8fdb\u884ckerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser \u5c06\u5b89\u88c5\u76ee\u5f55samples\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\u590d\u5236\u5230instanceA/config\u76ee\u5f55\u4e0b\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u5185\u5bb9 > \u914d\u7f6ePWX Publisher\u53ef\u53c2\u8003Informatica \u5b98\u65b9\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html cdcPublisherAvro.cfg\u6587\u4ef6\u914d\u7f6e\u5982\u4e0b - cdcPublisherCommon.cfg\u6587\u4ef6\u4e2d\u6307\u5b9a\u7aef\u53e3 - cdcPublisherKafka.cfg\u6587\u4ef6\u4e2d\u6307\u5b9akafka topic\u540d\u79f0\u4ee5\u53caproperties\u6587\u4ef6\u8def\u5f84 - cdcPowerExchange.cfg\u6587\u4ef6\u4e2d\u914d\u7f6e\u5982\u4e0b * Extract.pwxCapiConnectionName\u4e3a\u5728dbmover.cfg\u4e2dCAPI_CONNECTION\u914d\u7f6e\u7684name * Extract.pwxExtractionMapSchemaName \u4e3apwx \u6355\u83b7\u6620\u5c04\u4e2d\u7684schema\u540d\u79f0\uff0c\u901a\u5e38\u683c\u5f0f\u4e3a unninstance \u6216\u8005 dnninstance \uff0c\u8fd9\u91cc\u4e3a u8orcl * Extract.pwxNodeLocation \u914d\u7f6e\u4e3apwx\u8282\u70b9\u540d\u79f0 * Extract.pwxNodeUserId\uff0cExtract.pwxNodePwd\u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 * Extract.pwxXmapUserId\u4e3a\u8bbf\u95eepwx\u63d0\u53d6\u6620\u5c04\u7684\u7528\u6237\u540d\u5bc6\u7801 * \u4fee\u6539\u5b89\u88c5\u8def\u5f84bin\u76ee\u5f55\u4e0b\u7684PwxCDCPublisher.sh\u542f\u52a8\u811a\u672c\u6587\u4ef6,\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u52a0\u5165\u4e00\u884c RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" * \u542f\u52a8pwx CDC Publisher,\u5728bin\u76ee\u5f55\u4e0b\u6267\u884c sh PwxCDCPublisher.sh \u542f\u52a8kafka consumer\uff0c\u67e5\u770b\u6d88\u8d39\u5230\u7684\u6570\u636e \u00b6 \u5728FusionInsight HD Kafka \u5ba2\u6237\u7aef,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cinsert\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cupdate\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cdelete\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b Q&A \u00b6 1.\u82e5\u542f\u52a8pwxccl\u62a5\u9519\u5982\u4e0b A:\u68c0\u67e5\u5728oracle\u4e2d\u662f\u5426\u6267\u884c\u8fc7 exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u82e5\u6267\u884c\u6210\u529f\uff0c\u4ecd\u7136\u62a5\u9519\uff0c\u7ed9C##PWX\u7528\u6237\u8d4b\u4e88sysdba\u6743\u9650 grant sysdba to C##PWX","title":"10.2.0 <--> C80"},{"location":"Data_Integration/Informatica_PWX_CDC/#informatica-powerexchange-cdcfusioninsight","text":"","title":"Informatica PowerExchange CDC\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Informatica_PWX_CDC/#_1","text":"Informatica PowerexChange CDC 10.2.0 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_PWX_CDC/#_2","text":"Informatica PowerExchange CDC 10.2.0 Linux & Windows\u7248\u672c Informatica PowerExchange Publisher 1.2.0 Oracle database 11g jdk-7u71-linux-x64.rpm FusionInsight HD Kafka\u5ba2\u6237\u7aef","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Informatica_PWX_CDC/#_3","text":"\u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72oracle\u6570\u636e\u5e93\uff0c\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u540c\u65f6\u90e8\u7f72Informatica PWX CDC\uff0c\u5e76\u542f\u7528listener\u548clogger\u8fdb\u884c\u65e5\u5fd7\u76d1\u542c\uff0c\u518d\u5b89\u88c5PWX Publisher,\u5c06\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e\u4f20\u9001\u5230kafka\u7684topic\u4e2d\u3002 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u5b89\u88c5FusionInsight HD Kafka\u5ba2\u6237\u7aef\uff0c\u6d88\u8d39PWX Publisher\u4f20\u9001\u8fc7\u6765\u7684\u6570\u636e (\u53ef\u9009)\u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5PWX CDC\uff0c\u542f\u7528listener\uff0c\u542f\u52a8navigator\u56fe\u5f62\u5316\u754c\u9762\uff0c\u67e5\u770bPWX\u6355\u83b7\u5230\u7684\u6570\u636e.","title":"\u90e8\u7f72\u65b9\u6848"},{"location":"Data_Integration/Informatica_PWX_CDC/#_4","text":">\u6b64\u90e8\u5206\u914d\u7f6e\u8bf7\u53c2\u8003Informatica PowerExchange CDC\u6307\u5bfc\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-for-cdc-and-mainframe/10-2/_cdc-guide-for-linux-unix-and-windows_powerexchange-for-cdc-and-mainframe_10-2_ditamap/powerexchange_cdc_data_sources_1/oracle_cdc_with_logminer.html \u5207\u6362\u81f3oracle\u7528\u6237,\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: SHUTDOWN IMMEDIATE ; STARTUP MOUNT ; ALTER DATABASE ARCHIVELOG ; ALTER DATABASE OPEN ; SHUTDOWN IMMEDIATE : STARTUP ; archive log list ; >\u5efa\u8bae\u5728\u4e24\u6b21SHUTDOWN\u64cd\u4f5c\u4e4b\u524d\u5907\u4efd\u6570\u636e\u5e93. \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; \u62f7\u8d1dOracle Catalog \u81f3\u5f52\u6863\u65e5\u5fd7\u4e2d EXECUTE SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u521b\u5efa\u666e\u901a\u7528\u6237C##PWX,\u8d4b\u4e88\u521b\u5efa\u8868\u7684\u6743\u9650\uff0c\u8fde\u63a5\u81f3\u6570\u636e\u5e93 \u521b\u5efa\u6d4b\u8bd5\u8868,\u5411\u8868\u4e2d\u63d2\u5165\u4e00\u4e9b\u6570\u636e.","title":"\u6570\u636e\u5e93\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PWX_CDC/#informatica-pwx-cdc-pwx-publisher","text":"","title":"Informatica PWX CDC &amp; PWX Publisher \u5b89\u88c5\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PWX_CDC/#linuxinformatica-pwx-cdc","text":"\u83b7\u53d6\u5b89\u88c5\u5305 pwx1020_linux_em64t.tar . \u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8bbe\u7f6e\u5b89\u88c5\u8def\u5f84\u5373\u53ef,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/PowerExchange/10.2.0 .","title":"\u5728Linux\u4e0a\u5b89\u88c5Informatica PWX CDC"},{"location":"Data_Integration/Informatica_PWX_CDC/#_5","text":"\u6253\u5f00\u914d\u7f6e\u6587\u4ef6 vi ~/.bash_profile \u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWX_CONFIG=/opt/PowerExchange10.2.0/dbmover.cfg export PWX_HOME=/opt/PowerExchange10.2.0 PATH=$PATH:$HOME/bin:/usr/lib/oracle/12.1/client64/bin:/opt/PowerExchange10.2.0 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:/opt/PowerExchange10.2.0 export NLS_LANG=AMERICAN_AMERICA.ZHS16GBK * \u6267\u884c source ~/.bash_profile ,source\u73af\u5883\u53d8\u91cf * \u6267\u884c dtlinfo ,\u68c0\u67e5\u5b89\u88c5\u4ee5\u53ca\u914d\u7f6e\u662f\u5426\u6210\u529f","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/Informatica_PWX_CDC/#dbmovercfgpwxcclcfg","text":"\u4fee\u6539PWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6\u5982\u4e0b nodeln \u4e3a\u81ea\u5b9a\u4e49\u7684\u76d1\u542c\u8282\u70b9\u540d ORACLEID\u4e2d\u7684\u7b2c\u4e8c\u4e2aORCL\uff0c\u4e3a\u88ab\u76d1\u542c\u7684\u6570\u636e\u5e93\u540d\u79f0\uff0c\u6b64\u5904\u4e3a\u9ed8\u8ba4\u7684ORCL CAPT_PATH\u6307\u5b9a\u4e86CDC\u7684\u63a7\u5236\u6587\u4ef6\u8def\u5f84\uff0c\u9700\u63d0\u524d\u521b\u5efa\u597d\u76f8\u5e94\u76ee\u5f55 \u6307\u5b9aSVCNODE\u548cCMDNODE\u540d\u79f0 \u4fee\u6539pwxccl.cfg\u6587\u4ef6\u5982\u4e0b CONDENSENAME\u9700\u8981\u548cdbmover.cfg\u6587\u4ef6\u4e2dSVCNODE\u914d\u7f6e\u4fdd\u6301\u4e00\u81f4 DBID \u4e3a\u6570\u636e\u5e93\u540d\u79f0 CAPTURE_NODE \u4e3a\u8fdb\u884c\u6355\u83b7\u8282\u70b9\u540d\u79f0 CAPTURE_NODE_UID \u4e3a\u767b\u5f55\u6570\u636e\u5e93\u7684\u7528\u6237\u540d CAPTURE_NODE_PWD \u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 \u542f\u52a8listener\u4ee5\u53calogger","title":"\u914d\u7f6edbmover.cfg\u4e0epwxccl.cfg\u6587\u4ef6"},{"location":"Data_Integration/Informatica_PWX_CDC/#pwx-cdc-oracle","text":"### \u5728Windows\u4e0a\u5b89\u88c5Informatica PWX CDC Windows\u4e0a\u5b89\u88c5Informatica PWX CDC\u4e3b\u8981\u662f\u53ef\u4ee5\u4f7f\u7528Navigator\u754c\u9762,\u67e5\u770b\u6355\u83b7\u5230\u7684\u65e5\u5fd7\u6570\u636e. \u83b7\u53d6\u5b89\u88c5\u5305\u4e4b\u540e\u53cc\u51fb\u8fdb\u884c\u5b89\u88c5,\u4fee\u6539\u7cfb\u7edf\u73af\u5883\u53d8\u91cfPATH,\u6dfb\u52a0PWX\u5b89\u88c5\u76ee\u5f55. * \u6dfb\u52a0\u73af\u5883\u53d8\u91cfPWX_CONFIG,\u8bbe\u7f6e\u4e3aPWX\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684dbmover.cfg\u6587\u4ef6 * \u914d\u7f6edbmover.cfg\u6587\u4ef6 - \u914d\u7f6elistener\u540d\u79f0,\u6dfb\u52a0\u670d\u52a1\u7aeflistener\u914d\u7f6e\u4fe1\u606f - \u6307\u5b9a\u76d1\u542c\u6570\u636e\u5e93\u540d\u79f0 - \u8bbe\u7f6e\u63a7\u5236\u6587\u4ef6\u8def\u5f84 * \u542f\u52a8listener * \u4ece\u5f00\u59cb\u83dc\u5355\u680f\u542f\u52a8Navigator * \u5728\u83dc\u5355\u680f\u8d44\u6e90->\u6570\u636e\u6355\u83b7->\u6ce8\u518c\u7ec4\uff0c\u53f3\u952e\u65b0\u5efa\u6ce8\u518c\u7ec4\uff0c\u586b\u5199\u4fe1\u606f\u5982\u4e0b - \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 - \u4f4d\u7f6e\uff1aLinux\u670d\u52a1\u7aef\u76d1\u542c\u8282\u70b9\u540d\u79f0 - \u7c7b\u578b\uff1aORACLE - \u7528\u6237ID\u548c\u5bc6\u7801\uff1a\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 - \u96c6\u5408\u6807\u5fd7\u7b26\uff1a\u6570\u636e\u5e93\u540dORCL \u70b9\u51fb\u4e0b\u4e00\u6b65 \u540d\u79f0\uff1a\u81ea\u5b9a\u4e49 \u67b6\u6784\uff1aschema\u540d\u79f0\uff0c\u5373\u7528\u6237\u540d \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4f1a\u770b\u5230\u521a\u624d\u521b\u5efa\u7684test\u8868\uff0c\u53cc\u51fb\u8868\u540d\uff0c\u88ab\u9009\u5165\u53f3\u4fa7\uff0c\u9009\u62e9\u6240\u6709\u5217 \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u4fee\u6539\u72b6\u6001\u4e3a \u6d3b\u52a8 \uff0c\u52fe\u9009 \u7acb\u5373\u6267\u884cDDL ,\u70b9\u51fb\u5b8c\u6210 \u5728\u63d0\u53d6\u7ec4\uff0c\u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684orcl12,\u8fdb\u5165\u63d0\u53d6\u7ec4\u754c\u9762\uff0c\u53f3\u952e\uff0c\u6dfb\u52a0\u63d0\u53d6\u81ea\u5b9a\u4e49\uff0c\u586b\u5199\u6620\u5c04\u540d\u79f0\u4ee5\u53ca\u8868\u540d\u79f0 * \u70b9\u51fb\u4e0b\u4e00\u6b65\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u521b\u5efa\u7684\u6ce8\u518c \u70b9\u51fb\u6dfb\u52a0\uff0c\u5b8c\u6210 \u70b9\u51fb\u83dc\u5355\u680f\u56fe\u8868\uff0c\u6267\u884c\u884c\u6d4b\u8bd5,\u53ef\u770b\u5230\u6355\u83b7\u5230\u7684\u6570\u636e\u5e93\u65e5\u5fd7\u8bb0\u5f55","title":"PWX CDC \u6355\u83b7ORACLE\u65e5\u5fd7\u6570\u636e"},{"location":"Data_Integration/Informatica_PWX_CDC/#pwx-cdc-publisherkafka","text":"### \u4fee\u6539kafka\u914d\u7f6e\u6587\u4ef6 * \u4fee\u6539producer.properties\u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e sasl.mechanism = GSSAPI key.serializer = org.apache.kafka.common.serialization.StringSerializer value.serializer = org.apache.kafka.common.serialization.ByteArraySerializer key.deserializer = org.apache.kafka.common.serialization.StringDeserializer value.deserializer = org.apache.kafka.common.serialization.StringDeserializer * \u4fee\u6539jaas.conf\u6587\u4ef6\u5982\u4e0b ![](assets/Informatica_PWX_CDC/14cae.png) \u521b\u5efa\u4e00\u4e2akafka topic, pwxtopic cd /opt/hadoopclient/Kafka/kafka/bin kafka-topics.sh --create --zookeeper 172.16.4.21:24002/kafka --partitions 2 --replication-factor 2 --topic pwxtopic ### \u5b89\u88c5\u914d\u7f6eInformatica PWX Publisher * \u83b7\u53d6\u5b89\u88c5\u5305 pwxcdcpub120_linux_x64.tar.gz ,\u4ee5root\u7528\u6237\u8eab\u4efd\u89e3\u538b\u81f3\u5b89\u88c5\u76ee\u5f55\u5373\u53ef \u4ee5root\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u5728~/.bash_profile\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u914d\u7f6e export PWXPUB_HOME=/opt/pwxcdcpub120_linux_x64 export KAFKA_CLIENT_LIBS=/opt/hadoopclient/Kafka/kafka/libs export PWX_LICENSE=/opt/pwx1020.key \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0csource\u73af\u5883\u53d8\u91cf,\u8fdb\u884ckerberos\u8ba4\u8bc1 source ~/.bash_profile source /opt/hadoopclien/bigdata_env kinit developuser \u5c06\u5b89\u88c5\u76ee\u5f55samples\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\u590d\u5236\u5230instanceA/config\u76ee\u5f55\u4e0b\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u5185\u5bb9 > \u914d\u7f6ePWX Publisher\u53ef\u53c2\u8003Informatica \u5b98\u65b9\u6587\u6863 https://docs.informatica.com/data-integration/powerexchange-cdc-publisher/1-1/user-guide/configuring-powerexchange-cdc-publisher.html cdcPublisherAvro.cfg\u6587\u4ef6\u914d\u7f6e\u5982\u4e0b - cdcPublisherCommon.cfg\u6587\u4ef6\u4e2d\u6307\u5b9a\u7aef\u53e3 - cdcPublisherKafka.cfg\u6587\u4ef6\u4e2d\u6307\u5b9akafka topic\u540d\u79f0\u4ee5\u53caproperties\u6587\u4ef6\u8def\u5f84 - cdcPowerExchange.cfg\u6587\u4ef6\u4e2d\u914d\u7f6e\u5982\u4e0b * Extract.pwxCapiConnectionName\u4e3a\u5728dbmover.cfg\u4e2dCAPI_CONNECTION\u914d\u7f6e\u7684name * Extract.pwxExtractionMapSchemaName \u4e3apwx \u6355\u83b7\u6620\u5c04\u4e2d\u7684schema\u540d\u79f0\uff0c\u901a\u5e38\u683c\u5f0f\u4e3a unninstance \u6216\u8005 dnninstance \uff0c\u8fd9\u91cc\u4e3a u8orcl * Extract.pwxNodeLocation \u914d\u7f6e\u4e3apwx\u8282\u70b9\u540d\u79f0 * Extract.pwxNodeUserId\uff0cExtract.pwxNodePwd\u4e3a\u5bf9\u5e94\u6570\u636e\u5e93\u7528\u6237\u5bc6\u7801 * Extract.pwxXmapUserId\u4e3a\u8bbf\u95eepwx\u63d0\u53d6\u6620\u5c04\u7684\u7528\u6237\u540d\u5bc6\u7801 * \u4fee\u6539\u5b89\u88c5\u8def\u5f84bin\u76ee\u5f55\u4e0b\u7684PwxCDCPublisher.sh\u542f\u52a8\u811a\u672c\u6587\u4ef6,\u5728\u4ee5\u4e0b\u4f4d\u7f6e\u52a0\u5165\u4e00\u884c RUN=\"$RUN -Djava.security.auth.login.config=/opt/hadoopclient/Kafka/kafka/config/jaas.conf\" * \u542f\u52a8pwx CDC Publisher,\u5728bin\u76ee\u5f55\u4e0b\u6267\u884c sh PwxCDCPublisher.sh","title":"\u4f7f\u7528PWX CDC publisher\u5bf9\u63a5Kafka"},{"location":"Data_Integration/Informatica_PWX_CDC/#kafka-consumer","text":"\u5728FusionInsight HD Kafka \u5ba2\u6237\u7aef,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8consumer source /opt/hadoopclient/bigdata_env kinit developuser cd /opt/hadoopclient/Kafka/kafka/bin ./kafka-console-consumer.sh --bootstrapserver 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic pwxtopic --new-consumer --consumer.config ../config/consumer.properties \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cinsert\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cupdate\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b \u5728oracle\u6570\u636e\u6e90\u4e2d\u6267\u884cdelete\u64cd\u4f5c,\u5728kafka\u4e2d\u53ef\u4ee5\u770b\u5230\u6570\u636e\u6355\u83b7\u5982\u4e0b","title":"\u542f\u52a8kafka consumer\uff0c\u67e5\u770b\u6d88\u8d39\u5230\u7684\u6570\u636e"},{"location":"Data_Integration/Informatica_PWX_CDC/#qa","text":"1.\u82e5\u542f\u52a8pwxccl\u62a5\u9519\u5982\u4e0b A:\u68c0\u67e5\u5728oracle\u4e2d\u662f\u5426\u6267\u884c\u8fc7 exec SYS.DBMS_LOGMNR_D.BUILD(options => sys.dbms_logmnr_d.store_in_redo_logs); \u82e5\u6267\u884c\u6210\u529f\uff0c\u4ecd\u7136\u62a5\u9519\uff0c\u7ed9C##PWX\u7528\u6237\u8d4b\u4e88sysdba\u6743\u9650 grant sysdba to C##PWX","title":"Q&amp;A"},{"location":"Data_Integration/Informatica_PowerCenter/","text":"Informatica PowerCenter\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive) \u73af\u5883\u4fe1\u606f \u00b6 Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD \u5ba2\u6237\u7aef \u90e8\u7f72\u65b9\u6848 \u00b6 \u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72Informatica Server\uff0c\u5e76\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5Informatica PowerCenter Client \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u00b6 \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88HDFS,Hive\u6240\u6709\u6743\u9650\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.confh\u548cuser.keytab\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/ \u76ee\u5f55\u4e0b \u5728Linux\u4e0a\u5b89\u88c5Oracle database \u4ee5\u53ca Informatica Server \u00b6 \u521b\u5efaoracle \u7528\u6237\uff0c\u5b89\u88c5oracle \u6570\u636e\u5e93 \u521b\u5efainfa\u7528\u6237\uff0c\u4f7f\u7528 sqlplus / as sysdba \u767b\u5f55\u81f3oracle\u6570\u636e\u5e93\u4e2d\uff0c\u6267\u884c\u4ee5\u4e0bsql\u8bed\u53e5 create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp; grant dba to domain_user,pwc_user,mdl_user; \u83b7\u53d6Informatica Server\u5b89\u88c5\u5305\u5e76\u4e0a\u4f20\u81f3\u8282\u70b9,\u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8fdb\u884c\u5b89\u88c5,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /home/infa/Informatica/10.2.0 \u3002 \u5b89\u88c5\u5b8c\u6210\u540e\uff0cInformatica Server\u4f1a\u81ea\u884c\u542f\u52a8\uff0c\u5728\u6d4f\u89c8\u5668\u8f93\u5165ip:6008\u7aef\u53e3\uff0c\u6253\u5f00Administrator \u7ba1\u7406\u754c\u9762\uff0c\u8f93\u5165\u5b89\u88c5\u65f6\u8bbe\u7f6e\u7684\u7528\u6237\u540d\u5bc6\u7801\u8fdb\u884c\u767b\u5f55\u3002\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u6216\u505c\u6b62Informatica Server\u3002 su - infa /home/infa/Informatica/10.2.0/tomcat/bin/infaservice.sh startup /home/infa/Informatica/10.2.0/tomcat/bin/infaservice.sh shutdown \u8bf4\u660e\uff1a\u5982\u679c\u4f7f\u7528\u975einfa\u7528\u6237\u542f\u52a8\u6216\u8005\u505c\u6b62Informatica Server\u4e4b\u540e\uff0c\u4f1a\u5bfc\u81f4 /home/infa/Informatica/10.2.0 \u76ee\u5f55\u4e0b\u4e00\u4e9b\u6587\u4ef6\u7684\u6240\u6709\u8005\u53d1\u751f\u53d8\u5316\uff0c\u5bfc\u81f4\u4e0b\u4e00\u6b21\u542f\u52a8\u6216\u8005\u505c\u6b62\u5931\u8d25\u3002\u53ef\u6267\u884c chown -R infa:oinstall /home/infa/Informatica/10.2.0 \u4fee\u590d\u3002 Informatica Server\u914d\u7f6e \u00b6 \u521b\u5efaPowerCenter \u5b58\u50a8\u5e93 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter \u5b58\u50a8\u5e93 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u6570\u636e\u5e93\u4fe1\u606f\uff0c\u5b8c\u6210 - \u70b9\u51fb\u53f3\u4e0a\u89d2\u6309\u94ae\u542f\u7528\u5b58\u50a8\u5e93\uff0c\u5e76\u4e3a\u5b58\u50a8\u5e93\u521b\u5efa\u5185\u5bb9 \u5728\u5b58\u50a8\u5e93\u5c5e\u6027\u4e2d\uff0c\u4fee\u6539\u64cd\u4f5c\u7c7b\u578b\u4e3a\u666e\u901a\uff0c\u5e76\u91cd\u542f\u670d\u52a1 \u521b\u5efaPowerCenter \u6570\u636e\u96c6\u6210\u670d\u52a1 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter\u96c6\u6210\u670d\u52a1 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u5b58\u50a8\u5e93\u4fe1\u606f\uff0c\u70b9\u51fb\u5b8c\u6210\uff0c\u5e76\u542f\u7528\u670d\u52a1 \u5728infa server\u521b\u5efadevelopuser \u5728\u5b89\u5168\u9875\u7b7e\u4e0b\uff0c\u521b\u5efa\u4e00\u4e2a\u7528\u6237\uff0c\u540d\u4e3adevelopuser\uff0c\u4e0eHadoop\u96c6\u7fa4\u7528\u6237\u4fdd\u6301\u4e00\u81f4 \u4fee\u6539\u7528\u6237\u7684\u4f18\u5148\u7ea7\u4ee5\u53ca\u7528\u6237\u7ec4 \u5728infa Server \u8fdb\u884cHadoop\u914d\u7f6e \u5c06 /opt \u76ee\u5f55\u4e0b\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u81f3 /etc \u76ee\u5f55\u4e0b\u4ee5\u53cainformatica\u5b89\u88c5\u76ee\u5f55 ${INFA_HOME}java/jre/lib/security/ \u4e0b\uff0c\u5e76\u8d4b\u4e88infa\u7528\u6237\u6539\u6587\u4ef6\u7684\u8bfb\u53d6\u6743\u9650 \u4ee5infa\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\uff0c\u4f8b\u5982 /opt/pwx-hadoop/conf \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u4ee5\u4e0b\u914d\u7f6e\u6587\u4ef6\uff0c\u653e\u81f3 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650\u81f3775 cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/pwx-hadoop/conf cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/pwx-hadoop/conf cp /opt/hadoopclient/Hive/config/hive-site.xml /opt/pwx-hadoop/conf cp /opt/hadoopclient/Yarn/config/mapred-site.xml /opt/pwx-hadoop/conf chmod -R 775 /opt/pwx-hadoop/conf chown infa:oinstall /opt/pwx-hadoop/conf/* \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884cKerberos\u8ba4\u8bc1\uff0c\u5e76\u6307\u5b9acache\u6587\u4ef6\uff0cinfa\u7528\u6237\u9700\u8981\u5bf9\u6307\u5b9a\u7684\u8def\u5f84\u6709\u8bfb\u5199\u6743\u9650 source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser chown infa:oinstall /home/infa/krb5cc_developuser \u4fee\u6539 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\u7684 core-site.xml \u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> \u5728Administrator \u7ba1\u7406\u754c\u9762\uff0c\u4e3a\u96c6\u6210\u670d\u52a1\u521b\u5efa\u73af\u5883\u53d8\u91cf CLASSPATH=/opt/pwx-hadoop/conf \uff0c\u5e76\u91cd\u542f\u96c6\u6210\u670d\u52a1 \u8bf4\u660e\uff1a\u5982\u679c\u4f7f\u7528FusionInsight HD\u63d0\u4f9b\u7684Hive ODBC\u9a71\u52a8\u8fde\u63a5Hive\uff0c\u9700\u8981\u521b\u5efa\u73af\u5883\u53d8\u91cf LD_PRELOAD=/usr/lib64/libodbchive.so \u3002 \u5220\u9664 /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ \u76ee\u5f55\u4e0bhive\u76f8\u5173\u7684jar\u5305\uff0c\u5e76\u5c06 /opt/hadoopclient/Hive/Beeline/lib \u4e0bhive\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u81f3\u8be5\u76ee\u5f55\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650 rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /opt/hadoopclient/Hive/Beeline/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* PowerCenter Client ~ From Oracle to HDFS/Hive \u00b6 PowerCenter Repository Manager\u914d\u7f6e \u00b6 \u83b7\u53d6PowerCenter Client\u5b89\u88c5\u5305\uff0c\u5b89\u88c5\u65f6\u9009\u53d6PowerCenter Client,\u542f\u52a8PowerCenter Repository Manager\uff0c\u9009\u62e9\u83dc\u5355\u680f\u4ed3\u5e93->\u914d\u7f6e\u57df\uff0c\u914d\u7f6e\u5b8c\u6210\u53ef\u4ee5\u770b\u5230\u4e4b\u524d\u521b\u5efa\u7684\u5b58\u50a8\u5e93 \u53cc\u51fb\u5b58\u50a8\u5e93\uff0c\u8f93\u5165\u5bc6\u7801\uff0c\u8fde\u63a5 \u9009\u62e9\u83dc\u5355\u680f\u6587\u4ef6\u5939,\u521b\u5efa\u6587\u4ef6\u5939 PowerCenter Designer\u521b\u5efamapping \u00b6 \u6253\u5f00PowerCenter Designer\uff0c\u53f3\u952e\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939\uff0c\u70b9\u51fbopen\uff0c\u6253\u5f00\u914d\u7f6e\u754c\u9762 - \u70b9\u51fb\u83dc\u5355\u680fSources->import from databases\uff0c\u5728ODBC\u6570\u636e\u6e90\u4e2d\u521b\u5efasitDSN\uff0c\u586b\u5199\u6570\u636e\u5e93\u76f8\u5173\u4fe1\u606f\uff0c\u6d4b\u8bd5\u8fde\u63a5 - \u9009\u62e9\u521a\u624d\u521b\u5efa\u7684\u6570\u636e\u6e90\uff0c\u586b\u5165\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801\uff0c\u8fde\u63a5\uff0c\u53ef\u4ee5\u770b\u5230\u6570\u636e\u5e93\u4e2d\u7684\u8868 - \u9009\u62e9target designer\uff0c\u62d6\u5165source\u4e2d\u7684\u8868 - \u53cc\u51fb\u8868\uff0c\u8bbe\u7f6e\u6570\u636e\u7c7b\u578b\u4e3aFlat File \u5728mapping\u8bbe\u7f6e\u9875\u9762\uff0c\u521b\u5efa\u65b0\u7684mapping\uff0c\u62d6\u5165source\u548ctarget\u8868\uff0c\u5e76\u8fde\u7ebf PowerCenter Workflow Manager\u8fd0\u884cworkflow \u00b6 \u5728\u83dc\u5355\u680f\u9009\u62e9task,\u65b0\u5efa\u4e00\u4e2atask,\u547d\u540d\u5e76\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684map \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165\u521a\u624d\u65b0\u5efa\u7684task\uff0c\u5e76\u8fde\u7ebf \u5728\u83dc\u5355\u680fconnection\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2aapplication connection,\u9009\u62e9Hadoop HDFS Connection \u5177\u4f53\u4fe1\u606f\u586b\u5199\u5982\u4e0b HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser \u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684task\uff0c\u5728mapping\u9009\u9879\u5361\uff0c\u70b9\u51fbtarget\uff0c\u8bbe\u7f6e\u5199\u5165\u7c7b\u578b\u4e3a HDFS Flat Write \uff0c\u5e76\u9009\u62e9\u8fde\u63a5\u4e3a\u521a\u624d\u521b\u5efa\u7684connection\uff0c\u5e76\u5728properties\u4e2d\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u4fdd\u5b58\u5f53\u524dworkflow\uff0c\u53f3\u952e\uff0c\u542f\u52a8workflow \u5728PowerCenter Workflow Monitor\u4e2d\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u60c5\u51b5 \u5728HDFS\u4e2d\u53ef\u4ee5\u770b\u5230\u5bfc\u5165\u7684\u6570\u636e \u5728task\u914d\u7f6e\u4e2d\u52fe\u9009\u5199\u5165Hive\u8868\uff0c\u586b\u5165\u4e4b\u524d\u521b\u5efa\u7684\u8868\u540d\uff0c\u8fd0\u884cworkflow \u5728Hive\u4e2d\u53ef\u4ee5\u770b\u5230\u8868\u4e2d\u7684\u6570\u636e PowerCenter Client ~ From Hive to Local \u00b6 \u4f7f\u7528FusionInsight HD\u63d0\u4f9b\u7684Hive ODBC\u9a71\u52a8\u5bf9\u63a5\u5e76\u8bfb\u53d6Hive\u8868\u7684\u6570\u636e\u4e0b\u8f7d\u81f3\u672c\u5730\u3002 \u5b89\u88c5FusionInsight HD\u7684Hive ODBC\u9a71\u52a8 \u00b6 \u4ece https://support.huawei.com/enterprise/zh/cloud-computing/fusioninsight-tool-pid-21624171/software/250981134?idAbsPath=fixnode01%7C7919749%7C7941815%7C19942925%7C21624171 \u4e0b\u8f7d FusionInsight_Hive_ODBC_Driver_6.5.1.3.zip \u3002 \u53c2\u8003\u4ea7\u54c1\u624b\u518c \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->Hive\u5f00\u53d1\u6307\u5357->\u73af\u5883\u51c6\u5907->\u914d\u7f6eODBC\u6837\u4f8b\u5de5\u7a0b->Linux\u73af\u5883 \u5b89\u88c5Hive ODBC\u9a71\u52a8\u3002\u6ce8\u610f\u4ee5\u4e0b\u51e0\u70b9\uff1a FusionInsight_Hive_ODBC_Driver_6.5.1.3.zip\u7248\u672c\u7684Hive ODBC\u4e0d\u652f\u6301TaiShan\u5e73\u53f0\uff0c\u4ee5\u53caX86\u5e73\u53f0\u4e0b\u768464\u4f4dWindows\u64cd\u4f5c\u7cfb\u7edf\u3001\u6b27\u62c9\u64cd\u4f5c\u7cfb\u7edf\u3001SuSE 12.x\u3001RedHat 7.x\u3002 Hive ODBC\u5b89\u88c5\u5b8c\u4e4b\u540e\uff0c\u9700\u8981\u6267\u884c ldd /usr/lib64/libodbchive.so \u786e\u8ba4\u6ca1\u6709\u5305\u4e22\u5931\u3002 \u8bf4\u660e\uff1a\u5982\u679cldd\u547d\u4ee4\u8fd4\u56de\u6709\u5305\u4e22\u5931\uff0c\u5219\u9700\u8981\u521b\u5efa\u5bf9\u5e94\u7684\u7b26\u53f7\u94fe\u63a5\u3002\u4f8b\u5982\uff1a ln -s /opt/hadoopclient/JDK/jdk-8u201/jre/lib/amd64/server/libjvm.so /usr/lib64/libjvm.so ln -s /usr/lib64/libssl.so.1.0.2k /usr/lib64/libssl.so.1.0.0 ln -s /usr/lib64/libcrypto.so.1.0.2k /usr/lib64/libcrypto.so.1.0.0 /etc/odbc.ini \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a [hivefi] DRIVER=/usr/lib64/libodbchive.so MODE=1 HOST=172.16.4.181:24002,172.16.4.182:24002,172.16.4.183:24002 PORT=24002 DATABASE=default PRINCIPAL=hive/hadoop.hadoop.com@HADOOP.COM FRAMED=0 NAMESPACE=hiveserver2 KRB5PATH=/opt/krb5.conf JAASPATH=/opt/jaas.conf \u4ee5\u4e0b\u4e09\u4e2a\u73af\u5883\u53d8\u91cf\u5728 /etc/profile \u4e2d\u8bbe\u7f6e\u4ee5\u4fbf\u6240\u6709\u7528\u6237\u90fd\u80fd\u5171\u4eab\u3002 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib:/usr/lib64 export ODBCINI=/etc/odbc.ini export ODBCCLASSPATH=/usr/local/hiveodbc/jars/zookeeper-3.5.1.jar:/usr/local/hiveodbc/jars/slf4j-api-1.7.10.jar:/usr/local/hiveodbc/jars/log4j-1.2.17.jar:/usr/local/hiveodbc/jars/slf4j-log4j12-1.7.5.jar:/usr/local/hiveodbc/jars/zk-helper.jar:/usr/local/hiveodbc/jars/commons-logging-1.2.jar \u4f7f\u7528Informatica\u63d0\u4f9b\u7684\u8c03\u8bd5\u5de5\u5177ssgodbc\u9a8c\u8bc1\u8fde\u63a5\u3002 su - infa kinit developuser export LD_PRELOAD=/usr/lib64/libodbchive.so /home/infa/Informatica/10.2.0/tools/debugtools/ssgodbc/linux64/ssgodbc.linux64 -d hivefi -u developuesr -p 'Huawei@123' -v kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 PowerCenter Repository Manager\u914d\u7f6e \u00b6 \u4ee5\u7ba1\u7406\u5458\u8eab\u4efd\u8fd0\u884cPowerCenter Repository Manager\u5e76\u8fde\u63a5\u4e0a\u5b58\u50a8\u5e93\u3002\u9009\u62e9\u83dc\u5355\u680f \u6587\u4ef6\u5939->\u521b\u5efa \u65b0\u5efa\u6587\u4ef6\u5939 Hive_to_Local \u3002 PowerCenter Designer\u521b\u5efamapping \u00b6 \u767b\u5f55FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u521b\u5efaHive\u8868class\u5e76\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS class(id INT,name STRING); INSERT INTO class VALUES (1,'Class1'); INSERT INTO class VALUES (2,'Class2'); INSERT INTO class VALUES (3,'Class3'); \u4ee5\u7ba1\u7406\u5458\u8eab\u4efd\u8fd0\u884cPowerCenter Designer\uff0c\u53f3\u952e\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939**Hive_to_Local**\uff0c\u70b9\u51fbopen\uff0c\u6253\u5f00\u914d\u7f6e\u754c\u9762\u3002\u70b9\u51fb\u83dc\u5355\u680f \u6e90->\u4ece\u6570\u636e\u5e93\u5bfc\u5165 \uff0c\u5728ODBC\u6570\u636e\u6e90\u4e2d\u9009\u62e9 Sample Cloudera Hive DSN \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684\u6309\u94ae\u3002 \u9009\u62e9 \u7cfb\u7edfDSN->Sample Cloudera Hive DSN \uff0c\u70b9\u51fb \u914d\u7f6e \u6309\u94ae\u3002 \u914d\u7f6eHive\u8fde\u63a5\u4fe1\u606f\u5982\u4e0b\uff0c\u70b9\u51fb Test \u6309\u94ae\uff0c\u8fd4\u56de\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fbOK\u6216\u8005\u786e\u5b9a\u6309\u94ae\u8fd4\u56de \u5bfc\u5165\u8868 \u754c\u9762\uff0c\u586b\u5165\u7528\u6237\u540d\uff1adevelopuser\uff0c\u6240\u6709\u8005\u9ed8\u8ba4\u4e0e\u7528\u6237\u540d\u4e00\u81f4\uff0c\u8f93\u5165developuser\u5bf9\u5e94\u7684\u5bc6\u7801\uff0c\u70b9\u51fb \u8fde\u63a5 \u6309\u94ae\u3002\u8fde\u63a5\u6210\u529f\u540e\uff0c\u70b9\u51fb\u201c\u663e\u793a\u6240\u6709\u8005\u201d\u4e0b\u9762\u7684 \u5168\u90e8 \u6309\u94ae\uff0c\u5219\u4f1a\u8fd4\u56dedefault\u6570\u636e\u5e93\u3002\u9009\u62e9default\u6570\u636e\u5e93\u8868 class \uff0c\u70b9\u51fb \u786e\u5b9a \u6309\u94ae\u3002 \u5728 Hive_to_Local->\u6e90->Sample Cloudera Hive DSN \u4e0b\u53ef\u770b\u5230\u65b0\u589e\u7684\u6e90\u8868class\u3002 \u9009\u62e9target designer\uff0c\u62d6\u5165source\u4e2d\u7684\u8868class\uff0c\u5219\u5728 Hive_to_Local->\u76ee\u6807 \u4e0b\u770b\u5230class\u3002 \u53cc\u51fb\u76ee\u6807class\uff0c\u8bbe\u7f6e\u6570\u636e\u7c7b\u578b\u4e3aFlat File\u3002 \u70b9\u51fb mapping designer \u56fe\u6807\uff0c\u70b9\u51fb\u83dc\u5355\u680f \u6620\u5c04->\u521b\u5efa \uff0c\u521b\u5efa\u65b0\u7684\u6620\u5c04\u201cmapping_hive_to_local\u201d\u3002 \u5c06\u6e90class\u548c\u76ee\u6807class\u62d6\u5165\u6620\u5c04\u201cmapping_hive_to_local\u201d\u4e2d\u5e76\u8fde\u7ebf\u3002 \u53cc\u51fb SQ_class \uff0c\u5728\u201c\u5c5e\u6027->Sql Query\u201d\u4e2d\uff0c\u201cODBC\u6570\u636e\u6e90\u201d\u9009\u62e9 Sample Cloudera Hive DSN \uff0c\u70b9\u51fb \u751f\u6210SQL \u6309\u94ae\uff0c\u7136\u540e\u786e\u5b9a\u3002\u201cCrtl+s\u201d\u4fdd\u5b58mapping\u3002 PowerCenter Workflow Manager\u8fd0\u884cworkflow \u00b6 \u4ee5\u7ba1\u7406\u5458\u8eab\u4efd\u8fd0\u884cPowerCenter Wrokflow\uff0c\u5728\u83dc\u5355\u680f\u9009\u62e9\u201c\u4efb\u52a1->\u521b\u5efa\u201d\uff0c\u547d\u540d\u4e3a\u201csession_hive_to_local\u201d\uff0c\u5e76\u9009\u62e9\u6620\u5c04\u201cmapping_hive_to_local\u201d\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u9009\u62e9\u83dc\u5355\u680f\u201c\u8fde\u63a5->\u5173\u7cfb\u201d\uff0c\u70b9\u51fb \u65b0\u5efa \uff0c\u9009\u62e9 ODBC \u540e\u70b9\u51fb \u786e\u5b9a \u3002 \u8fde\u63a5\u5bf9\u8c61\u5b9a\u4e49\u5982\u4e0b\uff1a \u540d\u79f0\uff1ahivefi\uff0c\u81ea\u5b9a\u4e49 \u7528\u6237\u540d\uff1adevelopuser \u5bc6\u7801\uff1a\u8f93\u5165developuser\u7684\u5bc6\u7801 \u8fde\u63a5\u5b57\u7b26\u4e32\uff1ahivefi\uff0c\u9700\u8981\u548c/etc/odbc.ini\u4e2d\u8bbe\u7f6e\u7684\u540d\u79f0\u4fdd\u6301\u4e00\u81f4 \u4ee3\u7801\u9875\uff1aUTF-8 encoding of Unicode \u5728\u201cTask Developer\u201d\u4e2d\u53cc\u51fb\u4f1a\u8bdd\u201csession_hive_to_local\u201d\uff0c\u70b9\u51fb \u6620\u5c04 \uff0c\u8bbe\u7f6e\u6e90SQ_class\u7684\u8fde\u63a5\u503c\u4e3a hivefi \uff0c\u76ee\u6807class1\u7684\u5c5e\u6027\u201cOutput file directory\u201d\u4e3a /home/infa \uff0c\u201cOutput filename\u201d\u4e3a class.out \u3002 \u70b9\u51fb\u201cWorkflow Designer\u201d\uff0c\u9009\u62e9\u83dc\u5355\u680f\u201c\u5de5\u4f5c\u6d41\u201d\uff0c\u70b9\u51fb \u65b0\u5efa \uff0c\u65b0\u5efa\u4e00\u4e2a\u5de5\u4f5c\u6d41 wkf_hive_to_local \uff0c\u62d6\u5165\u4f1a\u8bdd\u201csession_hive_to_local\u201d\uff0c\u5e76\u4e0e\u201c\u542f\u52a8\u201d\u8fde\u7ebf\u3002 \u201cCtrl+s\u201d\u4fdd\u5b58\u201cwkf_hive_to_local\u201d\uff0c\u53f3\u952e\u201cwkf_hive_to_local\u201d\u9009\u62e9 \u542f\u52a8\u5de5\u4f5c\u6d41 \u3002 \u5728PowerCenter Workflow Monitor\u4e2d\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002 \u767b\u5f55Informatica\u5b89\u88c5\u8282\u70b9\u67e5\u770b\u4eceHive\u8868\u83b7\u53d6\u7684\u6570\u636e\u5b58\u50a8\u4e8e /home/infa/class.out \u3002","title":"10.2.0 <--> 6.5"},{"location":"Data_Integration/Informatica_PowerCenter/#informatica-powercenterfusioninsight-hd","text":"","title":"Informatica PowerCenter\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Integration/Informatica_PowerCenter/#_1","text":"Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive) Informatica PowerCenter 10.2.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Informatica_PowerCenter/#_2","text":"Informatica Server 10.2.0 Linux Informatica PowerCenter Client 10.2.0 Oracle database 11g FusionInsight HD \u5ba2\u6237\u7aef","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Informatica_PowerCenter/#_3","text":"\u4e00\u53f0Linux\u670d\u52a1\u5668\uff0c\u90e8\u7f72Informatica Server\uff0c\u5e76\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e00\u53f0Windows\u673a\u5668\uff0c\u5b89\u88c5Informatica PowerCenter Client","title":"\u90e8\u7f72\u65b9\u6848"},{"location":"Data_Integration/Informatica_PowerCenter/#_4","text":"","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Informatica_PowerCenter/#fusioninsight-hd","text":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88HDFS,Hive\u6240\u6709\u6743\u9650\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.confh\u548cuser.keytab\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/ \u76ee\u5f55\u4e0b","title":"\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef"},{"location":"Data_Integration/Informatica_PowerCenter/#linuxoracle-database-informatica-server","text":"\u521b\u5efaoracle \u7528\u6237\uff0c\u5b89\u88c5oracle \u6570\u636e\u5e93 \u521b\u5efainfa\u7528\u6237\uff0c\u4f7f\u7528 sqlplus / as sysdba \u767b\u5f55\u81f3oracle\u6570\u636e\u5e93\u4e2d\uff0c\u6267\u884c\u4ee5\u4e0bsql\u8bed\u53e5 create tablespace rep_data datafile '/u01/app/oracle/oradata/orcl/rep_data_01.dbf' size 512m ; create user pwc_user identified by pwc_user default tablespace rep_data temporary tablespace temp; create user mdl_user identified by mdl_user default tablespace rep_data temporary tablespace temp; create user domain_user identified by domain_user default tablespace rep_data temporary tablespace temp; grant dba to domain_user,pwc_user,mdl_user; \u83b7\u53d6Informatica Server\u5b89\u88c5\u5305\u5e76\u4e0a\u4f20\u81f3\u8282\u70b9,\u89e3\u538b\u5b89\u88c5\u5305\u4e4b\u540e\uff0c\u6267\u884c ./install.sh \uff0c\u6839\u636e\u63d0\u793a\u8fdb\u884c\u5b89\u88c5,\u8fd9\u91cc\u5b89\u88c5\u76ee\u5f55\u4e3a /home/infa/Informatica/10.2.0 \u3002 \u5b89\u88c5\u5b8c\u6210\u540e\uff0cInformatica Server\u4f1a\u81ea\u884c\u542f\u52a8\uff0c\u5728\u6d4f\u89c8\u5668\u8f93\u5165ip:6008\u7aef\u53e3\uff0c\u6253\u5f00Administrator \u7ba1\u7406\u754c\u9762\uff0c\u8f93\u5165\u5b89\u88c5\u65f6\u8bbe\u7f6e\u7684\u7528\u6237\u540d\u5bc6\u7801\u8fdb\u884c\u767b\u5f55\u3002\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8\u6216\u505c\u6b62Informatica Server\u3002 su - infa /home/infa/Informatica/10.2.0/tomcat/bin/infaservice.sh startup /home/infa/Informatica/10.2.0/tomcat/bin/infaservice.sh shutdown \u8bf4\u660e\uff1a\u5982\u679c\u4f7f\u7528\u975einfa\u7528\u6237\u542f\u52a8\u6216\u8005\u505c\u6b62Informatica Server\u4e4b\u540e\uff0c\u4f1a\u5bfc\u81f4 /home/infa/Informatica/10.2.0 \u76ee\u5f55\u4e0b\u4e00\u4e9b\u6587\u4ef6\u7684\u6240\u6709\u8005\u53d1\u751f\u53d8\u5316\uff0c\u5bfc\u81f4\u4e0b\u4e00\u6b21\u542f\u52a8\u6216\u8005\u505c\u6b62\u5931\u8d25\u3002\u53ef\u6267\u884c chown -R infa:oinstall /home/infa/Informatica/10.2.0 \u4fee\u590d\u3002","title":"\u5728Linux\u4e0a\u5b89\u88c5Oracle database \u4ee5\u53ca Informatica Server"},{"location":"Data_Integration/Informatica_PowerCenter/#informatica-server","text":"\u521b\u5efaPowerCenter \u5b58\u50a8\u5e93 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter \u5b58\u50a8\u5e93 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u6570\u636e\u5e93\u4fe1\u606f\uff0c\u5b8c\u6210 - \u70b9\u51fb\u53f3\u4e0a\u89d2\u6309\u94ae\u542f\u7528\u5b58\u50a8\u5e93\uff0c\u5e76\u4e3a\u5b58\u50a8\u5e93\u521b\u5efa\u5185\u5bb9 \u5728\u5b58\u50a8\u5e93\u5c5e\u6027\u4e2d\uff0c\u4fee\u6539\u64cd\u4f5c\u7c7b\u578b\u4e3a\u666e\u901a\uff0c\u5e76\u91cd\u542f\u670d\u52a1 \u521b\u5efaPowerCenter \u6570\u636e\u96c6\u6210\u670d\u52a1 \u5728\u7ba1\u7406\u754c\u9762\uff0cdomain\u4e0b\u53f3\u952e\u65b0\u5efa\u4e00\u4e2aPowerCenter\u96c6\u6210\u670d\u52a1 - \u6307\u5b9a\u540d\u79f0\u7b49\u4fe1\u606f\uff0c\u4e0b\u4e00\u6b65 - \u6307\u5b9a\u5b58\u50a8\u5e93\u4fe1\u606f\uff0c\u70b9\u51fb\u5b8c\u6210\uff0c\u5e76\u542f\u7528\u670d\u52a1 \u5728infa server\u521b\u5efadevelopuser \u5728\u5b89\u5168\u9875\u7b7e\u4e0b\uff0c\u521b\u5efa\u4e00\u4e2a\u7528\u6237\uff0c\u540d\u4e3adevelopuser\uff0c\u4e0eHadoop\u96c6\u7fa4\u7528\u6237\u4fdd\u6301\u4e00\u81f4 \u4fee\u6539\u7528\u6237\u7684\u4f18\u5148\u7ea7\u4ee5\u53ca\u7528\u6237\u7ec4 \u5728infa Server \u8fdb\u884cHadoop\u914d\u7f6e \u5c06 /opt \u76ee\u5f55\u4e0b\u7684krb5.conf\u6587\u4ef6\u590d\u5236\u81f3 /etc \u76ee\u5f55\u4e0b\u4ee5\u53cainformatica\u5b89\u88c5\u76ee\u5f55 ${INFA_HOME}java/jre/lib/security/ \u4e0b\uff0c\u5e76\u8d4b\u4e88infa\u7528\u6237\u6539\u6587\u4ef6\u7684\u8bfb\u53d6\u6743\u9650 \u4ee5infa\u7528\u6237\u767b\u5f55\u8282\u70b9\uff0c\u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\uff0c\u4f8b\u5982 /opt/pwx-hadoop/conf \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u4ee5\u4e0b\u914d\u7f6e\u6587\u4ef6\uff0c\u653e\u81f3 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650\u81f3775 cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/pwx-hadoop/conf cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/pwx-hadoop/conf cp /opt/hadoopclient/Hive/config/hive-site.xml /opt/pwx-hadoop/conf cp /opt/hadoopclient/Yarn/config/mapred-site.xml /opt/pwx-hadoop/conf chmod -R 775 /opt/pwx-hadoop/conf chown infa:oinstall /opt/pwx-hadoop/conf/* \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884cKerberos\u8ba4\u8bc1\uff0c\u5e76\u6307\u5b9acache\u6587\u4ef6\uff0cinfa\u7528\u6237\u9700\u8981\u5bf9\u6307\u5b9a\u7684\u8def\u5f84\u6709\u8bfb\u5199\u6743\u9650 source /opt/hadoopclient/bigdata_env kinit -c /home/infa/krb5cc_developuser developuser chown infa:oinstall /home/infa/krb5cc_developuser \u4fee\u6539 /opt/pwx-hadoop/conf \u76ee\u5f55\u4e2d\u7684 core-site.xml \u6587\u4ef6\uff0c\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e <property> <name>hadoop.security.kerberos.ticket.cache.path</name> <value>home/infa/krb5cc_developuser</value> <description>Path to the Kerberos ticket cache. </description> </property> \u5728Administrator \u7ba1\u7406\u754c\u9762\uff0c\u4e3a\u96c6\u6210\u670d\u52a1\u521b\u5efa\u73af\u5883\u53d8\u91cf CLASSPATH=/opt/pwx-hadoop/conf \uff0c\u5e76\u91cd\u542f\u96c6\u6210\u670d\u52a1 \u8bf4\u660e\uff1a\u5982\u679c\u4f7f\u7528FusionInsight HD\u63d0\u4f9b\u7684Hive ODBC\u9a71\u52a8\u8fde\u63a5Hive\uff0c\u9700\u8981\u521b\u5efa\u73af\u5883\u53d8\u91cf LD_PRELOAD=/usr/lib64/libodbchive.so \u3002 \u5220\u9664 /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/ \u76ee\u5f55\u4e0bhive\u76f8\u5173\u7684jar\u5305\uff0c\u5e76\u5c06 /opt/hadoopclient/Hive/Beeline/lib \u4e0bhive\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u81f3\u8be5\u76ee\u5f55\uff0c\u5e76\u4fee\u6539\u6587\u4ef6\u6743\u9650 rm -f /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive* cp /opt/hadoopclient/Hive/Beeline/lib/hive* /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib chown infa:oinstall /home/infa/Informatica/10.2.0/services/shared/hadoop/hortonworks_2.5/lib/hive*","title":"Informatica Server\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-client-from-oracle-to-hdfshive","text":"","title":"PowerCenter Client ~ From Oracle to HDFS/Hive"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-repository-manager","text":"\u83b7\u53d6PowerCenter Client\u5b89\u88c5\u5305\uff0c\u5b89\u88c5\u65f6\u9009\u53d6PowerCenter Client,\u542f\u52a8PowerCenter Repository Manager\uff0c\u9009\u62e9\u83dc\u5355\u680f\u4ed3\u5e93->\u914d\u7f6e\u57df\uff0c\u914d\u7f6e\u5b8c\u6210\u53ef\u4ee5\u770b\u5230\u4e4b\u524d\u521b\u5efa\u7684\u5b58\u50a8\u5e93 \u53cc\u51fb\u5b58\u50a8\u5e93\uff0c\u8f93\u5165\u5bc6\u7801\uff0c\u8fde\u63a5 \u9009\u62e9\u83dc\u5355\u680f\u6587\u4ef6\u5939,\u521b\u5efa\u6587\u4ef6\u5939","title":"PowerCenter Repository Manager\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-designermapping","text":"\u6253\u5f00PowerCenter Designer\uff0c\u53f3\u952e\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939\uff0c\u70b9\u51fbopen\uff0c\u6253\u5f00\u914d\u7f6e\u754c\u9762 - \u70b9\u51fb\u83dc\u5355\u680fSources->import from databases\uff0c\u5728ODBC\u6570\u636e\u6e90\u4e2d\u521b\u5efasitDSN\uff0c\u586b\u5199\u6570\u636e\u5e93\u76f8\u5173\u4fe1\u606f\uff0c\u6d4b\u8bd5\u8fde\u63a5 - \u9009\u62e9\u521a\u624d\u521b\u5efa\u7684\u6570\u636e\u6e90\uff0c\u586b\u5165\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801\uff0c\u8fde\u63a5\uff0c\u53ef\u4ee5\u770b\u5230\u6570\u636e\u5e93\u4e2d\u7684\u8868 - \u9009\u62e9target designer\uff0c\u62d6\u5165source\u4e2d\u7684\u8868 - \u53cc\u51fb\u8868\uff0c\u8bbe\u7f6e\u6570\u636e\u7c7b\u578b\u4e3aFlat File \u5728mapping\u8bbe\u7f6e\u9875\u9762\uff0c\u521b\u5efa\u65b0\u7684mapping\uff0c\u62d6\u5165source\u548ctarget\u8868\uff0c\u5e76\u8fde\u7ebf","title":"PowerCenter Designer\u521b\u5efamapping"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-workflow-managerworkflow","text":"\u5728\u83dc\u5355\u680f\u9009\u62e9task,\u65b0\u5efa\u4e00\u4e2atask,\u547d\u540d\u5e76\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684map \u65b0\u5efa\u4e00\u4e2aworkflow\uff0c\u62d6\u5165\u521a\u624d\u65b0\u5efa\u7684task\uff0c\u5e76\u8fde\u7ebf \u5728\u83dc\u5355\u680fconnection\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2aapplication connection,\u9009\u62e9Hadoop HDFS Connection \u5177\u4f53\u4fe1\u606f\u586b\u5199\u5982\u4e0b HDFS Connection URI\uff1ahdfs://namenodeip:25000 Hive URL : jdbc:hive2://172.16.4.21:21066/default;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.keytab=/opt/user.keytab;user.principal=developuser Hive User Name: developuser \u53cc\u51fb\u521a\u624d\u521b\u5efa\u7684task\uff0c\u5728mapping\u9009\u9879\u5361\uff0c\u70b9\u51fbtarget\uff0c\u8bbe\u7f6e\u5199\u5165\u7c7b\u578b\u4e3a HDFS Flat Write \uff0c\u5e76\u9009\u62e9\u8fde\u63a5\u4e3a\u521a\u624d\u521b\u5efa\u7684connection\uff0c\u5e76\u5728properties\u4e2d\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e \u4fdd\u5b58\u5f53\u524dworkflow\uff0c\u53f3\u952e\uff0c\u542f\u52a8workflow \u5728PowerCenter Workflow Monitor\u4e2d\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u60c5\u51b5 \u5728HDFS\u4e2d\u53ef\u4ee5\u770b\u5230\u5bfc\u5165\u7684\u6570\u636e \u5728task\u914d\u7f6e\u4e2d\u52fe\u9009\u5199\u5165Hive\u8868\uff0c\u586b\u5165\u4e4b\u524d\u521b\u5efa\u7684\u8868\u540d\uff0c\u8fd0\u884cworkflow \u5728Hive\u4e2d\u53ef\u4ee5\u770b\u5230\u8868\u4e2d\u7684\u6570\u636e","title":"PowerCenter Workflow Manager\u8fd0\u884cworkflow"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-client-from-hive-to-local","text":"\u4f7f\u7528FusionInsight HD\u63d0\u4f9b\u7684Hive ODBC\u9a71\u52a8\u5bf9\u63a5\u5e76\u8bfb\u53d6Hive\u8868\u7684\u6570\u636e\u4e0b\u8f7d\u81f3\u672c\u5730\u3002","title":"PowerCenter Client ~ From Hive to Local"},{"location":"Data_Integration/Informatica_PowerCenter/#fusioninsight-hdhive-odbc","text":"\u4ece https://support.huawei.com/enterprise/zh/cloud-computing/fusioninsight-tool-pid-21624171/software/250981134?idAbsPath=fixnode01%7C7919749%7C7941815%7C19942925%7C21624171 \u4e0b\u8f7d FusionInsight_Hive_ODBC_Driver_6.5.1.3.zip \u3002 \u53c2\u8003\u4ea7\u54c1\u624b\u518c \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->Hive\u5f00\u53d1\u6307\u5357->\u73af\u5883\u51c6\u5907->\u914d\u7f6eODBC\u6837\u4f8b\u5de5\u7a0b->Linux\u73af\u5883 \u5b89\u88c5Hive ODBC\u9a71\u52a8\u3002\u6ce8\u610f\u4ee5\u4e0b\u51e0\u70b9\uff1a FusionInsight_Hive_ODBC_Driver_6.5.1.3.zip\u7248\u672c\u7684Hive ODBC\u4e0d\u652f\u6301TaiShan\u5e73\u53f0\uff0c\u4ee5\u53caX86\u5e73\u53f0\u4e0b\u768464\u4f4dWindows\u64cd\u4f5c\u7cfb\u7edf\u3001\u6b27\u62c9\u64cd\u4f5c\u7cfb\u7edf\u3001SuSE 12.x\u3001RedHat 7.x\u3002 Hive ODBC\u5b89\u88c5\u5b8c\u4e4b\u540e\uff0c\u9700\u8981\u6267\u884c ldd /usr/lib64/libodbchive.so \u786e\u8ba4\u6ca1\u6709\u5305\u4e22\u5931\u3002 \u8bf4\u660e\uff1a\u5982\u679cldd\u547d\u4ee4\u8fd4\u56de\u6709\u5305\u4e22\u5931\uff0c\u5219\u9700\u8981\u521b\u5efa\u5bf9\u5e94\u7684\u7b26\u53f7\u94fe\u63a5\u3002\u4f8b\u5982\uff1a ln -s /opt/hadoopclient/JDK/jdk-8u201/jre/lib/amd64/server/libjvm.so /usr/lib64/libjvm.so ln -s /usr/lib64/libssl.so.1.0.2k /usr/lib64/libssl.so.1.0.0 ln -s /usr/lib64/libcrypto.so.1.0.2k /usr/lib64/libcrypto.so.1.0.0 /etc/odbc.ini \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a [hivefi] DRIVER=/usr/lib64/libodbchive.so MODE=1 HOST=172.16.4.181:24002,172.16.4.182:24002,172.16.4.183:24002 PORT=24002 DATABASE=default PRINCIPAL=hive/hadoop.hadoop.com@HADOOP.COM FRAMED=0 NAMESPACE=hiveserver2 KRB5PATH=/opt/krb5.conf JAASPATH=/opt/jaas.conf \u4ee5\u4e0b\u4e09\u4e2a\u73af\u5883\u53d8\u91cf\u5728 /etc/profile \u4e2d\u8bbe\u7f6e\u4ee5\u4fbf\u6240\u6709\u7528\u6237\u90fd\u80fd\u5171\u4eab\u3002 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib:/usr/lib64 export ODBCINI=/etc/odbc.ini export ODBCCLASSPATH=/usr/local/hiveodbc/jars/zookeeper-3.5.1.jar:/usr/local/hiveodbc/jars/slf4j-api-1.7.10.jar:/usr/local/hiveodbc/jars/log4j-1.2.17.jar:/usr/local/hiveodbc/jars/slf4j-log4j12-1.7.5.jar:/usr/local/hiveodbc/jars/zk-helper.jar:/usr/local/hiveodbc/jars/commons-logging-1.2.jar \u4f7f\u7528Informatica\u63d0\u4f9b\u7684\u8c03\u8bd5\u5de5\u5177ssgodbc\u9a8c\u8bc1\u8fde\u63a5\u3002 su - infa kinit developuser export LD_PRELOAD=/usr/lib64/libodbchive.so /home/infa/Informatica/10.2.0/tools/debugtools/ssgodbc/linux64/ssgodbc.linux64 -d hivefi -u developuesr -p 'Huawei@123' -v","title":"\u5b89\u88c5FusionInsight HD\u7684Hive ODBC\u9a71\u52a8"},{"location":"Data_Integration/Informatica_PowerCenter/#kinit","text":"\u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002","title":"kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-repository-manager_1","text":"\u4ee5\u7ba1\u7406\u5458\u8eab\u4efd\u8fd0\u884cPowerCenter Repository Manager\u5e76\u8fde\u63a5\u4e0a\u5b58\u50a8\u5e93\u3002\u9009\u62e9\u83dc\u5355\u680f \u6587\u4ef6\u5939->\u521b\u5efa \u65b0\u5efa\u6587\u4ef6\u5939 Hive_to_Local \u3002","title":"PowerCenter Repository Manager\u914d\u7f6e"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-designermapping_1","text":"\u767b\u5f55FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u521b\u5efaHive\u8868class\u5e76\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS class(id INT,name STRING); INSERT INTO class VALUES (1,'Class1'); INSERT INTO class VALUES (2,'Class2'); INSERT INTO class VALUES (3,'Class3'); \u4ee5\u7ba1\u7406\u5458\u8eab\u4efd\u8fd0\u884cPowerCenter Designer\uff0c\u53f3\u952e\u521a\u624d\u521b\u5efa\u7684\u6587\u4ef6\u5939**Hive_to_Local**\uff0c\u70b9\u51fbopen\uff0c\u6253\u5f00\u914d\u7f6e\u754c\u9762\u3002\u70b9\u51fb\u83dc\u5355\u680f \u6e90->\u4ece\u6570\u636e\u5e93\u5bfc\u5165 \uff0c\u5728ODBC\u6570\u636e\u6e90\u4e2d\u9009\u62e9 Sample Cloudera Hive DSN \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684\u6309\u94ae\u3002 \u9009\u62e9 \u7cfb\u7edfDSN->Sample Cloudera Hive DSN \uff0c\u70b9\u51fb \u914d\u7f6e \u6309\u94ae\u3002 \u914d\u7f6eHive\u8fde\u63a5\u4fe1\u606f\u5982\u4e0b\uff0c\u70b9\u51fb Test \u6309\u94ae\uff0c\u8fd4\u56de\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fbOK\u6216\u8005\u786e\u5b9a\u6309\u94ae\u8fd4\u56de \u5bfc\u5165\u8868 \u754c\u9762\uff0c\u586b\u5165\u7528\u6237\u540d\uff1adevelopuser\uff0c\u6240\u6709\u8005\u9ed8\u8ba4\u4e0e\u7528\u6237\u540d\u4e00\u81f4\uff0c\u8f93\u5165developuser\u5bf9\u5e94\u7684\u5bc6\u7801\uff0c\u70b9\u51fb \u8fde\u63a5 \u6309\u94ae\u3002\u8fde\u63a5\u6210\u529f\u540e\uff0c\u70b9\u51fb\u201c\u663e\u793a\u6240\u6709\u8005\u201d\u4e0b\u9762\u7684 \u5168\u90e8 \u6309\u94ae\uff0c\u5219\u4f1a\u8fd4\u56dedefault\u6570\u636e\u5e93\u3002\u9009\u62e9default\u6570\u636e\u5e93\u8868 class \uff0c\u70b9\u51fb \u786e\u5b9a \u6309\u94ae\u3002 \u5728 Hive_to_Local->\u6e90->Sample Cloudera Hive DSN \u4e0b\u53ef\u770b\u5230\u65b0\u589e\u7684\u6e90\u8868class\u3002 \u9009\u62e9target designer\uff0c\u62d6\u5165source\u4e2d\u7684\u8868class\uff0c\u5219\u5728 Hive_to_Local->\u76ee\u6807 \u4e0b\u770b\u5230class\u3002 \u53cc\u51fb\u76ee\u6807class\uff0c\u8bbe\u7f6e\u6570\u636e\u7c7b\u578b\u4e3aFlat File\u3002 \u70b9\u51fb mapping designer \u56fe\u6807\uff0c\u70b9\u51fb\u83dc\u5355\u680f \u6620\u5c04->\u521b\u5efa \uff0c\u521b\u5efa\u65b0\u7684\u6620\u5c04\u201cmapping_hive_to_local\u201d\u3002 \u5c06\u6e90class\u548c\u76ee\u6807class\u62d6\u5165\u6620\u5c04\u201cmapping_hive_to_local\u201d\u4e2d\u5e76\u8fde\u7ebf\u3002 \u53cc\u51fb SQ_class \uff0c\u5728\u201c\u5c5e\u6027->Sql Query\u201d\u4e2d\uff0c\u201cODBC\u6570\u636e\u6e90\u201d\u9009\u62e9 Sample Cloudera Hive DSN \uff0c\u70b9\u51fb \u751f\u6210SQL \u6309\u94ae\uff0c\u7136\u540e\u786e\u5b9a\u3002\u201cCrtl+s\u201d\u4fdd\u5b58mapping\u3002","title":"PowerCenter Designer\u521b\u5efamapping"},{"location":"Data_Integration/Informatica_PowerCenter/#powercenter-workflow-managerworkflow_1","text":"\u4ee5\u7ba1\u7406\u5458\u8eab\u4efd\u8fd0\u884cPowerCenter Wrokflow\uff0c\u5728\u83dc\u5355\u680f\u9009\u62e9\u201c\u4efb\u52a1->\u521b\u5efa\u201d\uff0c\u547d\u540d\u4e3a\u201csession_hive_to_local\u201d\uff0c\u5e76\u9009\u62e9\u6620\u5c04\u201cmapping_hive_to_local\u201d\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u9009\u62e9\u83dc\u5355\u680f\u201c\u8fde\u63a5->\u5173\u7cfb\u201d\uff0c\u70b9\u51fb \u65b0\u5efa \uff0c\u9009\u62e9 ODBC \u540e\u70b9\u51fb \u786e\u5b9a \u3002 \u8fde\u63a5\u5bf9\u8c61\u5b9a\u4e49\u5982\u4e0b\uff1a \u540d\u79f0\uff1ahivefi\uff0c\u81ea\u5b9a\u4e49 \u7528\u6237\u540d\uff1adevelopuser \u5bc6\u7801\uff1a\u8f93\u5165developuser\u7684\u5bc6\u7801 \u8fde\u63a5\u5b57\u7b26\u4e32\uff1ahivefi\uff0c\u9700\u8981\u548c/etc/odbc.ini\u4e2d\u8bbe\u7f6e\u7684\u540d\u79f0\u4fdd\u6301\u4e00\u81f4 \u4ee3\u7801\u9875\uff1aUTF-8 encoding of Unicode \u5728\u201cTask Developer\u201d\u4e2d\u53cc\u51fb\u4f1a\u8bdd\u201csession_hive_to_local\u201d\uff0c\u70b9\u51fb \u6620\u5c04 \uff0c\u8bbe\u7f6e\u6e90SQ_class\u7684\u8fde\u63a5\u503c\u4e3a hivefi \uff0c\u76ee\u6807class1\u7684\u5c5e\u6027\u201cOutput file directory\u201d\u4e3a /home/infa \uff0c\u201cOutput filename\u201d\u4e3a class.out \u3002 \u70b9\u51fb\u201cWorkflow Designer\u201d\uff0c\u9009\u62e9\u83dc\u5355\u680f\u201c\u5de5\u4f5c\u6d41\u201d\uff0c\u70b9\u51fb \u65b0\u5efa \uff0c\u65b0\u5efa\u4e00\u4e2a\u5de5\u4f5c\u6d41 wkf_hive_to_local \uff0c\u62d6\u5165\u4f1a\u8bdd\u201csession_hive_to_local\u201d\uff0c\u5e76\u4e0e\u201c\u542f\u52a8\u201d\u8fde\u7ebf\u3002 \u201cCtrl+s\u201d\u4fdd\u5b58\u201cwkf_hive_to_local\u201d\uff0c\u53f3\u952e\u201cwkf_hive_to_local\u201d\u9009\u62e9 \u542f\u52a8\u5de5\u4f5c\u6d41 \u3002 \u5728PowerCenter Workflow Monitor\u4e2d\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002 \u767b\u5f55Informatica\u5b89\u88c5\u8282\u70b9\u67e5\u770b\u4eceHive\u8868\u83b7\u53d6\u7684\u6570\u636e\u5b58\u50a8\u4e8e /home/infa/class.out \u3002","title":"PowerCenter Workflow Manager\u8fd0\u884cworkflow"},{"location":"Data_Integration/Kafka_Manager/","text":"Kafka Manager\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kafka Manager 1.3.3.21 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) Kafka Manager 1.3.3.21 \u2194 FusionInsight HD 6.5 (Kafka) \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5JDK1.8\u53ca\u4ee5\u4e0a\u7248\u672c \u4e0b\u8f7dkafka-manager\u6e90\u7801 \u4e0b\u8f7d\u5730\u5740\u4e3a https://github.com/yahoo/kafka-manager \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55kafka-manager yum\u5b89\u88c5sbt yum install sbt \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u83b7\u53d6kafka\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6,\u767b\u5f55FusionInsight\u96c6\u7fa4\u8282\u70b9,\u5c06/opt/huawei/Bigdata/om-server_V100R002C80SPC200/apache-tomcat-8.5.28/conf/security/kafka.keytab\u6587\u4ef6\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u5e76\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef\u8282\u70b9/opt/hadoopclient/\u76ee\u5f55\u4e0b \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237kafkauser\uff0c\u5e76\u9009\u62e9kafka\u548ckafkaadmin\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.conf\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/hadoopclient/ \u76ee\u5f55\u4e0b kafka-manager\u7f16\u8bd1\u53ca\u914d\u7f6e \u00b6 \u4fee\u6539\u6e90\u7801 \u8fdb\u5165\u5b89\u88c5\u76ee\u5f55/kafka-manager/app/kafka/manager/jmx\uff0c\u4fee\u6539KafkaJMX.scala\u6587\u4ef6\u4e2d\u5199\u6b7b\u7684jmx\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u5c06'jmxrmi'\u4fee\u6539\u4e3a'kafka',\u5982\u4e0b\u56fe\uff1a \u7f16\u8bd1kafka-manager,\u83b7\u53d6\u538b\u7f29\u5305 cd /opt/kafka-manager ./sbt clean dist cp /opt/kafka-manager/target/universal/kafka-manager-1.3.3.21.zip /opt cd /opt uzip kafka-manager-1.3.3.21.zip cd /opt/kafka-manager-1.3.3.21 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6conf/application.conf,'kafka-manager.zkhosts'\u4fee\u6539\u4e3azookeeper\u96c6\u7fa4\u8282\u70b9IP:\u7aef\u53e3,FI\u96c6\u7fa4\u9ed8\u8ba4\u7aef\u53e3\u4e3a24002 kafka-manager.zkhosts=\"172.21.3.115:24002\" \u65b0\u5efaconf/jaas.conf\u6587\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; \u5c06kafka-manager\u7684lib\u5e93\u4e2dzookeeper\u7684jar\u5305\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e2dzookeeper\u7684jar\u5305,\u5e76\u91cd\u547d\u540d cp /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar /opt/kafka-manager-1.3.3.21/lib cd /opt/kafka-manager-1.3.3.21/lib rm org.apache.zookeeper.zookeeper-3.4.10.jar mv zookeeper-3.5.1.jar org.apache.zookeeper.zookeeper-3.4.10.jar kafka-manager\u4f7f\u7528 \u00b6 \u542f\u52a8kafka-manager cd /opt/kafka-manager-1.3.3.21 nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Djava.security.auth.login.config=conf/jaas.conf -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com & > \u53ef\u901a\u8fc7-Dhttp.port=port\u6307\u5b9a\u8bbf\u95ee\u7aef\u53e3,\u9ed8\u8ba4\u4e3a9000 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165172.21.3.48:9000\uff0c\u5373\u53ef\u8bbf\u95eekafka-manager \u70b9\u51fbcluster->Add Cluster\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199 Cluster Name:\u81ea\u5b9a\u4e49 Cluster Zookeeper Hosts:ZooKeeper\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f,\u53ef\u5199\u591a\u4e2a\u6216\u8005\u4e00\u4e2a\u8282\u70b9,\u4e00\u5b9a\u8981\u52a0\u4e0akafka\u540e\u7f00 Kafka Version:\u5f53\u524dFI\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684\u662f0.11.0.1\uff0c\u9009\u62e9\u6700\u63a5\u8fd1\u76840.11.0.2\u5373\u53ef \u52fe\u9009Enable JMX\u590d\u9009\u6846 \u5c06\u4ee5\u4e0b\u51e0\u4e2asize\u5927\u5c0f\u8bbe\u7f6e\u4e3a\u5927\u4e8e\u7b49\u4e8e2 \u5176\u4ed6\u8bbe\u7f6e\u53ef\u4ee5\u4fdd\u6301\u9ed8\u8ba4\u6216\u8005\u6839\u636e\u9700\u8981\u4fee\u6539,\u70b9\u51fbSave\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u521b\u5efa\u6210\u529f \u70b9\u51fbGo to cluster view\uff0c\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u76f8\u5173\u4fe1\u606f \u5728Brokers\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u7684brokers\u60c5\u51b5 \u5728Topic->Create\u83dc\u5355\u680f\u53ef\u4ee5\u521b\u5efa\u65b0\u7684topic \u5728Topic->List\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u6240\u6709\u7684topic","title":"1.3.3.21 <--> 6.5"},{"location":"Data_Integration/Kafka_Manager/#kafka-managerfusioninsight","text":"","title":"Kafka Manager\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Kafka_Manager/#_1","text":"Kafka Manager 1.3.3.21 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) Kafka Manager 1.3.3.21 \u2194 FusionInsight HD 6.5 (Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Kafka_Manager/#_2","text":"\u5b89\u88c5JDK1.8\u53ca\u4ee5\u4e0a\u7248\u672c \u4e0b\u8f7dkafka-manager\u6e90\u7801 \u4e0b\u8f7d\u5730\u5740\u4e3a https://github.com/yahoo/kafka-manager \u89e3\u538b\u540e\u5f97\u5230\u5b89\u88c5\u76ee\u5f55kafka-manager yum\u5b89\u88c5sbt yum install sbt \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u76ee\u5f55\u4e3a/opt/hadoopclient \u83b7\u53d6kafka\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6,\u767b\u5f55FusionInsight\u96c6\u7fa4\u8282\u70b9,\u5c06/opt/huawei/Bigdata/om-server_V100R002C80SPC200/apache-tomcat-8.5.28/conf/security/kafka.keytab\u6587\u4ef6\u4e0b\u8f7d\u5230\u672c\u5730\uff0c\u5e76\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef\u8282\u70b9/opt/hadoopclient/\u76ee\u5f55\u4e0b \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237kafkauser\uff0c\u5e76\u9009\u62e9kafka\u548ckafkaadmin\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6,\u5c06krb5.conf\u6587\u4ef6\u4e0a\u4f20\u5230\u5ba2\u6237\u7aef\u8282\u70b9\u7684 /opt/hadoopclient/ \u76ee\u5f55\u4e0b","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kafka_Manager/#kafka-manager","text":"\u4fee\u6539\u6e90\u7801 \u8fdb\u5165\u5b89\u88c5\u76ee\u5f55/kafka-manager/app/kafka/manager/jmx\uff0c\u4fee\u6539KafkaJMX.scala\u6587\u4ef6\u4e2d\u5199\u6b7b\u7684jmx\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u5c06'jmxrmi'\u4fee\u6539\u4e3a'kafka',\u5982\u4e0b\u56fe\uff1a \u7f16\u8bd1kafka-manager,\u83b7\u53d6\u538b\u7f29\u5305 cd /opt/kafka-manager ./sbt clean dist cp /opt/kafka-manager/target/universal/kafka-manager-1.3.3.21.zip /opt cd /opt uzip kafka-manager-1.3.3.21.zip cd /opt/kafka-manager-1.3.3.21 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6conf/application.conf,'kafka-manager.zkhosts'\u4fee\u6539\u4e3azookeeper\u96c6\u7fa4\u8282\u70b9IP:\u7aef\u53e3,FI\u96c6\u7fa4\u9ed8\u8ba4\u7aef\u53e3\u4e3a24002 kafka-manager.zkhosts=\"172.21.3.115:24002\" \u65b0\u5efaconf/jaas.conf\u6587\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/hadoopclient/kafka.keytab\" principal=\"kafka/hadoop.hadoop.com@HADOOP.COM\" storeKey=true useTicketCache=false; }; \u5c06kafka-manager\u7684lib\u5e93\u4e2dzookeeper\u7684jar\u5305\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e2dzookeeper\u7684jar\u5305,\u5e76\u91cd\u547d\u540d cp /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar /opt/kafka-manager-1.3.3.21/lib cd /opt/kafka-manager-1.3.3.21/lib rm org.apache.zookeeper.zookeeper-3.4.10.jar mv zookeeper-3.5.1.jar org.apache.zookeeper.zookeeper-3.4.10.jar","title":"kafka-manager\u7f16\u8bd1\u53ca\u914d\u7f6e"},{"location":"Data_Integration/Kafka_Manager/#kafka-manager_1","text":"\u542f\u52a8kafka-manager cd /opt/kafka-manager-1.3.3.21 nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Djava.security.auth.login.config=conf/jaas.conf -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com & > \u53ef\u901a\u8fc7-Dhttp.port=port\u6307\u5b9a\u8bbf\u95ee\u7aef\u53e3,\u9ed8\u8ba4\u4e3a9000 \u5728\u6d4f\u89c8\u5668\u5730\u5740\u680f\u8f93\u5165172.21.3.48:9000\uff0c\u5373\u53ef\u8bbf\u95eekafka-manager \u70b9\u51fbcluster->Add Cluster\uff0c\u8fdb\u884c\u5982\u4e0b\u586b\u5199 Cluster Name:\u81ea\u5b9a\u4e49 Cluster Zookeeper Hosts:ZooKeeper\u96c6\u7fa4\u8282\u70b9\u4fe1\u606f,\u53ef\u5199\u591a\u4e2a\u6216\u8005\u4e00\u4e2a\u8282\u70b9,\u4e00\u5b9a\u8981\u52a0\u4e0akafka\u540e\u7f00 Kafka Version:\u5f53\u524dFI\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684\u662f0.11.0.1\uff0c\u9009\u62e9\u6700\u63a5\u8fd1\u76840.11.0.2\u5373\u53ef \u52fe\u9009Enable JMX\u590d\u9009\u6846 \u5c06\u4ee5\u4e0b\u51e0\u4e2asize\u5927\u5c0f\u8bbe\u7f6e\u4e3a\u5927\u4e8e\u7b49\u4e8e2 \u5176\u4ed6\u8bbe\u7f6e\u53ef\u4ee5\u4fdd\u6301\u9ed8\u8ba4\u6216\u8005\u6839\u636e\u9700\u8981\u4fee\u6539,\u70b9\u51fbSave\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u521b\u5efa\u6210\u529f \u70b9\u51fbGo to cluster view\uff0c\u53ef\u4ee5\u770b\u5230\u96c6\u7fa4\u76f8\u5173\u4fe1\u606f \u5728Brokers\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u7684brokers\u60c5\u51b5 \u5728Topic->Create\u83dc\u5355\u680f\u53ef\u4ee5\u521b\u5efa\u65b0\u7684topic \u5728Topic->List\u83dc\u5355\u680f\u53ef\u4ee5\u770b\u5230\u5f53\u524d\u96c6\u7fa4\u6240\u6709\u7684topic","title":"kafka-manager\u4f7f\u7528"},{"location":"Data_Integration/Kettle_6.1/","text":"Kettle\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD V100R002C70U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD V100R002C80U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD 6.5 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight MRS 8.0 (HDFS/Hive) Kerberos\u652f\u6301\u80fd\u529b\u8bf4\u660e \u00b6 Pentaho(7.0-9.0)\u76ee\u524d\u4ec5\u4ec5\u5728\u4f01\u4e1a\u7248\uff08EE\uff09\u652f\u6301Kerberos\u8ba4\u8bc1\u7684Hadoop, Pentaho\u793e\u533a\u7248\uff08CE\uff09\u4e0d\u652f\u6301Kerberos\u8ba4\u8bc1\u7684Hadoop\uff0c\u76f8\u5173\u7b54\u590d\u53c2\u8003\u4ee5\u4e0b\u94fe\u63a5\uff1a https://forums.pentaho.com/threads/230953-Is-Kerberos-auth-Enterprise-only/ Kettle 6.1\uff0c\u6240\u4ee5\u867d\u7136\u4ee3\u7801\u4e0a\u6ca1\u6709\u652f\u6301Kerberos\u8ba4\u8bc1\uff0c\u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u624b\u52a8\u5728OS\u5c42\u9762\u8fdb\u884cKerberos\u8ba4\u8bc1\u6765\u8fde\u63a5\u5b89\u5168\u96c6\u7fa4\uff0c6.1\u4ee5\u540e\u7684CE\u7248\u672c\u7531\u4e8e\u67b6\u6784\u8c03\u6574\uff0c\u65e0\u6cd5\u901a\u8fc7\u8bfb\u53d6OS\u4e0a\u7684Kerberos\u8ba4\u8bc1\u4fe1\u606f\u8fde\u63a5\u5b89\u5168\u96c6\u7fa4\u3002 \u73af\u5883\u51c6\u5907 \u00b6 Linux\u5e73\u53f0 \u00b6 \u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5CentOS6.5 Desktop \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kettle \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/ Windows\u5e73\u53f0 \u00b6 \u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u590d\u5236 C:\\Windows \u76ee\u5f55\u4e0b\uff0c\u91cd\u547d\u540d\u4e3akrb5.ini \u6dfb\u52a0\u7cfb\u7edf\u73af\u5883\u53d8\u91cfKRB5_CONFIG\uff08\u53ef\u9009\u6b65\u9aa4\uff09 KRB5_CONFIG=C:\\Windows \u914d\u7f6e\u5e76\u542f\u52a8Kettle \u00b6 \u4ece\u4ee5\u4e0b\u5730\u5740 https://sourceforge.net/projects/pentaho/files/Data%20Integration/ \u4e0b\u8f7dKettle6.1\u7248\u672c \u89e3\u538b\u5f97\u5230data-integration\u76ee\u5f55 \u66ff\u6362pentaho-big-data-plugin\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u7528\u89e3\u538b\u76ee\u5f55\u4e0b Hive/jdbc-examples/conf/core-site.xml \u6587\u4ef6 \u66ff\u6362 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23 \u76ee\u5f55\u4e0b\u7684core-site.xml\u6587\u4ef6 \u66ff\u6362Hive\u76f8\u5173jar\u5305 \u5c06 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib \u4e0b\u7684hive\u76f8\u5173\u7684jar\u5305 \u66ff\u6362\u6210Hive\u5ba2\u6237\u7aef\u4e0bjdbc-examples/lib\u76ee\u5f55\u4e0b\u7684\u4ee5\u4e0bjar\u5305 \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 Kerberos\u8ba4\u8bc1\uff08\u53ef\u9009\u6b65\u9aa4\uff09 \u5728\u5bf9\u63a5Hive\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e\uff0c\u6216\u8005\u5728jdbc URL\u4e2d\u6307\u5b9aprincipal\u548ckeytab\u6587\u4ef6\u8fdb\u884c\u8ba4\u8bc1\uff08\u5bf9\u63a5HDFS\u65f6\uff0c\u53ea\u80fd\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff09 \u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff0c\u9700\u8981\u5728\u542f\u52a8kettle\u524d\u5148\u5b8c\u6210\u8ba4\u8bc1\u3002 \u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1akettle\u53ea\u5728\u542f\u52a8\u65f6\u8bfb\u53d6\u4e00\u6b21\u7968\u636e\uff0c\u800c\u4e0d\u662f\u8fde\u63a5\u65f6\u5b9e\u65f6\u8bfb\u53d6\u5f53\u524d\u7968\u636e\u4fe1\u606f\uff0c\u6240\u4ee5\u5f53kettle\u542f\u52a8\u65f6\u83b7\u53d6\u7684\u7968\u636e\u8fc7\u671f\u4ee5\u540e\uff0c\u8fde\u63a5Hive\u4f1a\u5931\u8d25\uff0c\u5fc5\u987b\u91cd\u542fkettle\u3002 \u542f\u52a8kettle Linux\u5e73\u53f0 VNC\u767b\u5f55CentOS\u684c\u9762\uff0c\u6253\u5f00Terminal cd /opt/data-integration/ ./spoon.sh Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat \u5bf9\u63a5Hive \u00b6 \u521b\u5efaHive\u8fde\u63a5 \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u5982\u679c\u8981\u5728\u8fde\u63a5Hive\u65f6\u4f7f\u7528keytab\u6587\u4ef6\u8ba4\u8bc1\uff0c\u589e\u52a0user.principal\u548cuser.keytab\u4e24\u4e2a\u53c2\u6570\uff1a \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0cHadoop\u7248\u672c\u9009\u7528HDP2.3 \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5 \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u4ee5hive -> postgresql\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3ahive2postgres.ktr \u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c \u8f93\u51fa -> \u8868\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u4fee\u6539postgresql\u8868\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u51fa \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5\u4e2d \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6\u76ee\u6807\u8868\u914d\u7f6e \u5982\u4e0b\uff08\u9700\u8981\u5148\u5728postgresql\u6570\u636e\u5e93\u521b\u5efa\u76ee\u6807\u8868\uff09 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a postgresql\u6570\u636e\u5e93\u67e5\u770b\uff1a \u5199\u5165Hive\u6570\u636e \u00b6 \u4ee5oracle -> hive\u4e3a\u4f8b \u6dfb\u52a0Oracle JDBC Driver \u4ece http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html \u4e0b\u8f7d\u5bf9\u5e94\u7248\u672c\u7684jdbc Driver\uff0c\u653e\u5230 data-integration/lib \u76ee\u5f55\u4e0b\uff0c\u91cd\u542fkettle \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3aoracle2hive.ktr \u521b\u5efaOracle\u8fde\u63a5 \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS kettle_export ( id int , name string ); \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u4fee\u6539\u6b65\u9aa4\u914d\u7f6e Oracle\u8868\u8f93\u5165\u914d\u7f6e Hive\u8868\u8f93\u51fa\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516513\u6761\u6570\u636e\uff0c\u7528\u65f64min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6 \u5bf9\u63a5HDFS \u00b6 \u521b\u5efaHadoop Cluster \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199NameNode\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f25000\uff0c\u5982\u679cNaneNode\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f26004\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP\u3002 \u70b9\u51fb \u6d4b\u8bd5 kettle6.1\u4e0d\u652f\u6301HDFS NameNode\u548cYarn ResourceManager\u7684HA\u914d\u7f6e \u5bfc\u5165HDFS\u6587\u4ef6 \u00b6 \u4ee5postgresql -> HDFS\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3apostgres2hdfs.ktr \u53c2\u8003\u524d\u9762\u7ae0\u8282\u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS sample_kettle_hdfs_test ( code string , description string , total_emp int , salary int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"[,]\" ) STORED AS TEXTFILE ; \u5982\u679c\u6570\u636e\u4e2d\u542b\u6709\u201d,\u201d\uff0c\u5217\u5206\u9694\u7b26\u4e0d\u53ef\u4ee5\u4f7f\u7528\u9ed8\u8ba4\u7684\u201d,\u201d\uff0c\u672c\u6837\u4f8b\u4f7f\u7528\u591a\u5b57\u8282\u5206\u9694\u7b26\u201d[,]\u201d \u4fee\u6539postgresql\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u8868 \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9\u5230hive\u8868\u5bf9\u5e94\u7684hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u5206\u9694\u7b26\u8bbe\u7f6e\u4e0e\u524d\u9762\u521b\u5efa\u7684Hive\u8868\u76f8\u540c\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u5bfc\u5165\u7684HDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 kettle\u4f1a\u81ea\u52a8\u626b\u63cf\u6587\u4ef6\u4e2d\u7684\u5b57\u6bb5\u7c7b\u578b\u548c\u957f\u5ea6 \u53ef\u4ee5\u624b\u52a8\u4fee\u6539\u5b57\u6bb5\u540d\u79f0\u548c\u957f\u5ea6 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6","title":"6.1 <--> 8.0"},{"location":"Data_Integration/Kettle_6.1/#kettlefusioninsight","text":"","title":"Kettle\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Kettle_6.1/#_1","text":"Kettle 6.1 \u2194 FusionInsight HD V100R002C60U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD V100R002C70U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD V100R002C80U10 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight HD 6.5 (HDFS/Hive) Kettle 6.1 \u2194 FusionInsight MRS 8.0 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Kettle_6.1/#kerberos","text":"Pentaho(7.0-9.0)\u76ee\u524d\u4ec5\u4ec5\u5728\u4f01\u4e1a\u7248\uff08EE\uff09\u652f\u6301Kerberos\u8ba4\u8bc1\u7684Hadoop, Pentaho\u793e\u533a\u7248\uff08CE\uff09\u4e0d\u652f\u6301Kerberos\u8ba4\u8bc1\u7684Hadoop\uff0c\u76f8\u5173\u7b54\u590d\u53c2\u8003\u4ee5\u4e0b\u94fe\u63a5\uff1a https://forums.pentaho.com/threads/230953-Is-Kerberos-auth-Enterprise-only/ Kettle 6.1\uff0c\u6240\u4ee5\u867d\u7136\u4ee3\u7801\u4e0a\u6ca1\u6709\u652f\u6301Kerberos\u8ba4\u8bc1\uff0c\u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u624b\u52a8\u5728OS\u5c42\u9762\u8fdb\u884cKerberos\u8ba4\u8bc1\u6765\u8fde\u63a5\u5b89\u5168\u96c6\u7fa4\uff0c6.1\u4ee5\u540e\u7684CE\u7248\u672c\u7531\u4e8e\u67b6\u6784\u8c03\u6574\uff0c\u65e0\u6cd5\u901a\u8fc7\u8bfb\u53d6OS\u4e0a\u7684Kerberos\u8ba4\u8bc1\u4fe1\u606f\u8fde\u63a5\u5b89\u5168\u96c6\u7fa4\u3002","title":"Kerberos\u652f\u6301\u80fd\u529b\u8bf4\u660e"},{"location":"Data_Integration/Kettle_6.1/#_2","text":"","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kettle_6.1/#linux","text":"\u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5CentOS6.5 Desktop \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kettle \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/","title":"Linux\u5e73\u53f0"},{"location":"Data_Integration/Kettle_6.1/#windows","text":"\u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u590d\u5236 C:\\Windows \u76ee\u5f55\u4e0b\uff0c\u91cd\u547d\u540d\u4e3akrb5.ini \u6dfb\u52a0\u7cfb\u7edf\u73af\u5883\u53d8\u91cfKRB5_CONFIG\uff08\u53ef\u9009\u6b65\u9aa4\uff09 KRB5_CONFIG=C:\\Windows","title":"Windows\u5e73\u53f0"},{"location":"Data_Integration/Kettle_6.1/#kettle","text":"\u4ece\u4ee5\u4e0b\u5730\u5740 https://sourceforge.net/projects/pentaho/files/Data%20Integration/ \u4e0b\u8f7dKettle6.1\u7248\u672c \u89e3\u538b\u5f97\u5230data-integration\u76ee\u5f55 \u66ff\u6362pentaho-big-data-plugin\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u7528\u89e3\u538b\u76ee\u5f55\u4e0b Hive/jdbc-examples/conf/core-site.xml \u6587\u4ef6 \u66ff\u6362 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23 \u76ee\u5f55\u4e0b\u7684core-site.xml\u6587\u4ef6 \u66ff\u6362Hive\u76f8\u5173jar\u5305 \u5c06 data-integration/plugins/pentaho-big-data-plugin/hadoop-configurations/hdp23/lib \u4e0b\u7684hive\u76f8\u5173\u7684jar\u5305 \u66ff\u6362\u6210Hive\u5ba2\u6237\u7aef\u4e0bjdbc-examples/lib\u76ee\u5f55\u4e0b\u7684\u4ee5\u4e0bjar\u5305 \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 Kerberos\u8ba4\u8bc1\uff08\u53ef\u9009\u6b65\u9aa4\uff09 \u5728\u5bf9\u63a5Hive\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e\uff0c\u6216\u8005\u5728jdbc URL\u4e2d\u6307\u5b9aprincipal\u548ckeytab\u6587\u4ef6\u8fdb\u884c\u8ba4\u8bc1\uff08\u5bf9\u63a5HDFS\u65f6\uff0c\u53ea\u80fd\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff09 \u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7684\u7968\u636e\uff0c\u9700\u8981\u5728\u542f\u52a8kettle\u524d\u5148\u5b8c\u6210\u8ba4\u8bc1\u3002 \u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1akettle\u53ea\u5728\u542f\u52a8\u65f6\u8bfb\u53d6\u4e00\u6b21\u7968\u636e\uff0c\u800c\u4e0d\u662f\u8fde\u63a5\u65f6\u5b9e\u65f6\u8bfb\u53d6\u5f53\u524d\u7968\u636e\u4fe1\u606f\uff0c\u6240\u4ee5\u5f53kettle\u542f\u52a8\u65f6\u83b7\u53d6\u7684\u7968\u636e\u8fc7\u671f\u4ee5\u540e\uff0c\u8fde\u63a5Hive\u4f1a\u5931\u8d25\uff0c\u5fc5\u987b\u91cd\u542fkettle\u3002 \u542f\u52a8kettle Linux\u5e73\u53f0 VNC\u767b\u5f55CentOS\u684c\u9762\uff0c\u6253\u5f00Terminal cd /opt/data-integration/ ./spoon.sh Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat","title":"\u914d\u7f6e\u5e76\u542f\u52a8Kettle"},{"location":"Data_Integration/Kettle_6.1/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/Kettle_6.1/#hive_1","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u5982\u679c\u4f7f\u7528\u672c\u5730\u7f13\u5b58\u7968\u636e\uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u5982\u679c\u8981\u5728\u8fde\u63a5Hive\u65f6\u4f7f\u7528keytab\u6587\u4ef6\u8ba4\u8bc1\uff0c\u589e\u52a0user.principal\u548cuser.keytab\u4e24\u4e2a\u53c2\u6570\uff1a \u6d4b\u8bd5\u8fde\u63a5\u65f6\uff0cHadoop\u7248\u672c\u9009\u7528HDP2.3 \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5","title":"\u521b\u5efaHive\u8fde\u63a5"},{"location":"Data_Integration/Kettle_6.1/#hive_2","text":"\u4ee5hive -> postgresql\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3ahive2postgres.ktr \u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c \u8f93\u51fa -> \u8868\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u4fee\u6539postgresql\u8868\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u51fa \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5\u4e2d \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6\u76ee\u6807\u8868\u914d\u7f6e \u5982\u4e0b\uff08\u9700\u8981\u5148\u5728postgresql\u6570\u636e\u5e93\u521b\u5efa\u76ee\u6807\u8868\uff09 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a postgresql\u6570\u636e\u5e93\u67e5\u770b\uff1a","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_6.1/#hive_3","text":"\u4ee5oracle -> hive\u4e3a\u4f8b \u6dfb\u52a0Oracle JDBC Driver \u4ece http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html \u4e0b\u8f7d\u5bf9\u5e94\u7248\u672c\u7684jdbc Driver\uff0c\u653e\u5230 data-integration/lib \u76ee\u5f55\u4e0b\uff0c\u91cd\u542fkettle \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3aoracle2hive.ktr \u521b\u5efaOracle\u8fde\u63a5 \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS kettle_export ( id int , name string ); \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u4fee\u6539\u6b65\u9aa4\u914d\u7f6e Oracle\u8868\u8f93\u5165\u914d\u7f6e Hive\u8868\u8f93\u51fa\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516513\u6761\u6570\u636e\uff0c\u7528\u65f64min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6","title":"\u5199\u5165Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_6.1/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/Kettle_6.1/#hadoop-cluster","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199NameNode\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f25000\uff0c\u5982\u679cNaneNode\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f26004\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP\u3002 \u70b9\u51fb \u6d4b\u8bd5 kettle6.1\u4e0d\u652f\u6301HDFS NameNode\u548cYarn ResourceManager\u7684HA\u914d\u7f6e","title":"\u521b\u5efaHadoop Cluster"},{"location":"Data_Integration/Kettle_6.1/#hdfs_1","text":"\u4ee5postgresql -> HDFS\u4e3a\u4f8b \u5c06\u4e0a\u9762\u521b\u5efa\u7684\u8f6c\u6362\u4fdd\u5b58\u4e3apostgres2hdfs.ktr \u53c2\u8003\u524d\u9762\u7ae0\u8282\u521b\u5efapostgresql\u8fde\u63a5 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u8868\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u521b\u5efa\u5f85\u5bfc\u5165\u7684Hive\u8868 CREATE TABLE IF NOT EXISTS sample_kettle_hdfs_test ( code string , description string , total_emp int , salary int ) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES ( \"field.delim\" = \"[,]\" ) STORED AS TEXTFILE ; \u5982\u679c\u6570\u636e\u4e2d\u542b\u6709\u201d,\u201d\uff0c\u5217\u5206\u9694\u7b26\u4e0d\u53ef\u4ee5\u4f7f\u7528\u9ed8\u8ba4\u7684\u201d,\u201d\uff0c\u672c\u6837\u4f8b\u4f7f\u7528\u591a\u5b57\u8282\u5206\u9694\u7b26\u201d[,]\u201d \u4fee\u6539postgresql\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684postgresql\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684\u8868 \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9\u5230hive\u8868\u5bf9\u5e94\u7684hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u5206\u9694\u7b26\u8bbe\u7f6e\u4e0e\u524d\u9762\u521b\u5efa\u7684Hive\u8868\u76f8\u540c\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u5bfc\u5165\u7684HDFS\u6587\u4ef6\uff1a \u67e5\u770bHive\u8868\u6570\u636e\uff1a","title":"\u5bfc\u5165HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_6.1/#hdfs_2","text":"\u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 kettle\u4f1a\u81ea\u52a8\u626b\u63cf\u6587\u4ef6\u4e2d\u7684\u5b57\u6bb5\u7c7b\u578b\u548c\u957f\u5ea6 \u53ef\u4ee5\u624b\u52a8\u4fee\u6539\u5b57\u6bb5\u540d\u79f0\u548c\u957f\u5ea6 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u9009\u62e9 \u672c\u5730\u6267\u884c \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/","text":"Kettle\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kettle 8.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Kettle 8.1.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) \u8bf4\u660e\uff1aKettle 8.1.0\u4ec5\u9650POC\u4f7f\u7528 Windows\u5e73\u53f0 \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u96c6\u7fa4\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e\uff0c\u4f8b\u5982\u7528\u6237\u4e3a developuser ; \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u540e\u7f00\u540d\u4fee\u6539\u4e3aini,\u5c06krb5.ini\u6587\u4ef6\u590d\u5236\u5230 C:\\Windows \u76ee\u5f55\u4e0b. \u914d\u7f6e\u5e76\u542f\u52a8Kettle \u00b6 \u8f6f\u4ef6\u83b7\u53d6 \u6253\u5f00\u4ee5\u4e0b\u5730\u5740[ https://github.com/pentaho/pentaho-kettle/tree/8.0 ] ( https://github.com/pentaho/pentaho-kettle/tree/8.0 ), \u9009\u62e9DownloadZip\u4e0b\u8f7dKettle8.0\u7248\u672c \u89e3\u538b\u5f97\u5230pdi-ce-8.0.0.0-28; \u83b7\u53d6FusionInsight\u7684\u9002\u914d\u5305\u6587\u4ef6 pentaho-hadoop-shims-hdp26-8.1.0.0-SNAPSHOT.jar \u548c pentaho-hadoop-shims-hdp26-hbase-comparators-8.1.0.0-SNAPSHOT.jar ,\u66ff\u6362\u76ee\u5f55 \\data-integration\\plugins\\pentaho-big-data-plugin\\hadoop-configurations\\hdp26\\ \u4e0b\u7684\u539f\u6709\u6587\u4ef6; \u66ff\u6362hdp26\\lib\u76ee\u5f55\u4e0bHive\u76f8\u5173\u7684jar\u5305\u4ee5\u53cahdp26\\lib\\client\u76ee\u5f55\u4e0bhdfs\u76f8\u5173\u7684jar\u5305 \u5e76\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u5305 \u83b7\u53d6FusionInsightHD\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cHbase\u7b49\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230Fi28\u9002\u914d\u5305\u7684\u6587\u4ef6\u5939\u91cc; \u4fee\u6539core-site.xml\u6587\u4ef6\u4e2d\u4ee5\u4e0b\u5b57\u6bb5\uff1a <name>fs.defaultFS</name> <value>hdfs://hacluster</value> \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6\u53ca\u914d\u7f6e \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 \u4fee\u6539Fi28\u9002\u914d\u5305\u4e2d config.properties \u6587\u4ef6: pentaho.authentication.default.kerberos.keytabLocation=C:/kerberos/user.keytab pentaho.authentication.default.kerberos.conf=C:/kerberos/krb5.conf pentaho.authentication.default.kerberos.principal=developuser@HADOOP.COM \u542f\u52a8kettle Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat,\u8fdb\u5165\u754c\u9762\u540e\uff0c\u5728\u4e0a\u65b9\u83dc\u5355\u680f\u9009\u62e9\u5de5\u5177->Hadoop Distribution,\u9009\u62e9 HortonWorks HDP 2.6.x \u5bf9\u63a5Hive \u00b6 \u521b\u5efaHive\u8fde\u63a5 \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u4e3a\u8fde\u63a5\u547d\u540d\uff0c\u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u70b9\u51fb\u6d4b\u8bd5\uff0c\u663e\u793a\u4ee5\u4e0b\u7a97\u53e3\uff0c\u8868\u660e\u6d4b\u8bd5\u6210\u529f \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5 \u8bfb\u53d6Hive\u6570\u636e \u00b6 \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8868\u8f93\u5165 \u548c \u6587\u672c\u6587\u4ef6\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4; \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u9009\u62e9 \u662f ,\u8be5\u8868\u7684\u5b57\u6bb5\u5c06\u4f1a\u5305\u542b\u5728SQL\u8bed\u53e5\u4e2d\uff0c \u53ef\u4ee5\u70b9\u51fb \u9884\u89c8 \uff0c\u5e76\u9009\u62e9\u884c\u6570\uff0c\u9884\u89c8Hive\u8868\u4e2d\u7684\u6570\u636e \u4fee\u6539\u6587\u672c\u6587\u4ef6\u8f93\u51fa\u914d\u7f6e \u5728\u6587\u4ef6\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u8f93\u51fa\u6587\u4ef6\u540d\u79f0\uff0c\u6269\u5c55\u540d\uff1a \u5728\u5185\u5bb9\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u6587\u4ef6\u8f93\u51fa\u65f6\u5c5e\u6027 \u5728\u5b57\u6bb5\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 \uff0c\u83b7\u5f97\u6587\u4ef6\u5b57\u6bb5\u5185\u5bb9\uff0c\u53ef\u4ee5\u70b9\u51fb \u6700\u5c0f\u5bbd\u5ea6 ,\u4f7f\u5b57\u6bb5\u5bbd\u5ea6\u6700\u5c0f \u70b9\u51fb \u786e\u5b9a ,\u4fdd\u5b58\u8bbe\u7f6e\u3002 \u8fd0\u884c\u8f6c\u6362\uff0c\u5728\u4e3b\u754c\u9762\u70b9\u51fb\u5de5\u5177\u680f\u5de6\u4fa7\u7684\u4e09\u89d2\u5f62\u8fd0\u884c\u6309\u94ae \u6267\u884c\u7ed3\u679c\uff1a \u5199\u5165Hive\u6570\u636e \u00b6 \u4ee5\u672c\u5730\u6587\u672c\u6587\u4ef6 -> hive\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahive_out.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4\uff0c\u5c06\u6587\u672c\u6587\u4ef6\u8f93\u5165\u548c\u8868\u8f93\u51fa\u4e24\u4e2a\u6b65\u9aa4\u62d6\u5165\u5de5\u4f5c\u533a\uff0c\u8fde\u63a5\u4e24\u4e2a\u6b65\u9aa4; \u53cc\u51fb\u6587\u672c\u6587\u4ef6\u8f93\u5165\uff0c\u5728\u6587\u4ef6\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u9700\u8981\u4e0a\u4f20\u7684\u672c\u5730\u6587\u4ef6\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u6587\u4ef6\u88ab\u6dfb\u52a0\u81f3\u4e0b\u65b9\u9009\u4e2d\u7684\u6587\u4ef6; \u5728\u5185\u5bb9\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u9650\u5b9a\u7b26\u3001\u7f16\u7801\u7b49\u7b49 \u5728\u5b57\u6bb5\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 \uff0c\u83b7\u5f97\u5b57\u6bb5\u540e\uff0c\u53ef\u4ee5\u70b9\u51fb Minimal Width \u4f7f\u5b57\u6bb5\u5bbd\u5ea6\u6700\u5c0f \u70b9\u51fb \u786e\u5b9a ,\u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb\u8868\u8f93\u51fa\uff0c\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u8bbe\u7f6e\u76ee\u6807\u8868\uff0c\u8be5\u8868\u9700\u8981\u5df2\u7ecf\u5728Hive\u4e2d\u521b\u5efa\u597d\uff0c\u5e76\u4e14\u5b57\u6bb5\u4e0e\u672c\u5730\u6587\u4ef6\u4fdd\u6301\u4e00\u81f4; \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6\u4e4b\u540e\u518d\u8f7d\u5165Hive\u8868 \u5bf9\u63a5HDFS \u00b6 \u521b\u5efaHadoop Cluster \u00b6 \u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199hacluster; JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f21066,\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP; ZooKeeper\u7684Hostname \u586b\u5199ZooKeeper\u7684\u4e3b\u8282\u70b9IP\uff0c\u7aef\u53e3\u53f7\u662f24002\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP; Oozie\u7684URL\u586b\u5199oozie WebUI\u7684\u5730\u5740. \u70b9\u51fb \u6d4b\u8bd5 \u5bfc\u5165HDFS\u6587\u4ef6 \u00b6 \u4ee5\u672c\u5730\u6587\u4ef6 -> HDFS\u4e3a\u4f8b \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u6587\u672c\u6587\u4ef6\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u6587\u672c\u6587\u4ef6\u8f93\u5165\u914d\u7f6e\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u914d\u7f6e \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u8bbe\u7f6e\u5206\u9694\u7b26\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5\uff0c\u5e76\u8bbe\u7f6e\u6700\u5c0f\u5bbd\u5ea6 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770bHDFS\u6587\u4ef6 \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6 \u5bf9\u63a5HBase \u00b6 \u5bfc\u5165HBASE\u6587\u4ef6 \u00b6 \u4ee5\u672c\u5730\u6587\u4ef6 -> HBase\u4e3a\u4f8b \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u6587\u672c\u6587\u4ef6\u8f93\u5165 \uff0c\u548c Big Data -> HBase Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u6587\u672c\u6587\u4ef6\u8f93\u5165\u914d\u7f6e\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u914d\u7f6e\uff0c\u6ce8\u610f\u5728\u96c6\u7fa4HBase\u4e2d\u8981\u6709\u548c\u5bfc\u5165\u7684\u8868\u76f8\u540c\u7684\u7a7a\u8868\uff0c\u6307\u660e\u5b57\u6bb5\u548c\u5217\u7c07. \u4fee\u6539 HBase Output \u914d\u7f6e \u53cc\u51fb HBase Output \u6b65\u9aa4\uff0c\u5728 Configure connection \u9875\u7b7e\u4e0b\uff0c\u9009\u62e9\u5df2\u7ecf\u914d\u7f6e\u597d\u7684Hadoop\u96c6\u7fa4\uff0c\u70b9\u51fb Get table name \uff0c\u83b7\u53d6\u8981\u8f93\u51fa\u7684\u8868,\u70b9\u51fb Get mapping for specified table \u83b7\u53d6\u8be5\u8868\u5bf9\u5e94\u7684mapping. \u82e5\u8be5\u8868\u6ca1\u6709\u521b\u5efamapping,\u5728 Create/Edit Mappings \u9875\u7b7e\u521b\u5efamapping,\u6307\u5b9a\u5404\u9879\u5c5e\u6027 \u70b9\u51fb \u786e\u5b9a \uff0c\u4fdd\u5b58\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae,\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u96c6\u7fa4\u4e2d\u7684HBase\u6587\u4ef6 \u6267\u884c hbase shell count 'customer' \u8bfb\u53d6HBASE\u6587\u4ef6 \u00b6 \u4ee5HBase -> \u6587\u672c\u6587\u4ef6\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahbase.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> HBase Input \u548c \u8f93\u51fa -> \u6587\u672c\u6587\u4ef6\u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 HBase Input\u914d\u7f6e \u53cc\u51fb HBase Input \u6b65\u9aa4\uff0c\u5728 Configure query \u9875\u7b7e\uff0c\u9009\u62e9\u5df2\u7ecf\u8fde\u63a5\u597d\u7684Hadoop\u96c6\u7fa4\uff0c\u82e5\u65e0\u5df2\u7ecf\u8fde\u63a5\u7684\u96c6\u7fa4\uff0c\u70b9\u51fb new ,\u53c2\u7167\u4e0a\u9762\u7ae0\u8282Hadoop\u96c6\u7fa4\u914d\u7f6e\uff0c\u914d\u7f6e\u8fde\u63a5\u96c6\u7fa4; \u5728 Create/Edit Mappings \u9875\u7b7e\uff0c\u70b9\u51fb Get table names ,\u83b7\u53d6\u96c6\u7fa4\u4e2d\u7684Hbase\u8868\uff0c\u9009\u62e9\u8981\u8bfb\u53d6\u7684\u8868\uff0c\u5728 Mapping name \u4e0b\u62c9\u9009\u62e9\u4e0e\u8be5\u8868\u5173\u8054\u7684map\uff0c\u82e5\u6ca1\u6709\uff0c\u81ea\u5b9a\u4e49\u4e00\u4e2amap\u7684\u540d\u5b57\uff0c\u586b\u5199\u5b57\u6bb5\u548c\u5217\u7c07\uff0c\u5e76\u6307\u5b9a\u5b57\u6bb5\u662f\u5426\u4e3akey\uff0c\u5b57\u6bb5\u7c7b\u578b. \u56de\u5230 Configure query \u9875\u7b7e,\u70b9\u51fb Get mapped table names ,\u9009\u62e9\u8981\u8bfb\u53d6\u7684\u8868\uff0c\u70b9\u51fb Get mappings for the specified table \u83b7\u53d6\u8be5\u8868\u5bf9\u5e94\u7684mapping\uff0c\u70b9\u51fb\u53f3\u4e0b\u89d2 Get Key/Feilds Info \uff0c\u83b7\u53d6\u5bf9\u5e94\u7684\u8868\u7684\u4fe1\u606f. \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539\u6587\u672c\u6587\u4ef6\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u6587\u672c\u6587\u4ef6\u8f93\u51fa \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\uff0c\u586b\u5199\u6587\u4ef6\u540d\u548c\u6269\u5c55\u540d; \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 ,\u8bbe\u7f6e\u6700\u5c0f\u5bbd\u5ea6(\u53ef\u9009) \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684\u6587\u4ef6 Linux\u5e73\u53f0 \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5RedHat 6.5 \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u8282\u70b9IP host1 \u8282\u70b9IP host2 \u8282\u70b9IP host3 \u82e5\u662f\u684c\u9762\u7248\u64cd\u4f5c\u7cfb\u7edf\uff0cKettle\u5bf9\u63a5\u53c2\u7167\u4e0a\u9762\u7ae0\u8282Windows\u7cfb\u7edf\u4e0b\u7684\u5bf9\u63a5\u65b9\u5f0f. \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\uff0c\u5728\u6709\u56fe\u5f62\u754c\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e0b\uff0c\u914d\u7f6e\u597dKettle\u4e0eFi\u96c6\u7fa4\u7684\u8fde\u63a5\uff0c\u6d4b\u8bd5\u8fde\u901a\u6027,\u5c06Kettle\u7684 data-integration \u76ee\u5f55\u4ee5\u53ca\u5176\u4e0b\u6240\u6709\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u7684 opt \u76ee\u5f55\u4e0b. \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/ Hive\u5bf9\u63a5 \u00b6 \u5bfc\u51faHive\u8868 \u00b6 \u4ee5Hive->\u6587\u672c\u6587\u4ef6\u4e3a\u4f8b \u5728\u6709\u56fe\u5f62\u754c\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2a\u8f6c\u6362\uff0c\u5728\u5de5\u4f5c\u533a\u4e2d\u653e\u5165 \u8868\u8f93\u5165 \u548c \u6587\u672c\u6587\u4ef6\u8f93\u51fa \uff0c\u4fdd\u5b58\u4e3ahive.ktr; \u70b9\u51fb \u8868\u8f93\u5165 \uff0c\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u4e2d\u5173\u4e8eHive\u8fde\u63a5\u7684\u914d\u7f6e\uff0c\u53ea\u9700\u4fee\u6539\u8fde\u63a5\u9009\u9879\u4e2d user.keytab \u6587\u4ef6\u6240\u5728\u8def\u5f84\uff0c\u4fee\u6539\u4e3a /etc/user.keytab \u5c06hive.ktr\u8f6c\u6362\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u4e0bKettle\u7684 data-integration \u6587\u4ef6\u5939\u4e0b\uff0c \u6839\u636eKettle\u7248\u672c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 cd /opt/data-integration/ \u5bf9\u4e8eKettle-8.0\u7248\u672c,\u6267\u884c\u4ee5\u4e0b\u811a\u672c\u6e05\u9664cache\uff08\u53c2\u89c1FAQ1\uff09 sed -i \"s/^org.pentaho\\.clean\\.karaf\\.cache=false/org\\.pentaho\\.clean\\.karaf\\.cache=true/g\" /opt/data-integration/system/karaf/etc/custom.properties \u53ef\u5c06\u5176\u4fdd\u5b58\u4e3a\u811a\u672c\u6587\u4ef6\uff0c\u6bcf\u6b21\u6267\u884c\u547d\u4ee4\u524d\u5148\u6267\u884c\u8be5\u811a\u672c \u7136\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fd0\u884c\u7a0b\u5e8f ./kitchen.sh -file=hive.ktr \u5bf9\u4e8eKettle-8.1\u7248\u672c,\u624b\u52a8\u5220\u9664 /data-integration/system/karaf/caches/pan/data-1 \u76ee\u5f55\u4e0b\u7684cache\u6587\u4ef6 \u7136\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fd0\u884c\u7a0b\u5e8f ./pan.sh -file=hive.ktr * \u6267\u884c\u7ed3\u679c\u5982\u4e0b \u5bfc\u51fa\u7684\u8868\u5728 data-integration/ \u76ee\u5f55\u4e0b \u4e0a\u4f20\u6587\u4ef6\u81f3Hive \u00b6 \u540cWindows\u64cd\u4f5c\u7cfb\u7edf\u4e0b\u521b\u5efaktr\u6587\u4ef6\u64cd\u4f5c\uff0c\u5728\u9009\u62e9\u9700\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u65f6\uff0c\u4fee\u6539\u672c\u5730\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5728Hive\u8fde\u63a5\u9009\u9879\u914d\u7f6e\u4fee\u6539\u4e2d user.keytab \u6587\u4ef6\u7684\u8def\u5f84\u4e3a /etc/user.keytab \u5373\u53ef\uff0c\u5c06ktr\u6587\u4ef6\u7f6e\u4e8eLinux\u7cfb\u7edf\u4e2d data-integration \u6587\u4ef6\u5939\u4e0b\uff0c\u6267\u884c\u547d\u4ee4\u540c\u4e0a\u5c0f\u8282\u4e2d\u64cd\u4f5c\u3002 HDFS & HBase\u6587\u4ef6\u8f93\u51fa \u00b6 \u5c06\u4e0a\u9762\u7ae0\u8282\u521b\u5efa\u7684ktr\u8f6c\u6362\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u4e0bKettle\u7684 data-integration \u6587\u4ef6\u5939\u4e0b\uff0c\u6839\u636eKettle\u7248\u672c\u6267\u884c\u547d\u4ee4(\u540chive)\u5373\u53ef \u6267\u884c\u7ed3\u679c\u5982\u4e0b \u5bfc\u51fa\u7684\u8868\u5728 data-integration/ \u76ee\u5f55\u4e0b \u4e0a\u4f20\u6587\u4ef6\u81f3HDFS & HBase \u00b6 \u540c \u4e0a\u4f20\u6587\u4ef6\u81f3Hive \u64cd\u4f5c\uff0c\u4fee\u6539\u672c\u5730\u6587\u4ef6\u8def\u5f84\u5373\u53ef\u3002 FAQ \u00b6 1.\u5728Linux\u7cfb\u7edf\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u8f6c\u6362\u6216\u8005\u4efb\u52a1\uff0cKettle\u90fd\u4f1a\u751f\u6210\u4e00\u4e9bCache\u6587\u4ef6\uff0c\u5728\u6267\u884c\u4e0b\u4e00\u6b21\u8f6c\u6362/\u4efb\u52a1\u4e4b\u524d\uff0c\u9700\u8981\u6e05\u9664\u8fd9\u4e9bCache\uff0c\u5426\u5728HDFS Hive \u548cHBase\u8fdb\u884c\u8fde\u63a5\u4f20\u8f93\u65f6\u4f1a\u51fa\u9519","title":"Kettle 8.x"},{"location":"Data_Integration/Kettle_8.x/#kettlefusioninsight","text":"","title":"Kettle\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Kettle_8.x/#_1","text":"Kettle 8.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Kettle 8.1.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) \u8bf4\u660e\uff1aKettle 8.1.0\u4ec5\u9650POC\u4f7f\u7528","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Kettle_8.x/#windows","text":"","title":"Windows\u5e73\u53f0"},{"location":"Data_Integration/Kettle_8.x/#_2","text":"\u5b89\u88c5JDK8 \u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728PATH\u73af\u5883\u53d8\u91cf\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; \u83b7\u53d6Kerberos\u914d\u7f6e\u6587\u4ef6 \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u96c6\u7fa4\u7528\u6237\u7684\u8ba4\u8bc1\u51ed\u636e\uff0c\u4f8b\u5982\u7528\u6237\u4e3a developuser ; \u89e3\u538b\u540e\u5f97\u5230Kerberos\u914d\u7f6e\u6587\u4ef6krb5.conf\u548c\u7528\u6237\u5bc6\u94a5\u6587\u4ef6user.keytab \u5c06krb5.conf\u6587\u4ef6\u540e\u7f00\u540d\u4fee\u6539\u4e3aini,\u5c06krb5.ini\u6587\u4ef6\u590d\u5236\u5230 C:\\Windows \u76ee\u5f55\u4e0b.","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kettle_8.x/#kettle","text":"\u8f6f\u4ef6\u83b7\u53d6 \u6253\u5f00\u4ee5\u4e0b\u5730\u5740[ https://github.com/pentaho/pentaho-kettle/tree/8.0 ] ( https://github.com/pentaho/pentaho-kettle/tree/8.0 ), \u9009\u62e9DownloadZip\u4e0b\u8f7dKettle8.0\u7248\u672c \u89e3\u538b\u5f97\u5230pdi-ce-8.0.0.0-28; \u83b7\u53d6FusionInsight\u7684\u9002\u914d\u5305\u6587\u4ef6 pentaho-hadoop-shims-hdp26-8.1.0.0-SNAPSHOT.jar \u548c pentaho-hadoop-shims-hdp26-hbase-comparators-8.1.0.0-SNAPSHOT.jar ,\u66ff\u6362\u76ee\u5f55 \\data-integration\\plugins\\pentaho-big-data-plugin\\hadoop-configurations\\hdp26\\ \u4e0b\u7684\u539f\u6709\u6587\u4ef6; \u66ff\u6362hdp26\\lib\u76ee\u5f55\u4e0bHive\u76f8\u5173\u7684jar\u5305\u4ee5\u53cahdp26\\lib\\client\u76ee\u5f55\u4e0bhdfs\u76f8\u5173\u7684jar\u5305 \u5e76\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u5305 \u83b7\u53d6FusionInsightHD\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6 \u4e0b\u8f7dFusionInsightHD\u5ba2\u6237\u7aef\u5e76\u89e3\u538b \u89e3\u538b\u540e\uff0c\u8fdb\u5165HDFS\uff0cHive\uff0cHbase\u7b49\u7ec4\u4ef6\u7684config\u76ee\u5f55\uff0c\u627e\u5230\u5982\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u590d\u5236\u5230Fi28\u9002\u914d\u5305\u7684\u6587\u4ef6\u5939\u91cc; \u4fee\u6539core-site.xml\u6587\u4ef6\u4e2d\u4ee5\u4e0b\u5b57\u6bb5\uff1a <name>fs.defaultFS</name> <value>hdfs://hacluster</value> \u83b7\u53d6\u7528\u6237keytab\u6587\u4ef6\u53ca\u914d\u7f6e \u5728FI\u7ba1\u7406\u754c\u9762\u4e0b\u8f7d\u7528\u6237\u7684keytab\u6587\u4ef6\u5230\u672c\u5730 \u4fee\u6539Fi28\u9002\u914d\u5305\u4e2d config.properties \u6587\u4ef6: pentaho.authentication.default.kerberos.keytabLocation=C:/kerberos/user.keytab pentaho.authentication.default.kerberos.conf=C:/kerberos/krb5.conf pentaho.authentication.default.kerberos.principal=developuser@HADOOP.COM \u542f\u52a8kettle Windows\u5e73\u53f0 \u53cc\u51fbdata-integration\u76ee\u5f55\u4e0b\u7684Spoon.bat,\u8fdb\u5165\u754c\u9762\u540e\uff0c\u5728\u4e0a\u65b9\u83dc\u5355\u680f\u9009\u62e9\u5de5\u5177->Hadoop Distribution,\u9009\u62e9 HortonWorks HDP 2.6.x","title":"\u914d\u7f6e\u5e76\u542f\u52a8Kettle"},{"location":"Data_Integration/Kettle_8.x/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/Kettle_8.x/#hive_1","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728\u9875\u7b7e\u4e2d\u9009\u62e9 \u8f6c\u6362 -> DB\u8fde\u63a5 \uff0c\u53f3\u952e\u9009\u62e9 \u65b0\u5efa \u4e3a\u8fde\u63a5\u547d\u540d\uff0c\u8fde\u63a5\u7c7b\u578b\u9009\u62e9Hive 2\uff0c\u586b\u5199\u4e3b\u673a\u540d\u3001\u7aef\u53e3\u53f7\u3001\u6570\u636e\u5e93\u540d \u70b9\u51fb\u5de6\u4fa7 \u9009\u9879 \uff0c\u586b\u5199\u4ee5\u4e0b\u53c2\u6570\uff1a \u70b9\u51fb\u6d4b\u8bd5\uff0c\u663e\u793a\u4ee5\u4e0b\u7a97\u53e3\uff0c\u8868\u660e\u6d4b\u8bd5\u6210\u529f \u8fde\u63a5\u6d4b\u8bd5\u6210\u529f\u540e\uff0c\u70b9\u51fb \u786e\u8ba4 \u4fdd\u5b58\u8fde\u63a5","title":"\u521b\u5efaHive\u8fde\u63a5"},{"location":"Data_Integration/Kettle_8.x/#hive_2","text":"\u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8868\u8f93\u5165 \u548c \u6587\u672c\u6587\u4ef6\u8f93\u51fa \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4; \u4fee\u6539Hive\u8868\u8f93\u5165\u914d\u7f6e \u53cc\u51fb \u8868\u8f93\u5165 \u6b65\u9aa4\uff0c \u6570\u636e\u5e93\u8fde\u63a5 \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684hive\u8fde\u63a5\uff0c\u70b9\u51fb \u83b7\u53d6SQL\u67e5\u8be2\u8bed\u53e5 \uff0c\u9009\u62e9\u9700\u8981\u5bfc\u5165\u7684hive\u8868 \u9009\u62e9 \u662f ,\u8be5\u8868\u7684\u5b57\u6bb5\u5c06\u4f1a\u5305\u542b\u5728SQL\u8bed\u53e5\u4e2d\uff0c \u53ef\u4ee5\u70b9\u51fb \u9884\u89c8 \uff0c\u5e76\u9009\u62e9\u884c\u6570\uff0c\u9884\u89c8Hive\u8868\u4e2d\u7684\u6570\u636e \u4fee\u6539\u6587\u672c\u6587\u4ef6\u8f93\u51fa\u914d\u7f6e \u5728\u6587\u4ef6\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u8f93\u51fa\u6587\u4ef6\u540d\u79f0\uff0c\u6269\u5c55\u540d\uff1a \u5728\u5185\u5bb9\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u6587\u4ef6\u8f93\u51fa\u65f6\u5c5e\u6027 \u5728\u5b57\u6bb5\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 \uff0c\u83b7\u5f97\u6587\u4ef6\u5b57\u6bb5\u5185\u5bb9\uff0c\u53ef\u4ee5\u70b9\u51fb \u6700\u5c0f\u5bbd\u5ea6 ,\u4f7f\u5b57\u6bb5\u5bbd\u5ea6\u6700\u5c0f \u70b9\u51fb \u786e\u5b9a ,\u4fdd\u5b58\u8bbe\u7f6e\u3002 \u8fd0\u884c\u8f6c\u6362\uff0c\u5728\u4e3b\u754c\u9762\u70b9\u51fb\u5de5\u5177\u680f\u5de6\u4fa7\u7684\u4e09\u89d2\u5f62\u8fd0\u884c\u6309\u94ae \u6267\u884c\u7ed3\u679c\uff1a","title":"\u8bfb\u53d6Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_8.x/#hive_3","text":"\u4ee5\u672c\u5730\u6587\u672c\u6587\u4ef6 -> hive\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahive_out.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4\uff0c\u5c06\u6587\u672c\u6587\u4ef6\u8f93\u5165\u548c\u8868\u8f93\u51fa\u4e24\u4e2a\u6b65\u9aa4\u62d6\u5165\u5de5\u4f5c\u533a\uff0c\u8fde\u63a5\u4e24\u4e2a\u6b65\u9aa4; \u53cc\u51fb\u6587\u672c\u6587\u4ef6\u8f93\u5165\uff0c\u5728\u6587\u4ef6\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u9700\u8981\u4e0a\u4f20\u7684\u672c\u5730\u6587\u4ef6\uff0c\u70b9\u51fb \u6dfb\u52a0 \uff0c\u6587\u4ef6\u88ab\u6dfb\u52a0\u81f3\u4e0b\u65b9\u9009\u4e2d\u7684\u6587\u4ef6; \u5728\u5185\u5bb9\u9009\u9879\u5361\u4e2d\uff0c\u8bbe\u7f6e\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u9650\u5b9a\u7b26\u3001\u7f16\u7801\u7b49\u7b49 \u5728\u5b57\u6bb5\u9009\u9879\u5361\u4e2d\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 \uff0c\u83b7\u5f97\u5b57\u6bb5\u540e\uff0c\u53ef\u4ee5\u70b9\u51fb Minimal Width \u4f7f\u5b57\u6bb5\u5bbd\u5ea6\u6700\u5c0f \u70b9\u51fb \u786e\u5b9a ,\u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb\u8868\u8f93\u51fa\uff0c\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u521b\u5efahive\u8fde\u63a5 \u8bbe\u7f6e\u76ee\u6807\u8868\uff0c\u8be5\u8868\u9700\u8981\u5df2\u7ecf\u5728Hive\u4e2d\u521b\u5efa\u597d\uff0c\u5e76\u4e14\u5b57\u6bb5\u4e0e\u672c\u5730\u6587\u4ef6\u4fdd\u6301\u4e00\u81f4; \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c \u6267\u884c\u7ed3\u679c\uff1a\u5411Hive\u8868\u5199\u516510\u6761\u6570\u636e\uff0c\u7528\u65f62min+ \u67e5\u770bHive\u8868\u6570\u636e\uff1a \u8bf4\u660e\uff1a\u5411Hive\u8868\u4e2d\u5199\u5165\u6570\u636e\uff0c\u6bcf\u63d2\u5165\u4e00\u6761\u6570\u636e\u4f1a\u8d77\u4e00\u4e2aMR\u4efb\u52a1\uff0c\u6240\u4ee5\u6548\u7387\u7279\u522b\u4f4e\uff0c\u4e0d\u63a8\u8350\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165HDFS\u6587\u4ef6\u4e4b\u540e\u518d\u8f7d\u5165Hive\u8868","title":"\u5199\u5165Hive\u6570\u636e"},{"location":"Data_Integration/Kettle_8.x/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/Kettle_8.x/#hadoop-cluster","text":"\u9009\u62e9 \u6587\u4ef6 -> \u65b0\u5efa -> \u8f6c\u6362 \uff0c\u70b9\u51fb \u4e3b\u5bf9\u8c61\u6811 \u9875\u7b7e\uff0c\u5728 Hadoop Clusters \u53f3\u952e\u9009\u62e9 New Cluster HDFS\u7684Hostname\u586b\u5199hacluster; JobTracker\u7684Hostname \u586b\u5199 Yarn ResourceManager\u4e3b\u8282\u70b9\u7684IP\uff0c\u7aef\u53e3\u53f7\u662f21066,\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP; ZooKeeper\u7684Hostname \u586b\u5199ZooKeeper\u7684\u4e3b\u8282\u70b9IP\uff0c\u7aef\u53e3\u53f7\u662f24002\uff0c\u5982\u679cResourceManager\u53d1\u751f\u4e3b\u5907\u5207\u6362\uff0c\u9700\u8981\u4fee\u6539IP; Oozie\u7684URL\u586b\u5199oozie WebUI\u7684\u5730\u5740. \u70b9\u51fb \u6d4b\u8bd5","title":"\u521b\u5efaHadoop Cluster"},{"location":"Data_Integration/Kettle_8.x/#hdfs_1","text":"\u4ee5\u672c\u5730\u6587\u4ef6 -> HDFS\u4e3a\u4f8b \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u6587\u672c\u6587\u4ef6\u8f93\u5165 \uff0c\u548c Big Data -> Hadoop File Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u6587\u672c\u6587\u4ef6\u8f93\u5165\u914d\u7f6e\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u914d\u7f6e \u4fee\u6539Hadoop File Output\u914d\u7f6e \u53cc\u51fb Hadoop File Output \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\u4e0b\uff0c Hadoop Cluster \u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u96c6\u7fa4\uff0c Folder/File \u9009\u62e9hdfs\u76ee\u5f55\uff0c\u6587\u4ef6\u540d\u53ef\u4ee5\u4efb\u610f\u6307\u5b9a \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u8bbe\u7f6e\u5206\u9694\u7b26\uff0c\u52fe\u9009 \u5feb\u901f\u6570\u636e\u5b58\u50a8\uff08\u65e0\u683c\u5f0f\uff09 \uff08\u5426\u5219\u4fdd\u5b58\u7684\u6587\u4ef6\u4e2d\u4f1a\u6309\u5b57\u6bb5\u957f\u5ea6\u586b\u5145\u7a7a\u683c\uff09 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5\uff0c\u5e76\u8bbe\u7f6e\u6700\u5c0f\u5bbd\u5ea6 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770bHDFS\u6587\u4ef6","title":"\u5bfc\u5165HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/#hdfs_2","text":"\u4ee5HDFS -> Excel\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahdfs2excel.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> Hadoop File Input \u548c \u8f93\u51fa -> Microsoft Excel \u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 Hadoop File Input\u914d\u7f6e \u53cc\u51fb Hadoop File Input \u6b65\u9aa4\uff0c \u6587\u4ef6 \u9875\u7b7e\uff0c\u9009\u62e9\u5f85\u5bfc\u51fa\u7684\u6587\u4ef6\uff0c\u6587\u4ef6\u7c7b\u578b\u652f\u6301CSV\uff08txt\u4e5f\u53ef\u4ee5\uff09\u548cFixed\uff08\u56fa\u5b9a\u5217\u5bbd\uff09 \u70b9\u51fb \u5185\u5bb9 \u9875\u7b7e\uff0c\u9009\u62e9\u6587\u4ef6\u7c7b\u578b\u3001\u5206\u9694\u7b26\u3001\u7f16\u7801\u65b9\u5f0f\u7b49 \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539Microsoft Excel\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb Microsoft Excel \u8f93\u51fa \u6b65\u9aa4\uff0c\u9009\u62e9\u6587\u4ef6\u4fdd\u5b58\u4f4d\u7f6e\u548c\u6587\u4ef6\u540d \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u83b7\u53d6\u5b57\u6bb5 \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684excel\u6587\u4ef6","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/#hbase","text":"","title":"\u5bf9\u63a5HBase"},{"location":"Data_Integration/Kettle_8.x/#hbase_1","text":"\u4ee5\u672c\u5730\u6587\u4ef6 -> HBase\u4e3a\u4f8b \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 \u8f93\u5165 -> \u6587\u672c\u6587\u4ef6\u8f93\u5165 \uff0c\u548c Big Data -> HBase Output \u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u6587\u672c\u6587\u4ef6\u8f93\u5165\u914d\u7f6e\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u914d\u7f6e\uff0c\u6ce8\u610f\u5728\u96c6\u7fa4HBase\u4e2d\u8981\u6709\u548c\u5bfc\u5165\u7684\u8868\u76f8\u540c\u7684\u7a7a\u8868\uff0c\u6307\u660e\u5b57\u6bb5\u548c\u5217\u7c07. \u4fee\u6539 HBase Output \u914d\u7f6e \u53cc\u51fb HBase Output \u6b65\u9aa4\uff0c\u5728 Configure connection \u9875\u7b7e\u4e0b\uff0c\u9009\u62e9\u5df2\u7ecf\u914d\u7f6e\u597d\u7684Hadoop\u96c6\u7fa4\uff0c\u70b9\u51fb Get table name \uff0c\u83b7\u53d6\u8981\u8f93\u51fa\u7684\u8868,\u70b9\u51fb Get mapping for specified table \u83b7\u53d6\u8be5\u8868\u5bf9\u5e94\u7684mapping. \u82e5\u8be5\u8868\u6ca1\u6709\u521b\u5efamapping,\u5728 Create/Edit Mappings \u9875\u7b7e\u521b\u5efamapping,\u6307\u5b9a\u5404\u9879\u5c5e\u6027 \u70b9\u51fb \u786e\u5b9a \uff0c\u4fdd\u5b58\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae,\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c\uff1a \u67e5\u770b\u96c6\u7fa4\u4e2d\u7684HBase\u6587\u4ef6 \u6267\u884c hbase shell count 'customer'","title":"\u5bfc\u5165HBASE\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/#hbase_2","text":"\u4ee5HBase -> \u6587\u672c\u6587\u4ef6\u4e3a\u4f8b \u65b0\u5efa\u8f6c\u6362\uff0c\u4fdd\u5b58\u4e3ahbase.ktr \u6dfb\u52a0\u8f6c\u6362\u6b65\u9aa4 \u5728 \u6838\u5fc3\u5bf9\u8c61 \u9875\u7b7e\u4e0b\uff0c\u62d6\u52a8 Big Data -> HBase Input \u548c \u8f93\u51fa -> \u6587\u672c\u6587\u4ef6\u8f93\u51fa \uff0c\u4e24\u4e2a\u6b65\u9aa4\u5230\u5de5\u4f5c\u533a\uff0c\u5e76\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u3002 \u4fee\u6539 HBase Input\u914d\u7f6e \u53cc\u51fb HBase Input \u6b65\u9aa4\uff0c\u5728 Configure query \u9875\u7b7e\uff0c\u9009\u62e9\u5df2\u7ecf\u8fde\u63a5\u597d\u7684Hadoop\u96c6\u7fa4\uff0c\u82e5\u65e0\u5df2\u7ecf\u8fde\u63a5\u7684\u96c6\u7fa4\uff0c\u70b9\u51fb new ,\u53c2\u7167\u4e0a\u9762\u7ae0\u8282Hadoop\u96c6\u7fa4\u914d\u7f6e\uff0c\u914d\u7f6e\u8fde\u63a5\u96c6\u7fa4; \u5728 Create/Edit Mappings \u9875\u7b7e\uff0c\u70b9\u51fb Get table names ,\u83b7\u53d6\u96c6\u7fa4\u4e2d\u7684Hbase\u8868\uff0c\u9009\u62e9\u8981\u8bfb\u53d6\u7684\u8868\uff0c\u5728 Mapping name \u4e0b\u62c9\u9009\u62e9\u4e0e\u8be5\u8868\u5173\u8054\u7684map\uff0c\u82e5\u6ca1\u6709\uff0c\u81ea\u5b9a\u4e49\u4e00\u4e2amap\u7684\u540d\u5b57\uff0c\u586b\u5199\u5b57\u6bb5\u548c\u5217\u7c07\uff0c\u5e76\u6307\u5b9a\u5b57\u6bb5\u662f\u5426\u4e3akey\uff0c\u5b57\u6bb5\u7c7b\u578b. \u56de\u5230 Configure query \u9875\u7b7e,\u70b9\u51fb Get mapped table names ,\u9009\u62e9\u8981\u8bfb\u53d6\u7684\u8868\uff0c\u70b9\u51fb Get mappings for the specified table \u83b7\u53d6\u8be5\u8868\u5bf9\u5e94\u7684mapping\uff0c\u70b9\u51fb\u53f3\u4e0b\u89d2 Get Key/Feilds Info \uff0c\u83b7\u53d6\u5bf9\u5e94\u7684\u8868\u7684\u4fe1\u606f. \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u4fee\u6539\u6587\u672c\u6587\u4ef6\u8f93\u51fa\u914d\u7f6e \u53cc\u51fb \u6587\u672c\u6587\u4ef6\u8f93\u51fa \u6b65\u9aa4\uff0c\u5728 \u6587\u4ef6 \u9875\u7b7e\uff0c\u586b\u5199\u6587\u4ef6\u540d\u548c\u6269\u5c55\u540d; \u70b9\u51fb \u5b57\u6bb5 \u9875\u7b7e\uff0c\u70b9\u51fb \u83b7\u53d6\u5b57\u6bb5 ,\u8bbe\u7f6e\u6700\u5c0f\u5bbd\u5ea6(\u53ef\u9009) \u70b9\u51fb \u786e\u5b9a \u6309\u94ae\uff0c\u4fdd\u5b58\u914d\u7f6e \u8fd0\u884c\u8f6c\u6362 \u4fdd\u5b58\u914d\u7f6e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u542f\u52a8\u8f6c\u6362 \u6267\u884c\u7ed3\u679c \u67e5\u770b\u5bfc\u51fa\u7684\u6587\u4ef6","title":"\u8bfb\u53d6HBASE\u6587\u4ef6"},{"location":"Data_Integration/Kettle_8.x/#linux","text":"","title":"Linux\u5e73\u53f0"},{"location":"Data_Integration/Kettle_8.x/#_3","text":"\u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf \u5b89\u88c5RedHat 6.5 \u7981\u7528\u9632\u706b\u5899\uff0cSELinux \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u4f7f\u7528 vi /etc/hosts \u6dfb\u52a0\u672c\u5730\u4e3b\u673a\u540d\u89e3\u6790 \u8282\u70b9IP host1 \u8282\u70b9IP host2 \u8282\u70b9IP host3 \u82e5\u662f\u684c\u9762\u7248\u64cd\u4f5c\u7cfb\u7edf\uff0cKettle\u5bf9\u63a5\u53c2\u7167\u4e0a\u9762\u7ae0\u8282Windows\u7cfb\u7edf\u4e0b\u7684\u5bf9\u63a5\u65b9\u5f0f. \u53c2\u8003\u4e0a\u9762\u7ae0\u8282\uff0c\u5728\u6709\u56fe\u5f62\u754c\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e0b\uff0c\u914d\u7f6e\u597dKettle\u4e0eFi\u96c6\u7fa4\u7684\u8fde\u63a5\uff0c\u6d4b\u8bd5\u8fde\u901a\u6027,\u5c06Kettle\u7684 data-integration \u76ee\u5f55\u4ee5\u53ca\u5176\u4e0b\u6240\u6709\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u7684 opt \u76ee\u5f55\u4e0b. \u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef \u4e0b\u8f7d\u5b8c\u6574\u5ba2\u6237\u7aef\uff0c\u5b89\u88c5\u81f3\u76ee\u5f55 /opt/hadoopclient \u4f7f\u7528 vi /etc/profile \u7f16\u8f91\u4ee5\u4e0b\u5185\u5bb9\u63d2\u5165\u5230\u6587\u4ef6\u672b\u5c3e source /opt/hadoopclient/bigdata_env \u5c06krb5.conf\u653e\u5728/etc\u76ee\u5f55\u4e0b cp /opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf /etc/","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Kettle_8.x/#hive_4","text":"","title":"Hive\u5bf9\u63a5"},{"location":"Data_Integration/Kettle_8.x/#hive_5","text":"\u4ee5Hive->\u6587\u672c\u6587\u4ef6\u4e3a\u4f8b \u5728\u6709\u56fe\u5f62\u754c\u9762\u7684\u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u65b0\u5efa\u4e00\u4e2a\u8f6c\u6362\uff0c\u5728\u5de5\u4f5c\u533a\u4e2d\u653e\u5165 \u8868\u8f93\u5165 \u548c \u6587\u672c\u6587\u4ef6\u8f93\u51fa \uff0c\u4fdd\u5b58\u4e3ahive.ktr; \u70b9\u51fb \u8868\u8f93\u5165 \uff0c\u53c2\u8003\u4e0a\u9762\u7ae0\u8282\u4e2d\u5173\u4e8eHive\u8fde\u63a5\u7684\u914d\u7f6e\uff0c\u53ea\u9700\u4fee\u6539\u8fde\u63a5\u9009\u9879\u4e2d user.keytab \u6587\u4ef6\u6240\u5728\u8def\u5f84\uff0c\u4fee\u6539\u4e3a /etc/user.keytab \u5c06hive.ktr\u8f6c\u6362\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u4e0bKettle\u7684 data-integration \u6587\u4ef6\u5939\u4e0b\uff0c \u6839\u636eKettle\u7248\u672c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 cd /opt/data-integration/ \u5bf9\u4e8eKettle-8.0\u7248\u672c,\u6267\u884c\u4ee5\u4e0b\u811a\u672c\u6e05\u9664cache\uff08\u53c2\u89c1FAQ1\uff09 sed -i \"s/^org.pentaho\\.clean\\.karaf\\.cache=false/org\\.pentaho\\.clean\\.karaf\\.cache=true/g\" /opt/data-integration/system/karaf/etc/custom.properties \u53ef\u5c06\u5176\u4fdd\u5b58\u4e3a\u811a\u672c\u6587\u4ef6\uff0c\u6bcf\u6b21\u6267\u884c\u547d\u4ee4\u524d\u5148\u6267\u884c\u8be5\u811a\u672c \u7136\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fd0\u884c\u7a0b\u5e8f ./kitchen.sh -file=hive.ktr \u5bf9\u4e8eKettle-8.1\u7248\u672c,\u624b\u52a8\u5220\u9664 /data-integration/system/karaf/caches/pan/data-1 \u76ee\u5f55\u4e0b\u7684cache\u6587\u4ef6 \u7136\u540e\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u8fd0\u884c\u7a0b\u5e8f ./pan.sh -file=hive.ktr * \u6267\u884c\u7ed3\u679c\u5982\u4e0b \u5bfc\u51fa\u7684\u8868\u5728 data-integration/ \u76ee\u5f55\u4e0b","title":"\u5bfc\u51faHive\u8868"},{"location":"Data_Integration/Kettle_8.x/#hive_6","text":"\u540cWindows\u64cd\u4f5c\u7cfb\u7edf\u4e0b\u521b\u5efaktr\u6587\u4ef6\u64cd\u4f5c\uff0c\u5728\u9009\u62e9\u9700\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u65f6\uff0c\u4fee\u6539\u672c\u5730\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u5728Hive\u8fde\u63a5\u9009\u9879\u914d\u7f6e\u4fee\u6539\u4e2d user.keytab \u6587\u4ef6\u7684\u8def\u5f84\u4e3a /etc/user.keytab \u5373\u53ef\uff0c\u5c06ktr\u6587\u4ef6\u7f6e\u4e8eLinux\u7cfb\u7edf\u4e2d data-integration \u6587\u4ef6\u5939\u4e0b\uff0c\u6267\u884c\u547d\u4ee4\u540c\u4e0a\u5c0f\u8282\u4e2d\u64cd\u4f5c\u3002","title":"\u4e0a\u4f20\u6587\u4ef6\u81f3Hive"},{"location":"Data_Integration/Kettle_8.x/#hdfs-hbase","text":"\u5c06\u4e0a\u9762\u7ae0\u8282\u521b\u5efa\u7684ktr\u8f6c\u6362\u6587\u4ef6\u4e0a\u4f20\u81f3Linux\u7cfb\u7edf\u4e0bKettle\u7684 data-integration \u6587\u4ef6\u5939\u4e0b\uff0c\u6839\u636eKettle\u7248\u672c\u6267\u884c\u547d\u4ee4(\u540chive)\u5373\u53ef \u6267\u884c\u7ed3\u679c\u5982\u4e0b \u5bfc\u51fa\u7684\u8868\u5728 data-integration/ \u76ee\u5f55\u4e0b","title":"HDFS &amp; HBase\u6587\u4ef6\u8f93\u51fa"},{"location":"Data_Integration/Kettle_8.x/#hdfs-hbase_1","text":"\u540c \u4e0a\u4f20\u6587\u4ef6\u81f3Hive \u64cd\u4f5c\uff0c\u4fee\u6539\u672c\u5730\u6587\u4ef6\u8def\u5f84\u5373\u53ef\u3002","title":"\u4e0a\u4f20\u6587\u4ef6\u81f3HDFS &amp; HBase"},{"location":"Data_Integration/Kettle_8.x/#faq","text":"1.\u5728Linux\u7cfb\u7edf\u4e2d\uff0c\u6bcf\u6267\u884c\u4e00\u6b21\u8f6c\u6362\u6216\u8005\u4efb\u52a1\uff0cKettle\u90fd\u4f1a\u751f\u6210\u4e00\u4e9bCache\u6587\u4ef6\uff0c\u5728\u6267\u884c\u4e0b\u4e00\u6b21\u8f6c\u6362/\u4efb\u52a1\u4e4b\u524d\uff0c\u9700\u8981\u6e05\u9664\u8fd9\u4e9bCache\uff0c\u5426\u5728HDFS Hive \u548cHBase\u8fdb\u884c\u8fde\u63a5\u4f20\u8f93\u65f6\u4f1a\u51fa\u9519","title":"FAQ"},{"location":"Data_Integration/Knime/","text":"Knime \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Knime 3.6.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/Spark) Knime 4.1.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive/Spark) Knime 4.1.0 \u2194 FusionInsight MRS 8.0 (HDFS/Hive) \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001Hive\u3001Spark2x\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06krb5.conf\u548cuser.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u96c6\u7fa4->\u66f4\u591a->\u4e0b\u8f7d\u5ba2\u6237\u7aef \uff0c\u9009\u62e9\u201c\u4ec5\u914d\u7f6e\u6587\u4ef6\u201d\u4e0b\u8f7d\u96c6\u7fa4\u7684\u914d\u7f6e\u6587\u4ef6\u81f3\u672c\u5730\u5e76\u89e3\u538b\u3002\u5e76\u5c06\u89e3\u538b\u540e\u7684 ..\\FusionInsight_Cluster_1_Services_ClientConfig_ConfigFiles\\HDFS\\config \u76ee\u5f55\u7684 core-site.xml \u548c hdfs-site.xml \u653e\u5728 C:\\ecotesting\\hadoopConfig \u76ee\u5f55\u4e0b\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f\u3002 \u5b89\u88c5\u5e76\u914d\u7f6eJDK \u5b89\u88c5JDK8 \u65b0\u589e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728\u7cfb\u7edf\u73af\u5883\u53d8\u91cfPATH\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; Knime\u5b89\u88c5\u548c\u914d\u7f6e \u00b6 \u4e0b\u8f7dKnime \u00b6 \u4eceKnime\u5b98\u7f51 https://www.knime.com/downloads/download-knime \u4e0b\u8f7d\u64cd\u4f5c\u7cfb\u7edf\u5bf9\u5e94\u7684\u7248\u672c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002\u672c\u6587\u4f7f\u7528\u7684\u662f64\u4f4d\u7684Knime Analytics Platform for Windows (installer)\u3002 \u5b89\u88c5Knime extension \u00b6 \u5b89\u88c5\u5e76\u542f\u52a8Knime Analytics Platform\uff0c\u70b9\u51fb\u83dc\u5355\u680f File->Install Knime extensions \u3002\u641c\u7d22 big data \uff0c\u5728\u7ed3\u679c\u4e2d\u9009\u62e9 KNIME Big Data Extensions \u3002\u7136\u540e\u70b9\u51fb next \u3002 \u9009\u62e9 accept licence \uff0c\u70b9\u51fb finish \u5f00\u59cb\u5b89\u88c5\u3002 \u5728\u53f3\u4e0b\u89d2\u53ef\u4ee5\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6\u3002 \u8bf4\u660e\uff1a\u7531\u4e8e\u7f51\u7edc\u539f\u56e0\u5b89\u88c5\u5931\u8d25\u65f6\uff0c\u53ef\u91cd\u590d\u4ee5\u4e0a\u6b65\u9aa4\u76f4\u81f3\u5b89\u88c5\u6210\u529f\u4e3a\u6b62\u3002 \u5b89\u88c5\u5b8c\u6210\u540e\u91cd\u542fKnime Analytics Platform\u3002 \u914d\u7f6eKnime \u00b6 \u5728Knime\u7684\u5b89\u88c5\u76ee\u5f55\u4e2d\uff0c\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u201cknime.ini\u201d\uff0c\u5728\u672b\u5c3e\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9\uff1a -Djava.security.krb5.conf=C:\\developuser\\krb5.conf \u91cd\u542fKnime Analytics Platform\uff0c\u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preferences->KNIME->Big Data->Hadoop \uff0c\u5728 Hadoop Configuration \u4e2d\u586b\u5165\u672c\u5730\u4fdd\u5b58\u7684HDFS\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6\u8def\u5f84\uff0c\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preferences->KNIME->Kerberos \uff0c\u586b\u5165\u5982\u4e0b\u914d\u7f6e\u540e\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 \u201cKerberos Configuration\u201d\u9009\u62e9 Use Kerberos client configuration file (krb5.conf) \u5e76\u8f93\u5165\u6587\u4ef6\u8def\u5f84 C:\\developuser\\krb5.conf \u201cHow to log in\u201d\u9009\u62e9 With keytab \u5e76\u586b\u5165kerberos\u8ba4\u8bc1\u7528\u6237\u540d developuser \u548c\u672c\u5730keytab\u6587\u4ef6\u7684\u8def\u5f84 C:\\developuser\\user.keytab \uff0c Knime\u8fde\u63a5HDFS \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1 \u5efa\u7acbHDFS\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow \uff0c\u547d\u540d\u4e3a\u201cHDFSConnection\u201d\u540e\u4fdd\u5b58\u3002 \u5728Node Repository\u4e2d\u641c\u7d22 hdfs \u3002 \u5c06 HDFS Connection \u8282\u70b9\u62d6\u5165\u201cHDFSConnection\u201d\u5de5\u4f5c\u533a\u3002 \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\uff1a Host: HDFS\u7684NameNode\u4e3b\u8282\u70b9IP Port: 25000 Authentication: Kerberos \u70b9\u51fb Test connection \uff0c\u663e\u793a\u5982\u4e0b\uff0c\u8868\u793a\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u8bfb\u53d6HDFS\u6587\u4ef6 \u00b6 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 Download \u8282\u70b9\uff0c\u5c06\u5176\u4e0e HDFS Connection \u76f8\u8fde\u3002 \u53cc\u51fb Download \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4eceHDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u7684\u6587\u4ef6\uff08\u4f8b\u5982 /temp.csv \uff09\u4ee5\u53ca\u6587\u4ef6\u7684\u672c\u5730\u4fdd\u5b58\u8def\u5f84\u3002 \u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1\u3002 \u8bf4\u660e\uff1a\u5982\u679c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4efb\u52a1\uff0c\u9009\u4e2d\u8282\u70b9\uff0c\u53f3\u952e\u9009\u62e9\u201cReset\u201d\u540e\u8be5\u8282\u70b9\u70b9\u5373\u53ef\u91cd\u65b0\u6267\u884c\u3002 \u67e5\u770b temp.csv \u5df2\u4e0b\u8f7d\u81f3\u672c\u5730\u6307\u5b9a\u76ee\u5f55\u3002 \u4e0a\u4f20\u6587\u4ef6\u81f3HDFS \u00b6 \u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u653e\u5728\u672c\u5730\u7684\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u4f8b\u5982 C:\\KnimeData \u3002 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 List Files , String to URI \u4ee5\u53ca Upload \u8282\u70b9\uff0c\u5c06\u5176\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5\u3002 \u53cc\u51fb List Files \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4e0a\u4f20\u6587\u4ef6\u7684\u672c\u5730\u8def\u5f84\uff0c\u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb Upload \u8282\u70b9\uff0c\u9009\u62e9\u5728HDFS\u4e2d\u6587\u4ef6\u4fdd\u5b58\u7684\u8def\u5f84\uff0c\u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1\u3002 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770bHDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u6240\u4e0a\u4f20\u7684\u6587\u4ef6\u3002 Knime\u8fde\u63a5Hive \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1 \u5efa\u7acbHive\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow \uff0c\u547d\u540d\u4e3a\u201cHiveConnection\u201d\u540e\u4fdd\u5b58\u3002 \u5728\u201cHiveConnection\u201d\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Hive Connector \u8282\u70b9\u3002 \u53cc\u51fb Hive Connector \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\uff1a Hostname: Hive\u7684HiveServer\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9 Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos \u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u5199\u5165Hive\u8868 \u00b6 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u8282\u70b9\uff0c\u5e76\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5\u3002 \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 Host: HDFS\u7684NameNode\u4e3b\u8282\u70b9IP Port: 25000 Authentication: Kerberos \u53cc\u51fb File Reader \u8282\u70b9\uff0c\u9009\u62e9\u672c\u5730\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\uff0c\u4f8b\u5982Knime\u5de5\u4f5c\u533a C:\\Users\\wangna\\knime-workspace \u4e2d\u5df2\u9ed8\u8ba4\u4e0b\u8f7d\u4e0b\u6765\u7684 C:\\Users\\wangna\\knime-workspace\\Example Workflows\\TheData\\Basics\\adult.csv \u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb Hive Loader \u8282\u70b9\uff0c\u9009\u62e9\u6587\u4ef6\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u7684\u8def\u5f84\u4ee5\u53ca\u8868\u540d\uff0c\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1\u3002 \u5728FusionInsight\u5ba2\u6237\u7aef\u4f7f\u7528beeline\u67e5\u770b\u5bfc\u5165Hive\u4e2d\u7684\u8868\u3002 Knime\u8fde\u63a5Spark2x \u00b6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u8bf4\u660e\uff1aFusionInsight\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient \uff0c\u4e14\u80fd\u4f7f\u7528spark-submit\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u3002 \u5b89\u88c5Spark Job Server \u00b6 \u8bf4\u660e\uff1a\u4ee5\u4e0b\u64cd\u4f5c\u6b65\u9aa4\u4ee5CentOS 7.x\u4e3a\u4f8b\uff0c\u5176\u4ed6\u64cd\u4f5c\u7cfb\u7edf\u8bf7\u53c2\u8003KNIME\u5b98\u65b9\u6587\u6863 https://download.knime.org/store/3.6/knime_extension_for_apache_spark_2.3.0.pdf \u6267\u884c\u76f8\u5e94\u7684\u547d\u4ee4\u3002 \u4ece https://docs.knime.com/latest/bigdata_extensions_admin_guide/index.html#_overview \u7684\u201cSpark Jobserver downloads\u201d\u7ae0\u8282\uff0c\u6839\u636e\u96c6\u7fa4\u4ee5\u53ca\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u83b7\u53d6\u5bf9\u5e94\u7684 Spark Job Server \u5b89\u88c5\u5305\u3002\u57fa\u4e8eFusionInsight HD 6.5.1\u7684Spark2x\u7684\u7248\u672c\u4e3a2.3.2\uff0c\u6545\u9009\u62e9\u4e0b\u8f7d CDH 5.9 - 5.15 (Apache Spark 2.3) \uff0c\u4e0b\u8f7d\u6587\u4ef6\u540d\u4e3a spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh.tar.gz \u3002 \u5c06\u4e0b\u8f7d\u7684 spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh.tar.gz \u4e0a\u4f20\u81f3\u5df2\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\u8282\u70b9\uff0c\u4f8b\u5982 /opt \u76ee\u5f55\u4e0b\u3002\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5\u914d\u7f6e\uff1a LINKNAME=spark2-job-server tar -xvf /opt/spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh.tar.gz -C /opt ln -s /opt/spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh /opt/${LINKNAME} useradd -d /opt/${LINKNAME}/ -M -r -s /bin/false spark-job-server su -l -c \"hdfs dfs -mkdir -p /user/spark-job-server ; hdfs dfs -chown -R spark-job-server /user/spark-job-server\" spark-job-server chown -R spark-job-server:spark-job-server /opt/${LINKNAME} /opt/spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh \u914d\u7f6eSpark Job Server \u00b6 \u5c06 C:\\developuser\\krb5.conf \u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /etc \u76ee\u5f55\u4e0b\u3002\u542f\u52a8spark job server\u65f6\u8c03\u7528\u201ckerberos-ticket-renewer.sh\u201d\u4ea7\u751f\u7968\u636e\u4f9d\u8d56\u4e8e /etc/krb5.conf \u6587\u4ef6\u3002 \u5c06 C:\\developuser\\user.keytab \u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt/spark2-job-server \u76ee\u5f55\u4e0b\u3002 \u4fee\u6539 /opt/spark2-job-server/spark-job-server-init.d \u8bbe\u7f6eJava\u548cSpark\u8fd0\u884c\u7684\u73af\u5883\u53d8\u91cf\u3002\u5728\u201cjobserver_start()\u201d\u5f00\u59cb\u5904\u65b0\u589e source /opt/hadoopclient/bigdata_env \u3002 \u4fee\u6539 /opt/spark2-job-server/environment.conf \u6587\u4ef6\u3002\u8bbe\u7f6e master = \"yarn-client\" \uff0c\u4ee5yarn-client\u6a21\u5f0f\u8fd0\u884cspark\u3002 \u4fee\u6539 /opt/spark2-job-server/settings.sh \u6587\u4ef6\u3002 \u8bbe\u7f6eSPARK_HOME\u3002\u7531\u4e8e\u4e0d\u5b58\u5728\u76ee\u5f55 /opt/cloudera/parcels/SPARK2/ \uff0c\u6545else\u8bed\u53e5\u7684SPARK_HOME\u751f\u6548\u3002\u5728else\u8bed\u53e5\u4e2d\u8bbe\u7f6e SPARK_HOME=/opt/hadoopclient/Spark2x/spark \u3002 \u5c06 HADOOP_CONF_DIR \u7684\u8bbe\u7f6e\u6ce8\u91ca\u6216\u5220\u9664\u3002 \u8bf4\u660e\uff1ayarn-client\u6a21\u5f0f\u8fd0\u884cspark\u9700\u8981\u4ece\u53c2\u6570HADOOP_CONF_DIR\u8bbe\u7f6e\u7684\u76ee\u5f55\u52a0\u8f7d\u76f8\u5e94\u7684\u73af\u5883\u53d8\u91cf\u548c\u914d\u7f6e\u6587\u4ef6\u3002\u542f\u52a8spark job server(\u6267\u884cspark-job-server-init.d start\u6216\u8005systemctl start spark2-job-server)\u65f6\u6267\u884c\u547d\u4ee4\u201csource /opt/hadoopclient/bigdata_env\u201d\u5df2\u8bbe\u7f6eHADOOP_CONF_DIR\u4e3a\u6b63\u786e\u7684\u503c\u3002\u56e0\u6b64\u9700\u8981\u5c06settings.sh\u6587\u4ef6\u4e2dHADOOP_CONF_DIR\u8bbe\u7f6e\u7684\u503c\u6ce8\u91ca\u4ee5\u514d\u5c06\u6b63\u786e\u7684\u503c\u8986\u76d6\uff0c\u5bfc\u81f4\u9519\u8bef\u3002 \u8bbe\u7f6e JOBSERVER_KEYTAB \u548c JOBSERVER_PRINCIPAL \u3002 export JOBSERVER_KEYTAB=/opt/spark2-job-server/user.keytab export JOBSERVER_PRINCIPAL=developuser@HADOOP.COM \u767b\u5f55FusionInsight Manager\uff0c\u4fee\u6539HDFS\u7684core-site.xml\u6587\u4ef6\u914d\u7f6e\u3002 \u4e3b\u9875\u9762\u9009\u62e9 \u96c6\u7fa4->HDFS \uff0c\u5728\u5de6\u4fa7\u9009\u62e9 \u914d\u7f6e->\u5168\u90e8\u914d\u7f6e->\u81ea\u5b9a\u4e49 \uff0c\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570 hadoop.proxyuser.spark-job-server.hosts = * hadoop.proxyuser.spark-job-server.groups = * \u4fdd\u5b58\u914d\u7f6e\uff0c\u5e76\u91cd\u542fHDFS\u4ee5\u53ca\u4f9d\u8d56\u7684\u670d\u52a1\u3002 \u542f\u52a8\u548c\u505c\u6b62Spark Job Server \u00b6 \u8bbe\u7f6e\u4ece\u7cfb\u7edf\u542f\u52a8\u6216\u505c\u6b62Spark Job Server\u3002 ln -s /opt/spark2-job-server/spark-job-server-init.d /etc/init.d/spark2-job-server systemctl daemon-reload systemctl enable spark2-job-server \u542f\u52a8Spark Job Server\u3002 systemctl start spark2-job-server ps -ef |grep spark2 \u8bf4\u660e\uff1a \u542f\u52a8Spark Job Server\u7684\u53e6\u5916\u4e00\u79cd\u65b9\u5f0f\u662f\u6267\u884c\u547d\u4ee4 /opt/spark2-job-server/spark-job-server-init.d start \u5728 /tmp \u76ee\u5f55\u4e0b\u53ef\u4ee5\u67e5\u770b\u7f13\u5b58\u7684\u7968\u636e\u548c\u5176\u4ed6\u6570\u636e\u3002 \u5728 /var/log/spark2-job-server \u76ee\u5f55\u4e0b\u53ef\u4ee5\u67e5\u770b\u8fd0\u884c\u7684\u65e5\u5fd7\u3002 \u542f\u52a8\u540e\u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165 http://ip:8090 \uff0c\u5176\u4e2dip\u4e3a\u5b89\u88c5\u8282\u70b9\u7684IP\uff0c\u8fd4\u56de\u4ee5\u4e0b\u754c\u9762\u5219\u8868\u793a\u542f\u52a8\u6210\u529f\u3002 \u505c\u6b62Spark Job Server\u3002 systemctl stop spark2-job-server \u5efa\u7acbSpark\u8fde\u63a5 \u00b6 \u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow \uff0c\u547d\u540d\u4e3a\u201cSparkContext\u201d\u540e\u4fdd\u5b58\u3002\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Create Spark Context \u8282\u70b9\u548c\u4e00\u4e2a Destroy Spark Context \u8282\u70b9\u5e76\u8fde\u63a5\u3002 \u53cc\u51fb Create Spark Context \u8282\u70b9\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e\u3002 \u5728Context Settings\u9875\u9762 Spark version\uff1a\u9009\u62e9\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684Spark\u7248\u672c2.3 Context name\uff1a\u81ea\u5b9a\u4e49Spark Context\u540d\u5b57\u4e3aknimeSparkContext \u5728Connection Settings\u9875\u9762 Jobserver URL\uff1a http://ip:8090/ \uff0c\u5176\u4e2dIP\u4e3aSpark Job Server \u6240\u5728\u8282\u70b9IP Authentication: None \u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fb\u83dc\u5355\u680f \u6309\u94ae\uff0c\u6d4b\u8bd5\u8fde\u63a5\u662f\u5426\u6709\u9519\uff0c\u82e5\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u8282\u70b9\u914d\u7f6e\u65e0\u8bef\u3002 \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00Jobserver URL\u4e2d\u914d\u7f6e\u7684\u5730\u5740\uff0c\u53ef\u4ee5\u8fdb\u5165Spark Job Server UI\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u5efa\u7acb\u7684Spark Context\uff0c\u663e\u793a\u5982\u4e0b\uff1a \u767b\u5f55Funsion Manager\u8fdb\u5165Yarn ResourceManager WebUI\u67e5\u770bapplication\u7684\u65e5\u5fd7\uff0cFinalStatus\u663e\u793a\u4e3a SUCCEEDED \u3002 Spark\u5e94\u7528\u5b9e\u4f8b \u00b6 Spark\u5e94\u7528\u5b9e\u4f8b\u4e0b\u8f7d\u5730\u5740 https://www.knime.com/nodeguide/big-data/spark-executor \u3002 Hive to Spark to Hive \u00b6 \u4e0b\u8f7d\u5b9e\u4f8b\u201cHive to Spark to Hive\u201d\uff0c\u4e0b\u8f7d\u6587\u4ef6\u540d\u4e3a 05_Hive_to_Spark_to_Hive.knwf \u3002 \u70b9\u51fbKNIME Analytics Platform\u7684\u83dc\u5355\u680f File->Import KNIME Workflow \u5bfc\u5165 05_Hive_to_Spark_to_Hive.knwf \u3002 \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 Host: HDFS\u7684NameNode\u4e3b\u8282\u70b9IP Port: 25000 Authentication: Kerberos \u53cc\u51fb File Reader \u8282\u70b9\uff0c\u9009\u62e9\u5c06\u8981\u4e0a\u4f20\u7684\u672c\u5730\u6587\u4ef6\uff0c\u4f8b\u5982Knime\u5de5\u4f5c\u533a C:\\Users\\wangna\\knime-workspace \u4e2d\u5df2\u9ed8\u8ba4\u4e0b\u8f7d\u4e0b\u6765\u7684 C:\\ecotesting\\knime-workspace\\Example Workflows\\TheData\\Customers\\ContractData.csv \u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb Hive Connector \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 Hostname: Hive\u7684HiveServer\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u7684IP Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos \u53cc\u51fb Hive Loader \u8282\u70b9\uff0c\u9009\u62e9\u6587\u4ef6\u8981\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u8def\u5f84\u4ee5\u53ca\u8868\u540d\uff0c\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb Create Spark Context \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u5728Context Settings\u9875\u9762 Spark version\uff1a\u9009\u62e9\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684Spark\u7248\u672c2.3 Context name\uff1a\u81ea\u5b9a\u4e49Spark Context\u540d\u5b57\u4e3aknimeSparkContext \u5728Connection Settings\u9875\u9762 Jobserver URL\uff1a http://ip:8090/ \uff0c\u5176\u4e2dIP\u4e3aSpark Job Server \u6240\u5728\u8282\u70b9IP Authentication: None Hive to Spark \u3001 Spark to Hive \u548c Database Connection Table Reader \u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e\u4e0d\u53d8\u3002\u53cc\u51fb Spark to Hive \u8282\u70b9\u53ef\u67e5\u770b\u521b\u5efa\u7684Hive\u8868\u7684\u201cTable name\u201d\u4e3a knimeTest \u3002 \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1\u3002 \u767b\u5f55Funsion Manager\u8fdb\u5165Yarn ResourceManager WebUI\u67e5\u770bapplication\u7684\u65e5\u5fd7\uff0cState\u663e\u793a\u4e3a RUNNING \u3002 \u5728FusionInsight\u5ba2\u6237\u7aef\u4f7f\u7528beeline\u67e5\u770b\u5bfc\u5165Hive\u4e2d\u7684\u8868\u3002\u901a\u8fc7 Hive Loader \u8282\u70b9\u5bfc\u5165\u7684\u8868 contactdata \u4ee5\u53ca Spark to Hive \u8282\u70b9\u5bfc\u5165\u7684\u8868 knimetest \u5747\u5df2\u5bfc\u5165Hive\u3002 select count(*) from contactdata; select count(*) from knimetest; FAQ \u00b6 \u542f\u52a8Spark Job Server\u65f6\uff0c\u8fd4\u56dekinit: command not found \u00b6 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c/opt/spark2-job-server/spark-job-server-init.d start\u65f6\u8fd4\u56de kinit: command not found\u3002 \u5728Knime Analytics Platform\u4e2d\u8fd0\u884cCreate Spark Context(Jobserver)\u65f6\u5931\u8d25\uff0c\u8fd4\u56de Possible reason: Incompatible Jobserver version, malconfigured Spark Jobserver\u3002 \u67e5\u770b\u8fd0\u884c\u65e5\u5fd7/var/log/spark2-job-server/jobserver-knimeSparkContext7418932986808208865/spark-job-server.log\u8fd4\u56de\u4ee5\u4e0b\u9519\u8bef\uff1a DestHost:destPort 172-16-4-22:26004 , LocalHost:localPort 172-16-5-105/172.16.5.105:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] , while invoking ApplicationClientProtocolPBClientImpl.getNewApplication over 167 after 2 failover attempts. Trying to failover after sleeping for 43502ms. \u3010\u95ee\u9898\u5206\u6790\u3011 \u4ecespark-job-server-init.d\u53ef\u4ee5\u770b\u51fa\u4f7f\u7528spark-job-server\u7528\u6237\u542f\u52a8Spark Job Server\u3002spark-job-server\u7528\u6237\u6ca1\u6709\u6743\u9650\u4f7f\u7528FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u7684kinit\u3002\u5982\u679c\u64cd\u4f5c\u7cfb\u7edf\u6ca1\u6709\u5b89\u88c5krb5-workstation\uff0c\u5219\u4f1a\u8fd4\u56dekinit: command not found\uff0c\u65e0\u6cd5\u4ea7\u751f\u8ba4\u8bc1\u7968\u636e\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u65b9\u5f0f\u4e00\uff1a\u6267\u884c yum install krb5-workstation \u5b89\u88c5krb5-workstation\u3002 \u65b9\u5f0f\u4e8c\uff1a\u5728/opt/spark2-job-server/ spark-job-server-init.d\u7684\u201cjobserver_start()\u201d\u5f00\u59cb\u5904\u65b0\u589e source /opt/hadoopclient/bigdata_env \u3002 \u8fd0\u884cCreate Spark Context(Jobserver)\u65f6\u5931\u8d25\uff0c\u8fd4\u56deCan't get Kerberos realm \u00b6 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5f53/opt/spark2-job-server/environment.conf\u8bbe\u7f6emaster = \"yarn-client\"\uff0c\u5728Knime Analytics Platform\u4e2d\u8fd0\u884cCreate Spark Context(Jobserver)\u65f6\u5931\u8d25\uff0c\u8fd4\u56de Yarn application has already ended! It might have been killed or unable to launch application master. \u767b\u5f55Funsion Manager\u8fdb\u5165Yarn ResourceManager WebUI\u67e5\u770bapplication\u7684\u65e5\u5fd7\uff0c\u8fd4\u56de\u9519\u8bef\u5982\u4e0b\uff1a ``` INFO SecurityManager: SecurityManager: authentication enabled; ui acls enabled; users with view permissions: Set(admin, developuser); groups with view permissions: Set(); users with modify permissions: Set(admin, developuser); groups with modify permissions: Set() Exception in thread \"main\" java.lang.IllegalArgumentException: Can't get Kerberos realm at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:65) at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:318) at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:364) at org.apache.spark.deploy.SparkHadoopUtil. (SparkHadoopUtil.scala:53) at org.apache.spark.deploy.SparkHadoopUtil .instance .instance lzycompute(SparkHadoopUtil.scala:409) at org.apache.spark.deploy.SparkHadoopUtil .instance(SparkHadoopUtil.scala:409) at org.apache.spark.deploy.SparkHadoopUtil .instance(SparkHadoopUtil.scala:409) at org.apache.spark.deploy.SparkHadoopUtil .get(SparkHadoopUtil.scala:430) at org.apache.spark.SecurityManager. (SecurityManager.scala:259) at org.apache.spark.deploy.yarn.ApplicationMaster. (ApplicationMaster.scala:69) at org.apache.spark.deploy.yarn.ApplicationMaster .main(ApplicationMaster.scala:811) at org.apache.spark.deploy.yarn.ExecutorLauncher .main(ApplicationMaster.scala:811) at org.apache.spark.deploy.yarn.ExecutorLauncher .main(ApplicationMaster.scala:844) at org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala) Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:110) at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:63) ... 11 more Caused by: KrbException: Cannot locate default realm at sun.security.krb5.Config.getDefaultRealm(Config.java:1029) ... 17 more ``` \u3010\u95ee\u9898\u5206\u6790\u3011 yarn-client\u6a21\u5f0f\u8fd0\u884cspark\u9700\u8981\u4ece\u53c2\u6570HADOOP_CONF_DIR\u8bbe\u7f6e\u7684\u76ee\u5f55\u52a0\u8f7d\u76f8\u5e94\u7684\u73af\u5883\u53d8\u91cf\u548c\u914d\u7f6e\u6587\u4ef6\uff0c \u8be5\u95ee\u9898\u662f\u7531\u4e8eHADOOP_CONF_DIR\u8bbe\u7f6e\u4e0d\u6b63\u786e\u5f15\u8d77\u7684 \u3002 \u5728Knime Analytics Platform\u4e2d\u8fd0\u884cCreate Spark Context(Jobserver)\u65f6\uff0c\u8c03\u7528/opt/spark2-job-server/manager_start.sh\u63d0\u4ea4spark\u4efb\u52a1\u3002 \u5728manager_start.sh\u4e2d\u4f7f\u7528FusionInsight\u5ba2\u6237\u7aef\u7684spark-submit\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u4e4b\u524d\u8c03\u7528\u4e86/opt/spark2-job-server/setting.sh\u3002\u5728/opt/spark2-job-server/setting.sh\u9ed8\u8ba4\u8bbe\u7f6eHADOOP_CONF_DIR=/etc/hive/conf\u3002 \u4f7f\u7528FusionInsight\u5ba2\u6237\u7aef\u7684spark-submit\u63d0\u4ea4\u4efb\u52a1\u65f6\uff0c\u73af\u5883\u53d8\u91cfHADOOP_CONF_DIR\u6b63\u786e\u503c\u5e94\u8bbe\u7f6e\u4e3a/opt/hadoopclient/HDFS/hadoop/etc/hadoop\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5728/opt/spark2-job-server/ spark-job-server-init.d\u7684\u201cjobserver_start()\u201d\u5f00\u59cb\u5904\u65b0\u589e source /opt/hadoopclient/bigdata_env \u3002 \u5728/opt/spark2-job-server/setting.sh\u4e2d \u5c06HADOOP_CONF_DIR\u7684\u8bbe\u7f6e\u6ce8\u91ca\u6216\u8005\u5220\u9664 \u3002\u6216\u8005\u8bbe\u7f6e\u4e3a\u6b63\u786e\u7684\u503c HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u3002","title":"4.1.0 <--> 8.0"},{"location":"Data_Integration/Knime/#knime-fusioninsight","text":"","title":"Knime \u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Knime/#_1","text":"Knime 3.6.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/Spark) Knime 4.1.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive/Spark) Knime 4.1.0 \u2194 FusionInsight MRS 8.0 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Knime/#_2","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001Hive\u3001Spark2x\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06krb5.conf\u548cuser.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u96c6\u7fa4->\u66f4\u591a->\u4e0b\u8f7d\u5ba2\u6237\u7aef \uff0c\u9009\u62e9\u201c\u4ec5\u914d\u7f6e\u6587\u4ef6\u201d\u4e0b\u8f7d\u96c6\u7fa4\u7684\u914d\u7f6e\u6587\u4ef6\u81f3\u672c\u5730\u5e76\u89e3\u538b\u3002\u5e76\u5c06\u89e3\u538b\u540e\u7684 ..\\FusionInsight_Cluster_1_Services_ClientConfig_ConfigFiles\\HDFS\\config \u76ee\u5f55\u7684 core-site.xml \u548c hdfs-site.xml \u653e\u5728 C:\\ecotesting\\hadoopConfig \u76ee\u5f55\u4e0b\u3002 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f\u3002 \u5b89\u88c5\u5e76\u914d\u7f6eJDK \u5b89\u88c5JDK8 \u65b0\u589e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf JAVA_HOME= C:\\\\Program Files\\\\Java\\\\jdk1.8.0_112 \u5728\u7cfb\u7edf\u73af\u5883\u53d8\u91cfPATH\u6dfb\u52a0 %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin;","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Knime/#knime","text":"","title":"Knime\u5b89\u88c5\u548c\u914d\u7f6e"},{"location":"Data_Integration/Knime/#knime_1","text":"\u4eceKnime\u5b98\u7f51 https://www.knime.com/downloads/download-knime \u4e0b\u8f7d\u64cd\u4f5c\u7cfb\u7edf\u5bf9\u5e94\u7684\u7248\u672c\u4e0b\u8f7d\u5e76\u5b89\u88c5\u3002\u672c\u6587\u4f7f\u7528\u7684\u662f64\u4f4d\u7684Knime Analytics Platform for Windows (installer)\u3002","title":"\u4e0b\u8f7dKnime"},{"location":"Data_Integration/Knime/#knime-extension","text":"\u5b89\u88c5\u5e76\u542f\u52a8Knime Analytics Platform\uff0c\u70b9\u51fb\u83dc\u5355\u680f File->Install Knime extensions \u3002\u641c\u7d22 big data \uff0c\u5728\u7ed3\u679c\u4e2d\u9009\u62e9 KNIME Big Data Extensions \u3002\u7136\u540e\u70b9\u51fb next \u3002 \u9009\u62e9 accept licence \uff0c\u70b9\u51fb finish \u5f00\u59cb\u5b89\u88c5\u3002 \u5728\u53f3\u4e0b\u89d2\u53ef\u4ee5\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6\u3002 \u8bf4\u660e\uff1a\u7531\u4e8e\u7f51\u7edc\u539f\u56e0\u5b89\u88c5\u5931\u8d25\u65f6\uff0c\u53ef\u91cd\u590d\u4ee5\u4e0a\u6b65\u9aa4\u76f4\u81f3\u5b89\u88c5\u6210\u529f\u4e3a\u6b62\u3002 \u5b89\u88c5\u5b8c\u6210\u540e\u91cd\u542fKnime Analytics Platform\u3002","title":"\u5b89\u88c5Knime extension"},{"location":"Data_Integration/Knime/#knime_2","text":"\u5728Knime\u7684\u5b89\u88c5\u76ee\u5f55\u4e2d\uff0c\u4fee\u6539\u914d\u7f6e\u6587\u4ef6\u201cknime.ini\u201d\uff0c\u5728\u672b\u5c3e\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9\uff1a -Djava.security.krb5.conf=C:\\developuser\\krb5.conf \u91cd\u542fKnime Analytics Platform\uff0c\u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preferences->KNIME->Big Data->Hadoop \uff0c\u5728 Hadoop Configuration \u4e2d\u586b\u5165\u672c\u5730\u4fdd\u5b58\u7684HDFS\u7684 hdfs-site.xml \u548c core-site.xml \u6587\u4ef6\u8def\u5f84\uff0c\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 File->Preferences->KNIME->Kerberos \uff0c\u586b\u5165\u5982\u4e0b\u914d\u7f6e\u540e\u70b9\u51fb Apply and Close \u4fdd\u5b58\u914d\u7f6e\u3002 \u201cKerberos Configuration\u201d\u9009\u62e9 Use Kerberos client configuration file (krb5.conf) \u5e76\u8f93\u5165\u6587\u4ef6\u8def\u5f84 C:\\developuser\\krb5.conf \u201cHow to log in\u201d\u9009\u62e9 With keytab \u5e76\u586b\u5165kerberos\u8ba4\u8bc1\u7528\u6237\u540d developuser \u548c\u672c\u5730keytab\u6587\u4ef6\u7684\u8def\u5f84 C:\\developuser\\user.keytab \uff0c","title":"\u914d\u7f6eKnime"},{"location":"Data_Integration/Knime/#knimehdfs","text":"","title":"Knime\u8fde\u63a5HDFS"},{"location":"Data_Integration/Knime/#_3","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#hdfs","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow \uff0c\u547d\u540d\u4e3a\u201cHDFSConnection\u201d\u540e\u4fdd\u5b58\u3002 \u5728Node Repository\u4e2d\u641c\u7d22 hdfs \u3002 \u5c06 HDFS Connection \u8282\u70b9\u62d6\u5165\u201cHDFSConnection\u201d\u5de5\u4f5c\u533a\u3002 \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\uff1a Host: HDFS\u7684NameNode\u4e3b\u8282\u70b9IP Port: 25000 Authentication: Kerberos \u70b9\u51fb Test connection \uff0c\u663e\u793a\u5982\u4e0b\uff0c\u8868\u793a\u8fde\u63a5\u6210\u529f\u3002 \u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002","title":"\u5efa\u7acbHDFS\u8fde\u63a5"},{"location":"Data_Integration/Knime/#hdfs_1","text":"\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 Download \u8282\u70b9\uff0c\u5c06\u5176\u4e0e HDFS Connection \u76f8\u8fde\u3002 \u53cc\u51fb Download \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4eceHDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u7684\u6587\u4ef6\uff08\u4f8b\u5982 /temp.csv \uff09\u4ee5\u53ca\u6587\u4ef6\u7684\u672c\u5730\u4fdd\u5b58\u8def\u5f84\u3002 \u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1\u3002 \u8bf4\u660e\uff1a\u5982\u679c\u9700\u8981\u91cd\u65b0\u6267\u884c\u4efb\u52a1\uff0c\u9009\u4e2d\u8282\u70b9\uff0c\u53f3\u952e\u9009\u62e9\u201cReset\u201d\u540e\u8be5\u8282\u70b9\u70b9\u5373\u53ef\u91cd\u65b0\u6267\u884c\u3002 \u67e5\u770b temp.csv \u5df2\u4e0b\u8f7d\u81f3\u672c\u5730\u6307\u5b9a\u76ee\u5f55\u3002","title":"\u8bfb\u53d6HDFS\u6587\u4ef6"},{"location":"Data_Integration/Knime/#hdfs_2","text":"\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\u653e\u5728\u672c\u5730\u7684\u4e00\u4e2a\u6587\u4ef6\u5939\u4e2d\uff0c\u4f8b\u5982 C:\\KnimeData \u3002 \u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165 List Files , String to URI \u4ee5\u53ca Upload \u8282\u70b9\uff0c\u5c06\u5176\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5\u3002 \u53cc\u51fb List Files \u8282\u70b9\uff0c\u9009\u62e9\u8981\u4e0a\u4f20\u6587\u4ef6\u7684\u672c\u5730\u8def\u5f84\uff0c\u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb Upload \u8282\u70b9\uff0c\u9009\u62e9\u5728HDFS\u4e2d\u6587\u4ef6\u4fdd\u5b58\u7684\u8def\u5f84\uff0c\u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1\u3002 \u5728\u670d\u52a1\u5668\u4e0a\u67e5\u770bHDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\u6240\u4e0a\u4f20\u7684\u6587\u4ef6\u3002","title":"\u4e0a\u4f20\u6587\u4ef6\u81f3HDFS"},{"location":"Data_Integration/Knime/#knimehive","text":"","title":"Knime\u8fde\u63a5Hive"},{"location":"Data_Integration/Knime/#_4","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 \u5df2\u5b8c\u6210\u672c\u673a\u7684Kerberos\u8ba4\u8bc1","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#hive","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow \uff0c\u547d\u540d\u4e3a\u201cHiveConnection\u201d\u540e\u4fdd\u5b58\u3002 \u5728\u201cHiveConnection\u201d\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Hive Connector \u8282\u70b9\u3002 \u53cc\u51fb Hive Connector \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\uff1a Hostname: Hive\u7684HiveServer\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9 Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos \u70b9\u51fb OK \uff0c\u4fdd\u5b58\u914d\u7f6e\u3002","title":"\u5efa\u7acbHive\u8fde\u63a5"},{"location":"Data_Integration/Knime/#hive_1","text":"\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u8282\u70b9\uff0c\u5e76\u8fdb\u884c\u5982\u4e0b\u8fde\u63a5\u3002 \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 Host: HDFS\u7684NameNode\u4e3b\u8282\u70b9IP Port: 25000 Authentication: Kerberos \u53cc\u51fb File Reader \u8282\u70b9\uff0c\u9009\u62e9\u672c\u5730\u5c06\u8981\u4e0a\u4f20\u7684\u6587\u4ef6\uff0c\u4f8b\u5982Knime\u5de5\u4f5c\u533a C:\\Users\\wangna\\knime-workspace \u4e2d\u5df2\u9ed8\u8ba4\u4e0b\u8f7d\u4e0b\u6765\u7684 C:\\Users\\wangna\\knime-workspace\\Example Workflows\\TheData\\Basics\\adult.csv \u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb Hive Loader \u8282\u70b9\uff0c\u9009\u62e9\u6587\u4ef6\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u7684\u8def\u5f84\u4ee5\u53ca\u8868\u540d\uff0c\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1\u3002 \u5728FusionInsight\u5ba2\u6237\u7aef\u4f7f\u7528beeline\u67e5\u770b\u5bfc\u5165Hive\u4e2d\u7684\u8868\u3002","title":"\u5199\u5165Hive\u8868"},{"location":"Data_Integration/Knime/#knimespark2x","text":"","title":"Knime\u8fde\u63a5Spark2x"},{"location":"Data_Integration/Knime/#_5","text":"\u5df2\u7ecf\u5b8c\u6210Knime 3.6.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u8bf4\u660e\uff1aFusionInsight\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient \uff0c\u4e14\u80fd\u4f7f\u7528spark-submit\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Knime/#spark-job-server","text":"\u8bf4\u660e\uff1a\u4ee5\u4e0b\u64cd\u4f5c\u6b65\u9aa4\u4ee5CentOS 7.x\u4e3a\u4f8b\uff0c\u5176\u4ed6\u64cd\u4f5c\u7cfb\u7edf\u8bf7\u53c2\u8003KNIME\u5b98\u65b9\u6587\u6863 https://download.knime.org/store/3.6/knime_extension_for_apache_spark_2.3.0.pdf \u6267\u884c\u76f8\u5e94\u7684\u547d\u4ee4\u3002 \u4ece https://docs.knime.com/latest/bigdata_extensions_admin_guide/index.html#_overview \u7684\u201cSpark Jobserver downloads\u201d\u7ae0\u8282\uff0c\u6839\u636e\u96c6\u7fa4\u4ee5\u53ca\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u83b7\u53d6\u5bf9\u5e94\u7684 Spark Job Server \u5b89\u88c5\u5305\u3002\u57fa\u4e8eFusionInsight HD 6.5.1\u7684Spark2x\u7684\u7248\u672c\u4e3a2.3.2\uff0c\u6545\u9009\u62e9\u4e0b\u8f7d CDH 5.9 - 5.15 (Apache Spark 2.3) \uff0c\u4e0b\u8f7d\u6587\u4ef6\u540d\u4e3a spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh.tar.gz \u3002 \u5c06\u4e0b\u8f7d\u7684 spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh.tar.gz \u4e0a\u4f20\u81f3\u5df2\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\u8282\u70b9\uff0c\u4f8b\u5982 /opt \u76ee\u5f55\u4e0b\u3002\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5\u914d\u7f6e\uff1a LINKNAME=spark2-job-server tar -xvf /opt/spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh.tar.gz -C /opt ln -s /opt/spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh /opt/${LINKNAME} useradd -d /opt/${LINKNAME}/ -M -r -s /bin/false spark-job-server su -l -c \"hdfs dfs -mkdir -p /user/spark-job-server ; hdfs dfs -chown -R spark-job-server /user/spark-job-server\" spark-job-server chown -R spark-job-server:spark-job-server /opt/${LINKNAME} /opt/spark-job-server-0.7.0.3-KNIME_spark-2.3_cdh","title":"\u5b89\u88c5Spark Job Server"},{"location":"Data_Integration/Knime/#spark-job-server_1","text":"\u5c06 C:\\developuser\\krb5.conf \u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /etc \u76ee\u5f55\u4e0b\u3002\u542f\u52a8spark job server\u65f6\u8c03\u7528\u201ckerberos-ticket-renewer.sh\u201d\u4ea7\u751f\u7968\u636e\u4f9d\u8d56\u4e8e /etc/krb5.conf \u6587\u4ef6\u3002 \u5c06 C:\\developuser\\user.keytab \u4e0a\u4f20\u81f3\u5b89\u88c5\u8282\u70b9\u7684 /opt/spark2-job-server \u76ee\u5f55\u4e0b\u3002 \u4fee\u6539 /opt/spark2-job-server/spark-job-server-init.d \u8bbe\u7f6eJava\u548cSpark\u8fd0\u884c\u7684\u73af\u5883\u53d8\u91cf\u3002\u5728\u201cjobserver_start()\u201d\u5f00\u59cb\u5904\u65b0\u589e source /opt/hadoopclient/bigdata_env \u3002 \u4fee\u6539 /opt/spark2-job-server/environment.conf \u6587\u4ef6\u3002\u8bbe\u7f6e master = \"yarn-client\" \uff0c\u4ee5yarn-client\u6a21\u5f0f\u8fd0\u884cspark\u3002 \u4fee\u6539 /opt/spark2-job-server/settings.sh \u6587\u4ef6\u3002 \u8bbe\u7f6eSPARK_HOME\u3002\u7531\u4e8e\u4e0d\u5b58\u5728\u76ee\u5f55 /opt/cloudera/parcels/SPARK2/ \uff0c\u6545else\u8bed\u53e5\u7684SPARK_HOME\u751f\u6548\u3002\u5728else\u8bed\u53e5\u4e2d\u8bbe\u7f6e SPARK_HOME=/opt/hadoopclient/Spark2x/spark \u3002 \u5c06 HADOOP_CONF_DIR \u7684\u8bbe\u7f6e\u6ce8\u91ca\u6216\u5220\u9664\u3002 \u8bf4\u660e\uff1ayarn-client\u6a21\u5f0f\u8fd0\u884cspark\u9700\u8981\u4ece\u53c2\u6570HADOOP_CONF_DIR\u8bbe\u7f6e\u7684\u76ee\u5f55\u52a0\u8f7d\u76f8\u5e94\u7684\u73af\u5883\u53d8\u91cf\u548c\u914d\u7f6e\u6587\u4ef6\u3002\u542f\u52a8spark job server(\u6267\u884cspark-job-server-init.d start\u6216\u8005systemctl start spark2-job-server)\u65f6\u6267\u884c\u547d\u4ee4\u201csource /opt/hadoopclient/bigdata_env\u201d\u5df2\u8bbe\u7f6eHADOOP_CONF_DIR\u4e3a\u6b63\u786e\u7684\u503c\u3002\u56e0\u6b64\u9700\u8981\u5c06settings.sh\u6587\u4ef6\u4e2dHADOOP_CONF_DIR\u8bbe\u7f6e\u7684\u503c\u6ce8\u91ca\u4ee5\u514d\u5c06\u6b63\u786e\u7684\u503c\u8986\u76d6\uff0c\u5bfc\u81f4\u9519\u8bef\u3002 \u8bbe\u7f6e JOBSERVER_KEYTAB \u548c JOBSERVER_PRINCIPAL \u3002 export JOBSERVER_KEYTAB=/opt/spark2-job-server/user.keytab export JOBSERVER_PRINCIPAL=developuser@HADOOP.COM \u767b\u5f55FusionInsight Manager\uff0c\u4fee\u6539HDFS\u7684core-site.xml\u6587\u4ef6\u914d\u7f6e\u3002 \u4e3b\u9875\u9762\u9009\u62e9 \u96c6\u7fa4->HDFS \uff0c\u5728\u5de6\u4fa7\u9009\u62e9 \u914d\u7f6e->\u5168\u90e8\u914d\u7f6e->\u81ea\u5b9a\u4e49 \uff0c\u6dfb\u52a0\u4ee5\u4e0b\u4e24\u4e2a\u53c2\u6570 hadoop.proxyuser.spark-job-server.hosts = * hadoop.proxyuser.spark-job-server.groups = * \u4fdd\u5b58\u914d\u7f6e\uff0c\u5e76\u91cd\u542fHDFS\u4ee5\u53ca\u4f9d\u8d56\u7684\u670d\u52a1\u3002","title":"\u914d\u7f6eSpark Job Server"},{"location":"Data_Integration/Knime/#spark-job-server_2","text":"\u8bbe\u7f6e\u4ece\u7cfb\u7edf\u542f\u52a8\u6216\u505c\u6b62Spark Job Server\u3002 ln -s /opt/spark2-job-server/spark-job-server-init.d /etc/init.d/spark2-job-server systemctl daemon-reload systemctl enable spark2-job-server \u542f\u52a8Spark Job Server\u3002 systemctl start spark2-job-server ps -ef |grep spark2 \u8bf4\u660e\uff1a \u542f\u52a8Spark Job Server\u7684\u53e6\u5916\u4e00\u79cd\u65b9\u5f0f\u662f\u6267\u884c\u547d\u4ee4 /opt/spark2-job-server/spark-job-server-init.d start \u5728 /tmp \u76ee\u5f55\u4e0b\u53ef\u4ee5\u67e5\u770b\u7f13\u5b58\u7684\u7968\u636e\u548c\u5176\u4ed6\u6570\u636e\u3002 \u5728 /var/log/spark2-job-server \u76ee\u5f55\u4e0b\u53ef\u4ee5\u67e5\u770b\u8fd0\u884c\u7684\u65e5\u5fd7\u3002 \u542f\u52a8\u540e\u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165 http://ip:8090 \uff0c\u5176\u4e2dip\u4e3a\u5b89\u88c5\u8282\u70b9\u7684IP\uff0c\u8fd4\u56de\u4ee5\u4e0b\u754c\u9762\u5219\u8868\u793a\u542f\u52a8\u6210\u529f\u3002 \u505c\u6b62Spark Job Server\u3002 systemctl stop spark2-job-server","title":"\u542f\u52a8\u548c\u505c\u6b62Spark Job Server"},{"location":"Data_Integration/Knime/#spark","text":"\u5728Knime\u83dc\u5355\u680f\u4e2d\u9009\u62e9 File->New->New KNIME Workflow \uff0c\u547d\u540d\u4e3a\u201cSparkContext\u201d\u540e\u4fdd\u5b58\u3002\u5728\u5de5\u4f5c\u533a\u4e2d\u62d6\u5165\u4e00\u4e2a Create Spark Context \u8282\u70b9\u548c\u4e00\u4e2a Destroy Spark Context \u8282\u70b9\u5e76\u8fde\u63a5\u3002 \u53cc\u51fb Create Spark Context \u8282\u70b9\u8fdb\u884c\u5982\u4e0b\u914d\u7f6e\u3002 \u5728Context Settings\u9875\u9762 Spark version\uff1a\u9009\u62e9\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684Spark\u7248\u672c2.3 Context name\uff1a\u81ea\u5b9a\u4e49Spark Context\u540d\u5b57\u4e3aknimeSparkContext \u5728Connection Settings\u9875\u9762 Jobserver URL\uff1a http://ip:8090/ \uff0c\u5176\u4e2dIP\u4e3aSpark Job Server \u6240\u5728\u8282\u70b9IP Authentication: None \u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u70b9\u51fb\u83dc\u5355\u680f \u6309\u94ae\uff0c\u6d4b\u8bd5\u8fde\u63a5\u662f\u5426\u6709\u9519\uff0c\u82e5\u663e\u793a\u5982\u4e0b\uff0c\u8868\u660e\u8282\u70b9\u914d\u7f6e\u65e0\u8bef\u3002 \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00Jobserver URL\u4e2d\u914d\u7f6e\u7684\u5730\u5740\uff0c\u53ef\u4ee5\u8fdb\u5165Spark Job Server UI\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u521a\u624d\u5efa\u7acb\u7684Spark Context\uff0c\u663e\u793a\u5982\u4e0b\uff1a \u767b\u5f55Funsion Manager\u8fdb\u5165Yarn ResourceManager WebUI\u67e5\u770bapplication\u7684\u65e5\u5fd7\uff0cFinalStatus\u663e\u793a\u4e3a SUCCEEDED \u3002","title":"\u5efa\u7acbSpark\u8fde\u63a5"},{"location":"Data_Integration/Knime/#spark_1","text":"Spark\u5e94\u7528\u5b9e\u4f8b\u4e0b\u8f7d\u5730\u5740 https://www.knime.com/nodeguide/big-data/spark-executor \u3002","title":"Spark\u5e94\u7528\u5b9e\u4f8b"},{"location":"Data_Integration/Knime/#hive-to-spark-to-hive","text":"\u4e0b\u8f7d\u5b9e\u4f8b\u201cHive to Spark to Hive\u201d\uff0c\u4e0b\u8f7d\u6587\u4ef6\u540d\u4e3a 05_Hive_to_Spark_to_Hive.knwf \u3002 \u70b9\u51fbKNIME Analytics Platform\u7684\u83dc\u5355\u680f File->Import KNIME Workflow \u5bfc\u5165 05_Hive_to_Spark_to_Hive.knwf \u3002 \u53cc\u51fb HDFS Connection \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 Host: HDFS\u7684NameNode\u4e3b\u8282\u70b9IP Port: 25000 Authentication: Kerberos \u53cc\u51fb File Reader \u8282\u70b9\uff0c\u9009\u62e9\u5c06\u8981\u4e0a\u4f20\u7684\u672c\u5730\u6587\u4ef6\uff0c\u4f8b\u5982Knime\u5de5\u4f5c\u533a C:\\Users\\wangna\\knime-workspace \u4e2d\u5df2\u9ed8\u8ba4\u4e0b\u8f7d\u4e0b\u6765\u7684 C:\\ecotesting\\knime-workspace\\Example Workflows\\TheData\\Customers\\ContractData.csv \u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb Hive Connector \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 Hostname: Hive\u7684HiveServer\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u7684IP Port: 21066 Parameter: principal=hive/hadoop.hadoop.com@HADOOP.COM;saslQop=auth-conf;auth=KERBEROS; Authentication: Use Kerberos \u53cc\u51fb Hive Loader \u8282\u70b9\uff0c\u9009\u62e9\u6587\u4ef6\u8981\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u8def\u5f84\u4ee5\u53ca\u8868\u540d\uff0c\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u53cc\u51fb Create Spark Context \u8282\u70b9\uff0c\u586b\u5199\u914d\u7f6e\u5982\u4e0b\u3002\u70b9\u51fb OK \u4fdd\u5b58\u914d\u7f6e\u3002 \u5728Context Settings\u9875\u9762 Spark version\uff1a\u9009\u62e9\u96c6\u7fa4\u4e2d\u4f7f\u7528\u7684Spark\u7248\u672c2.3 Context name\uff1a\u81ea\u5b9a\u4e49Spark Context\u540d\u5b57\u4e3aknimeSparkContext \u5728Connection Settings\u9875\u9762 Jobserver URL\uff1a http://ip:8090/ \uff0c\u5176\u4e2dIP\u4e3aSpark Job Server \u6240\u5728\u8282\u70b9IP Authentication: None Hive to Spark \u3001 Spark to Hive \u548c Database Connection Table Reader \u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e\u4e0d\u53d8\u3002\u53cc\u51fb Spark to Hive \u8282\u70b9\u53ef\u67e5\u770b\u521b\u5efa\u7684Hive\u8868\u7684\u201cTable name\u201d\u4e3a knimeTest \u3002 \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 \u6267\u884c\u4efb\u52a1\u3002 \u767b\u5f55Funsion Manager\u8fdb\u5165Yarn ResourceManager WebUI\u67e5\u770bapplication\u7684\u65e5\u5fd7\uff0cState\u663e\u793a\u4e3a RUNNING \u3002 \u5728FusionInsight\u5ba2\u6237\u7aef\u4f7f\u7528beeline\u67e5\u770b\u5bfc\u5165Hive\u4e2d\u7684\u8868\u3002\u901a\u8fc7 Hive Loader \u8282\u70b9\u5bfc\u5165\u7684\u8868 contactdata \u4ee5\u53ca Spark to Hive \u8282\u70b9\u5bfc\u5165\u7684\u8868 knimetest \u5747\u5df2\u5bfc\u5165Hive\u3002 select count(*) from contactdata; select count(*) from knimetest;","title":"Hive to Spark to Hive"},{"location":"Data_Integration/Knime/#faq","text":"","title":"FAQ"},{"location":"Data_Integration/Knime/#spark-job-serverkinit-command-not-found","text":"\u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6267\u884c/opt/spark2-job-server/spark-job-server-init.d start\u65f6\u8fd4\u56de kinit: command not found\u3002 \u5728Knime Analytics Platform\u4e2d\u8fd0\u884cCreate Spark Context(Jobserver)\u65f6\u5931\u8d25\uff0c\u8fd4\u56de Possible reason: Incompatible Jobserver version, malconfigured Spark Jobserver\u3002 \u67e5\u770b\u8fd0\u884c\u65e5\u5fd7/var/log/spark2-job-server/jobserver-knimeSparkContext7418932986808208865/spark-job-server.log\u8fd4\u56de\u4ee5\u4e0b\u9519\u8bef\uff1a DestHost:destPort 172-16-4-22:26004 , LocalHost:localPort 172-16-5-105/172.16.5.105:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS] , while invoking ApplicationClientProtocolPBClientImpl.getNewApplication over 167 after 2 failover attempts. Trying to failover after sleeping for 43502ms. \u3010\u95ee\u9898\u5206\u6790\u3011 \u4ecespark-job-server-init.d\u53ef\u4ee5\u770b\u51fa\u4f7f\u7528spark-job-server\u7528\u6237\u542f\u52a8Spark Job Server\u3002spark-job-server\u7528\u6237\u6ca1\u6709\u6743\u9650\u4f7f\u7528FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u7684kinit\u3002\u5982\u679c\u64cd\u4f5c\u7cfb\u7edf\u6ca1\u6709\u5b89\u88c5krb5-workstation\uff0c\u5219\u4f1a\u8fd4\u56dekinit: command not found\uff0c\u65e0\u6cd5\u4ea7\u751f\u8ba4\u8bc1\u7968\u636e\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u65b9\u5f0f\u4e00\uff1a\u6267\u884c yum install krb5-workstation \u5b89\u88c5krb5-workstation\u3002 \u65b9\u5f0f\u4e8c\uff1a\u5728/opt/spark2-job-server/ spark-job-server-init.d\u7684\u201cjobserver_start()\u201d\u5f00\u59cb\u5904\u65b0\u589e source /opt/hadoopclient/bigdata_env \u3002","title":"\u542f\u52a8Spark Job Server\u65f6\uff0c\u8fd4\u56dekinit: command not found"},{"location":"Data_Integration/Knime/#create-spark-contextjobservercant-get-kerberos-realm","text":"\u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5f53/opt/spark2-job-server/environment.conf\u8bbe\u7f6emaster = \"yarn-client\"\uff0c\u5728Knime Analytics Platform\u4e2d\u8fd0\u884cCreate Spark Context(Jobserver)\u65f6\u5931\u8d25\uff0c\u8fd4\u56de Yarn application has already ended! It might have been killed or unable to launch application master. \u767b\u5f55Funsion Manager\u8fdb\u5165Yarn ResourceManager WebUI\u67e5\u770bapplication\u7684\u65e5\u5fd7\uff0c\u8fd4\u56de\u9519\u8bef\u5982\u4e0b\uff1a ``` INFO SecurityManager: SecurityManager: authentication enabled; ui acls enabled; users with view permissions: Set(admin, developuser); groups with view permissions: Set(); users with modify permissions: Set(admin, developuser); groups with modify permissions: Set() Exception in thread \"main\" java.lang.IllegalArgumentException: Can't get Kerberos realm at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:65) at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:318) at org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:364) at org.apache.spark.deploy.SparkHadoopUtil. (SparkHadoopUtil.scala:53) at org.apache.spark.deploy.SparkHadoopUtil .instance .instance lzycompute(SparkHadoopUtil.scala:409) at org.apache.spark.deploy.SparkHadoopUtil .instance(SparkHadoopUtil.scala:409) at org.apache.spark.deploy.SparkHadoopUtil .instance(SparkHadoopUtil.scala:409) at org.apache.spark.deploy.SparkHadoopUtil .get(SparkHadoopUtil.scala:430) at org.apache.spark.SecurityManager. (SecurityManager.scala:259) at org.apache.spark.deploy.yarn.ApplicationMaster. (ApplicationMaster.scala:69) at org.apache.spark.deploy.yarn.ApplicationMaster .main(ApplicationMaster.scala:811) at org.apache.spark.deploy.yarn.ExecutorLauncher .main(ApplicationMaster.scala:811) at org.apache.spark.deploy.yarn.ExecutorLauncher .main(ApplicationMaster.scala:844) at org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala) Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:110) at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:63) ... 11 more Caused by: KrbException: Cannot locate default realm at sun.security.krb5.Config.getDefaultRealm(Config.java:1029) ... 17 more ``` \u3010\u95ee\u9898\u5206\u6790\u3011 yarn-client\u6a21\u5f0f\u8fd0\u884cspark\u9700\u8981\u4ece\u53c2\u6570HADOOP_CONF_DIR\u8bbe\u7f6e\u7684\u76ee\u5f55\u52a0\u8f7d\u76f8\u5e94\u7684\u73af\u5883\u53d8\u91cf\u548c\u914d\u7f6e\u6587\u4ef6\uff0c \u8be5\u95ee\u9898\u662f\u7531\u4e8eHADOOP_CONF_DIR\u8bbe\u7f6e\u4e0d\u6b63\u786e\u5f15\u8d77\u7684 \u3002 \u5728Knime Analytics Platform\u4e2d\u8fd0\u884cCreate Spark Context(Jobserver)\u65f6\uff0c\u8c03\u7528/opt/spark2-job-server/manager_start.sh\u63d0\u4ea4spark\u4efb\u52a1\u3002 \u5728manager_start.sh\u4e2d\u4f7f\u7528FusionInsight\u5ba2\u6237\u7aef\u7684spark-submit\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u4e4b\u524d\u8c03\u7528\u4e86/opt/spark2-job-server/setting.sh\u3002\u5728/opt/spark2-job-server/setting.sh\u9ed8\u8ba4\u8bbe\u7f6eHADOOP_CONF_DIR=/etc/hive/conf\u3002 \u4f7f\u7528FusionInsight\u5ba2\u6237\u7aef\u7684spark-submit\u63d0\u4ea4\u4efb\u52a1\u65f6\uff0c\u73af\u5883\u53d8\u91cfHADOOP_CONF_DIR\u6b63\u786e\u503c\u5e94\u8bbe\u7f6e\u4e3a/opt/hadoopclient/HDFS/hadoop/etc/hadoop\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u5728/opt/spark2-job-server/ spark-job-server-init.d\u7684\u201cjobserver_start()\u201d\u5f00\u59cb\u5904\u65b0\u589e source /opt/hadoopclient/bigdata_env \u3002 \u5728/opt/spark2-job-server/setting.sh\u4e2d \u5c06HADOOP_CONF_DIR\u7684\u8bbe\u7f6e\u6ce8\u91ca\u6216\u8005\u5220\u9664 \u3002\u6216\u8005\u8bbe\u7f6e\u4e3a\u6b63\u786e\u7684\u503c HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u3002","title":"\u8fd0\u884cCreate Spark Context(Jobserver)\u65f6\u5931\u8d25\uff0c\u8fd4\u56deCan't get Kerberos realm"},{"location":"Data_Integration/OceanSource/","text":"\u4e2d\u65b0\u8d5b\u514bOceanSource\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 OceanSource 1.0 \u2194 FusionInsight HD V100R002C80 (HBase/Hive/Kafka/ElasticSearch/Redis/GaussDB)","title":"1.0 <--> C80"},{"location":"Data_Integration/OceanSource/#oceansourcefusioninsight","text":"","title":"\u4e2d\u65b0\u8d5b\u514bOceanSource\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/OceanSource/#_1","text":"OceanSource 1.0 \u2194 FusionInsight HD V100R002C80 (HBase/Hive/Kafka/ElasticSearch/Redis/GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Oracle_GoldenGate/","text":"Oracle GoldenGate\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Oracle GoldenGate 12.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C80SPC100 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.2 \u2194 FusionInsight MRS 8.0 (HDFS/Flume/Kafka) \u73af\u5883\u4fe1\u606f \u00b6 \u8f6f\u4ef6\u4fe1\u606f \u00b6 Oracle GoldenGate 12.2.0.1.1 for Oracle database Oracle GoldenGate 12.2.0.1.1 for BigData Oracle database 12.1.0.2.0 jdk-7u71-linux-x64.rpm FusionInsight V100R002C60U20 \u786c\u4ef6\u4fe1\u606f \u00b6 \u6e90\u7aefOGG VM: 162.1.115.68 Redhat6.5 \uff08\u5305\u542bOracle DB12c\u7684\u6570\u636e\u5e93\uff09 \u76ee\u6807\u7aefOGG VM: 162.1.115.69 Redhat6.5\uff08\u5305\u542bHadoop\u7684\u5ba2\u6237\u7aef\uff09 \u62d3\u6734\u7ed3\u6784 \u00b6 \u6d4b\u8bd5\u62d3\u6734\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a \u6d4b\u8bd5\u8868 \u00b6 \u6e90\u7aef\u6d4b\u8bd5\u8868\uff1a \u5728\u6e90\u7aefOracle\u7684PDBORCL\u6570\u636e\u5e93\u7684test\u7528\u6237\u4e0b\u521b\u5efatest1\u8868\uff0c\u5176\u4e2dID\u4e3a\u4e3b\u952e OGG for Oracle\u5b89\u88c5 \u00b6 \u524d\u7f6e\u6761\u4ef6\uff1a\u5b8c\u6210oracle12c\u6570\u636e\u5e93\u7684\u5b89\u88c5\uff08IP\uff1a162.1.115.68\uff09 \u8f6f\u4ef6\u7248\u672c\uff1alinuxamd64_12102_database_1of2.zip, linuxamd64_12102_database_1of2.zip \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Oracle \u00b6 \u5c06fbo_ggs_Linux_x64_shiphome.zip\u4e0a\u4f20\u81f3oracle\u5ba2\u6237\u7aef\uff08ip\uff1a162.1.115.68\uff09 /home/oracle \u76ee\u5f55\u4e0b\uff0c\u5207\u6362\u81f3oracle\u7528\u6237\uff0c\u89e3\u538b\u751f\u6210bo_ggs_Linux_x64_shiphome\u76ee\u5f55\u3002 \u5728 /home/oracle/fbo_ggs_Linux_x64_shiphome/Disk1 \u76ee\u5f55\u4e0b\uff0c\u8fd0\u884c ./runInstaller \u5b89\u88c5\u6210\u529f\uff0c/home/orcle/OGG/\u662fOGG for Oracle\u7684\u5b89\u88c5\u76ee\u5f55\u3002 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u5207\u6362\u5230oracle\u7528\u6237 su - oracle vi .bash_profile \u6587\u4ef6.bash_profile\u5185\u5bb9\u5982\u4e0b\uff1a ```shell # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH= PATH: PATH: HOME/bin export PATH PATH= PATH: PATH: HOME/bin:/u01/app/oracle/product/12.1.0/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_1 export ORACLE_SID=orcl export LD_LIBRARY_PATH=$ORACLE_HOME/lib ``` \u8fd0\u884cOGG \u6253\u5f00\u6570\u636e\u5e93\u5f52\u6863\u53ca\u5f00\u542f\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7 \u00b6 \u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: shutdown immediate ; startup mount ; alter database archivelog ; alter database open ; archive log list ; \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; Enabling Oracle GoldenGate in the Database: show parameter enable_goldengate_replication ; alter system set enable_goldengate_replication = true scope = both ; \u914d\u7f6eDB12c PDB\u7684tnsname\u4fe1\u606f vi $ORACLE_HOME/network/admin/tnsnames.ora \uff1a \u5728\u6570\u636e\u5e93\u4e2d\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 \u00b6 \u4f7f\u7528 sqlplus / as sysdba \u767b\u9646\u6570\u636e\u5e93\u540e\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 create user c ## ogg identified by welcome1 ; grant dba to c ## ogg container = all ; grant create session , connect , resource to c ## ogg container = all ; grant alter any table to c ## ogg container = all ; grant alter system to c ## ogg container = all ; exec dbms_goldengate_auth . grant_admin_privilege ( 'c##ogg' , container => 'all' ); \u914d\u7f6eGoldenGate \u767b\u9646\u6570\u636e\u5e93\u7684\u522b\u540d \u00b6 \u5728GoldenGate\u4e2d\u521b\u5efa\u7528\u6237\u522b\u540d\uff0c\u7528\u4e8e\u767b\u5f55Oracle\u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u5e93\u65e5\u5fd7\uff1a add credentialstore ALTER CREDENTIALSTORE ADD USER c ## ogg PASSWORD welcome1 ALIAS ogg_src \u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u522b\u540dogg_src\u767b\u9646\u6570\u636e\u5e93\u4e86\uff1a dblogin useridalias ogg_src C##ogg\u662fOracle DB12c\u7684\u666e\u901a\u7528\u6237\uff0c\u53ef\u4ee5\u8bbf\u95ee\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\u3002 \u521b\u5efatest\u7528\u6237\u548ctest1\u8868 \u00b6 test\u7528\u6237\u662f\u57fa\u4e8epdborcl\u6570\u636e\u5e93\u5b9e\u4f8b\u7684\uff1a \u767b\u9646\u6570\u636e\u5e93 Sqlplus / as sysdba \u521b\u5efa\u7528\u6237 alter session set container = pdborcl ; alter database open ; create user test identified by welcome1 ; grant resource , connect to test ; CREATE TABLESPACE test DATAFILE '/u01/app/oracle/oradata/orcl/pdborcl/test01.dbf' SIZE 500 M UNIFORM SIZE 128 k ; alter user test quota unlimited on test ; alter user test quota unlimited on users ; \u521b\u5efa\u6d4b\u8bd5\u8868 conn test / welcome1 @ pdborcl ; create table test1 ( id number primary key , name varchar2 ( 50 )); \u914d\u7f6eGoldenGate\u6355\u83b7\u8fdb\u7a0b \u00b6 \u7f16\u8f91eora.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cedit param eora\u547d\u4ee4\uff1a GGSCI> edit param eora GGSCI> edit param mgr GGSCI> edit param phdfs GGSCI> edit param phbase GGSCI> edit param pkafka GGSCI> edit param pflume \u7f16\u8f91 diroby/eora.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/eora.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) \u4f7f\u7528oracle\u7528\u6237\u521b\u5efadiroby\u76ee\u5f55\uff1a cd /home/oracle/OGG/ mkdir diroby GGSCI> shell vi diroby/eora.oby \u6ce8\u610f\u8fdb\u7a0b\u540deora\u548c\u6570\u636e\u6587\u4ef6dirdat/eo\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cobey diroby/eora.oby\u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0beora\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/eora.oby \u628a\u6355\u83b7\u8fdb\u7a0beora\u6ce8\u518c\u5230pdborcl\u6570\u636e\u5e93\u4e2d\uff1a GGSCI> dblogin useridalias ogg_src GGSCI> register extract eora database container(pdborcl) \u4e3apdborcl.test\u4e0b\u7684\u6240\u6709\u8868\u6dfb\u52a0\u8868\u7ea7\u9644\u52a0\u65e5\u5fd7\uff1a GGSCI> add schematrandata pdborcl.test allcols \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0beora: GGSCI> start eora \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HDFS\u5904\u7406\u3002 \u7f16\u8f91phdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phdfs \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phdfs.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phdfs.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phdfs.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phdfs**\u548c\u6570\u636e\u6587\u4ef6dirdat/rs\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phdfs.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphdfs\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phdfs.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphdfs: GGSCI> start phdfs \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HBASE\u5904\u7406\u3002 \u7f16\u8f91phbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phbase \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phbase.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phbase.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phbase.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phbase**\u548c\u6570\u636e\u6587\u4ef6dirdat/se\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phbase.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphbase\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phbase.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphbase: GGSCI> start phbase \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate FLUME\u5904\u7406\u3002 \u7f16\u8f91pflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pflume \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pflume.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pflume.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pflume.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pflume**\u548c\u6570\u636e\u6587\u4ef6dirdat/rf\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pflume.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpflume\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/pflume.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpflume: GGSCI> start pflume \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka \u00b6 \u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate Kafka\u5904\u7406\u3002 \u7f16\u8f91pkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pkafka \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pkafka.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pkafka.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pkafka.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pkafka**\u548c\u6570\u636e\u6587\u4ef6dirdat/rk\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pkafka.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpkafka\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/ pkafka.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpkafka: GGSCI> start pkafka \u67e5\u770bGoldenGate\u8fdb\u7a0b\u8fd0\u884c\u72b6\u6001 \u00b6 \u67e5\u770bGoldenGate\u8fdb\u7a0b\u72b6\u6001\uff1a(EORCL\u662f\u4e0eELK\u5bf9\u63a5\u7684\u8fdb\u7a0b) GGSCI> info all \u67e5\u770b\u67d0\u4e2a\u8fdb\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a GGSCI> info eora detail \u67e5\u770bGoldenGate\u7684\u7edf\u8ba1\u4fe1\u606f\uff1a GGSCI> stats eora, latest \u67e5\u770bGoldenGate\u8fdb\u7a0b\u62a5\u544a\uff0c\u7528\u4e8e\u5b9a\u4f4d\u95ee\u9898\uff1a GGSCI> view report eora OGG for Bigdata\u5b89\u88c5 \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7d\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728Bigdata\u5ba2\u6237\u7aef\u673a\u5668\u4e0a\uff08ip\uff1a162.1.115.69\uff09\u6309\u7167FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u3002\u5c06\u5ba2\u6237\u7aefJDK\u66ff\u6362\u62101.7\u7248\u672c\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5oracle JDK1.7 \u5c06krb5.conf\u653e\u5728/etc/\u76ee\u5f55\u4e0b \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Bigdata \u5c06122011_ggs_Adapters_Linux_x64.zip\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef/opt\u76ee\u5f55\u4e0b\uff1a unzip 122011_ggs_Adapters_Linux_x64.zip \u5c06\u89e3\u538b\u540e\u7684ggs_Adapters_Linux_x64.tar\u89e3\u538b\u5230/opt/OGG_HADOOP\u76ee\u5f55\u4e0b\uff1a \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u66f4\u6539\u73af\u5883\u53d8\u91cf\uff0c\u7f16\u8f91\u6839\u76ee\u5f55\u4e0b vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ] ; then . ~/.bashrc fi # User specific environment and startup programs export JAVA_HOME = /usr/java/jdk1.7.0_40 #export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre export CLASSPATH = $CLASSPATH : $JAVA_HOME /lib: $JAVA_HOME /jre/lib PATH = $JAVA_HOME /bin: $PATH : $HOME /bin export PATH #export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server/libjvm.so:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/java/jdk1.7.0_40/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.7.0_40/jre/lib/amd64/server:/usr/java/jdk1.7.0_40/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib: $LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/local/lib: $LD_LIBRARY_PATH Source\u73af\u5883\u53d8\u91cf\uff0c source .bash_profile . \u5c06 /opt/OGG_HADOOP/AdapterExamples/big-data \u4e0b\u7684\u56db\u4e2a\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/OGG_HADOOP/dirprm \u76ee\u5f55\u4e0b\u3002 \u914d\u7f6eGoldenGate\u7ba1\u7406\u8fdb\u7a0b \u00b6 \u7f16\u8f91mgr.prm GGSCI> edit param mgr GGSCI>start mgr GGSCI>info all \u914d\u7f6eGoldenGate HDFS \u590d\u5236\u8fdb\u7a0b \u00b6 \u7f16\u8f91rhdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhdfs \u547d\u4ee4\uff1a GGSCI> edit param rhdfs \u7f16\u8f91hdfs.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hdfs.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hdfs.props \u9700\u8981\u5728HDFS\u4e2d\u521b\u5efa/ogg1\u76ee\u5f55\u3002 \u5c06hdfs.keytab\u6587\u4ef6\u62f7\u8d1d\u5230/opt/OGG_HADOOP/dirprm\u76ee\u5f55\u4e2d\uff1a \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhdfs\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhdfs, exttrail dirdat/rs GGSCI>info all GGSCI>start rhdfs GGSCI>info all \u914d\u7f6eGoldenGate HBase \u590d\u5236\u8fdb\u7a0b \u00b6 \u7f16\u8f91rhbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhbase \u547d\u4ee4\uff1a GGSCI> edit param rhbase \u7f16\u8f91hbase.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hbase.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hbase.props \u62f7\u8d1dhbase.keytab\u548cjaas.conf\u5230 /opt/OGG_HADOOP/dirprm/ \u4e0b\uff1a jaas.conf \u6587\u4ef6 \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhbase\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhbase, exttrail dirdat/se GGSCI>start rhbase GGSCI>info all \u914d\u7f6eGoldenGate Kafka \u590d\u5236\u8fdb\u7a0b \u00b6 \u521b\u5efakafka\u6d88\u606f\uff0c\u8fdb\u5165FusionInsight\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin Kafka\u521b\u5efa\u6d88\u606f\uff1a ./kafka-topics.sh --create --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --replication-factor 1 --partitions 1 --topic test Kafka\u67e5\u770b\u6d88\u606f\uff1a ./kafka-topics.sh --list --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test Kafka\u7ed9\u6d88\u606f\u6388\u6743\uff1a ./kafka-acls.sh --authorizer-properties zookeeper.connect=162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --add --operation All --allow-principal User:* --cluster --topic test \u7f16\u8f91rkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rkafka \u547d\u4ee4\uff1a GGSCI> edit param rkafka \u7f16\u8f91kafka.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/kafka.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/kafka.props \u5176\u4e2d gg.handler.kafkahandler.BlockingSend \u5c5e\u6027\u63a7\u5236\u540c\u6b65\u548c\u5f02\u6b65\uff0c\u9ed8\u8ba4false\uff0c\u5f02\u6b65\u3002 GGSCI> shell vi dirprm/custom_kafka_producer.properties \u4fee\u6539Kafka\u91cc\u7684\u914d\u7f6e\uff0c\u5c06\u5982\u4e0b\u9009\u9879\u4fee\u6539\u4e3aTrue \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brkafka\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rkafka, exttrail dirdat/rk GGSCI>start rkafka GGSCI>info all \u914d\u7f6eGoldenGate Flume \u590d\u5236\u8fdb\u7a0b \u00b6 \u5b89\u88c5Flume\u5ba2\u6237\u7aef\uff0c\u914d\u7f6e\u975e\u52a0\u5bc6\u4f20\u8f93 \u914d\u7f6eServer\u7684\u914d\u7f6e\u6587\u4ef6properties.properties \u5bfc\u51fa\u7684properties.properties\u6587\u4ef6\uff0c\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a \u53ef\u4ee5\u5728HDFS\u4e2d\u589e\u52a0/ogg/flume\u76ee\u5f55 \u5c06\u6b64properties.properties\u6587\u4ef6\u4e0a\u4f20\u81f3FusionInsight\u3002 \u7f16\u8f91rflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rflume \u547d\u4ee4\uff1a GGSCI> edit param rflume \u7f16\u8f91flume.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/flume.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/flume.props gg.handler.flumehandler.PropagateSchema=false \u63a7\u5236DDL gg.handler.flumehandler.format.WrapMessageInGenericAvroMessage=false \u76f8\u540cSCHAME\u6253\u5305 GGSCI> shell vi dirprm/custom-flume-rpc.properties \u62f7\u8d1dflume.keytab\u6587\u4ef6\u5230 /opt/OGG_HADOOP/dirprm/ \u76ee\u5f55\u4e0b \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brflume\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rflume, exttrail dirdat/rf GGSCI>start rflume GGSCI>info all \u6d4b\u8bd5\u7ed3\u679c \u00b6 Oracle\u7aef\u542f\u52a8\u6240\u6709\u7684\u4f20\u8f93\u8fdb\u7a0b \u00b6 \u786e\u4fdd\u6240\u6709\u4f20\u8f93\u8fdb\u7a0b\u5747\u5df2\u7ecf\u6b63\u5e38\u542f\u52a8 \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aInsert\u64cd\u4f5c \u00b6 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aUpdate\u64cd\u4f5c \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a \u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aDelete\u64cd\u4f5c \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0chadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"12.3 <--> C80"},{"location":"Data_Integration/Oracle_GoldenGate/#oracle-goldengatefusioninsight","text":"","title":"Oracle GoldenGate\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Oracle_GoldenGate/#_1","text":"Oracle GoldenGate 12.2 \u2194 FusionInsight HD V100R002C60U20 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.3 \u2194 FusionInsight HD V100R002C80SPC100 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.2 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Flume/Kafka) Oracle GoldenGate 12.2 \u2194 FusionInsight MRS 8.0 (HDFS/Flume/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Oracle_GoldenGate/#_2","text":"","title":"\u73af\u5883\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_3","text":"Oracle GoldenGate 12.2.0.1.1 for Oracle database Oracle GoldenGate 12.2.0.1.1 for BigData Oracle database 12.1.0.2.0 jdk-7u71-linux-x64.rpm FusionInsight V100R002C60U20","title":"\u8f6f\u4ef6\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_4","text":"\u6e90\u7aefOGG VM: 162.1.115.68 Redhat6.5 \uff08\u5305\u542bOracle DB12c\u7684\u6570\u636e\u5e93\uff09 \u76ee\u6807\u7aefOGG VM: 162.1.115.69 Redhat6.5\uff08\u5305\u542bHadoop\u7684\u5ba2\u6237\u7aef\uff09","title":"\u786c\u4ef6\u4fe1\u606f"},{"location":"Data_Integration/Oracle_GoldenGate/#_5","text":"\u6d4b\u8bd5\u62d3\u6734\u7ed3\u6784\u5982\u4e0b\u56fe\u6240\u793a\uff1a","title":"\u62d3\u6734\u7ed3\u6784"},{"location":"Data_Integration/Oracle_GoldenGate/#_6","text":"\u6e90\u7aef\u6d4b\u8bd5\u8868\uff1a \u5728\u6e90\u7aefOracle\u7684PDBORCL\u6570\u636e\u5e93\u7684test\u7528\u6237\u4e0b\u521b\u5efatest1\u8868\uff0c\u5176\u4e2dID\u4e3a\u4e3b\u952e","title":"\u6d4b\u8bd5\u8868"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-oracle","text":"\u524d\u7f6e\u6761\u4ef6\uff1a\u5b8c\u6210oracle12c\u6570\u636e\u5e93\u7684\u5b89\u88c5\uff08IP\uff1a162.1.115.68\uff09 \u8f6f\u4ef6\u7248\u672c\uff1alinuxamd64_12102_database_1of2.zip, linuxamd64_12102_database_1of2.zip","title":"OGG for Oracle\u5b89\u88c5"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-oracle_1","text":"\u5c06fbo_ggs_Linux_x64_shiphome.zip\u4e0a\u4f20\u81f3oracle\u5ba2\u6237\u7aef\uff08ip\uff1a162.1.115.68\uff09 /home/oracle \u76ee\u5f55\u4e0b\uff0c\u5207\u6362\u81f3oracle\u7528\u6237\uff0c\u89e3\u538b\u751f\u6210bo_ggs_Linux_x64_shiphome\u76ee\u5f55\u3002 \u5728 /home/oracle/fbo_ggs_Linux_x64_shiphome/Disk1 \u76ee\u5f55\u4e0b\uff0c\u8fd0\u884c ./runInstaller \u5b89\u88c5\u6210\u529f\uff0c/home/orcle/OGG/\u662fOGG for Oracle\u7684\u5b89\u88c5\u76ee\u5f55\u3002","title":"\u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Oracle"},{"location":"Data_Integration/Oracle_GoldenGate/#_7","text":"\u5207\u6362\u5230oracle\u7528\u6237 su - oracle vi .bash_profile \u6587\u4ef6.bash_profile\u5185\u5bb9\u5982\u4e0b\uff1a ```shell # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH= PATH: PATH: HOME/bin export PATH PATH= PATH: PATH: HOME/bin:/u01/app/oracle/product/12.1.0/db_1/bin export PATH umask 022 export ORACLE_BASE=/u01/app/oracle export ORACLE_HOME=/u01/app/oracle/product/12.1.0/db_1 export ORACLE_SID=orcl export LD_LIBRARY_PATH=$ORACLE_HOME/lib ``` \u8fd0\u884cOGG","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"Data_Integration/Oracle_GoldenGate/#_8","text":"\u4f7f\u7528Sqlplus / as sysdba\u767b\u9646Oracle\u6e90\u7aef\u6570\u636e\u5e93\u540e\u6253\u5f00Archive Log: shutdown immediate ; startup mount ; alter database archivelog ; alter database open ; archive log list ; \u6e90\u7aef\u6570\u636e\u5e93\u6253\u5f00\u6570\u636e\u5e93\u7ea7\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7\u53caforce logging\uff1a SELECT supplemental_log_data_min , force_logging FROM v$database ; alter database add supplemental log data ; alter database force logging ; \u5207\u6362\u65e5\u5fd7\u4ee5\u4f7f\u9644\u52a0\u65e5\u5fd7\u751f\u6548\uff1a ALTER SYSTEM switch logfile ; Enabling Oracle GoldenGate in the Database: show parameter enable_goldengate_replication ; alter system set enable_goldengate_replication = true scope = both ; \u914d\u7f6eDB12c PDB\u7684tnsname\u4fe1\u606f vi $ORACLE_HOME/network/admin/tnsnames.ora \uff1a","title":"\u6253\u5f00\u6570\u636e\u5e93\u5f52\u6863\u53ca\u5f00\u542f\u6700\u5c0f\u9644\u52a0\u65e5\u5fd7"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg","text":"\u4f7f\u7528 sqlplus / as sysdba \u767b\u9646\u6570\u636e\u5e93\u540e\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650 create user c ## ogg identified by welcome1 ; grant dba to c ## ogg container = all ; grant create session , connect , resource to c ## ogg container = all ; grant alter any table to c ## ogg container = all ; grant alter system to c ## ogg container = all ; exec dbms_goldengate_auth . grant_admin_privilege ( 'c##ogg' , container => 'all' );","title":"\u5728\u6570\u636e\u5e93\u4e2d\u521b\u5efaogg\u7528\u6237\u5e76\u8d4b\u4e88\u6743\u9650"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate","text":"\u5728GoldenGate\u4e2d\u521b\u5efa\u7528\u6237\u522b\u540d\uff0c\u7528\u4e8e\u767b\u5f55Oracle\u6570\u636e\u5e93\u8bfb\u53d6\u6570\u636e\u5e93\u65e5\u5fd7\uff1a add credentialstore ALTER CREDENTIALSTORE ADD USER c ## ogg PASSWORD welcome1 ALIAS ogg_src \u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u522b\u540dogg_src\u767b\u9646\u6570\u636e\u5e93\u4e86\uff1a dblogin useridalias ogg_src C##ogg\u662fOracle DB12c\u7684\u666e\u901a\u7528\u6237\uff0c\u53ef\u4ee5\u8bbf\u95ee\u591a\u4e2a\u6570\u636e\u5e93\u5b9e\u4f8b\u3002","title":"\u914d\u7f6eGoldenGate \u767b\u9646\u6570\u636e\u5e93\u7684\u522b\u540d"},{"location":"Data_Integration/Oracle_GoldenGate/#testtest1","text":"test\u7528\u6237\u662f\u57fa\u4e8epdborcl\u6570\u636e\u5e93\u5b9e\u4f8b\u7684\uff1a \u767b\u9646\u6570\u636e\u5e93 Sqlplus / as sysdba \u521b\u5efa\u7528\u6237 alter session set container = pdborcl ; alter database open ; create user test identified by welcome1 ; grant resource , connect to test ; CREATE TABLESPACE test DATAFILE '/u01/app/oracle/oradata/orcl/pdborcl/test01.dbf' SIZE 500 M UNIFORM SIZE 128 k ; alter user test quota unlimited on test ; alter user test quota unlimited on users ; \u521b\u5efa\u6d4b\u8bd5\u8868 conn test / welcome1 @ pdborcl ; create table test1 ( id number primary key , name varchar2 ( 50 ));","title":"\u521b\u5efatest\u7528\u6237\u548ctest1\u8868"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_1","text":"\u7f16\u8f91eora.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cedit param eora\u547d\u4ee4\uff1a GGSCI> edit param eora GGSCI> edit param mgr GGSCI> edit param phdfs GGSCI> edit param phbase GGSCI> edit param pkafka GGSCI> edit param pflume \u7f16\u8f91 diroby/eora.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/eora.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) \u4f7f\u7528oracle\u7528\u6237\u521b\u5efadiroby\u76ee\u5f55\uff1a cd /home/oracle/OGG/ mkdir diroby GGSCI> shell vi diroby/eora.oby \u6ce8\u610f\u8fdb\u7a0b\u540deora\u548c\u6570\u636e\u6587\u4ef6dirdat/eo\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884cobey diroby/eora.oby\u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0beora\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/eora.oby \u628a\u6355\u83b7\u8fdb\u7a0beora\u6ce8\u518c\u5230pdborcl\u6570\u636e\u5e93\u4e2d\uff1a GGSCI> dblogin useridalias ogg_src GGSCI> register extract eora database container(pdborcl) \u4e3apdborcl.test\u4e0b\u7684\u6240\u6709\u8868\u6dfb\u52a0\u8868\u7ea7\u9644\u52a0\u65e5\u5fd7\uff1a GGSCI> add schematrandata pdborcl.test allcols \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0beora: GGSCI> start eora","title":"\u914d\u7f6eGoldenGate\u6355\u83b7\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatephdfs","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HDFS\u5904\u7406\u3002 \u7f16\u8f91phdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phdfs \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phdfs.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phdfs.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phdfs.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phdfs**\u548c\u6570\u636e\u6587\u4ef6dirdat/rs\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phdfs.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphdfs\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phdfs.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphdfs: GGSCI> start phdfs","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphdfs"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatephbase","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate HBASE\u5904\u7406\u3002 \u7f16\u8f91phbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param phbase \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/phbase.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/phbase.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/phbase.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**phbase**\u548c\u6570\u636e\u6587\u4ef6dirdat/se\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/phbase.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bphbase\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/phbase.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bphbase: GGSCI> start phbase","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bphbase"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatepflume","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate FLUME\u5904\u7406\u3002 \u7f16\u8f91pflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pflume \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pflume.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pflume.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pflume.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pflume**\u548c\u6570\u636e\u6587\u4ef6dirdat/rf\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pflume.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpflume\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/pflume.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpflume: GGSCI> start pflume","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpflume"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengatepkafka","text":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka\uff0c\u5c06OGG\u751f\u6210\u7684\u6570\u636e\u6587\u4ef6\u4f20\u9012\u7ed9\u76ee\u6807\u7aefGoldenGate Kafka\u5904\u7406\u3002 \u7f16\u8f91pkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param pkafka \u547d\u4ee4\uff1a \u7f16\u8f91 diroby/pkafka.oby \u6587\u4ef6\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi diroby/pkafka.oby \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi diroby/pkafka.oby \u6ce8\u610f\u8fdb\u7a0b\u540d**pkafka**\u548c\u6570\u636e\u6587\u4ef6dirdat/rk\u7684\u5bf9\u5e94\u5173\u7cfb \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c obey diroby/pkafka.oby \u547d\u4ee4\uff0c\u628a\u6355\u83b7\u8fdb\u7a0bpkafka\u52a0\u5165\u5230\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> obey diroby/ pkafka.oby \u542f\u52a8GoldenGate\u6355\u83b7\u8fdb\u7a0bpkafka: GGSCI> start pkafka","title":"\u914d\u7f6eGoldenGate\u4f20\u8f93\u8fdb\u7a0bpkafka"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_2","text":"\u67e5\u770bGoldenGate\u8fdb\u7a0b\u72b6\u6001\uff1a(EORCL\u662f\u4e0eELK\u5bf9\u63a5\u7684\u8fdb\u7a0b) GGSCI> info all \u67e5\u770b\u67d0\u4e2a\u8fdb\u7a0b\u7684\u8be6\u7ec6\u4fe1\u606f\uff1a GGSCI> info eora detail \u67e5\u770bGoldenGate\u7684\u7edf\u8ba1\u4fe1\u606f\uff1a GGSCI> stats eora, latest \u67e5\u770bGoldenGate\u8fdb\u7a0b\u62a5\u544a\uff0c\u7528\u4e8e\u5b9a\u4f4d\u95ee\u9898\uff1a GGSCI> view report eora","title":"\u67e5\u770bGoldenGate\u8fdb\u7a0b\u8fd0\u884c\u72b6\u6001"},{"location":"Data_Integration/Oracle_GoldenGate/#ogg-for-bigdata","text":"","title":"OGG for Bigdata\u5b89\u88c5"},{"location":"Data_Integration/Oracle_GoldenGate/#_9","text":"\u4e0b\u8f7d\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728Bigdata\u5ba2\u6237\u7aef\u673a\u5668\u4e0a\uff08ip\uff1a162.1.115.69\uff09\u6309\u7167FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u3002\u5c06\u5ba2\u6237\u7aefJDK\u66ff\u6362\u62101.7\u7248\u672c\u3002 \u4e0b\u8f7d\u5e76\u5b89\u88c5oracle JDK1.7 \u5c06krb5.conf\u653e\u5728/etc/\u76ee\u5f55\u4e0b \u4e0b\u8f7d\u5e76\u5b89\u88c5OGG for Bigdata \u5c06122011_ggs_Adapters_Linux_x64.zip\u4e0a\u4f20\u81f3\u5ba2\u6237\u7aef/opt\u76ee\u5f55\u4e0b\uff1a unzip 122011_ggs_Adapters_Linux_x64.zip \u5c06\u89e3\u538b\u540e\u7684ggs_Adapters_Linux_x64.tar\u89e3\u538b\u5230/opt/OGG_HADOOP\u76ee\u5f55\u4e0b\uff1a \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u66f4\u6539\u73af\u5883\u53d8\u91cf\uff0c\u7f16\u8f91\u6839\u76ee\u5f55\u4e0b vi .bash_profile # .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ] ; then . ~/.bashrc fi # User specific environment and startup programs export JAVA_HOME = /usr/java/jdk1.7.0_40 #export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre export CLASSPATH = $CLASSPATH : $JAVA_HOME /lib: $JAVA_HOME /jre/lib PATH = $JAVA_HOME /bin: $PATH : $HOME /bin export PATH #export LD_LIBRARY_PATH=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server/libjvm.so:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/server:/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.9.x86_64/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib:$LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/java/jdk1.7.0_40/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.7.0_40/jre/lib/amd64/server:/usr/java/jdk1.7.0_40/jre/lib/amd64/libjsig.so:/root/OGG_PostgreSQL/lib: $LD_LIBRARY_PATH export LD_LIBRARY_PATH = /usr/local/lib: $LD_LIBRARY_PATH Source\u73af\u5883\u53d8\u91cf\uff0c source .bash_profile . \u5c06 /opt/OGG_HADOOP/AdapterExamples/big-data \u4e0b\u7684\u56db\u4e2a\u76ee\u5f55\u4e0b\u7684\u6240\u6709\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/OGG_HADOOP/dirprm \u76ee\u5f55\u4e0b\u3002","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate_3","text":"\u7f16\u8f91mgr.prm GGSCI> edit param mgr GGSCI>start mgr GGSCI>info all","title":"\u914d\u7f6eGoldenGate\u7ba1\u7406\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-hdfs","text":"\u7f16\u8f91rhdfs.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhdfs \u547d\u4ee4\uff1a GGSCI> edit param rhdfs \u7f16\u8f91hdfs.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hdfs.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hdfs.props \u9700\u8981\u5728HDFS\u4e2d\u521b\u5efa/ogg1\u76ee\u5f55\u3002 \u5c06hdfs.keytab\u6587\u4ef6\u62f7\u8d1d\u5230/opt/OGG_HADOOP/dirprm\u76ee\u5f55\u4e2d\uff1a \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhdfs\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhdfs, exttrail dirdat/rs GGSCI>info all GGSCI>start rhdfs GGSCI>info all","title":"\u914d\u7f6eGoldenGate HDFS \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-hbase","text":"\u7f16\u8f91rhbase.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rhbase \u547d\u4ee4\uff1a GGSCI> edit param rhbase \u7f16\u8f91hbase.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/hbase.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/hbase.props \u62f7\u8d1dhbase.keytab\u548cjaas.conf\u5230 /opt/OGG_HADOOP/dirprm/ \u4e0b\uff1a jaas.conf \u6587\u4ef6 \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brhbase\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rhbase, exttrail dirdat/se GGSCI>start rhbase GGSCI>info all","title":"\u914d\u7f6eGoldenGate HBase \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-kafka","text":"\u521b\u5efakafka\u6d88\u606f\uff0c\u8fdb\u5165FusionInsight\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin Kafka\u521b\u5efa\u6d88\u606f\uff1a ./kafka-topics.sh --create --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --replication-factor 1 --partitions 1 --topic test Kafka\u67e5\u770b\u6d88\u606f\uff1a ./kafka-topics.sh --list --zookeeper 162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test Kafka\u7ed9\u6d88\u606f\u6388\u6743\uff1a ./kafka-acls.sh --authorizer-properties zookeeper.connect=162.1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --add --operation All --allow-principal User:* --cluster --topic test \u7f16\u8f91rkafka.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rkafka \u547d\u4ee4\uff1a GGSCI> edit param rkafka \u7f16\u8f91kafka.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/kafka.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/kafka.props \u5176\u4e2d gg.handler.kafkahandler.BlockingSend \u5c5e\u6027\u63a7\u5236\u540c\u6b65\u548c\u5f02\u6b65\uff0c\u9ed8\u8ba4false\uff0c\u5f02\u6b65\u3002 GGSCI> shell vi dirprm/custom_kafka_producer.properties \u4fee\u6539Kafka\u91cc\u7684\u914d\u7f6e\uff0c\u5c06\u5982\u4e0b\u9009\u9879\u4fee\u6539\u4e3aTrue \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brkafka\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rkafka, exttrail dirdat/rk GGSCI>start rkafka GGSCI>info all","title":"\u914d\u7f6eGoldenGate Kafka \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#goldengate-flume","text":"\u5b89\u88c5Flume\u5ba2\u6237\u7aef\uff0c\u914d\u7f6e\u975e\u52a0\u5bc6\u4f20\u8f93 \u914d\u7f6eServer\u7684\u914d\u7f6e\u6587\u4ef6properties.properties \u5bfc\u51fa\u7684properties.properties\u6587\u4ef6\uff0c\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a \u53ef\u4ee5\u5728HDFS\u4e2d\u589e\u52a0/ogg/flume\u76ee\u5f55 \u5c06\u6b64properties.properties\u6587\u4ef6\u4e0a\u4f20\u81f3FusionInsight\u3002 \u7f16\u8f91rflume.prm\uff0c\u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c edit param rflume \u547d\u4ee4\uff1a GGSCI> edit param rflume \u7f16\u8f91flume.props, \u5728GGSCI\u547d\u4ee4\u884c\u4e0b\u8fd0\u884c shell vi dirprm/flume.props \u547d\u4ee4\uff1a(shell\u4e4b\u540e\u63a5\u64cd\u4f5c\u7cfb\u7edf\u547d\u4ee4) GGSCI> shell vi dirprm/flume.props gg.handler.flumehandler.PropagateSchema=false \u63a7\u5236DDL gg.handler.flumehandler.format.WrapMessageInGenericAvroMessage=false \u76f8\u540cSCHAME\u6253\u5305 GGSCI> shell vi dirprm/custom-flume-rpc.properties \u62f7\u8d1dflume.keytab\u6587\u4ef6\u5230 /opt/OGG_HADOOP/dirprm/ \u76ee\u5f55\u4e0b \u628aGoldenGate\u590d\u5236\u8fdb\u7a0brflume\u52a0\u5165\u5230GoldenGate\u7ba1\u7406\u8005\u8fdb\u7a0b\u4e2d\uff1a GGSCI> add replicat rflume, exttrail dirdat/rf GGSCI>start rflume GGSCI>info all","title":"\u914d\u7f6eGoldenGate Flume \u590d\u5236\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#_10","text":"","title":"\u6d4b\u8bd5\u7ed3\u679c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracle","text":"\u786e\u4fdd\u6240\u6709\u4f20\u8f93\u8fdb\u7a0b\u5747\u5df2\u7ecf\u6b63\u5e38\u542f\u52a8","title":"Oracle\u7aef\u542f\u52a8\u6240\u6709\u7684\u4f20\u8f93\u8fdb\u7a0b"},{"location":"Data_Integration/Oracle_GoldenGate/#oracleinsert","text":"su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aInsert\u64cd\u4f5c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracleupdate","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0c hadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aUpdate\u64cd\u4f5c"},{"location":"Data_Integration/Oracle_GoldenGate/#oracledelete","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4 su \u2013 oracle source .bash_profile sqlplus test/welcome1@pdborcl \u67e5\u770bHDFS\u540c\u6b65\u60c5\u51b5\uff0chadoop fs \u2013ls /ogg1 \u67e5\u770bHBase\u540c\u6b65\u60c5\u51b5 hbase shell \u67e5\u770bkafka\u7ed3\u679c\uff0c\u8fdb\u5165kafka\u5ba2\u6237\u7aef /opt/hadoopclient/Kafka/kafka/bin \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a ./kafka-console-consumer.sh --zookeeper 162 .1.93.101:24002,162.1.93.102:24002,162.1.93.103:24002/kafka --topic test --from-beginning \u5728HDFS\u4e2d\u67e5\u770bflume\u8fd0\u884c\u7ed3\u679c\uff1a\u67e5\u770b/ogg/flume/\u4e0b\u6570\u636e\u6587\u4ef6\uff1a","title":"\u5728Oracle\u6570\u636e\u5e93\u6e90\u7aef\u505aDelete\u64cd\u4f5c"},{"location":"Data_Integration/Pentaho/","text":"Pentaho\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Pentaho EE 7.1 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/Hive) Pentaho EE 8.0 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive) Kerberos\u652f\u6301\u80fd\u529b\u8bf4\u660e \u00b6 Pentaho(7.0-9.0)\u76ee\u524d\u4ec5\u4ec5\u5728\u4f01\u4e1a\u7248\uff08EE\uff09\u652f\u6301Kerberos\u8ba4\u8bc1\u7684Hadoop, Pentaho\u793e\u533a\u7248\uff08CE\uff09\u4e0d\u652f\u6301Kerberos\u8ba4\u8bc1\u7684Hadoop\uff0c\u76f8\u5173\u7b54\u590d\u53c2\u8003\u4ee5\u4e0b\u94fe\u63a5\uff1a https://forums.pentaho.com/threads/230953-Is-Kerberos-auth-Enterprise-only/ Kettle 6.1\uff0c\u6240\u4ee5\u867d\u7136\u4ee3\u7801\u4e0a\u6ca1\u6709\u652f\u6301Kerberos\u8ba4\u8bc1\uff0c\u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u624b\u52a8\u5728OS\u5c42\u9762\u8fdb\u884cKerberos\u8ba4\u8bc1\u6765\u8fde\u63a5\u5b89\u5168\u96c6\u7fa4\uff0c6.1\u4ee5\u540e\u7684CE\u7248\u672c\u7531\u4e8e\u67b6\u6784\u8c03\u6574\uff0c\u65e0\u6cd5\u901a\u8fc7\u8bfb\u53d6OS\u4e0a\u7684Kerberos\u8ba4\u8bc1\u4fe1\u606f\u8fde\u63a5\u5b89\u5168\u96c6\u7fa4\u3002","title":"8.0 <--> C60"},{"location":"Data_Integration/Pentaho/#pentahofusioninsight","text":"","title":"Pentaho\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Pentaho/#_1","text":"Pentaho EE 7.1 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/Hive) Pentaho EE 8.0 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Pentaho/#kerberos","text":"Pentaho(7.0-9.0)\u76ee\u524d\u4ec5\u4ec5\u5728\u4f01\u4e1a\u7248\uff08EE\uff09\u652f\u6301Kerberos\u8ba4\u8bc1\u7684Hadoop, Pentaho\u793e\u533a\u7248\uff08CE\uff09\u4e0d\u652f\u6301Kerberos\u8ba4\u8bc1\u7684Hadoop\uff0c\u76f8\u5173\u7b54\u590d\u53c2\u8003\u4ee5\u4e0b\u94fe\u63a5\uff1a https://forums.pentaho.com/threads/230953-Is-Kerberos-auth-Enterprise-only/ Kettle 6.1\uff0c\u6240\u4ee5\u867d\u7136\u4ee3\u7801\u4e0a\u6ca1\u6709\u652f\u6301Kerberos\u8ba4\u8bc1\uff0c\u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u624b\u52a8\u5728OS\u5c42\u9762\u8fdb\u884cKerberos\u8ba4\u8bc1\u6765\u8fde\u63a5\u5b89\u5168\u96c6\u7fa4\uff0c6.1\u4ee5\u540e\u7684CE\u7248\u672c\u7531\u4e8e\u67b6\u6784\u8c03\u6574\uff0c\u65e0\u6cd5\u901a\u8fc7\u8bfb\u53d6OS\u4e0a\u7684Kerberos\u8ba4\u8bc1\u4fe1\u606f\u8fde\u63a5\u5b89\u5168\u96c6\u7fa4\u3002","title":"Kerberos\u652f\u6301\u80fd\u529b\u8bf4\u660e"},{"location":"Data_Integration/SharePlex/","text":"SharePlex\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SharePlex 9.2.1 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) SharePlex 9.2.1 \u2194 FusionInsight HD 6.5 (Kafka) \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post \u00b6 \u6839\u636e\u5bf9\u63a5\u6a21\u5f0f\u4e0d\u540c\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u5bf9\u63a5\u65b9\u5f0f \u4f7f\u7528Kafka\u666e\u901a\u6a21\u5f0f\u5bf9\u63a5 \u4f7f\u7528Kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5 \u666e\u901a\u6a21\u5f0f\u5bf9\u63a5 \u00b6 \u4fee\u6539Kafka\u8ba4\u8bc1\u65b9\u5f0f \u5728FusionInsight\u670d\u52a1\u7aef\u4fee\u6539Kafka\u7684\u914d\u7f6e\u53c2\u6570 allow.everyone.if.no.acl.found \u4e3a True \uff0c\u5141\u8bb8\u4f7f\u752821005\u7684\u666e\u901a\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post\u914d\u7f6e target x.kafka set kafka broker = 172.16.6.11:21005 target x.kafka set kafka security.protocol = PLAINTEXT target x.kafka set kafka topic = splex \u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5 \u00b6 \u4fee\u6539/etc/hosts\u6587\u4ef6 \u5728SharePlex Linux for Open Target\u4e0a\u4fee\u6539hadoop\u96c6\u7fa4\u6240\u6709\u7684\u4e3b\u673aIP\u5730\u5740\u90fd\u5bf9\u5e94\u5230 hadoop.hadoop.com ,\u4f7f\u5f97\u89e3\u6790\u5bf9\u5e94kafka service \u7684kerberos principal\u80fd\u591f\u6b63\u786e\u586b\u5199\u4e3a kafka/hadoop.hadoop.com@HADOOP.COM \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post\u914d\u7f6e target x.kafka set kafka broker = 172.16.6.11:21007 target x.kafka set kafka sasl.kerberos.keytab = /home/quest/user.keytab target x.kafka set kafka sasl.kerberos.kinit.cmd = kinit -k -t \"%{sasl.kerberos.keytab}\" %{sasl.kerberos.principal} target x.kafka set kafka sasl.kerberos.min.time.before.relogin = 60000 target x.kafka set kafka sasl.kerberos.principal = developuser@HADOOP.COM target x.kafka set kafka sasl.kerberos.service.name = kafka target x.kafka set kafka sasl.mechanisms = GSSAPI target x.kafka set kafka security.protocol = SASL_PLAINTEXT target x.kafka set kafka threshold_size = 10000 target x.kafka set kafka topic = splex \u542f\u52a8kafka\u7684post\u8fdb\u7a0b \u00b6 \u6267\u884c start post \u6267\u884c show status \u9a8c\u8bc1Oracle\u7684\u64cd\u4f5c\u662f\u5426\u540c\u6b65\u5230Kafka \u00b6 \u5728Oracle\u7aef\u8fdb\u884c\u589e\u5220\u6539\u64cd\u4f5c \u67e5\u770bKafka\u7684Topic\u4e2d\u662f\u5426\u4e0a\u62a5\u5bf9\u5e94\u7684JSON\u6d88\u606f","title":"9.2.1 <--> 6.5"},{"location":"Data_Integration/SharePlex/#shareplexfusioninsight","text":"","title":"SharePlex\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/SharePlex/#_1","text":"SharePlex 9.2.1 \u2194 FusionInsight HD V100R002C80SPC200 (Kafka) SharePlex 9.2.1 \u2194 FusionInsight HD 6.5 (Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/SharePlex/#shareplexkafkapost","text":"\u6839\u636e\u5bf9\u63a5\u6a21\u5f0f\u4e0d\u540c\uff0c\u9009\u62e9\u4e0d\u540c\u7684\u5bf9\u63a5\u65b9\u5f0f \u4f7f\u7528Kafka\u666e\u901a\u6a21\u5f0f\u5bf9\u63a5 \u4f7f\u7528Kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5","title":"\u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post"},{"location":"Data_Integration/SharePlex/#_2","text":"\u4fee\u6539Kafka\u8ba4\u8bc1\u65b9\u5f0f \u5728FusionInsight\u670d\u52a1\u7aef\u4fee\u6539Kafka\u7684\u914d\u7f6e\u53c2\u6570 allow.everyone.if.no.acl.found \u4e3a True \uff0c\u5141\u8bb8\u4f7f\u752821005\u7684\u666e\u901a\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post\u914d\u7f6e target x.kafka set kafka broker = 172.16.6.11:21005 target x.kafka set kafka security.protocol = PLAINTEXT target x.kafka set kafka topic = splex","title":"\u666e\u901a\u6a21\u5f0f\u5bf9\u63a5"},{"location":"Data_Integration/SharePlex/#_3","text":"\u4fee\u6539/etc/hosts\u6587\u4ef6 \u5728SharePlex Linux for Open Target\u4e0a\u4fee\u6539hadoop\u96c6\u7fa4\u6240\u6709\u7684\u4e3b\u673aIP\u5730\u5740\u90fd\u5bf9\u5e94\u5230 hadoop.hadoop.com ,\u4f7f\u5f97\u89e3\u6790\u5bf9\u5e94kafka service \u7684kerberos principal\u80fd\u591f\u6b63\u786e\u586b\u5199\u4e3a kafka/hadoop.hadoop.com@HADOOP.COM \u914d\u7f6eSharePlex\u4e0a\u7684kafka\u7684post\u914d\u7f6e target x.kafka set kafka broker = 172.16.6.11:21007 target x.kafka set kafka sasl.kerberos.keytab = /home/quest/user.keytab target x.kafka set kafka sasl.kerberos.kinit.cmd = kinit -k -t \"%{sasl.kerberos.keytab}\" %{sasl.kerberos.principal} target x.kafka set kafka sasl.kerberos.min.time.before.relogin = 60000 target x.kafka set kafka sasl.kerberos.principal = developuser@HADOOP.COM target x.kafka set kafka sasl.kerberos.service.name = kafka target x.kafka set kafka sasl.mechanisms = GSSAPI target x.kafka set kafka security.protocol = SASL_PLAINTEXT target x.kafka set kafka threshold_size = 10000 target x.kafka set kafka topic = splex","title":"\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5"},{"location":"Data_Integration/SharePlex/#kafkapost","text":"\u6267\u884c start post \u6267\u884c show status","title":"\u542f\u52a8kafka\u7684post\u8fdb\u7a0b"},{"location":"Data_Integration/SharePlex/#oraclekafka","text":"\u5728Oracle\u7aef\u8fdb\u884c\u589e\u5220\u6539\u64cd\u4f5c \u67e5\u770bKafka\u7684Topic\u4e2d\u662f\u5426\u4e0a\u62a5\u5bf9\u5e94\u7684JSON\u6d88\u606f","title":"\u9a8c\u8bc1Oracle\u7684\u64cd\u4f5c\u662f\u5426\u540c\u6b65\u5230Kafka"},{"location":"Data_Integration/TIBCO_BusinessWorks/","text":"TIBCO Business Works(BW) 5.13\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Tibco BW 5.13 \u2194 FusionInsight HD 6.5 (GaussDB) \u73af\u5883\u51c6\u5907 \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5Tibco BW5.13\uff0c\u53c2\u8003Tibco\u5b98\u65b9\u6587\u6863 https://docs.tibco.com/pub/activematrix_businessworks/5.13.0/doc/pdf/TIB_BW_5.13.0_installation.pdf?id=0 \u5b89\u88c5\u5b8c\u6210\u540e\u542f\u52a8TIBCO Designer\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7a7a\u767d\u5de5\u7a0b \u8fde\u63a5GaussDB \u00b6 \u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u653e\u5728TIBCO\u5b89\u88c5\u76ee\u5f55 tpcl\\version\\jdbc \u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:\\tibco\\tpcl\\5.10\\jdbc \u76ee\u5f55\u4e0b \u5728TIBCO Designer\u4e2d\uff0c\u6dfb\u52a0 \u4e00\u4e2a\u65b0\u7684process\uff0c\u8fd9\u91cc\u53d6\u540d\u4e3a getTable \u5728\u5de5\u7a0b\u7684 Shared Resources \u4e2d\u6dfb\u52a0\u4e00\u4e2a JDBC Connection \uff0c\u547d\u540d\u4e3a Gauss Connection \uff0c\u5e76\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e JDBC Driver \u4e3apostgresql\u7684\u9a71\u52a8\u7c7b\u540d Database URL\u4e3apostgres\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u4e3ajdbc:postgresql://ip:port/postgresql,\u5bf9\u4e8eGaussDB\uff0c\u9ed8\u8ba4\u7aef\u53e3\u4e3a25308 UserName\u548cPassword\u4e3a\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801 \u70b9\u51fb Test Connection \uff0c\u5f39\u51fa\u8fde\u63a5\u6210\u529f\u7a97\u53e3 \u53cc\u51fb\u8fdb\u5165 getTable \uff0c\u4ece\u5de6\u4fa7\u7684activity\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u6d41\u7a0b \u5728 JDBC Query \u4e2d\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e,\u70b9\u51fb\u53f3\u4fa7\u641c\u7d22\u6807\u5fd7\uff0c\u5728\u5f39\u51fa\u7a97\u53e3\u4e2d\u9009\u62e9\u521a\u624d\u521b\u5efa\u7684 GAUSS COnnection ,\u8f93\u5165\u60f3\u8981\u6267\u884c\u7684SQL\u8bed\u53e5,\u8fd9\u91cc\u662f\u67e5\u8be2test\u8868\u4e2d\u7684\u6240\u6709\u5185\u5bb9 \u70b9\u51fb\u5de6\u4e0b\u89d2 fetch \uff0c\u4f1a\u5728Output\u9875\u7b7e\u4e2d\u770b\u5230\u8981\u67e5\u8be2\u7684\u8868\u7684\u5b57\u6bb5\u4fe1\u606f \u5728\u5de6\u4fa7 tester \u4e2d\u70b9\u51fb\u6267\u884c\u6309\u94ae\uff0c\u9009\u62e9 getTable \uff0c Load & Start Current \uff0c\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002 * \u5728 JDBC Query \u7684output\u4e2d\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c ![](assets/TIBCO_BusinessWorks/1289b.png) \u5bf9\u4e8eJDBC\u7684\u5176\u4ed6\u64cd\u4f5c\uff0c\u4f8b\u5982 JDBC Update\uff0cCall Procedure \u914d\u7f6e\u7c7b\u4f3c\uff0c\u53ef\u53c2\u8003TIBCO\u7684\u5b98\u65b9\u6587\u6863 http://tutorialspedia.com/jdbc-call-procedure-tutorial/","title":"5.13 <--> 6.5"},{"location":"Data_Integration/TIBCO_BusinessWorks/#tibco-business-worksbw-513fusioninsight-hd","text":"","title":"TIBCO Business Works(BW) 5.13\u5bf9\u63a5FusionInsight HD"},{"location":"Data_Integration/TIBCO_BusinessWorks/#_1","text":"Tibco BW 5.13 \u2194 FusionInsight HD 6.5 (GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/TIBCO_BusinessWorks/#_2","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5Tibco BW5.13\uff0c\u53c2\u8003Tibco\u5b98\u65b9\u6587\u6863 https://docs.tibco.com/pub/activematrix_businessworks/5.13.0/doc/pdf/TIB_BW_5.13.0_installation.pdf?id=0 \u5b89\u88c5\u5b8c\u6210\u540e\u542f\u52a8TIBCO Designer\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7a7a\u767d\u5de5\u7a0b","title":"\u73af\u5883\u51c6\u5907"},{"location":"Data_Integration/TIBCO_BusinessWorks/#gaussdb","text":"\u83b7\u53d6GaussDB\u7684\u9a71\u52a8\u5305gsjdbc4.jar\uff0c\u653e\u5728TIBCO\u5b89\u88c5\u76ee\u5f55 tpcl\\version\\jdbc \u76ee\u5f55\u4e0b\uff0c\u4f8b\u5982 C:\\tibco\\tpcl\\5.10\\jdbc \u76ee\u5f55\u4e0b \u5728TIBCO Designer\u4e2d\uff0c\u6dfb\u52a0 \u4e00\u4e2a\u65b0\u7684process\uff0c\u8fd9\u91cc\u53d6\u540d\u4e3a getTable \u5728\u5de5\u7a0b\u7684 Shared Resources \u4e2d\u6dfb\u52a0\u4e00\u4e2a JDBC Connection \uff0c\u547d\u540d\u4e3a Gauss Connection \uff0c\u5e76\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e JDBC Driver \u4e3apostgresql\u7684\u9a71\u52a8\u7c7b\u540d Database URL\u4e3apostgres\u8fde\u63a5\u5b57\u7b26\u4e32\uff0c\u683c\u5f0f\u4e3ajdbc:postgresql://ip:port/postgresql,\u5bf9\u4e8eGaussDB\uff0c\u9ed8\u8ba4\u7aef\u53e3\u4e3a25308 UserName\u548cPassword\u4e3a\u6570\u636e\u5e93\u7528\u6237\u540d\u5bc6\u7801 \u70b9\u51fb Test Connection \uff0c\u5f39\u51fa\u8fde\u63a5\u6210\u529f\u7a97\u53e3 \u53cc\u51fb\u8fdb\u5165 getTable \uff0c\u4ece\u5de6\u4fa7\u7684activity\u4e2d\u62d6\u5165\u4ee5\u4e0b\u51e0\u4e2a\u6d41\u7a0b \u5728 JDBC Query \u4e2d\u8fdb\u884c\u4ee5\u4e0b\u914d\u7f6e,\u70b9\u51fb\u53f3\u4fa7\u641c\u7d22\u6807\u5fd7\uff0c\u5728\u5f39\u51fa\u7a97\u53e3\u4e2d\u9009\u62e9\u521a\u624d\u521b\u5efa\u7684 GAUSS COnnection ,\u8f93\u5165\u60f3\u8981\u6267\u884c\u7684SQL\u8bed\u53e5,\u8fd9\u91cc\u662f\u67e5\u8be2test\u8868\u4e2d\u7684\u6240\u6709\u5185\u5bb9 \u70b9\u51fb\u5de6\u4e0b\u89d2 fetch \uff0c\u4f1a\u5728Output\u9875\u7b7e\u4e2d\u770b\u5230\u8981\u67e5\u8be2\u7684\u8868\u7684\u5b57\u6bb5\u4fe1\u606f \u5728\u5de6\u4fa7 tester \u4e2d\u70b9\u51fb\u6267\u884c\u6309\u94ae\uff0c\u9009\u62e9 getTable \uff0c Load & Start Current \uff0c\u53ef\u4ee5\u770b\u5230\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002 * \u5728 JDBC Query \u7684output\u4e2d\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c ![](assets/TIBCO_BusinessWorks/1289b.png) \u5bf9\u4e8eJDBC\u7684\u5176\u4ed6\u64cd\u4f5c\uff0c\u4f8b\u5982 JDBC Update\uff0cCall Procedure \u914d\u7f6e\u7c7b\u4f3c\uff0c\u53ef\u53c2\u8003TIBCO\u7684\u5b98\u65b9\u6587\u6863 http://tutorialspedia.com/jdbc-call-procedure-tutorial/","title":"\u8fde\u63a5GaussDB"},{"location":"Data_Integration/Talend_6.4.1/","text":"Talend\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase) Talend 6.4.1 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/Hetu) \u6ce8\uff1a\u56e0\u4e3aTalend 7.0.1\u7248\u672cbug\uff0cHive\u7ec4\u4ef6\u65e0\u6cd5\u5728\u7248\u672c7.0.1\u4e2d\u901a\u8fc7\uff0c\u5bf9\u63a5hive\u7ec4\u4ef6\u4f7f\u7528Talend 6.4.1\u7248\u672c \u5b89\u88c5Talend \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Talend 7.0.1 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5(\u53ef\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6) \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cfJAVA_HOME,Path \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5411FusionInsight HD\u96c6\u7fa4\u7ba1\u7406\u5458\u83b7\u53d6\u96c6\u7fa4Kerberos\u7684krb5.conf\u6587\u4ef6,\u628a\u76f8\u5e94\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini,\u5e76\u653e\u5230 C:\\Windows \u76ee\u5f55\u4e0b\uff08Talend\u9ed8\u8ba4\u4ece\u6b64\u76ee\u5f55\u4e0b\u67e5\u627e\uff09 \u4e0b\u8f7dTOS\u5e76\u4fee\u6539TOS\u542f\u52a8\u53c2\u6570 \u5728 https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dTOS\uff0c\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8TOS_BD\uff0c\u8fd0\u884cTOS_BD-win-x86_64.exe \u5b89\u88c5\u5fc5\u9700\u7684\u7b2c\u4e09\u65b9\u5e93 Talend\u8fde\u63a5HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HDFS\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6 HDFS Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6dfb\u52a0tHDFSConnection\u7ec4\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\uff1a 1: \u9009\u62e9Cloudera\uff0c\u7248\u672c\u4e3aCloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - \u6d4b\u8bd5\u7ed3\u679c\uff1a HDFS Get \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSGet\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a /tmp/talend_test \u8def\u5f84\u4e0b\u5df2\u7ecf\u4f20\u5165\u6587\u4ef6 out.csv \uff0c C:/SOFT \u4e3a\u672c\u5730\u8f93\u51fa\u6587\u4ef6\u8def\u5f84 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5230\u672c\u5730\u8def\u5f84 C:/SOFT \u4e0b\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c HDFS Put \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSPut\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u672c\u5730\u76ee\u5f55 C:/SOFT \u4e0b\u521b\u5efa\u6587\u4ef6 HDFSPut.txt , \u5185\u5bb9\u5982\u4e0b\uff1a It is create on local PC. \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767b\u5f55\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a Talend\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 6.4.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6 Hive Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bf9\u63a5Hive\u7ec4\u4ef6Talend\u7248\u672c\u9700\u89816.4.1 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fbDistritution\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u5bfc\u5165FusionInsight HD\u5ba2\u6237\u7aefHive\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 \u6d4b\u8bd5\u7ed3\u679c\uff1a Hive Create Table & Load \u64cd\u4f5c\u6b65\u9aa4 \u00b6 tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveCreateTable\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6e\u9700\u8981\u5bfc\u5165hive\u8868\u7684\u7ed3\u6784 tHiveLoad\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u63d0\u524d\u9700\u8981\u5411hdfs\u6587\u4ef6\u5b58\u50a8\u7cfb\u7edf /tmp/talend_test/ \u8def\u5f84\u4e0b\u4f20\u5165\u6587\u4ef6 out.csv out.csv \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b: \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5728\u96c6\u7fa4\u4e0a\u68c0\u67e5\u4f20\u5165\u7684\u8868 createdTableTalend Hive Input \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6d4b\u8bd5\u7ed3\u679c\uff1a Hive Row \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveRow\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u8fde\u63a5\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c Talend\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 HBase Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: \u7528eclipse\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\uff08\u6837\u4f8b\u4ee3\u7801\u8def\u5f84\u5982 C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example \uff09 \u5728Talend\u91cc\u63d2\u5165tHbaseConnection\u7ec4\u4ef6\uff0c\u70b9\u51fb\u7ec4\u4ef6\u8fdb\u884c\u8bbe\u7f6e \u9996\u5148\u70b9\u51fbtHBaseConnection\u56fe\u6807\u4e0b\u9762\u7684\u7ec4\u4ef6\u6309\u94ae\uff0c\u9009\u62e9\u7248\u672c\u4e3a Custom - Unsupported \u548c Hadoop 2 \uff0c\u518d\u70b9\u51fb\u7248\u672c\u65c1\u8fb9\u7684\u6309\u94ae\u5bfc\u5165jar\u5305\uff0c\u9700\u8981\u5bfc\u5165\u7684\u662f\u4e0a\u4e00\u6b65\u5bfc\u51fa\u7684hbase_loginUtil.jar\u4ee5\u53caFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801 hbase-example \u4e2d\u5f15\u7528\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 hbase-example \u6837\u4f8b\u4ee3\u7801\u4e2dlib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u5982\u4e0b\uff1a \u4f7f\u7528tLibraryLoad\u7ec4\u4ef6\u5bfc\u5165hbase_loginUtil.jar \u70b9\u51fb Advanced settings \u5728Import\u4e2d\u589e\u52a0 import com.huawei.hadoop.security.LoginUtil; tHBaseConnection\u914d\u7f6e\u5982\u4e0b: \u5f15\u5165tJava\u7ec4\u4ef6\u7528\u5b9a\u5236\u4ee3\u7801\u66ff\u4ee3Connection\u7ec4\u4ef6 \u4ee3\u7801\u5185\u5bb9\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - \u6d4b\u8bd5\u7ed3\u679c HBase Input Output \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tLibraryLoad\uff0ctHBaseConnection\uff0ctJava\u914d\u7f6e\u4e0d\u53d8 \u52a0\u5165tFileInputDelimited\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\uff0c\u6839\u636e\u9700\u8981\u5b58\u5165\u6587\u4ef6(out.csv)\u7684\u683c\u5f0f\u5b9a\u4e49\u5217\u548c\u7c7b\u578b out.csv \u6d4b\u8bd5\u6570\u636e\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u52a0\u5165tHBaseOutput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u7f16\u8f91\u8868\u7684\u67b6\u6784\uff1a tHBaseInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u540c\u6837\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u914d\u7f6e\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e \u6d4b\u8bd5\u7ed3\u679c \u68c0\u67e5\u96c6\u7fa4\u521b\u5efa\u7684HBase\u8868 hbaseInputOutputTest \u5728\u96c6\u7fa4\u4e0a\u4f7f\u7528\u4ee3\u7801 hbase shell scan 'hbaseInputOutputTest' Talend\u8fde\u63a5Hetu \u00b6 \u8bf4\u660e\uff1a talend 6.4.1\u5bf9\u63a5mrs 8.0.2 \u7684hetu\u7ec4\u4ef6 connection\u914d\u7f6e\uff1a \u6ce8\u610f\uff1ausername \u548c password\u5168\u90e8\u5220\u6389\uff0c\u4e0d\u80fd\u6709\u5185\u5bb9\uff0c\u5e26\u53cc\u5f15\u53f7\u201c\u201d\u90fd\u4e0d\u53ef\u4ee5\uff0c\u56e0\u4e3auri\u7684\u914d\u7f6e\u4e3a\uff1a \"jdbc:presto://172.16.10.131:24002,172.16.10.132:24002,172.16.10.133:24002/hive/default?serviceDiscoveryMode=zooKeeper&zooKeeperNamespace=hsbroker&deploymentMode=on_yarn&user=developuser&SSL=true&SSLTrustStorePath=E:/mrs_hetu_config/hetuserver.jks&KerberosConfigPath=E:/mrs_hetu_config/krb5.conf&KerberosPrincipal=developuser&KerberosKeytabPath=E:/mrs_hetu_config/user.keytab&KerberosRemoteServiceName=HTTP&KerberosServicePrincipalPattern=%24%7BSERVICE%7D%40%24%7BHOST%7D\" \u5df2\u7ecf\u542b\u6709\u8fde\u63a5\u5185\u5bb9\u4e86\uff0c\u4e0d\u7136\u7684\u8bdd\u4f1a\u62a5 no value present \u7684\u9519\u8bef\uff0c\u6216\u8005\u662f Connection property 'user' is both in the URL and an argument \u9519\u8bef\u5982\u4e0b\uff1a \u53ea\u914d\u4e86User\uff0c password\u6ca1\u6709\u914d\uff1a \u62a5\u9519\uff1a user,password\u90fd\u914d\u4e86\u4e14\u4e3a\u7a7a\u503c\uff1a \u62a5\u9519\uff1a \u542f\u52a8jvm\u53c2\u6570\u914d\u7f6e\u5982\u4e0b\uff1a \u7ed3\u679c\uff1a","title":"7.0.1 <--> C80"},{"location":"Data_Integration/Talend_6.4.1/#talendfusioninsight","text":"","title":"Talend\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Talend_6.4.1/#_1","text":"Talend 6.4.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase/Hive) Talend 7.0.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/HBase) Talend 6.4.1 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/Hetu) \u6ce8\uff1a\u56e0\u4e3aTalend 7.0.1\u7248\u672cbug\uff0cHive\u7ec4\u4ef6\u65e0\u6cd5\u5728\u7248\u672c7.0.1\u4e2d\u901a\u8fc7\uff0c\u5bf9\u63a5hive\u7ec4\u4ef6\u4f7f\u7528Talend 6.4.1\u7248\u672c","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#talend","text":"","title":"\u5b89\u88c5Talend"},{"location":"Data_Integration/Talend_6.4.1/#_2","text":"\u5b89\u88c5Talend 7.0.1","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5(\u53ef\u53c2\u8003\u4ea7\u54c1\u6587\u6863->\u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6)","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_6.4.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cfJAVA_HOME,Path \u914d\u7f6eKerberos\u8ba4\u8bc1 \u5411FusionInsight HD\u96c6\u7fa4\u7ba1\u7406\u5458\u83b7\u53d6\u96c6\u7fa4Kerberos\u7684krb5.conf\u6587\u4ef6,\u628a\u76f8\u5e94\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini,\u5e76\u653e\u5230 C:\\Windows \u76ee\u5f55\u4e0b\uff08Talend\u9ed8\u8ba4\u4ece\u6b64\u76ee\u5f55\u4e0b\u67e5\u627e\uff09 \u4e0b\u8f7dTOS\u5e76\u4fee\u6539TOS\u542f\u52a8\u53c2\u6570 \u5728 https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dTOS\uff0c\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff08\u5982 C:\\developuser\\jaas.conf \uff09\uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8TOS_BD\uff0c\u8fd0\u884cTOS_BD-win-x86_64.exe \u5b89\u88c5\u5fc5\u9700\u7684\u7b2c\u4e09\u65b9\u5e93","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#talendhdfs","text":"","title":"Talend\u8fde\u63a5HDFS"},{"location":"Data_Integration/Talend_6.4.1/#_5","text":"Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HDFS\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHDFS\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_6.4.1/#hdfs-connection","text":"\u6dfb\u52a0tHDFSConnection\u7ec4\u4ef6\uff0c\u914d\u7f6e\u5982\u4e0b: \u5177\u4f53\u914d\u7f6e\uff1a 1: \u9009\u62e9Cloudera\uff0c\u7248\u672c\u4e3aCloudera CDH 5.8(YARN mode) 2: \"hdfs://172.21.3.103:25000\" 3: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" 4: \"developuser\" 5: \"C:/developuser/user.keytab\" 6: \"hadoop.security.authentication\" -> \"kerberos\" \"hadoop.rpc.protection\" -> \"privacy\" - \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"HDFS Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hdfs-get","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSGet\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u96c6\u7fa4HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a /tmp/talend_test \u8def\u5f84\u4e0b\u5df2\u7ecf\u4f20\u5165\u6587\u4ef6 out.csv \uff0c C:/SOFT \u4e3a\u672c\u5730\u8f93\u51fa\u6587\u4ef6\u8def\u5f84 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5230\u672c\u5730\u8def\u5f84 C:/SOFT \u4e0b\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"HDFS Get \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hdfs-put","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHDFSConnection\u7ec4\u4ef6\u914d\u7f6e\u4e0d\u53d8 tHDFSPut\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u6d4b\u8bd5\u524d\u5728\u672c\u5730\u76ee\u5f55 C:/SOFT \u4e0b\u521b\u5efa\u6587\u4ef6 HDFSPut.txt , \u5185\u5bb9\u5982\u4e0b\uff1a It is create on local PC. \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767b\u5f55\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"HDFS Put \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#talendhive","text":"","title":"Talend\u8fde\u63a5Hive"},{"location":"Data_Integration/Talend_6.4.1/#_7","text":"Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD Hive\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#_8","text":"\u5df2\u7ecf\u5b8c\u6210Talend 6.4.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_6.4.1/#hive-connection","text":"\u5bf9\u63a5Hive\u7ec4\u4ef6Talend\u7248\u672c\u9700\u89816.4.1 \u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b 1: Custom-Unsuported 2: Hive2 3: \"172.21.3.103:24002,172.21.3.101:24002,172.21.3.102\" 4: \"24002\" 5: \"default\" 6: \"developuser\" 7: \";serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/SOFT/cfg/user.keytab\" \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fbDistritution\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u5bfc\u5165FusionInsight HD\u5ba2\u6237\u7aefHive\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"Hive Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hive-create-table-load","text":"tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveCreateTable\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6e\u9700\u8981\u5bfc\u5165hive\u8868\u7684\u7ed3\u6784 tHiveLoad\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u63d0\u524d\u9700\u8981\u5411hdfs\u6587\u4ef6\u5b58\u50a8\u7cfb\u7edf /tmp/talend_test/ \u8def\u5f84\u4e0b\u4f20\u5165\u6587\u4ef6 out.csv out.csv \u6587\u4ef6\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b: \u6d4b\u8bd5\u7ed3\u679c\uff1a \u5728\u96c6\u7fa4\u4e0a\u68c0\u67e5\u4f20\u5165\u7684\u8868 createdTableTalend","title":"Hive Create Table &amp; Load \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hive-input","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e tHiveClose\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"Hive Input \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hive-row","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tHiveConnection\u7ec4\u4ef6\u914d\u7f6e\u4fdd\u6301\u4e0d\u53d8 tHiveRow\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b \u6ce8\u610f\uff1a\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u6765\u914d\u7f6ehive\u8868\u7684\u7ed3\u6784 \u6d4b\u8bd5\u7ed3\u679c\uff1a \u8fde\u63a5\u5230\u96c6\u7fa4\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c","title":"Hive Row \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#talendhbase","text":"","title":"Talend\u8fde\u63a5HBase"},{"location":"Data_Integration/Talend_6.4.1/#_9","text":"Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FI HD HBase\u63a5\u53e3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_6.4.1/#_10","text":"\u5df2\u7ecf\u5b8c\u6210Talend 7.0.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_6.4.1/#hbase-connection","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: \u7528eclipse\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\uff08\u6837\u4f8b\u4ee3\u7801\u8def\u5f84\u5982 C:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\HBase\\hbase-example \uff09 \u5728Talend\u91cc\u63d2\u5165tHbaseConnection\u7ec4\u4ef6\uff0c\u70b9\u51fb\u7ec4\u4ef6\u8fdb\u884c\u8bbe\u7f6e \u9996\u5148\u70b9\u51fbtHBaseConnection\u56fe\u6807\u4e0b\u9762\u7684\u7ec4\u4ef6\u6309\u94ae\uff0c\u9009\u62e9\u7248\u672c\u4e3a Custom - Unsupported \u548c Hadoop 2 \uff0c\u518d\u70b9\u51fb\u7248\u672c\u65c1\u8fb9\u7684\u6309\u94ae\u5bfc\u5165jar\u5305\uff0c\u9700\u8981\u5bfc\u5165\u7684\u662f\u4e0a\u4e00\u6b65\u5bfc\u51fa\u7684hbase_loginUtil.jar\u4ee5\u53caFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801 hbase-example \u4e2d\u5f15\u7528\u7684\u6240\u6709jar\u5305\uff0c\u5982\u679c\u8fd8\u6709\u7f3a\u5931\u7684jar\u5305\uff0c\u53ef\u7528Talend\u81ea\u5e26\u7684\u7c7b\u5e93\u8fdb\u884c\u81ea\u52a8\u8865\u5168\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u624b\u52a8\u5bfc\u5165 hbase-example \u6837\u4f8b\u4ee3\u7801\u4e2dlib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u5982\u4e0b\uff1a \u4f7f\u7528tLibraryLoad\u7ec4\u4ef6\u5bfc\u5165hbase_loginUtil.jar \u70b9\u51fb Advanced settings \u5728Import\u4e2d\u589e\u52a0 import com.huawei.hadoop.security.LoginUtil; tHBaseConnection\u914d\u7f6e\u5982\u4e0b: \u5f15\u5165tJava\u7ec4\u4ef6\u7528\u5b9a\u5236\u4ee3\u7801\u66ff\u4ee3Connection\u7ec4\u4ef6 \u4ee3\u7801\u5185\u5bb9\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/SOFT/cfg/hbase-site.xml\")); System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); - \u6d4b\u8bd5\u7ed3\u679c","title":"HBase Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#hbase-input-output","text":"\u6574\u4e2a\u6d41\u7a0b\u5982\u56fe\u6240\u793a: tLibraryLoad\uff0ctHBaseConnection\uff0ctJava\u914d\u7f6e\u4e0d\u53d8 \u52a0\u5165tFileInputDelimited\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\uff0c\u6839\u636e\u9700\u8981\u5b58\u5165\u6587\u4ef6(out.csv)\u7684\u683c\u5f0f\u5b9a\u4e49\u5217\u548c\u7c7b\u578b out.csv \u6d4b\u8bd5\u6570\u636e\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq - \u52a0\u5165tHBaseOutput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff1a \u6ce8\u610f\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u7f16\u8f91\u8868\u7684\u67b6\u6784\uff1a tHBaseInput\u7ec4\u4ef6\u914d\u7f6e\u5982\u4e0b\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u540c\u6837\u9700\u8981\u70b9\u51fb\u7f16\u8f91\u67b6\u6784\u65c1\u8fb9\u7684\u6309\u94ae\u914d\u7f6e\u8868\u7684\u7ed3\u6784 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e \u6d4b\u8bd5\u7ed3\u679c \u68c0\u67e5\u96c6\u7fa4\u521b\u5efa\u7684HBase\u8868 hbaseInputOutputTest \u5728\u96c6\u7fa4\u4e0a\u4f7f\u7528\u4ee3\u7801 hbase shell scan 'hbaseInputOutputTest'","title":"HBase Input Output \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_6.4.1/#talendhetu","text":"\u8bf4\u660e\uff1a talend 6.4.1\u5bf9\u63a5mrs 8.0.2 \u7684hetu\u7ec4\u4ef6 connection\u914d\u7f6e\uff1a \u6ce8\u610f\uff1ausername \u548c password\u5168\u90e8\u5220\u6389\uff0c\u4e0d\u80fd\u6709\u5185\u5bb9\uff0c\u5e26\u53cc\u5f15\u53f7\u201c\u201d\u90fd\u4e0d\u53ef\u4ee5\uff0c\u56e0\u4e3auri\u7684\u914d\u7f6e\u4e3a\uff1a \"jdbc:presto://172.16.10.131:24002,172.16.10.132:24002,172.16.10.133:24002/hive/default?serviceDiscoveryMode=zooKeeper&zooKeeperNamespace=hsbroker&deploymentMode=on_yarn&user=developuser&SSL=true&SSLTrustStorePath=E:/mrs_hetu_config/hetuserver.jks&KerberosConfigPath=E:/mrs_hetu_config/krb5.conf&KerberosPrincipal=developuser&KerberosKeytabPath=E:/mrs_hetu_config/user.keytab&KerberosRemoteServiceName=HTTP&KerberosServicePrincipalPattern=%24%7BSERVICE%7D%40%24%7BHOST%7D\" \u5df2\u7ecf\u542b\u6709\u8fde\u63a5\u5185\u5bb9\u4e86\uff0c\u4e0d\u7136\u7684\u8bdd\u4f1a\u62a5 no value present \u7684\u9519\u8bef\uff0c\u6216\u8005\u662f Connection property 'user' is both in the URL and an argument \u9519\u8bef\u5982\u4e0b\uff1a \u53ea\u914d\u4e86User\uff0c password\u6ca1\u6709\u914d\uff1a \u62a5\u9519\uff1a user,password\u90fd\u914d\u4e86\u4e14\u4e3a\u7a7a\u503c\uff1a \u62a5\u9519\uff1a \u542f\u52a8jvm\u53c2\u6570\u914d\u7f6e\u5982\u4e0b\uff1a \u7ed3\u679c\uff1a","title":"Talend\u8fde\u63a5Hetu"},{"location":"Data_Integration/Talend_7.2.1/","text":"Talend\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Talend 7.2.1 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive) \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001HIVE\u3001HBASE\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06krb5.conf\u548cuser.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u590d\u5236krb5.conf\u6587\u4ef6\u5e76\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u653e\u5728 C:\\Windows \u76ee\u5f55\u4e0b\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002FusionInsight HD\u5ba2\u6237\u7aef\u89e3\u538b\u4e8e\u672c\u5730 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig Zookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff0c\u5982 C:\\developuser\\jaas.conf \uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u672c\u5730 C:\\Windows\\System32\\drivers\\etc\\hosts \u5df2\u6dfb\u52a0FusionInsight\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0ehostname\u7684\u6620\u5c04\u3002 \u672c\u5730\u5df2\u5b89\u88c5Hadoop\u670d\u52a1\uff08\u53ef\u4ece https://hadoop.apache.org/releases.html \u4e0b\u8f7dHadoop\u4e8c\u8fdb\u5236\uff09\uff0c\u8be5\u9879\u53ef\u9009\u3002\u5982\u679c\u672c\u5730\u6ca1\u5b89\u88c5Hadoop\u670d\u52a1\uff0ctalend\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0eHadoop\u76f8\u5173\u7684\u9519\u8bef\u65e5\u5fd7\uff0c\u4f46\u4e0d\u5f71\u54cd\u5b9e\u9645\u8fd0\u884c\u7ed3\u679c\u3002 \u5b89\u88c5Talend \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Talend Open Studio for Big Data \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4ece https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dWindow\u7248\u7684Talend\u3002 \u89e3\u538b\u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u70b9\u51fbTOS_BD-win-x86_64.exe\u542f\u52a8Talend Open Studio for Big Data\u3002\u70b9\u51fb \u6211\u540c\u610f \u3002 \u70b9\u51fb \u5b8c\u6210 \uff0c\u9ed8\u8ba4\u521b\u5efaLocal_Project\u7684\u5de5\u7a0b\u3002 \u9009\u62e9\u5b89\u88c5\u5fc5\u987b\u7684\u7b2c\u4e09\u65b9\u5e93\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u5728\u53f3\u4e0b\u89d2\u53ef\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6\u3002 \u521b\u5efaHadoop\u670d\u52a1 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u521b\u5efa\u5305\u542bHDFS\u3001HIVE\u3001HBASE\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u3001HBASE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 C:\\teland\\config \u76ee\u5f55\u4e0b\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\config \u7684hbase-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u521b\u5efaHadoop\u96c6\u7fa4 \u00b6 \u6253\u5f00 Talend Open Studio for Big Data \uff0c\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster \uff0c\u53f3\u952e Hadoop Cluster \u9009\u62e9 Create Hadoop Cluster \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201cFusionInsight\u201d\uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9 \u4ece\u672c\u5730\u6587\u4ef6\u5bfc\u5165\u914d\u7f6e \uff0c\u70b9\u51fb Next \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u76ee\u5f55 C:\\talend\\config \uff0c\u9ed8\u8ba4\u5168\u9009\uff0c\u70b9\u51fb Finish \u3002 \u201cDistribution\u201d\u9009\u62e9 Custom - Unsuported \uff0c\u70b9\u51fb\u4e0b\u62c9\u6846\u53f3\u8fb9\u7684 \u6309\u94ae\u5bfc\u5165HDFS\u3001HIVE\u3001HBASE\u76f8\u5173\u7684jar\u5305\u3002 \u70b9\u51fb Cancel \u53d6\u6d88\u81ea\u52a8\u5f39\u51fa\u7684\u201c\u5bfc\u5165\u81ea\u5b9a\u4e49\u7684\u5b9a\u4e49\u201d\u7a97\u53e3\u3002 \u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u70b9\u51fb \u6309\u94ae\u6dfb\u52a0HDFS\u76f8\u5173\u7684jar\u5305\u3002 \u9009\u62e9 \u5916\u90e8\u5e93 \uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\hdfs \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\uff0c\u70b9\u51fb OK \u5bfc\u5165jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common \u548c C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHive\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\jdbc \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHBase\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.3.1.tar.gz\\hbase\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002\u5b8c\u6210\u540e\uff0c\u70b9\u51fb OK \u3002 \u914d\u7f6eKerberos\u8ba4\u8bc1\u3002\u201cCustom->Authentication\u201d\u9009\u62e9 Kerberos \u3002 \u52fe\u9009 Authentication->Enable Kerberos security \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Namenode Principal = hdfs/hadoop.hadoop.com@HADOOP.COM \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u5907\u6ce8\uff1a Namenode Principal\u7684\u53d6\u503c\u4e3ahdfs-site.xml\u7684dfs.namenode.kerberos.principal\u7684value\u503c\uff1b \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u7684\u53d6\u503c\u4e3ayarn-site.xml\u7684yarn.resourcemanager.principal\u7684value\u503c\uff1b \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53\u7684\u53d6\u503c\u4e3amapred-site.xml\u7684mapreduce.jobhistory.principal\u7684value\u503c\u3002 \u52fe\u9009 Authentication->Use a keytab to authenticate \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Principal = developuser Keytab = C:/developuser/user.keytab \u5907\u6ce8\uff1a Principal\u4e3aFusionInsight Manager\u7684\u7528\u6237\u540d\uff0cKeytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u51ed\u636e\u3002 \u914d\u7f6eHadoop\u5c5e\u6027\uff0c\u70b9\u51fb Hadoop\u5c5e\u6027 \u53f3\u8fb9\u7684 \u6309\u94ae\u3002 \u70b9\u51fb \u6309\u94ae\uff0c\u589e\u52a0\u4ee5\u4e0bHadoop\u5c5e\u6027\u3002\u589e\u52a0\u5b8c\u6bd5\uff0c\u70b9\u51fb OK \u3002 \u589e\u52a0core-site.xml\u7684hadoop.security.authentication\u548chadoop.rpc.protection\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\uff1b \u589e\u52a0hdfs-site.xml\u7684dfs.namenode.rpc-address.hacluster.*\uff0cdfs.ha.namenodes.hacluster\u3001dfs.nameservices\u3001dfs.client.failover.proxy.provider.hacluster\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\u3002 \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a hadoop.security.authentication = Kerberos hadoop.rpc.protection = privacy dfs.namenode.rpc-address.hacluster.141 = euleros-hd02:25000 dfs.namenode.rpc-address.hacluster.142 = euleros-hd03:25000 dfs.ha.namenodes.hacluster = 141,142 dfs.nameservices = hacluster dfs.client.failover.proxy.provider.hacluster = org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider \u786e\u8ba4\u9ed8\u8ba4\u52fe\u9009 \u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u5c5e\u6027 \uff0c\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \u3002 \u68c0\u67e5\u8fd4\u56de100%\uff0c\u5219Hadoop\u96c6\u7fa4\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb Close \u3002\u5982\u679c\u8fd4\u56de\u9519\u8bef\u65e5\u5fd7\uff0c\u5219\u6839\u636e\u9519\u8bef\u65e5\u5fd7\u63d0\u793a\u4fee\u6b63\u95ee\u9898\u540e\uff0c\u91cd\u65b0\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \uff0c\u76f4\u81f3\u68c0\u67e5\u8fd4\u56de100%\u3002 \u70b9\u51fb Finish \uff0c\u5219\u53ef\u5728 \u5143\u6570\u636e->Hadoop Cluster \u770b\u5230\u65b0\u5efa\u7684\u201cFusionInsight\u201d\u96c6\u7fa4\uff0c\u5305\u542bHDFS\u3001HIVE\u3001HBASE\u670d\u52a1\u3002 \u914d\u7f6eHIVE\u670d\u52a1 \u00b6 \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight->Hive(1)->FusionInsight_HIVE \uff0c\u53f3\u952e FusionInsight_HIVE \u9009\u62e9 Edit Hive \u3002 \u70b9\u51fb Next \u3002 \u9700\u8981\u66f4\u65b0\u7684\u914d\u7f6e\u5982\u4e0b\uff0c\u5176\u4f59\u7684\u4fdd\u6301\u4e0d\u53d8\u3002 hive\u6a21\u5f0f = Standalone hive\u670d\u52a1\u5668\u7248\u672c = Hive Server2 -- jdbc:hive2:// \u767b\u5f55\u540d = developuser \u5bc6\u7801 = Huawei@123 \u670d\u52a1\u5668 = 172.16.4.21:24002,172.16.4.22:24002,172.16.4.23 \u7aef\u53e3 = 24002 DataBase = default \u9644\u52a0JDBC\u8bbe\u7f6e = ;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;user.principal=developuser;user.keytab=C:/developuser/user.keytab \u8bf4\u660e\uff1a\u4ee5\u4e0a\u4fe1\u606f\u53ef\u6839\u636eJDBC\u65b9\u5f0f\u8fde\u63a5Hive\u670d\u52a1\u7684\u914d\u7f6e\u586b\u5199\u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \uff0c\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 Talend\u5bf9\u63a5FusionInsight HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight HDFS\u63a5\u53e3\uff0c\u5e76\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\uff0c\u6216\u8005\u5c06\u672c\u5730\u6587\u4ef6\u4e0a\u4f20\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHDFS\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 HDFS Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chdfsConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hdfsConnection \uff0c\u5728Palette\u9762\u677f\u8f93\u5165\u201chdfsConnection\u201d\u641c\u7d22\uff0c\u5c06\u641c\u7d22\u8fd4\u56de\u7684\u201ctHDFSConnection\u201d\u7ec4\u4ef6\u62d6\u81f3Disigner\u533a\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHDFSConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002\u5982\u679c\u63d0\u793a\u201c\u6b64\u7ec4\u4ef6tHDFSConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002\u201d\uff0c\u5219\u70b9\u51fb \u5b89\u88c5 \u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \u3002 \u7b49\u5f85\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757\u4e0b\u8f7d\u5e76\u5b89\u88c5\u5b8c\u4e4b\u540e\uff0c\u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HDFS\u6210\u529f\u3002 HDFS Get\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cgetFromHdfs.csv\u201d\uff0c\u5185\u5bb9\u968f\u610f\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsGet\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSGet\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \uff0c\u53f3\u952e\u9009\u62e9 \u89e6\u53d1\u5668->\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6 \uff0c\u8fde\u63a5\u81f3tHDFSGet_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSGet_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile (\u53ef\u9009\u62e9\u4efb\u610f\u7684\u672c\u5730\u76ee\u5f55)\uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u5728HDFS\u7684/tmp\u76ee\u5f55\u4e0b\u9700\u8981\u83b7\u53d6\u7684\u6587\u4ef6\u540d\u79f0 getFromHdfs.csv \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsGet\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4eceFusionInsight HDFS\u4e0b\u8f7d\u6587\u4ef6\u6210\u529f\u3002 getFromHdfs.csv\u5df2\u4e0b\u8f7d\u81f3\u672c\u5730 C:\\talend\\testFile \u3002 HDFS Put \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4ece\u672c\u5730\u4e0a\u4f20\u6587\u4ef6\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsPut\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSPut\u7ec4\u4ef6\uff0ctHDFSConnection_1\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884ctHDFSPut_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSPut_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u9700\u8981\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684\u6587\u4ef6\u540d\u79f0 putToHdfs.csv \u3002 \u8bf4\u660e\uff1a C:/talend/testFile/putToHdfs.csv \u4e3a\u672c\u5730\u5df2\u5b58\u5728\u6587\u4ef6\uff0c\u5185\u5bb9\u968f\u610f\u3002 ![](assets/Talend_7.2.1/61c635ee.png) \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsPut\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793a\u4eceTalend\u4e0a\u4f20\u6587\u4ef6putToHdfs.csv\u81f3FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u68c0\u67e5putToHdfs.csv\u5df2\u4e0a\u4f20\u81f3 /tmp \u76ee\u5f55\u3002 Talend\u5bf9\u63a5FusionInsight Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8868\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHive\u670d\u52a1\u7684Hadoop\u96c6\u7fa4\u548c\u5b8c\u6210Hadoop\u96c6\u7fa4\u7684Hive\u670d\u52a1\u914d\u7f6e\u3002 Hive Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chiveConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hiveConnection \uff0c\u52a0\u5165tHiveConnection\u3001tHiveClose\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight Hive\u6210\u529f\u3002 Hive Create Table \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06/tmp/putToHdfs.csv\u7684\u6570\u636e\u4f20\u5165\u8868talendHiveCreate\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cputToHdfs.csv\u201d\u3002 putToHdfs.csv\u7684\u5185\u5bb9\u5982\u4e0b(\u5305\u542b\u4e24\u5217\uff0c\u4e24\u5217\u4e4b\u95f4\u7528\u5206\u53f7\u9694\u5f00)\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chiveCreateTable\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveCreateTable\u3001tHiveLoad\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveCreateTable_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u8868 \uff0c\u201c\u683c\u5f0f\u201d\u9009\u62e9 \u6587\u672c\u6587\u4ef6 \uff0c\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveLoad_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u201c\u52a0\u8f7d\u64cd\u4f5c\u201d\u9009\u62e9 \u52a0\u8f7d \uff0c\u201c\u6587\u4ef6\u8def\u5f84\u201d\u8f93\u5165 /tmp/putToHdfs.csv \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveCreateTable\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4f7f\u7528Hive\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06putToHdfs.csv\u7684\u6570\u636e\u8f93\u5165\u5230\u8868talendHiveCreate\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \u3002 Hive Input \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveInput\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveInput\u3001tHiveClose\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"select * from talendHiveCreate\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveInput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u67e5\u8be2\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 Hive Row \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u63d2\u5165\u6570\u636e\u81f3Hive\u8868\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveRow\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveRow\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveRow_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"insert into talendHiveCreate values(123,'shenzhen')\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveRow\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u63d2\u5165\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5df2\u5305\u542b\u65b0\u589e\u7684\u6570\u636e\u3002 Talend\u5bf9\u63a5FusionInsight HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FusionInsight HBase\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8be2\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHBase\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 \u5df2\u5728IntelliJ IDEA\u4f7f\u7528 Import project from external model ~ Eclipse \u65b9\u5f0f\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \uff0c\u5e76\u4e14\u8c03\u6d4bTestMain.java\u901a\u8fc7\u3002 HBase Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\u3002 \u5728IntelliJ IDEA\u6253\u5f00 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \u5de5\u7a0b\uff0c\u9009\u62e9 File > Project Structure... \u83dc\u5355\u9879\u3002 \u9009\u62e9 Artifacts->Add->JAR->Empty \u3002 \u5bfc\u51fajar\u5305\u7684\u540d\u79f0\u8bbe\u7f6e\u4e3a hbase-loginUtil.jar \uff0c\u201cOutput directory\u201d\u9009\u62e9 C:\\talend\\testFile \uff0c\u53cc\u51fb\u201cAvailable Elements\u201d\u7684 'hbase-example' compile output \u5c06\u5b83\u52a0\u8f7d\u5230\u5de6\u8fb9\u5217\u8868\uff0c\u70b9\u51fb OK \u3002 \u9009\u4e2d\u201chbase-example\u201d\u5de5\u7a0bcom.huawei.hadoop.security\u7684LoginUtil.java\uff0c\u9009\u62e9 Build->Build Artifacts... \u9009\u62e9 hbase-loginUtil.jar->Build \u3002 \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u5728\u672c\u5730 C:\\talend\\testFile \u4ea7\u751f\u201chbase-loginUtil.jar\u201d\u3002 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chbaseConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hbaseConnection \uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctLibraryLoad_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9\u6a21\u5757\u3002\u5728\u5f39\u51fa\u7a97\u53e3\uff0c\u9009\u62e9 \u6784\u5efa\u5e93\uff08local m2/nexus\uff09 \uff0c\u9009\u62e9 \u5b89\u88c5\u4e00\u4e2a\u65b0\u6a21\u5757 \u5e76\u9009\u62e9\u6587\u4ef6 C:\\talend\\testFile\\hbase-loginUtil.jar \uff0c\u7136\u540e\u70b9\u51fb \u68c0\u6d4b\u6a21\u5757\u5b89\u88c5\u72b6\u6001 \uff0c\u68c0\u6d4b\u6ca1\u95ee\u9898\u5219 OK \u6309\u94ae\u6fc0\u6d3b\uff0c\u70b9\u51fb OK \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseConnection_1\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HBASE \uff0c\u201c\u53d1\u884c\u7248\u7684Hadoop\u7248\u672c\u201d\u9009\u62e9 Hadoop 2 \u3002\u5982\u679c\u63d0\u793a \u6b64\u7ec4\u4ef6tHbaseConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002 \uff0c\u5219\u70b9\u51fb\u5b89\u88c5\u3002 \u8bf4\u660e\uff1a\u5982\u679c\u201c\u53d1\u884c\u7248\u7684Hadoop\u7248\u672c\u201d\u7684\u4e0b\u62c9\u6846\u6ca1\u6709\u201cHadoop 2\u201d\uff0c\u53ea\u80fd\u9009\u62e9\u201cHadoop 1\u201d\u3002\u5219\u786e\u8ba4\u521b\u5efaHadoop\u96c6\u7fa4\u65f6\uff0c\u5bf9\u4e8eHBase\u670d\u52a1\uff0c\u662f\u5426\u5df2\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.3.1.tar.gz\\hbase\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \uff0c\u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u8bf4\u660e\uff1a\u5982\u679c\u8fd8\u63d0\u793a \u6b64\u7ec4\u4ef6tHbaseConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002 \uff0c\u70b9\u51fb\u5b89\u88c5\uff0c\u5f39\u51fa\u662f jersey-client-1.9.jar \uff0c\u5219\u53ef\u4ee5\u5ffd\u7565\u5904\u7406\u3002\u4f5c\u4e1a\u8fd0\u884c\u540e\uff0c\u8be5\u63d0\u793a\u4f1a\u6d88\u5931\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctJava_1\u201d\u3002 \u5728\u201c\u57fa\u672c\u8bbe\u7f6e\u201d\u7684\u201c\u4ee3\u7801\u201d\u4e2d\u8f93\u5165HBase\u914d\u7f6e\u76f8\u5173\u7684\u4ee3\u7801\u3002 \u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); //\u8bbe\u7f6eKerberos\u8ba4\u8bc1\u7684\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84 System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); //\u589e\u52a0\u914d\u7f6e\u6587\u4ef6\uff0c\u6839\u636e\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u4f4d\u7f6e\u5237\u65b0 conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/hbase-site.xml\")); //\u8f93\u51fa\u914d\u7f6e\u5c5e\u6027 System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); //\u767b\u5f55 LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); \u5728\u201ctJava_1\u201d\u7684\u201c\u9ad8\u7ea7\u8bbe\u7f6e\u201d\u7684\u201c\u5bfc\u5165\u201d\u8f93\u5165 import com.huawei.hadoop.security.LoginUtil; \uff0c \u70b9\u51fb\u9009\u4e2d\u201ctHBaseClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\u3002 HBase Input Output \u64cd\u4f5c\u6b65\u9aa4 \u00b6 Talend\u901a\u8fc7FusionInsight HBase\u63a5\u53e3\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u521b\u5efa\u8868talendHbaseCreate\uff0c\u5c06\u672c\u5730 C:/talend/testFile/putToHdfs.csv \u7684\u6570\u636e\u4f20\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u786e\u8ba4\u672c\u5730\u5df2\u5b58\u5728 C:/talend/testFile/putToHdfs.csv \u3002 putToHdfs.csv\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chbaseInputOutput\u201d\uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u3001tFileInputDelimited\u3001tHBaseOutput\u3001tHBaseInput\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose_1\u7ec4\u4ef6\u7684\u914d\u7f6e\u8bf7\u53c2\u8003\u201cHBase Connection \u64cd\u4f5c\u6b65\u9aa4\u201d\uff0ctLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctFileInputDelimited_1\u201d\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u8bbe\u8ba1schema\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u6587\u4ef6\u540d/\u6d41\u201d\u8f93\u5165 C:/talend/testFile/putToHdfs.csv \uff0c\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseOutput_1\u201d\u3002 \u5728\u201c\u9ad8\u7ea7\u914d\u7f6e\u201d\u4e2d\uff0c\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522b\u4e3aid\u548cname\uff0c\u5217\u540d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u5728\u201c\u57fa\u672c\u914d\u7f6e\u201d\u4e2d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u8868 \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseInputOutput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\uff0c\u4e14\u521b\u5efa\u8868talendHbaseCreate\u5e76\u5c06\u672c\u5730\u6587\u4ef6\u6570\u636e\u8f93\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5HBase\u8868\u201ctalendHbaseCreate\u201d\u3002 hbase shell scan 'hbaseInputOutputTest' FAQ \u00b6 \u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u8fd4\u56deClient cannot authenticate via:[TOKEN, KERBEROS] \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u4f7f\u7528Talend\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u7684\u7ec4\u4ef6\uff08\u4f8b\u5982tHDFSGet_1\uff09\u91c7\u7528\u201c\u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5\u201d\uff0c\u73b0\u6709\u8fde\u63a5tHDFSConnection_1\u7684\u5c5e\u6027\u7c7b\u578b\u662f\u201c\u5b58\u50a8\u5e93\u201d\u65f6\uff0c\u8fd0\u884c\u65f6\u8fd4\u56dejava.io.IOException: DestHost:destPort euleros-hd03:25000 , LocalHost:localPort user-PC/172.16.5.106:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\uff0c\u4e14\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u5931\u8d25\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 tHDFSConnection_1\u4f7f\u7528\u7684\u5b58\u50a8\u5e93FusionInsight_HDFS\u6240\u5c5e\u7684Hadoop\u96c6\u7fa4FusionInsight\u6ca1\u6709\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002\u9700\u8981\u4fee\u6539Hadoop\u96c6\u7fa4FusionInsight\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002 * \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight \uff0c\u53f3\u952e FusionInsight \u9009\u62e9 Edit Hadoop Cluster \u3002 ![](assets/Talend_7.2.1/33ef12f1.png) * \u52fe\u9009`\u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u914d\u7f6e`\u3002 ![](assets/Talend_7.2.1/b6962abf.png) * \u70b9\u51fb`Yes`\u3002 ![](assets/Talend_7.2.1/9e6ea1d7.png) \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\u8fd4\u56deCannot modify dfs.client.use.datanode.hostname at runtime\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\uff0c\u8fd4\u56de\u7c7b\u4f3c\u7684\u9519\u8bef\uff1aError while processing statement: Cannot modify dfs.client.use.datanode.hostname at runtime. It is not in list of params that are allowed to be modified at runtime\u3002\u53ef\u80fd\u6d89\u53ca\u7684\u6709\u4ee5\u4e0b\u4e09\u4e2a\u5c5e\u6027\uff1a dfs.client.use.datanode.hostname\u3001mapred.job.name\u3001hive.query.name \u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u767b\u5f55FusionInsight Manager\uff0c\u5728Hive\u670d\u52a1\u7684\u914d\u7f6e\u53c2\u6570hive.security.authorization.sqlstd.confwhitelist.append\u65b0\u589e |dfs\\.client\\.use\\.datanode\\.hostname|mapred\\.job\\.name|hive\\.query\\.name \uff0c\u7136\u540e\u91cd\u542fHive\u670d\u52a1\u3002","title":"7.2.1 <--> 6.5"},{"location":"Data_Integration/Talend_7.2.1/#talendfusioninsight","text":"","title":"Talend\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Talend_7.2.1/#_1","text":"Talend 7.2.1 \u2194 FusionInsight HD 6.5 (HDFS/HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_2","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001HIVE\u3001HBASE\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06krb5.conf\u548cuser.keytab\u653e\u5728 C:\\developuser\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u590d\u5236krb5.conf\u6587\u4ef6\u5e76\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u653e\u5728 C:\\Windows \u76ee\u5f55\u4e0b\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002FusionInsight HD\u5ba2\u6237\u7aef\u89e3\u538b\u4e8e\u672c\u5730 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig Zookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff0c\u5982 C:\\developuser\\jaas.conf \uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"c:/developuser/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u672c\u5730 C:\\Windows\\System32\\drivers\\etc\\hosts \u5df2\u6dfb\u52a0FusionInsight\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0ehostname\u7684\u6620\u5c04\u3002 \u672c\u5730\u5df2\u5b89\u88c5Hadoop\u670d\u52a1\uff08\u53ef\u4ece https://hadoop.apache.org/releases.html \u4e0b\u8f7dHadoop\u4e8c\u8fdb\u5236\uff09\uff0c\u8be5\u9879\u53ef\u9009\u3002\u5982\u679c\u672c\u5730\u6ca1\u5b89\u88c5Hadoop\u670d\u52a1\uff0ctalend\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0eHadoop\u76f8\u5173\u7684\u9519\u8bef\u65e5\u5fd7\uff0c\u4f46\u4e0d\u5f71\u54cd\u5b9e\u9645\u8fd0\u884c\u7ed3\u679c\u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Talend_7.2.1/#talend","text":"","title":"\u5b89\u88c5Talend"},{"location":"Data_Integration/Talend_7.2.1/#_3","text":"\u5b89\u88c5Talend Open Studio for Big Data","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_4","text":"\u4ece https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dWindow\u7248\u7684Talend\u3002 \u89e3\u538b\u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u70b9\u51fbTOS_BD-win-x86_64.exe\u542f\u52a8Talend Open Studio for Big Data\u3002\u70b9\u51fb \u6211\u540c\u610f \u3002 \u70b9\u51fb \u5b8c\u6210 \uff0c\u9ed8\u8ba4\u521b\u5efaLocal_Project\u7684\u5de5\u7a0b\u3002 \u9009\u62e9\u5b89\u88c5\u5fc5\u987b\u7684\u7b2c\u4e09\u65b9\u5e93\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u5728\u53f3\u4e0b\u89d2\u53ef\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6\u3002","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hadoop","text":"","title":"\u521b\u5efaHadoop\u670d\u52a1"},{"location":"Data_Integration/Talend_7.2.1/#_5","text":"\u521b\u5efa\u5305\u542bHDFS\u3001HIVE\u3001HBASE\u670d\u52a1\u7684Hadoop\u96c6\u7fa4","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_6","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u3001HBASE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 C:\\teland\\config \u76ee\u5f55\u4e0b\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\config \u7684hbase-site.xml\u3002 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1/#_7","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hadoop_1","text":"\u6253\u5f00 Talend Open Studio for Big Data \uff0c\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster \uff0c\u53f3\u952e Hadoop Cluster \u9009\u62e9 Create Hadoop Cluster \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201cFusionInsight\u201d\uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9 \u4ece\u672c\u5730\u6587\u4ef6\u5bfc\u5165\u914d\u7f6e \uff0c\u70b9\u51fb Next \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u76ee\u5f55 C:\\talend\\config \uff0c\u9ed8\u8ba4\u5168\u9009\uff0c\u70b9\u51fb Finish \u3002 \u201cDistribution\u201d\u9009\u62e9 Custom - Unsuported \uff0c\u70b9\u51fb\u4e0b\u62c9\u6846\u53f3\u8fb9\u7684 \u6309\u94ae\u5bfc\u5165HDFS\u3001HIVE\u3001HBASE\u76f8\u5173\u7684jar\u5305\u3002 \u70b9\u51fb Cancel \u53d6\u6d88\u81ea\u52a8\u5f39\u51fa\u7684\u201c\u5bfc\u5165\u81ea\u5b9a\u4e49\u7684\u5b9a\u4e49\u201d\u7a97\u53e3\u3002 \u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u70b9\u51fb \u6309\u94ae\u6dfb\u52a0HDFS\u76f8\u5173\u7684jar\u5305\u3002 \u9009\u62e9 \u5916\u90e8\u5e93 \uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\hdfs \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\uff0c\u70b9\u51fb OK \u5bfc\u5165jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common \u548c C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHive\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\jdbc \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHBase\u201d\uff0c\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.3.1.tar.gz\\hbase\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002\u5b8c\u6210\u540e\uff0c\u70b9\u51fb OK \u3002 \u914d\u7f6eKerberos\u8ba4\u8bc1\u3002\u201cCustom->Authentication\u201d\u9009\u62e9 Kerberos \u3002 \u52fe\u9009 Authentication->Enable Kerberos security \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Namenode Principal = hdfs/hadoop.hadoop.com@HADOOP.COM \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u5907\u6ce8\uff1a Namenode Principal\u7684\u53d6\u503c\u4e3ahdfs-site.xml\u7684dfs.namenode.kerberos.principal\u7684value\u503c\uff1b \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u7684\u53d6\u503c\u4e3ayarn-site.xml\u7684yarn.resourcemanager.principal\u7684value\u503c\uff1b \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53\u7684\u53d6\u503c\u4e3amapred-site.xml\u7684mapreduce.jobhistory.principal\u7684value\u503c\u3002 \u52fe\u9009 Authentication->Use a keytab to authenticate \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Principal = developuser Keytab = C:/developuser/user.keytab \u5907\u6ce8\uff1a Principal\u4e3aFusionInsight Manager\u7684\u7528\u6237\u540d\uff0cKeytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u51ed\u636e\u3002 \u914d\u7f6eHadoop\u5c5e\u6027\uff0c\u70b9\u51fb Hadoop\u5c5e\u6027 \u53f3\u8fb9\u7684 \u6309\u94ae\u3002 \u70b9\u51fb \u6309\u94ae\uff0c\u589e\u52a0\u4ee5\u4e0bHadoop\u5c5e\u6027\u3002\u589e\u52a0\u5b8c\u6bd5\uff0c\u70b9\u51fb OK \u3002 \u589e\u52a0core-site.xml\u7684hadoop.security.authentication\u548chadoop.rpc.protection\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\uff1b \u589e\u52a0hdfs-site.xml\u7684dfs.namenode.rpc-address.hacluster.*\uff0cdfs.ha.namenodes.hacluster\u3001dfs.nameservices\u3001dfs.client.failover.proxy.provider.hacluster\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\u3002 \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a hadoop.security.authentication = Kerberos hadoop.rpc.protection = privacy dfs.namenode.rpc-address.hacluster.141 = euleros-hd02:25000 dfs.namenode.rpc-address.hacluster.142 = euleros-hd03:25000 dfs.ha.namenodes.hacluster = 141,142 dfs.nameservices = hacluster dfs.client.failover.proxy.provider.hacluster = org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider \u786e\u8ba4\u9ed8\u8ba4\u52fe\u9009 \u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u5c5e\u6027 \uff0c\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \u3002 \u68c0\u67e5\u8fd4\u56de100%\uff0c\u5219Hadoop\u96c6\u7fa4\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb Close \u3002\u5982\u679c\u8fd4\u56de\u9519\u8bef\u65e5\u5fd7\uff0c\u5219\u6839\u636e\u9519\u8bef\u65e5\u5fd7\u63d0\u793a\u4fee\u6b63\u95ee\u9898\u540e\uff0c\u91cd\u65b0\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \uff0c\u76f4\u81f3\u68c0\u67e5\u8fd4\u56de100%\u3002 \u70b9\u51fb Finish \uff0c\u5219\u53ef\u5728 \u5143\u6570\u636e->Hadoop Cluster \u770b\u5230\u65b0\u5efa\u7684\u201cFusionInsight\u201d\u96c6\u7fa4\uff0c\u5305\u542bHDFS\u3001HIVE\u3001HBASE\u670d\u52a1\u3002","title":"\u521b\u5efaHadoop\u96c6\u7fa4"},{"location":"Data_Integration/Talend_7.2.1/#hive","text":"\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight->Hive(1)->FusionInsight_HIVE \uff0c\u53f3\u952e FusionInsight_HIVE \u9009\u62e9 Edit Hive \u3002 \u70b9\u51fb Next \u3002 \u9700\u8981\u66f4\u65b0\u7684\u914d\u7f6e\u5982\u4e0b\uff0c\u5176\u4f59\u7684\u4fdd\u6301\u4e0d\u53d8\u3002 hive\u6a21\u5f0f = Standalone hive\u670d\u52a1\u5668\u7248\u672c = Hive Server2 -- jdbc:hive2:// \u767b\u5f55\u540d = developuser \u5bc6\u7801 = Huawei@123 \u670d\u52a1\u5668 = 172.16.4.21:24002,172.16.4.22:24002,172.16.4.23 \u7aef\u53e3 = 24002 DataBase = default \u9644\u52a0JDBC\u8bbe\u7f6e = ;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;user.principal=developuser;user.keytab=C:/developuser/user.keytab \u8bf4\u660e\uff1a\u4ee5\u4e0a\u4fe1\u606f\u53ef\u6839\u636eJDBC\u65b9\u5f0f\u8fde\u63a5Hive\u670d\u52a1\u7684\u914d\u7f6e\u586b\u5199\u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \uff0c\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002","title":"\u914d\u7f6eHIVE\u670d\u52a1"},{"location":"Data_Integration/Talend_7.2.1/#talendfusioninsight-hdfs","text":"","title":"Talend\u5bf9\u63a5FusionInsight HDFS"},{"location":"Data_Integration/Talend_7.2.1/#_8","text":"Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight HDFS\u63a5\u53e3\uff0c\u5e76\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\uff0c\u6216\u8005\u5c06\u672c\u5730\u6587\u4ef6\u4e0a\u4f20\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_9","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHDFS\u670d\u52a1\u7684Hadoop\u96c6\u7fa4","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1/#hdfs-connection","text":"\u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chdfsConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hdfsConnection \uff0c\u5728Palette\u9762\u677f\u8f93\u5165\u201chdfsConnection\u201d\u641c\u7d22\uff0c\u5c06\u641c\u7d22\u8fd4\u56de\u7684\u201ctHDFSConnection\u201d\u7ec4\u4ef6\u62d6\u81f3Disigner\u533a\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHDFSConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002\u5982\u679c\u63d0\u793a\u201c\u6b64\u7ec4\u4ef6tHDFSConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002\u201d\uff0c\u5219\u70b9\u51fb \u5b89\u88c5 \u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \u3002 \u7b49\u5f85\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757\u4e0b\u8f7d\u5e76\u5b89\u88c5\u5b8c\u4e4b\u540e\uff0c\u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HDFS\u6210\u529f\u3002","title":"HDFS Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hdfs-get","text":"\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cgetFromHdfs.csv\u201d\uff0c\u5185\u5bb9\u968f\u610f\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsGet\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSGet\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \uff0c\u53f3\u952e\u9009\u62e9 \u89e6\u53d1\u5668->\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6 \uff0c\u8fde\u63a5\u81f3tHDFSGet_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSGet_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile (\u53ef\u9009\u62e9\u4efb\u610f\u7684\u672c\u5730\u76ee\u5f55)\uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u5728HDFS\u7684/tmp\u76ee\u5f55\u4e0b\u9700\u8981\u83b7\u53d6\u7684\u6587\u4ef6\u540d\u79f0 getFromHdfs.csv \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsGet\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4eceFusionInsight HDFS\u4e0b\u8f7d\u6587\u4ef6\u6210\u529f\u3002 getFromHdfs.csv\u5df2\u4e0b\u8f7d\u81f3\u672c\u5730 C:\\talend\\testFile \u3002","title":"HDFS Get\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hdfs-put","text":"\u4ece\u672c\u5730\u4e0a\u4f20\u6587\u4ef6\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsPut\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSPut\u7ec4\u4ef6\uff0ctHDFSConnection_1\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884ctHDFSPut_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSPut_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u9700\u8981\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684\u6587\u4ef6\u540d\u79f0 putToHdfs.csv \u3002 \u8bf4\u660e\uff1a C:/talend/testFile/putToHdfs.csv \u4e3a\u672c\u5730\u5df2\u5b58\u5728\u6587\u4ef6\uff0c\u5185\u5bb9\u968f\u610f\u3002 ![](assets/Talend_7.2.1/61c635ee.png) \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsPut\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793a\u4eceTalend\u4e0a\u4f20\u6587\u4ef6putToHdfs.csv\u81f3FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u68c0\u67e5putToHdfs.csv\u5df2\u4e0a\u4f20\u81f3 /tmp \u76ee\u5f55\u3002","title":"HDFS Put \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#talendfusioninsight-hive","text":"","title":"Talend\u5bf9\u63a5FusionInsight Hive"},{"location":"Data_Integration/Talend_7.2.1/#_10","text":"Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8868\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_11","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHive\u670d\u52a1\u7684Hadoop\u96c6\u7fa4\u548c\u5b8c\u6210Hadoop\u96c6\u7fa4\u7684Hive\u670d\u52a1\u914d\u7f6e\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1/#hive-connection","text":"\u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chiveConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hiveConnection \uff0c\u52a0\u5165tHiveConnection\u3001tHiveClose\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight Hive\u6210\u529f\u3002","title":"Hive Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hive-create-table","text":"\u4f7f\u7528Talend\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06/tmp/putToHdfs.csv\u7684\u6570\u636e\u4f20\u5165\u8868talendHiveCreate\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cputToHdfs.csv\u201d\u3002 putToHdfs.csv\u7684\u5185\u5bb9\u5982\u4e0b(\u5305\u542b\u4e24\u5217\uff0c\u4e24\u5217\u4e4b\u95f4\u7528\u5206\u53f7\u9694\u5f00)\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chiveCreateTable\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveCreateTable\u3001tHiveLoad\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveCreateTable_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u8868 \uff0c\u201c\u683c\u5f0f\u201d\u9009\u62e9 \u6587\u672c\u6587\u4ef6 \uff0c\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveLoad_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u201c\u52a0\u8f7d\u64cd\u4f5c\u201d\u9009\u62e9 \u52a0\u8f7d \uff0c\u201c\u6587\u4ef6\u8def\u5f84\u201d\u8f93\u5165 /tmp/putToHdfs.csv \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveCreateTable\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4f7f\u7528Hive\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06putToHdfs.csv\u7684\u6570\u636e\u8f93\u5165\u5230\u8868talendHiveCreate\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \u3002","title":"Hive Create Table \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hive-input","text":"\u4f7f\u7528Talend\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveInput\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveInput\u3001tHiveClose\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"select * from talendHiveCreate\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveInput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u67e5\u8be2\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002","title":"Hive Input \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hive-row","text":"\u4f7f\u7528Talend\u63d2\u5165\u6570\u636e\u81f3Hive\u8868\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveRow\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveRow\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveRow_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"insert into talendHiveCreate values(123,'shenzhen')\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveRow\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u63d2\u5165\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5df2\u5305\u542b\u65b0\u589e\u7684\u6570\u636e\u3002","title":"Hive Row \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#talendfusioninsight-hbase","text":"","title":"Talend\u5bf9\u63a5FusionInsight HBase"},{"location":"Data_Integration/Talend_7.2.1/#_12","text":"Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FusionInsight HBase\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8be2\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1/#_13","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHBase\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 \u5df2\u5728IntelliJ IDEA\u4f7f\u7528 Import project from external model ~ Eclipse \u65b9\u5f0f\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \uff0c\u5e76\u4e14\u8c03\u6d4bTestMain.java\u901a\u8fc7\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1/#hbase-connection","text":"\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\u3002 \u5728IntelliJ IDEA\u6253\u5f00 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \u5de5\u7a0b\uff0c\u9009\u62e9 File > Project Structure... \u83dc\u5355\u9879\u3002 \u9009\u62e9 Artifacts->Add->JAR->Empty \u3002 \u5bfc\u51fajar\u5305\u7684\u540d\u79f0\u8bbe\u7f6e\u4e3a hbase-loginUtil.jar \uff0c\u201cOutput directory\u201d\u9009\u62e9 C:\\talend\\testFile \uff0c\u53cc\u51fb\u201cAvailable Elements\u201d\u7684 'hbase-example' compile output \u5c06\u5b83\u52a0\u8f7d\u5230\u5de6\u8fb9\u5217\u8868\uff0c\u70b9\u51fb OK \u3002 \u9009\u4e2d\u201chbase-example\u201d\u5de5\u7a0bcom.huawei.hadoop.security\u7684LoginUtil.java\uff0c\u9009\u62e9 Build->Build Artifacts... \u9009\u62e9 hbase-loginUtil.jar->Build \u3002 \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u5728\u672c\u5730 C:\\talend\\testFile \u4ea7\u751f\u201chbase-loginUtil.jar\u201d\u3002 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chbaseConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hbaseConnection \uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctLibraryLoad_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9\u6a21\u5757\u3002\u5728\u5f39\u51fa\u7a97\u53e3\uff0c\u9009\u62e9 \u6784\u5efa\u5e93\uff08local m2/nexus\uff09 \uff0c\u9009\u62e9 \u5b89\u88c5\u4e00\u4e2a\u65b0\u6a21\u5757 \u5e76\u9009\u62e9\u6587\u4ef6 C:\\talend\\testFile\\hbase-loginUtil.jar \uff0c\u7136\u540e\u70b9\u51fb \u68c0\u6d4b\u6a21\u5757\u5b89\u88c5\u72b6\u6001 \uff0c\u68c0\u6d4b\u6ca1\u95ee\u9898\u5219 OK \u6309\u94ae\u6fc0\u6d3b\uff0c\u70b9\u51fb OK \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseConnection_1\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HBASE \uff0c\u201c\u53d1\u884c\u7248\u7684Hadoop\u7248\u672c\u201d\u9009\u62e9 Hadoop 2 \u3002\u5982\u679c\u63d0\u793a \u6b64\u7ec4\u4ef6tHbaseConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002 \uff0c\u5219\u70b9\u51fb\u5b89\u88c5\u3002 \u8bf4\u660e\uff1a\u5982\u679c\u201c\u53d1\u884c\u7248\u7684Hadoop\u7248\u672c\u201d\u7684\u4e0b\u62c9\u6846\u6ca1\u6709\u201cHadoop 2\u201d\uff0c\u53ea\u80fd\u9009\u62e9\u201cHadoop 1\u201d\u3002\u5219\u786e\u8ba4\u521b\u5efaHadoop\u96c6\u7fa4\u65f6\uff0c\u5bf9\u4e8eHBase\u670d\u52a1\uff0c\u662f\u5426\u5df2\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-1.3.1.tar.gz\\hbase\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \uff0c\u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u8bf4\u660e\uff1a\u5982\u679c\u8fd8\u63d0\u793a \u6b64\u7ec4\u4ef6tHbaseConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002 \uff0c\u70b9\u51fb\u5b89\u88c5\uff0c\u5f39\u51fa\u662f jersey-client-1.9.jar \uff0c\u5219\u53ef\u4ee5\u5ffd\u7565\u5904\u7406\u3002\u4f5c\u4e1a\u8fd0\u884c\u540e\uff0c\u8be5\u63d0\u793a\u4f1a\u6d88\u5931\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctJava_1\u201d\u3002 \u5728\u201c\u57fa\u672c\u8bbe\u7f6e\u201d\u7684\u201c\u4ee3\u7801\u201d\u4e2d\u8f93\u5165HBase\u914d\u7f6e\u76f8\u5173\u7684\u4ee3\u7801\u3002 \u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); //\u8bbe\u7f6eKerberos\u8ba4\u8bc1\u7684\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84 System.setProperty(\"java.security.krb5.conf\", \"C:\\\\developuser\\\\krb5.conf\"); System.setProperty(\"java.security.auth.login.config\", \"C:/developuser/jaas.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); //\u589e\u52a0\u914d\u7f6e\u6587\u4ef6\uff0c\u6839\u636e\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u4f4d\u7f6e\u5237\u65b0 conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"C:/talend/config/hbase-site.xml\")); //\u8f93\u51fa\u914d\u7f6e\u5c5e\u6027 System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); //\u767b\u5f55 LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"C:\\\\developuser\\\\krb5.conf\"); LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"C:/developuser/user.keytab\", \"C:/developuser/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); \u5728\u201ctJava_1\u201d\u7684\u201c\u9ad8\u7ea7\u8bbe\u7f6e\u201d\u7684\u201c\u5bfc\u5165\u201d\u8f93\u5165 import com.huawei.hadoop.security.LoginUtil; \uff0c \u70b9\u51fb\u9009\u4e2d\u201ctHBaseClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\u3002","title":"HBase Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#hbase-input-output","text":"Talend\u901a\u8fc7FusionInsight HBase\u63a5\u53e3\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u521b\u5efa\u8868talendHbaseCreate\uff0c\u5c06\u672c\u5730 C:/talend/testFile/putToHdfs.csv \u7684\u6570\u636e\u4f20\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u786e\u8ba4\u672c\u5730\u5df2\u5b58\u5728 C:/talend/testFile/putToHdfs.csv \u3002 putToHdfs.csv\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chbaseInputOutput\u201d\uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u3001tFileInputDelimited\u3001tHBaseOutput\u3001tHBaseInput\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose_1\u7ec4\u4ef6\u7684\u914d\u7f6e\u8bf7\u53c2\u8003\u201cHBase Connection \u64cd\u4f5c\u6b65\u9aa4\u201d\uff0ctLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctFileInputDelimited_1\u201d\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u8bbe\u8ba1schema\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u6587\u4ef6\u540d/\u6d41\u201d\u8f93\u5165 C:/talend/testFile/putToHdfs.csv \uff0c\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseOutput_1\u201d\u3002 \u5728\u201c\u9ad8\u7ea7\u914d\u7f6e\u201d\u4e2d\uff0c\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522b\u4e3aid\u548cname\uff0c\u5217\u540d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u5728\u201c\u57fa\u672c\u914d\u7f6e\u201d\u4e2d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u8868 \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseInputOutput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\uff0c\u4e14\u521b\u5efa\u8868talendHbaseCreate\u5e76\u5c06\u672c\u5730\u6587\u4ef6\u6570\u636e\u8f93\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5HBase\u8868\u201ctalendHbaseCreate\u201d\u3002 hbase shell scan 'hbaseInputOutputTest'","title":"HBase Input Output \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1/#faq","text":"\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u8fd4\u56deClient cannot authenticate via:[TOKEN, KERBEROS] \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u4f7f\u7528Talend\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u7684\u7ec4\u4ef6\uff08\u4f8b\u5982tHDFSGet_1\uff09\u91c7\u7528\u201c\u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5\u201d\uff0c\u73b0\u6709\u8fde\u63a5tHDFSConnection_1\u7684\u5c5e\u6027\u7c7b\u578b\u662f\u201c\u5b58\u50a8\u5e93\u201d\u65f6\uff0c\u8fd0\u884c\u65f6\u8fd4\u56dejava.io.IOException: DestHost:destPort euleros-hd03:25000 , LocalHost:localPort user-PC/172.16.5.106:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\uff0c\u4e14\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u5931\u8d25\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 tHDFSConnection_1\u4f7f\u7528\u7684\u5b58\u50a8\u5e93FusionInsight_HDFS\u6240\u5c5e\u7684Hadoop\u96c6\u7fa4FusionInsight\u6ca1\u6709\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002\u9700\u8981\u4fee\u6539Hadoop\u96c6\u7fa4FusionInsight\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002 * \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight \uff0c\u53f3\u952e FusionInsight \u9009\u62e9 Edit Hadoop Cluster \u3002 ![](assets/Talend_7.2.1/33ef12f1.png) * \u52fe\u9009`\u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u914d\u7f6e`\u3002 ![](assets/Talend_7.2.1/b6962abf.png) * \u70b9\u51fb`Yes`\u3002 ![](assets/Talend_7.2.1/9e6ea1d7.png) \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\u8fd4\u56deCannot modify dfs.client.use.datanode.hostname at runtime\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\uff0c\u8fd4\u56de\u7c7b\u4f3c\u7684\u9519\u8bef\uff1aError while processing statement: Cannot modify dfs.client.use.datanode.hostname at runtime. It is not in list of params that are allowed to be modified at runtime\u3002\u53ef\u80fd\u6d89\u53ca\u7684\u6709\u4ee5\u4e0b\u4e09\u4e2a\u5c5e\u6027\uff1a dfs.client.use.datanode.hostname\u3001mapred.job.name\u3001hive.query.name \u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u767b\u5f55FusionInsight Manager\uff0c\u5728Hive\u670d\u52a1\u7684\u914d\u7f6e\u53c2\u6570hive.security.authorization.sqlstd.confwhitelist.append\u65b0\u589e |dfs\\.client\\.use\\.datanode\\.hostname|mapred\\.job\\.name|hive\\.query\\.name \uff0c\u7136\u540e\u91cd\u542fHive\u670d\u52a1\u3002","title":"FAQ"},{"location":"Data_Integration/Talend_7.2.1_MRS802/","text":"Talend\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Talend 7.2.1 \u2194 FusionInsight MRS 8.0 (HDFS/HBase/Hive) \u8bf4\u660e\uff1a talend 7.2.1\u7248\u672c\u4e0d\u652f\u6301\u5bf9\u63a5hetu \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001HIVE\u3001HBASE\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06krb5.conf\u548cuser.keytab\u653e\u5728 E:\\195config\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u590d\u5236krb5.conf\u6587\u4ef6\u5e76\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u653e\u5728 C:\\Windows \u76ee\u5f55\u4e0b\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002FusionInsight HD\u5ba2\u6237\u7aef\u89e3\u538b\u4e8e\u672c\u5730 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig Zookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff0c\u5982 E:\\195config\\jaas.conf \uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"E:\\195config/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u672c\u5730 C:\\Windows\\System32\\drivers\\etc\\hosts \u5df2\u6dfb\u52a0FusionInsight\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0ehostname\u7684\u6620\u5c04\u3002 \u672c\u5730\u5df2\u5b89\u88c5Hadoop\u670d\u52a1\uff08\u53ef\u4ece https://hadoop.apache.org/releases.html \u4e0b\u8f7dHadoop\u4e8c\u8fdb\u5236\uff09\uff0c\u8be5\u9879\u53ef\u9009\u3002\u5982\u679c\u672c\u5730\u6ca1\u5b89\u88c5Hadoop\u670d\u52a1\uff0ctalend\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0eHadoop\u76f8\u5173\u7684\u9519\u8bef\u65e5\u5fd7\uff0c\u4f46\u4e0d\u5f71\u54cd\u5b9e\u9645\u8fd0\u884c\u7ed3\u679c\u3002 \u5b89\u88c5Talend \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Talend Open Studio for Big Data \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4ece https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dWindow\u7248\u7684Talend\u3002 \u89e3\u538b\u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u70b9\u51fbTOS_BD-win-x86_64.exe\u542f\u52a8Talend Open Studio for Big Data\u3002\u70b9\u51fb \u6211\u540c\u610f \u3002 \u70b9\u51fb \u5b8c\u6210 \uff0c\u9ed8\u8ba4\u521b\u5efaLocal_Project\u7684\u5de5\u7a0b\u3002 \u9009\u62e9\u5b89\u88c5\u5fc5\u987b\u7684\u7b2c\u4e09\u65b9\u5e93\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u5728\u53f3\u4e0b\u89d2\u53ef\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6\u3002 \u521b\u5efaHadoop\u670d\u52a1 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u521b\u5efa\u5305\u542bHDFS\u3001HIVE\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 E:\\195config\\config \u76ee\u5f55\u4e0b\u3002 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u521b\u5efaHadoop\u96c6\u7fa4 \u00b6 \u6253\u5f00 Talend Open Studio for Big Data \uff0c\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster \uff0c\u53f3\u952e Hadoop Cluster \u9009\u62e9 Create Hadoop Cluster \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201cFusionInsight\u201d\uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9 \u4ece\u672c\u5730\u6587\u4ef6\u5bfc\u5165\u914d\u7f6e \uff0c\u70b9\u51fb Next \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u76ee\u5f55 E:\\195config\\config \uff0c\u9ed8\u8ba4\u5168\u9009\uff0c\u70b9\u51fb Finish \u3002 \u5148\u4fee\u6539Namenode URI\u7684\u503c\u4e3a hdfs://172.16.10.132:25000 \uff0c\u5176\u4e2d172.16.10.132\u4e3a\u4e3bnamenode ip\uff0c\u7136\u540e\u201cDistribution\u201d\u9009\u62e9 Custom - Unsuported \uff0c\u70b9\u51fb\u4e0b\u62c9\u6846\u53f3\u8fb9\u7684 \u6309\u94ae\u5bfc\u5165HDFS\u3001HIVE\u76f8\u5173\u7684jar\u5305\u3002 \u70b9\u51fb Cancel \u53d6\u6d88\u81ea\u52a8\u5f39\u51fa\u7684\u201c\u5bfc\u5165\u81ea\u5b9a\u4e49\u7684\u5b9a\u4e49\u201d\u7a97\u53e3\u3002 \u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u70b9\u51fb \u6309\u94ae\u6dfb\u52a0HDFS\u76f8\u5173\u7684jar\u5305\u3002 \u9009\u62e9 \u5916\u90e8\u5e93 \uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\hdfs \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\uff0c\u70b9\u51fb OK \u5bfc\u5165jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u5bfc\u5165 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common \u548c E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHive\u201d\uff0c\u5bfc\u5165 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\jdbc \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u914d\u7f6eKerberos\u8ba4\u8bc1\u3002\u201cCustom->Authentication\u201d\u9009\u62e9 Kerberos \u3002 \u52fe\u9009 Authentication->Enable Kerberos security \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Namenode Principal = hdfs/hadoop.hadoop.com@HADOOP.COM \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u5907\u6ce8\uff1a Namenode Principal\u7684\u53d6\u503c\u4e3ahdfs-site.xml\u7684dfs.namenode.kerberos.principal\u7684value\u503c\uff1b \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u7684\u53d6\u503c\u4e3ayarn-site.xml\u7684yarn.resourcemanager.principal\u7684value\u503c\uff1b \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53\u7684\u53d6\u503c\u4e3amapred-site.xml\u7684mapreduce.jobhistory.principal\u7684value\u503c\u3002 \u52fe\u9009 Authentication->Use a keytab to authenticate \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Principal = developuser Keytab = E:/195config/user.keytab \u5907\u6ce8\uff1a Principal\u4e3aFusionInsight Manager\u7684\u7528\u6237\u540d\uff0cKeytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u51ed\u636e\u3002 \u914d\u7f6eHadoop\u5c5e\u6027\uff0c\u70b9\u51fb Hadoop\u5c5e\u6027 \u53f3\u8fb9\u7684 \u6309\u94ae\u3002 \u70b9\u51fb \u6309\u94ae\uff0c\u589e\u52a0\u4ee5\u4e0bHadoop\u5c5e\u6027\u3002\u589e\u52a0\u5b8c\u6bd5\uff0c\u70b9\u51fb OK \u3002 \u589e\u52a0core-site.xml\u7684hadoop.security.authentication\u548chadoop.rpc.protection\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\uff1b \u589e\u52a0hdfs-site.xml\u7684dfs.namenode.rpc-address.hacluster.*\uff0cdfs.ha.namenodes.hacluster\u3001dfs.nameservices\u3001dfs.client.failover.proxy.provider.hacluster\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\u3002 \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a hadoop.security.authentication = Kerberos hadoop.rpc.protection = privacy \u786e\u8ba4\u9ed8\u8ba4\u52fe\u9009 \u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u5c5e\u6027 \uff0c\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \u3002 \u68c0\u67e5\u8fd4\u56de100%\uff0c\u5219Hadoop\u96c6\u7fa4\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb Close \u3002\u5982\u679c\u8fd4\u56de\u9519\u8bef\u65e5\u5fd7\uff0c\u5219\u6839\u636e\u9519\u8bef\u65e5\u5fd7\u63d0\u793a\u4fee\u6b63\u95ee\u9898\u540e\uff0c\u91cd\u65b0\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \uff0c\u76f4\u81f3\u68c0\u67e5\u8fd4\u56de100%\u3002 \u70b9\u51fb Finish \uff0c\u5219\u53ef\u5728 \u5143\u6570\u636e->Hadoop Cluster \u770b\u5230\u65b0\u5efa\u7684\u201cFusionInsight\u201d\u96c6\u7fa4\uff0c\u5305\u542bHDFS\u3001HIVE\u670d\u52a1\u3002 \u914d\u7f6eHIVE\u670d\u52a1 \u00b6 \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight->Hive(1)->FusionInsight_HIVE \uff0c\u53f3\u952e FusionInsight_HIVE \u9009\u62e9 Edit Hive \u3002 \u70b9\u51fb Next \u3002 \u9700\u8981\u66f4\u65b0\u7684\u914d\u7f6e\u5982\u4e0b\uff0c\u5176\u4f59\u7684\u4fdd\u6301\u4e0d\u53d8\u3002 hive\u6a21\u5f0f = Standalone hive\u670d\u52a1\u5668\u7248\u672c = Hive Server2 -- jdbc:hive2:// \u767b\u5f55\u540d = developuser \u5bc6\u7801 = Huawei@123 \u670d\u52a1\u5668 = 172.16.10.131:24002,172.16.10.132:24002,172.16.10.133 \u7aef\u53e3 = 24002 DataBase = default \u9644\u52a0JDBC\u8bbe\u7f6e = ;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;user.principal=developuser;user.keytab=E:/195config/user.keytab \u8bf4\u660e\uff1a\u4ee5\u4e0a\u4fe1\u606f\u53ef\u6839\u636eJDBC\u65b9\u5f0f\u8fde\u63a5Hive\u670d\u52a1\u7684\u914d\u7f6e\u586b\u5199\u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \uff0c\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 Talend\u5bf9\u63a5FusionInsight HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight HDFS\u63a5\u53e3\uff0c\u5e76\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\uff0c\u6216\u8005\u5c06\u672c\u5730\u6587\u4ef6\u4e0a\u4f20\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHDFS\u670d\u52a1\u7684Hadoop\u96c6\u7fa4 HDFS Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chdfsConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hdfsConnection \uff0c\u5728Palette\u9762\u677f\u8f93\u5165\u201chdfsConnection\u201d\u641c\u7d22\uff0c\u5c06\u641c\u7d22\u8fd4\u56de\u7684\u201ctHDFSConnection\u201d\u7ec4\u4ef6\u62d6\u81f3Disigner\u533a\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHDFSConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002\u5982\u679c\u63d0\u793a\u201c\u6b64\u7ec4\u4ef6tHDFSConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002\u201d\uff0c\u5219\u70b9\u51fb \u5b89\u88c5 \u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \u3002 \u7b49\u5f85\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757\u4e0b\u8f7d\u5e76\u5b89\u88c5\u5b8c\u4e4b\u540e\uff0c\u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HDFS\u6210\u529f\u3002 HDFS Get\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cgetFromHdfs.csv\u201d\uff0c\u5185\u5bb9\u968f\u610f\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsGet\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSGet\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \uff0c\u53f3\u952e\u9009\u62e9 \u89e6\u53d1\u5668->\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6 \uff0c\u8fde\u63a5\u81f3tHDFSGet_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSGet_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile (\u53ef\u9009\u62e9\u4efb\u610f\u7684\u672c\u5730\u76ee\u5f55)\uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u5728HDFS\u7684/tmp\u76ee\u5f55\u4e0b\u9700\u8981\u83b7\u53d6\u7684\u6587\u4ef6\u540d\u79f0 getFromHdfs.csv \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsGet\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4eceFusionInsight HDFS\u4e0b\u8f7d\u6587\u4ef6\u6210\u529f\u3002 getFromHdfs.csv\u5df2\u4e0b\u8f7d\u81f3\u672c\u5730 C:\\talend\\testFile \u3002 HDFS Put \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4ece\u672c\u5730\u4e0a\u4f20\u6587\u4ef6\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsPut\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSPut\u7ec4\u4ef6\uff0ctHDFSConnection_1\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884ctHDFSPut_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSPut_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u9700\u8981\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684\u6587\u4ef6\u540d\u79f0 putToHdfs.csv \u3002 \u8bf4\u660e\uff1a C:/talend/testFile/putToHdfs.csv \u4e3a\u672c\u5730\u5df2\u5b58\u5728\u6587\u4ef6\uff0c\u5185\u5bb9\u968f\u610f\u3002 ![](assets/Talend_7.2.1/61c635ee.png) \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsPut\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793a\u4eceTalend\u4e0a\u4f20\u6587\u4ef6putToHdfs.csv\u81f3FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u68c0\u67e5putToHdfs.csv\u5df2\u4e0a\u4f20\u81f3 /tmp \u76ee\u5f55\u3002 Talend\u5bf9\u63a5FusionInsight Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8868\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHive\u670d\u52a1\u7684Hadoop\u96c6\u7fa4\u548c\u5b8c\u6210Hadoop\u96c6\u7fa4\u7684Hive\u670d\u52a1\u914d\u7f6e\u3002 Hive Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chiveConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hiveConnection \uff0c\u52a0\u5165tHiveConnection\u3001tHiveClose\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight Hive\u6210\u529f\u3002 Hive Create Table \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06/tmp/putToHdfs.csv\u7684\u6570\u636e\u4f20\u5165\u8868talendHiveCreate\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cputToHdfs.csv\u201d\u3002 putToHdfs.csv\u7684\u5185\u5bb9\u5982\u4e0b(\u5305\u542b\u4e24\u5217\uff0c\u4e24\u5217\u4e4b\u95f4\u7528\u5206\u53f7\u9694\u5f00)\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chiveCreateTable\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveCreateTable\u3001tHiveLoad\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveCreateTable_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u8868 \uff0c\u201c\u683c\u5f0f\u201d\u9009\u62e9 \u6587\u672c\u6587\u4ef6 \uff0c\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveLoad_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u201c\u52a0\u8f7d\u64cd\u4f5c\u201d\u9009\u62e9 \u52a0\u8f7d \uff0c\u201c\u6587\u4ef6\u8def\u5f84\u201d\u8f93\u5165 /tmp/putToHdfs.csv \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveCreateTable\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4f7f\u7528Hive\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06putToHdfs.csv\u7684\u6570\u636e\u8f93\u5165\u5230\u8868talendHiveCreate\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \u3002 Hive Input \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveInput\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveInput\u3001tHiveClose\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"select * from talendHiveCreate\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveInput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u67e5\u8be2\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 Hive Row \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528Talend\u63d2\u5165\u6570\u636e\u81f3Hive\u8868\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveRow\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveRow\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveRow_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"insert into talendHiveCreate values(123,'shenzhen')\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveRow\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u63d2\u5165\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5df2\u5305\u542b\u65b0\u589e\u7684\u6570\u636e\u3002 Talend\u5bf9\u63a5FusionInsight HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FusionInsight HBase\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8be2\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u5728IntelliJ IDEA\u4f7f\u7528 Import project from external model ~ Eclipse \u65b9\u5f0f\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \uff0c\u5e76\u4e14\u8c03\u6d4bTestMain.java\u901a\u8fc7\u3002 HBase Connection \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\u3002 \u5728IntelliJ IDEA\u6253\u5f00 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \u5de5\u7a0b\uff0c\u9009\u62e9 File > Project Structure... \u83dc\u5355\u9879\u3002 \u9009\u62e9 Artifacts->Add->JAR->Empty \u3002 \u5bfc\u51fajar\u5305\u7684\u540d\u79f0\u8bbe\u7f6e\u4e3a hbase-loginUtil.jar \uff0c\u201cOutput directory\u201d\u9009\u62e9 C:\\talend\\testFile \uff0c\u53cc\u51fb\u201cAvailable Elements\u201d\u7684 'hbase-example' compile output \u5c06\u5b83\u52a0\u8f7d\u5230\u5de6\u8fb9\u5217\u8868\uff0c\u70b9\u51fb OK \u3002 \u9009\u4e2d\u201chbase-example\u201d\u5de5\u7a0bcom.huawei.hadoop.security\u7684LoginUtil.java\uff0c\u9009\u62e9 Build->Build Artifacts... \u9009\u62e9 hbase-loginUtil.jar->Build \u3002 \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u5728\u672c\u5730 C:\\talend\\testFile \u4ea7\u751f\u201chbase-loginUtil2.jar\u201d\u3002 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chbaseConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hbaseConnection \uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctLibraryLoad_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9\u6a21\u5757\u3002\u5728\u5f39\u51fa\u7a97\u53e3\uff0c\u9009\u62e9 \u6784\u5efa\u5e93\uff08local m2/nexus\uff09 \uff0c\u9009\u62e9 \u5b89\u88c5\u4e00\u4e2a\u65b0\u6a21\u5757 \u5e76\u9009\u62e9\u6587\u4ef6 C:\\talend\\testFile\\hbase-loginUtil.jar \uff0c\u7136\u540e\u70b9\u51fb \u68c0\u6d4b\u6a21\u5757\u5b89\u88c5\u72b6\u6001 \uff0c\u68c0\u6d4b\u6ca1\u95ee\u9898\u5219 OK \u6309\u94ae\u6fc0\u6d3b\uff0c\u70b9\u51fb OK \u3002 \u6ce8\u610f\uff1a\u5982\u679c\u5df2\u7ecf\u5b89\u88c5\u8fc7\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528talend\u7684mvn uri\u5bfc\u5165 \u70b9\u51fb\u9009\u4e2d\u201ctLibraryLoad_3\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9\u6a21\u5757\u3002\u5728\u5f39\u51fa\u7a97\u53e3\uff0c\u9009\u62e9 \u6784\u5efa\u5e93\uff08local m2/nexus\uff09 \uff0c\u9009\u62e9 \u5b89\u88c5\u4e00\u4e2a\u65b0\u6a21\u5757 \u5e76\u9009\u62e9\u6587\u4ef6 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-2.2.3.tar.gz\\hbase\\lib\\zookeeper-3.5.6-hw-ei-302002.jar \uff0c\u7136\u540e\u70b9\u51fb \u68c0\u6d4b\u6a21\u5757\u5b89\u88c5\u72b6\u6001 \uff0c\u68c0\u6d4b\u6ca1\u95ee\u9898\u5219 OK \u6309\u94ae\u6fc0\u6d3b\uff0c\u70b9\u51fb OK \u3002 \u6ce8\u610f\uff1a\u5982\u679c\u5df2\u7ecf\u5b89\u88c5\u8fc7\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528talend\u7684mvn uri\u5bfc\u5165 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseConnection_1\u201d\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a \u9009\u7528cdh 6.1.1\u4f5c\u4e3a\u5339\u914d\u7248\u672c\u5bf9\u63a5 \u70b9\u51fb\u9009\u4e2d\u201ctJava_1\u201d\u3002 \u5728\u201c\u57fa\u672c\u8bbe\u7f6e\u201d\u7684\u201c\u4ee3\u7801\u201d\u4e2d\u8f93\u5165HBase\u914d\u7f6e\u76f8\u5173\u7684\u4ee3\u7801\u3002 \u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); //\u8bbe\u7f6eKerberos\u8ba4\u8bc1\u7684\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84 System.setProperty(\"java.security.krb5.conf\", \"E:\\\\195config\\\\krb5.conf\"); System.setProperty(\"java.security.auth.login.config\", \"E:/195config/jaas.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); //\u589e\u52a0\u914d\u7f6e\u6587\u4ef6\uff0c\u6839\u636e\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u4f4d\u7f6e\u5237\u65b0 conf.addResource(new org.apache.hadoop.fs.Path(\"E:/195config/config/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"E:/195config/config/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"E:/195config/config/hbase-site.xml\")); //\u8f93\u51fa\u914d\u7f6e\u5c5e\u6027 System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); //\u767b\u5f55 LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"E:\\\\195config\\\\krb5.conf\"); //LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"E:/195config/user.keytab\", \"E:/195config/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); \u5728\u201ctJava_1\u201d\u7684\u201c\u9ad8\u7ea7\u8bbe\u7f6e\u201d\u7684\u201c\u5bfc\u5165\u201d\u8f93\u5165 import com.huawei.hadoop.security.LoginUtil; \uff0c \u70b9\u51fb\u9009\u4e2d\u201ctHBaseClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u9996\u5148\u5148\u5728Advanced settings\u4e0b\u9762\u81ea\u5b9a\u4e49jvm\u53c2\u6570\uff0c\u7136\u540e\u70b9\u51fb\u8fd0\u884c\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\u3002 HBase Input Output \u64cd\u4f5c\u6b65\u9aa4 \u00b6 Talend\u901a\u8fc7FusionInsight HBase\u63a5\u53e3\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u521b\u5efa\u8868talendHbaseCreate\uff0c\u5c06\u672c\u5730 E:/soft/talend/putToHdfs.csv \u7684\u6570\u636e\u4f20\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u786e\u8ba4\u672c\u5730\u5df2\u5b58\u5728 E:/soft/talend/putToHdfs.csv \u3002 putToHdfs.csv\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chbaseInputOutput\u201d\uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u3001tFileInputDelimited\u3001tHBaseOutput\u3001tHBaseInput\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose_1\u7ec4\u4ef6\u7684\u914d\u7f6e\u8bf7\u53c2\u8003\u201cHBase Connection \u64cd\u4f5c\u6b65\u9aa4\u201d\uff0ctLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctFileInputDelimited_1\u201d\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u8bbe\u8ba1schema\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u6587\u4ef6\u540d/\u6d41\u201d\u8f93\u5165 E:/soft/talend/putToHdfs.csv \uff0c\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseOutput_1\u201d\u3002 \u5728\u201c\u9ad8\u7ea7\u914d\u7f6e\u201d\u4e2d\uff0c\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522b\u4e3aid\u548cname\uff0c\u5217\u540d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u5728\u201c\u57fa\u672c\u914d\u7f6e\u201d\u4e2d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u8868 \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendhbase \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseInputOutput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\uff0c\u4e14\u521b\u5efa\u8868talendHbaseCreate\u5e76\u5c06\u672c\u5730\u6587\u4ef6\u6570\u636e\u8f93\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5HBase\u8868\u201ctalendHbaseCreate\u201d\u3002 hbase shell scan 'talendhbase' FAQ \u00b6 \u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u8fd4\u56deClient cannot authenticate via:[TOKEN, KERBEROS] \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u4f7f\u7528Talend\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u7684\u7ec4\u4ef6\uff08\u4f8b\u5982tHDFSGet_1\uff09\u91c7\u7528\u201c\u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5\u201d\uff0c\u73b0\u6709\u8fde\u63a5tHDFSConnection_1\u7684\u5c5e\u6027\u7c7b\u578b\u662f\u201c\u5b58\u50a8\u5e93\u201d\u65f6\uff0c\u8fd0\u884c\u65f6\u8fd4\u56dejava.io.IOException: DestHost:destPort euleros-hd03:25000 , LocalHost:localPort user-PC/172.16.5.106:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\uff0c\u4e14\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u5931\u8d25\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 tHDFSConnection_1\u4f7f\u7528\u7684\u5b58\u50a8\u5e93FusionInsight_HDFS\u6240\u5c5e\u7684Hadoop\u96c6\u7fa4FusionInsight\u6ca1\u6709\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002\u9700\u8981\u4fee\u6539Hadoop\u96c6\u7fa4FusionInsight\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002 * \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight \uff0c\u53f3\u952e FusionInsight \u9009\u62e9 Edit Hadoop Cluster \u3002 ![](assets/Talend_7.2.1/33ef12f1.png) * \u52fe\u9009`\u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u914d\u7f6e`\u3002 ![](assets/Talend_7.2.1/b6962abf.png) * \u70b9\u51fb`Yes`\u3002 ![](assets/Talend_7.2.1/9e6ea1d7.png) \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\u8fd4\u56deCannot modify dfs.client.use.datanode.hostname at runtime\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\uff0c\u8fd4\u56de\u7c7b\u4f3c\u7684\u9519\u8bef\uff1aError while processing statement: Cannot modify dfs.client.use.datanode.hostname at runtime. It is not in list of params that are allowed to be modified at runtime\u3002\u53ef\u80fd\u6d89\u53ca\u7684\u6709\u4ee5\u4e0b\u4e09\u4e2a\u5c5e\u6027\uff1a dfs.client.use.datanode.hostname\u3001mapred.job.name\u3001hive.query.name \u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u767b\u5f55FusionInsight Manager\uff0c\u5728Hive\u670d\u52a1\u7684\u914d\u7f6e\u53c2\u6570hive.security.authorization.sqlstd.confwhitelist.append\u65b0\u589e |dfs\\.client\\.use\\.datanode\\.hostname|mapred\\.job\\.name|hive\\.query\\.name \uff0c\u7136\u540e\u91cd\u542fHive\u670d\u52a1\u3002","title":"7.2.1 <--> 8.0"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#talendfusioninsight","text":"","title":"Talend\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_1","text":"Talend 7.2.1 \u2194 FusionInsight MRS 8.0 (HDFS/HBase/Hive) \u8bf4\u660e\uff1a talend 7.2.1\u7248\u672c\u4e0d\u652f\u6301\u5bf9\u63a5hetu","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_2","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHDFS\u3001HIVE\u3001HBASE\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u89e3\u538b\u540e\uff0c\u5c06krb5.conf\u548cuser.keytab\u653e\u5728 E:\\195config\\ \u76ee\u5f55\u4e0b(developuser\u6587\u4ef6\u5939\u4e0d\u5b58\u5728\u5219\u521b\u5efa)\uff0c\u590d\u5236krb5.conf\u6587\u4ef6\u5e76\u91cd\u547d\u540d\u4e3akrb5.ini\uff0c\u653e\u5728 C:\\Windows \u76ee\u5f55\u4e0b\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002FusionInsight HD\u5ba2\u6237\u7aef\u89e3\u538b\u4e8e\u672c\u5730 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig Zookeeper\u7684Kerberos\u8ba4\u8bc1\u9700\u8981\u6307\u5b9ajaas\u914d\u7f6e\u6587\u4ef6\u3002\u521b\u5efa\u8fde\u63a5zookeeper\u7684jaas\u914d\u7f6e\u6587\u4ef6\uff0c\u5982 E:\\195config\\jaas.conf \uff0c\u5185\u5bb9\u683c\u5f0f\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"E:\\195config/user.keytab\" principal=\"developuser@HADOOP.COM\" useTicketCache=false storeKey=true debug=true; }; \u672c\u5730 C:\\Windows\\System32\\drivers\\etc\\hosts \u5df2\u6dfb\u52a0FusionInsight\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0ehostname\u7684\u6620\u5c04\u3002 \u672c\u5730\u5df2\u5b89\u88c5Hadoop\u670d\u52a1\uff08\u53ef\u4ece https://hadoop.apache.org/releases.html \u4e0b\u8f7dHadoop\u4e8c\u8fdb\u5236\uff09\uff0c\u8be5\u9879\u53ef\u9009\u3002\u5982\u679c\u672c\u5730\u6ca1\u5b89\u88c5Hadoop\u670d\u52a1\uff0ctalend\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u4f1a\u51fa\u73b0\u4e0eHadoop\u76f8\u5173\u7684\u9519\u8bef\u65e5\u5fd7\uff0c\u4f46\u4e0d\u5f71\u54cd\u5b9e\u9645\u8fd0\u884c\u7ed3\u679c\u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#talend","text":"","title":"\u5b89\u88c5Talend"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_3","text":"\u5b89\u88c5Talend Open Studio for Big Data","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_4","text":"\u4ece https://www.talend.com/products/big-data/big-data-open-studio/ \u4e0b\u8f7dWindow\u7248\u7684Talend\u3002 \u89e3\u538b\u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u70b9\u51fbTOS_BD-win-x86_64.exe\u542f\u52a8Talend Open Studio for Big Data\u3002\u70b9\u51fb \u6211\u540c\u610f \u3002 \u70b9\u51fb \u5b8c\u6210 \uff0c\u9ed8\u8ba4\u521b\u5efaLocal_Project\u7684\u5de5\u7a0b\u3002 \u9009\u62e9\u5b89\u88c5\u5fc5\u987b\u7684\u7b2c\u4e09\u65b9\u5e93\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9 \u6211\u63a5\u53d7\u6240\u9009\u8bb8\u53ef\u534f\u8bae\u7684\u6761\u6b3e \uff0c\u70b9\u51fb \u5168\u90e8\u63a5\u53d7 \u3002 \u5728\u53f3\u4e0b\u89d2\u53ef\u770b\u5230\u5b89\u88c5\u8fdb\u5ea6\u3002","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hadoop","text":"","title":"\u521b\u5efaHadoop\u670d\u52a1"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_5","text":"\u521b\u5efa\u5305\u542bHDFS\u3001HIVE\u670d\u52a1\u7684Hadoop\u96c6\u7fa4","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_6","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aefHDFS\u3001HIVE\u4ee5\u4e0b\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u81f3 E:\\195config\\config \u76ee\u5f55\u4e0b\u3002 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\config \u7684hdfs-site.xml\u3001core-site.xml\u3002 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\config \u7684hive-site.xml\u3001hivemetastore-site.xml\u3002 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\Yarn\\config \u7684mapred-site.xml\u3001yarn-site.xml\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_7","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hadoop_1","text":"\u6253\u5f00 Talend Open Studio for Big Data \uff0c\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster \uff0c\u53f3\u952e Hadoop Cluster \u9009\u62e9 Create Hadoop Cluster \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201cFusionInsight\u201d\uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9 \u4ece\u672c\u5730\u6587\u4ef6\u5bfc\u5165\u914d\u7f6e \uff0c\u70b9\u51fb Next \u3002 \u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9\u76ee\u5f55 E:\\195config\\config \uff0c\u9ed8\u8ba4\u5168\u9009\uff0c\u70b9\u51fb Finish \u3002 \u5148\u4fee\u6539Namenode URI\u7684\u503c\u4e3a hdfs://172.16.10.132:25000 \uff0c\u5176\u4e2d172.16.10.132\u4e3a\u4e3bnamenode ip\uff0c\u7136\u540e\u201cDistribution\u201d\u9009\u62e9 Custom - Unsuported \uff0c\u70b9\u51fb\u4e0b\u62c9\u6846\u53f3\u8fb9\u7684 \u6309\u94ae\u5bfc\u5165HDFS\u3001HIVE\u76f8\u5173\u7684jar\u5305\u3002 \u70b9\u51fb Cancel \u53d6\u6d88\u81ea\u52a8\u5f39\u51fa\u7684\u201c\u5bfc\u5165\u81ea\u5b9a\u4e49\u7684\u5b9a\u4e49\u201d\u7a97\u53e3\u3002 \u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u70b9\u51fb \u6309\u94ae\u6dfb\u52a0HDFS\u76f8\u5173\u7684jar\u5305\u3002 \u9009\u62e9 \u5916\u90e8\u5e93 \uff0c\u70b9\u51fb \u6d4f\u89c8 \uff0c\u9009\u62e9 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\hdfs \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\uff0c\u70b9\u51fb OK \u5bfc\u5165jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHDFS/HCatalog/Oozie\u201d\uff0c\u5bfc\u5165 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common \u548c E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HDFS\\FusionInsight-Hadoop-3.1.1.tar.gz\\hadoop\\share\\hadoop\\common\\lib \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u6309\u7167\u540c\u6837\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u201cHive\u201d\uff0c\u5bfc\u5165 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\Hive\\jdbc \u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u3002 \u914d\u7f6eKerberos\u8ba4\u8bc1\u3002\u201cCustom->Authentication\u201d\u9009\u62e9 Kerberos \u3002 \u52fe\u9009 Authentication->Enable Kerberos security \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Namenode Principal = hdfs/hadoop.hadoop.com@HADOOP.COM \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53 = mapred/hadoop.hadoop.com@HADOOP.COM \u5907\u6ce8\uff1a Namenode Principal\u7684\u53d6\u503c\u4e3ahdfs-site.xml\u7684dfs.namenode.kerberos.principal\u7684value\u503c\uff1b \u8d44\u6e90\u7ba1\u7406\u5668\u4e3b\u4f53\u7684\u53d6\u503c\u4e3ayarn-site.xml\u7684yarn.resourcemanager.principal\u7684value\u503c\uff1b \u4f5c\u4e1a\u5386\u53f2\u8bb0\u5f55\u4e3b\u4f53\u7684\u53d6\u503c\u4e3amapred-site.xml\u7684mapreduce.jobhistory.principal\u7684value\u503c\u3002 \u52fe\u9009 Authentication->Use a keytab to authenticate \uff0c\u8f93\u5165\u4fe1\u606f\u5982\u4e0b\uff1a Principal = developuser Keytab = E:/195config/user.keytab \u5907\u6ce8\uff1a Principal\u4e3aFusionInsight Manager\u7684\u7528\u6237\u540d\uff0cKeytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u51ed\u636e\u3002 \u914d\u7f6eHadoop\u5c5e\u6027\uff0c\u70b9\u51fb Hadoop\u5c5e\u6027 \u53f3\u8fb9\u7684 \u6309\u94ae\u3002 \u70b9\u51fb \u6309\u94ae\uff0c\u589e\u52a0\u4ee5\u4e0bHadoop\u5c5e\u6027\u3002\u589e\u52a0\u5b8c\u6bd5\uff0c\u70b9\u51fb OK \u3002 \u589e\u52a0core-site.xml\u7684hadoop.security.authentication\u548chadoop.rpc.protection\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\uff1b \u589e\u52a0hdfs-site.xml\u7684dfs.namenode.rpc-address.hacluster.*\uff0cdfs.ha.namenodes.hacluster\u3001dfs.nameservices\u3001dfs.client.failover.proxy.provider.hacluster\u7684\u5c5e\u6027\u53ca\u5176\u5bf9\u5e94\u7684value\u503c\u3002 \u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a hadoop.security.authentication = Kerberos hadoop.rpc.protection = privacy \u786e\u8ba4\u9ed8\u8ba4\u52fe\u9009 \u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u5c5e\u6027 \uff0c\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \u3002 \u68c0\u67e5\u8fd4\u56de100%\uff0c\u5219Hadoop\u96c6\u7fa4\u914d\u7f6e\u6210\u529f\uff0c\u70b9\u51fb Close \u3002\u5982\u679c\u8fd4\u56de\u9519\u8bef\u65e5\u5fd7\uff0c\u5219\u6839\u636e\u9519\u8bef\u65e5\u5fd7\u63d0\u793a\u4fee\u6b63\u95ee\u9898\u540e\uff0c\u91cd\u65b0\u70b9\u51fb \u68c0\u67e5\u670d\u52a1 \uff0c\u76f4\u81f3\u68c0\u67e5\u8fd4\u56de100%\u3002 \u70b9\u51fb Finish \uff0c\u5219\u53ef\u5728 \u5143\u6570\u636e->Hadoop Cluster \u770b\u5230\u65b0\u5efa\u7684\u201cFusionInsight\u201d\u96c6\u7fa4\uff0c\u5305\u542bHDFS\u3001HIVE\u670d\u52a1\u3002","title":"\u521b\u5efaHadoop\u96c6\u7fa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hive","text":"\u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight->Hive(1)->FusionInsight_HIVE \uff0c\u53f3\u952e FusionInsight_HIVE \u9009\u62e9 Edit Hive \u3002 \u70b9\u51fb Next \u3002 \u9700\u8981\u66f4\u65b0\u7684\u914d\u7f6e\u5982\u4e0b\uff0c\u5176\u4f59\u7684\u4fdd\u6301\u4e0d\u53d8\u3002 hive\u6a21\u5f0f = Standalone hive\u670d\u52a1\u5668\u7248\u672c = Hive Server2 -- jdbc:hive2:// \u767b\u5f55\u540d = developuser \u5bc6\u7801 = Huawei@123 \u670d\u52a1\u5668 = 172.16.10.131:24002,172.16.10.132:24002,172.16.10.133 \u7aef\u53e3 = 24002 DataBase = default \u9644\u52a0JDBC\u8bbe\u7f6e = ;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;user.principal=developuser;user.keytab=E:/195config/user.keytab \u8bf4\u660e\uff1a\u4ee5\u4e0a\u4fe1\u606f\u53ef\u6839\u636eJDBC\u65b9\u5f0f\u8fde\u63a5Hive\u670d\u52a1\u7684\u914d\u7f6e\u586b\u5199\u3002 \u70b9\u51fb \u6d4b\u8bd5\u8fde\u63a5 \uff0c\u8fd4\u56de\u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \uff0c\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002","title":"\u914d\u7f6eHIVE\u670d\u52a1"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#talendfusioninsight-hdfs","text":"","title":"Talend\u5bf9\u63a5FusionInsight HDFS"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_8","text":"Talend\u4e2d\u914d\u7f6eHDFS\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight HDFS\u63a5\u53e3\uff0c\u5e76\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\uff0c\u6216\u8005\u5c06\u672c\u5730\u6587\u4ef6\u4e0a\u4f20\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_9","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHDFS\u670d\u52a1\u7684Hadoop\u96c6\u7fa4","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hdfs-connection","text":"\u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chdfsConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hdfsConnection \uff0c\u5728Palette\u9762\u677f\u8f93\u5165\u201chdfsConnection\u201d\u641c\u7d22\uff0c\u5c06\u641c\u7d22\u8fd4\u56de\u7684\u201ctHDFSConnection\u201d\u7ec4\u4ef6\u62d6\u81f3Disigner\u533a\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHDFSConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002\u5982\u679c\u63d0\u793a\u201c\u6b64\u7ec4\u4ef6tHDFSConnection\u9700\u8981\u81f3\u5c11\u5b89\u88c5\u4e00\u4e2a\u5916\u90e8jar\u3002\u201d\uff0c\u5219\u70b9\u51fb \u5b89\u88c5 \u3002 \u70b9\u51fb \u4e0b\u8f7d\u5e76\u5b89\u88c5\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757 \u3002 \u7b49\u5f85\u6240\u6709\u53ef\u7528\u7684\u6a21\u5757\u4e0b\u8f7d\u5e76\u5b89\u88c5\u5b8c\u4e4b\u540e\uff0c\u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HDFS\u6210\u529f\u3002","title":"HDFS Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hdfs-get","text":"\u4eceFusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0b\u8f7d\u6587\u4ef6\u81f3\u672c\u5730\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cgetFromHdfs.csv\u201d\uff0c\u5185\u5bb9\u968f\u610f\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsGet\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSGet\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \uff0c\u53f3\u952e\u9009\u62e9 \u89e6\u53d1\u5668->\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6 \uff0c\u8fde\u63a5\u81f3tHDFSGet_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSGet_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile (\u53ef\u9009\u62e9\u4efb\u610f\u7684\u672c\u5730\u76ee\u5f55)\uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u5728HDFS\u7684/tmp\u76ee\u5f55\u4e0b\u9700\u8981\u83b7\u53d6\u7684\u6587\u4ef6\u540d\u79f0 getFromHdfs.csv \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsGet\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4eceFusionInsight HDFS\u4e0b\u8f7d\u6587\u4ef6\u6210\u529f\u3002 getFromHdfs.csv\u5df2\u4e0b\u8f7d\u81f3\u672c\u5730 C:\\talend\\testFile \u3002","title":"HDFS Get\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hdfs-put","text":"\u4ece\u672c\u5730\u4e0a\u4f20\u6587\u4ef6\u81f3Fusion Insight\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chdfsPut\u201d\uff0c\u52a0\u5165tHDFSConnection\u3001tHDFSPut\u7ec4\u4ef6\uff0ctHDFSConnection_1\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884ctHDFSPut_1\u3002 \u70b9\u51fb\u9009\u4e2d tHDFSConnection_1 \u7ec4\u4ef6\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HDFS \u3002 \u70b9\u51fb\u9009\u4e2d tHDFSPut_1 \u7ec4\u4ef6\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHDFSConnection_1 \uff0c\u201c\u672c\u5730\u76ee\u5f55\u201d\u9009\u62e9 C:/talend/testFile \uff0c\u201cHDFS\u76ee\u5f55\u201d\u9009\u62e9 /tmp \uff0c\u5728\u201c\u6587\u4ef6\u63a9\u7801\u201d\u8f93\u5165\u9700\u8981\u4e0a\u4f20\u81f3HDFS\u6587\u4ef6\u7cfb\u7edf\u7684\u6587\u4ef6\u540d\u79f0 putToHdfs.csv \u3002 \u8bf4\u660e\uff1a C:/talend/testFile/putToHdfs.csv \u4e3a\u672c\u5730\u5df2\u5b58\u5728\u6587\u4ef6\uff0c\u5185\u5bb9\u968f\u610f\u3002 ![](assets/Talend_7.2.1/61c635ee.png) \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahdfsPut\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793a\u4eceTalend\u4e0a\u4f20\u6587\u4ef6putToHdfs.csv\u81f3FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u68c0\u67e5putToHdfs.csv\u5df2\u4e0a\u4f20\u81f3 /tmp \u76ee\u5f55\u3002","title":"HDFS Put \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#talendfusioninsight-hive","text":"","title":"Talend\u5bf9\u63a5FusionInsight Hive"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_10","text":"Talend\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8868\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_11","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u521b\u5efa\u5305\u542bHive\u670d\u52a1\u7684Hadoop\u96c6\u7fa4\u548c\u5b8c\u6210Hadoop\u96c6\u7fa4\u7684Hive\u670d\u52a1\u914d\u7f6e\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hive-connection","text":"\u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chiveConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hiveConnection \uff0c\u52a0\u5165tHiveConnection\u3001tHiveClose\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight Hive\u6210\u529f\u3002","title":"Hive Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hive-create-table","text":"\u4f7f\u7528Talend\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06/tmp/putToHdfs.csv\u7684\u6570\u636e\u4f20\u5165\u8868talendHiveCreate\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c hdfs dfs -ls /tmp \u547d\u4ee4\u786e\u8ba4 /tmp \u76ee\u5f55\u5df2\u5b58\u5728\u6587\u4ef6\u201cputToHdfs.csv\u201d\u3002 putToHdfs.csv\u7684\u5185\u5bb9\u5982\u4e0b(\u5305\u542b\u4e24\u5217\uff0c\u4e24\u5217\u4e4b\u95f4\u7528\u5206\u53f7\u9694\u5f00)\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chiveCreateTable\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveCreateTable\u3001tHiveLoad\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveCreateTable_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\u5219\u521b\u5efa\u8868 \uff0c\u201c\u683c\u5f0f\u201d\u9009\u62e9 \u6587\u672c\u6587\u4ef6 \uff0c\u5176\u4f59\u9009\u9879\u4e3a\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveLoad_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u201c\u52a0\u8f7d\u64cd\u4f5c\u201d\u9009\u62e9 \u52a0\u8f7d \uff0c\u201c\u6587\u4ef6\u8def\u5f84\u201d\u8f93\u5165 /tmp/putToHdfs.csv \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveCreateTable\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u4f7f\u7528Hive\u521b\u5efa\u8868talendHiveCreate\uff0c\u5e76\u5c06putToHdfs.csv\u7684\u6570\u636e\u8f93\u5165\u5230\u8868talendHiveCreate\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \u3002","title":"Hive Create Table \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hive-input","text":"\u4f7f\u7528Talend\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveInput\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveInput\u3001tHiveClose\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"select * from talendHiveCreate\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 tLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveInput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u67e5\u8be2\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002","title":"Hive Input \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hive-row","text":"\u4f7f\u7528Talend\u63d2\u5165\u6570\u636e\u81f3Hive\u8868\u3002 \u786e\u8ba4\u5df2\u5b58\u5728\u8868talendHiveCreate\u3002\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5982\u4e0b\u3002 \u521b\u5efa\u4f5c\u4e1a\u201chiveRow\u201d\uff0c\u52a0\u5165tHiveConnection\u3001tHiveRow\u3001tHiveClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveConnection_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u201c\u5c5e\u6027\u7c7b\u578b\u201d\u9009\u62e9 \u5b58\u50a8\u5e93 \uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9 FusionInsight_HIVE \u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveRow_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684\u6309\u94ae\u8bbe\u8ba1\u8868\u7ed3\u6784\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHiveCreate \uff0c\u201c\u67e5\u8be2\u201d\u8f93\u5165 \"insert into talendHiveCreate values(123,'shenzhen')\" \uff0c\u5176\u4f59\u9009\u9879\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHiveClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHiveConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahiveRow\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u63d2\u5165\u8868 createdTableTalend \u6570\u636e\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528beeline\u6267\u884c select * from talendHiveCreate; \u547d\u4ee4\u67e5\u8be2\u8868 createdTableTalend \uff0c\u8fd4\u56de\u6570\u636e\u5df2\u5305\u542b\u65b0\u589e\u7684\u6570\u636e\u3002","title":"Hive Row \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#talendfusioninsight-hbase","text":"","title":"Talend\u5bf9\u63a5FusionInsight HBase"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_12","text":"Talend\u4e2d\u914d\u7f6eHBase\u89e3\u6790\u5668\uff0c\u5bf9\u7684FusionInsight HBase\u63a5\u53e3\uff0c\u8fdb\u884c\u5efa\u8868\u3001\u67e5\u8be2\u3001\u63d2\u5165\u6570\u636e\u7b49\u64cd\u4f5c\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#_13","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c \u5df2\u5b8c\u6210Talend Open Studio for Big Data\u7684\u5b89\u88c5 \u5df2\u5728IntelliJ IDEA\u4f7f\u7528 Import project from external model ~ Eclipse \u65b9\u5f0f\u5bfc\u5165 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \uff0c\u5e76\u4e14\u8c03\u6d4bTestMain.java\u901a\u8fc7\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hbase-connection","text":"\u5bfc\u51faFusionInsight HD\u5ba2\u6237\u7aef\u4e2dHbase\u6837\u4f8b\u4ee3\u7801\u4e2d\u7684LoginUtil\u7c7b\u3002 \u5728IntelliJ IDEA\u6253\u5f00 C:\\talend\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\hbase-example \u5de5\u7a0b\uff0c\u9009\u62e9 File > Project Structure... \u83dc\u5355\u9879\u3002 \u9009\u62e9 Artifacts->Add->JAR->Empty \u3002 \u5bfc\u51fajar\u5305\u7684\u540d\u79f0\u8bbe\u7f6e\u4e3a hbase-loginUtil.jar \uff0c\u201cOutput directory\u201d\u9009\u62e9 C:\\talend\\testFile \uff0c\u53cc\u51fb\u201cAvailable Elements\u201d\u7684 'hbase-example' compile output \u5c06\u5b83\u52a0\u8f7d\u5230\u5de6\u8fb9\u5217\u8868\uff0c\u70b9\u51fb OK \u3002 \u9009\u4e2d\u201chbase-example\u201d\u5de5\u7a0bcom.huawei.hadoop.security\u7684LoginUtil.java\uff0c\u9009\u62e9 Build->Build Artifacts... \u9009\u62e9 hbase-loginUtil.jar->Build \u3002 \u7f16\u8bd1\u5b8c\u6210\u540e\uff0c\u5728\u672c\u5730 C:\\talend\\testFile \u4ea7\u751f\u201chbase-loginUtil2.jar\u201d\u3002 \u9009\u62e9 \u4f5c\u4e1a\u8bbe\u8ba1 \uff0c\u53f3\u952e\u9009\u62e9 \u521b\u5efa\u4f5c\u4e1a \u3002 \u201c\u540d\u79f0\u201d\u8f93\u5165\u201chbaseConnection\u201d\uff0c\u70b9\u51fb Finish \u3002 \u9009\u62e9\u4f5c\u4e1a hbaseConnection \uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctLibraryLoad_1\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9\u6a21\u5757\u3002\u5728\u5f39\u51fa\u7a97\u53e3\uff0c\u9009\u62e9 \u6784\u5efa\u5e93\uff08local m2/nexus\uff09 \uff0c\u9009\u62e9 \u5b89\u88c5\u4e00\u4e2a\u65b0\u6a21\u5757 \u5e76\u9009\u62e9\u6587\u4ef6 C:\\talend\\testFile\\hbase-loginUtil.jar \uff0c\u7136\u540e\u70b9\u51fb \u68c0\u6d4b\u6a21\u5757\u5b89\u88c5\u72b6\u6001 \uff0c\u68c0\u6d4b\u6ca1\u95ee\u9898\u5219 OK \u6309\u94ae\u6fc0\u6d3b\uff0c\u70b9\u51fb OK \u3002 \u6ce8\u610f\uff1a\u5982\u679c\u5df2\u7ecf\u5b89\u88c5\u8fc7\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528talend\u7684mvn uri\u5bfc\u5165 \u70b9\u51fb\u9009\u4e2d\u201ctLibraryLoad_3\u201d\uff0c\u5207\u6362\u81f3\u201c\u7ec4\u4ef6\u201d\uff0c\u70b9\u51fb\u53f3\u8fb9\u7684 \u6309\u94ae\u9009\u62e9\u6a21\u5757\u3002\u5728\u5f39\u51fa\u7a97\u53e3\uff0c\u9009\u62e9 \u6784\u5efa\u5e93\uff08local m2/nexus\uff09 \uff0c\u9009\u62e9 \u5b89\u88c5\u4e00\u4e2a\u65b0\u6a21\u5757 \u5e76\u9009\u62e9\u6587\u4ef6 E:\\195config\\FusionInsight_Cluster_1_Services_ClientConfig\\HBase\\FusionInsight-HBase-2.2.3.tar.gz\\hbase\\lib\\zookeeper-3.5.6-hw-ei-302002.jar \uff0c\u7136\u540e\u70b9\u51fb \u68c0\u6d4b\u6a21\u5757\u5b89\u88c5\u72b6\u6001 \uff0c\u68c0\u6d4b\u6ca1\u95ee\u9898\u5219 OK \u6309\u94ae\u6fc0\u6d3b\uff0c\u70b9\u51fb OK \u3002 \u6ce8\u610f\uff1a\u5982\u679c\u5df2\u7ecf\u5b89\u88c5\u8fc7\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528talend\u7684mvn uri\u5bfc\u5165 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseConnection_1\u201d\uff0c\u914d\u7f6e\u5982\u4e0b\uff1a \u9009\u7528cdh 6.1.1\u4f5c\u4e3a\u5339\u914d\u7248\u672c\u5bf9\u63a5 \u70b9\u51fb\u9009\u4e2d\u201ctJava_1\u201d\u3002 \u5728\u201c\u57fa\u672c\u8bbe\u7f6e\u201d\u7684\u201c\u4ee3\u7801\u201d\u4e2d\u8f93\u5165HBase\u914d\u7f6e\u76f8\u5173\u7684\u4ee3\u7801\u3002 \u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a org.apache.hadoop.conf.Configuration conf = org.apache.hadoop.hbase.HBaseConfiguration.create(); //\u8bbe\u7f6eKerberos\u8ba4\u8bc1\u7684\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84 System.setProperty(\"java.security.krb5.conf\", \"E:\\\\195config\\\\krb5.conf\"); System.setProperty(\"java.security.auth.login.config\", \"E:/195config/jaas.conf\"); conf.set(\"hadoop.security.authentication\",\"Kerberos\"); //\u589e\u52a0\u914d\u7f6e\u6587\u4ef6\uff0c\u6839\u636e\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u4f4d\u7f6e\u5237\u65b0 conf.addResource(new org.apache.hadoop.fs.Path(\"E:/195config/config/core-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"E:/195config/config/hdfs-site.xml\")); conf.addResource(new org.apache.hadoop.fs.Path(\"E:/195config/config/hbase-site.xml\")); //\u8f93\u51fa\u914d\u7f6e\u5c5e\u6027 System.out.println(\"=====\"); System.out.println(org.apache.hadoop.hbase.security.User.isHBaseSecurityEnabled(conf)); //\u767b\u5f55 LoginUtil.setJaasConf(\"developuser\", \"developuser\", \"E:\\\\195config\\\\krb5.conf\"); //LoginUtil.setZookeeperServerPrincipal(\"zookeeper.server.principal\", \"zookeeper/hadoop.hadoop.com\"); LoginUtil.login(\"developuser\", \"E:/195config/user.keytab\", \"E:/195config/krb5.conf\", conf); globalMap.put(\"conn_tHbaseConnection_1\", conf); \u5728\u201ctJava_1\u201d\u7684\u201c\u9ad8\u7ea7\u8bbe\u7f6e\u201d\u7684\u201c\u5bfc\u5165\u201d\u8f93\u5165 import com.huawei.hadoop.security.LoginUtil; \uff0c \u70b9\u51fb\u9009\u4e2d\u201ctHBaseClose_1\u201d\uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseConnection\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u9996\u5148\u5148\u5728Advanced settings\u4e0b\u9762\u81ea\u5b9a\u4e49jvm\u53c2\u6570\uff0c\u7136\u540e\u70b9\u51fb\u8fd0\u884c\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\u3002","title":"HBase Connection \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#hbase-input-output","text":"Talend\u901a\u8fc7FusionInsight HBase\u63a5\u53e3\u5bf9\u63a5\u6210\u529f\u540e\uff0c\u521b\u5efa\u8868talendHbaseCreate\uff0c\u5c06\u672c\u5730 E:/soft/talend/putToHdfs.csv \u7684\u6570\u636e\u4f20\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u786e\u8ba4\u672c\u5730\u5df2\u5b58\u5728 E:/soft/talend/putToHdfs.csv \u3002 putToHdfs.csv\u5185\u5bb9\u5982\u4e0b\uff1a 1;EcitQU 2;Hyy6RC 3;zju1jR 4;R9fex9 5;EU2mVq \u521b\u5efa\u4f5c\u4e1a\u201chbaseInputOutput\u201d\uff0c\u52a0\u5165tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose\u3001tFileInputDelimited\u3001tHBaseOutput\u3001tHBaseInput\u3001tLogRow\u7ec4\u4ef6\uff0c\u4e0a\u4e00\u4e2a\u7ec4\u4ef6\u7684\u5b50\u4f5c\u4e1a\u6b63\u5e38\u65f6\u6267\u884c\u4e0b\u4e00\u4e2a\u7ec4\u4ef6\u3002 tLibraryLoad\u3001tHBaseConnection\u3001tJava\u3001tHBaseClose_1\u7ec4\u4ef6\u7684\u914d\u7f6e\u8bf7\u53c2\u8003\u201cHBase Connection \u64cd\u4f5c\u6b65\u9aa4\u201d\uff0ctLogRow\u7ec4\u4ef6\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctFileInputDelimited_1\u201d\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u8bbe\u8ba1schema\u4e3a\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\uff0c\u201c\u6587\u4ef6\u540d/\u6d41\u201d\u8f93\u5165 E:/soft/talend/putToHdfs.csv \uff0c\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseOutput_1\u201d\u3002 \u5728\u201c\u9ad8\u7ea7\u914d\u7f6e\u201d\u4e2d\uff0c\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522b\u4e3aid\u548cname\uff0c\u5217\u540d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u5728\u201c\u57fa\u672c\u914d\u7f6e\u201d\u4e2d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendHbaseCreate \uff0c\u201c\u8868\u64cd\u4f5c\u201d\u9009\u62e9 \u5982\u679c\u8868\u4e0d\u5b58\u5728\uff0c\u5219\u521b\u5efa\u8868 \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u8981\u4e0d\u8fd0\u884c\u65f6\u4f1a\u8fd4\u56de\u8bed\u6cd5\u9519\u8bef\u3002 \u70b9\u51fb\u9009\u4e2d\u201ctHBaseInput_1\u201d\uff0c\u52fe\u9009 \u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5 \uff0c\u201c\u7ec4\u4ef6\u5217\u8868\u201d\u9009\u62e9 tHBaseConnection_1 \uff0c\u201c\u8868\u540d\u79f0\u201d\u8f93\u5165 talendhbase \uff0c\u8f93\u5165id\u548cname\u5bf9\u5e94\u7684\u201c\u65cf\u540d\u79f0\u201d\uff0c\u201c\u65cf\u540d\u79f0\u201d\u5fc5\u987b\u8981\u7528\u53cc\u5f15\u53f7\u5305\u62ec\uff0c\u70b9\u51fb\u201c\u7f16\u8f91schema\u201d\u53f3\u8fb9\u7684 \u6309\u94ae\u589e\u52a0\u4e24\u5217\uff0c\u5217\u540d\u5206\u522bid\u548cname\u3002 \u5207\u6362\u81f3\u201c\u8fd0\u884c\uff08\u4f5c\u4e1ahbaseInputOutput\uff09\u201d\uff0c\u70b9\u51fb \u8fd0\u884c \u6309\u94ae\u3002\u8fd4\u56de\u7ed3\u679c\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5219\u8868\u793aTalend\u5bf9\u63a5FusionInsight HBase\u6210\u529f\uff0c\u4e14\u521b\u5efa\u8868talendHbaseCreate\u5e76\u5c06\u672c\u5730\u6587\u4ef6\u6570\u636e\u8f93\u5165\u8868talendHbaseCreate\uff0c\u5e76\u4e14\u4ece\u8868talendHbaseCreate\u67e5\u8be2\u8fd4\u56de\u6570\u636e\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u68c0\u67e5HBase\u8868\u201ctalendHbaseCreate\u201d\u3002 hbase shell scan 'talendhbase'","title":"HBase Input Output \u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/Talend_7.2.1_MRS802/#faq","text":"\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u8fd4\u56deClient cannot authenticate via:[TOKEN, KERBEROS] \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u4f7f\u7528Talend\u5411FusionInsight HDFS\u6587\u4ef6\u7cfb\u7edf\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u65f6\uff0c\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u7684\u7ec4\u4ef6\uff08\u4f8b\u5982tHDFSGet_1\uff09\u91c7\u7528\u201c\u4f7f\u7528\u4e00\u4e2a\u73b0\u6709\u8fde\u63a5\u201d\uff0c\u73b0\u6709\u8fde\u63a5tHDFSConnection_1\u7684\u5c5e\u6027\u7c7b\u578b\u662f\u201c\u5b58\u50a8\u5e93\u201d\u65f6\uff0c\u8fd0\u884c\u65f6\u8fd4\u56dejava.io.IOException: DestHost:destPort euleros-hd03:25000 , LocalHost:localPort user-PC/172.16.5.106:0. Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]\uff0c\u4e14\u4e0a\u4f20\u6216\u8005\u4e0b\u8f7d\u6587\u4ef6\u5931\u8d25\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 tHDFSConnection_1\u4f7f\u7528\u7684\u5b58\u50a8\u5e93FusionInsight_HDFS\u6240\u5c5e\u7684Hadoop\u96c6\u7fa4FusionInsight\u6ca1\u6709\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002\u9700\u8981\u4fee\u6539Hadoop\u96c6\u7fa4FusionInsight\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684Hadoop\u914d\u7f6e\u3002 * \u9009\u62e9 \u5143\u6570\u636e->Hadoop Cluster->FusionInsight \uff0c\u53f3\u952e FusionInsight \u9009\u62e9 Edit Hadoop Cluster \u3002 ![](assets/Talend_7.2.1/33ef12f1.png) * \u52fe\u9009`\u4f7f\u7528\u81ea\u5b9a\u4e49Hadoop\u914d\u7f6e`\u3002 ![](assets/Talend_7.2.1/b6962abf.png) * \u70b9\u51fb`Yes`\u3002 ![](assets/Talend_7.2.1/9e6ea1d7.png) \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\u8fd4\u56deCannot modify dfs.client.use.datanode.hostname at runtime\u3002 \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5FusionInsight Hive\u63a5\u53e3\u521b\u5efa\u8868\u7684\u65f6\u5019\uff0c\u8fd4\u56de\u7c7b\u4f3c\u7684\u9519\u8bef\uff1aError while processing statement: Cannot modify dfs.client.use.datanode.hostname at runtime. It is not in list of params that are allowed to be modified at runtime\u3002\u53ef\u80fd\u6d89\u53ca\u7684\u6709\u4ee5\u4e0b\u4e09\u4e2a\u5c5e\u6027\uff1a dfs.client.use.datanode.hostname\u3001mapred.job.name\u3001hive.query.name \u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u767b\u5f55FusionInsight Manager\uff0c\u5728Hive\u670d\u52a1\u7684\u914d\u7f6e\u53c2\u6570hive.security.authorization.sqlstd.confwhitelist.append\u65b0\u589e |dfs\\.client\\.use\\.datanode\\.hostname|mapred\\.job\\.name|hive\\.query\\.name \uff0c\u7136\u540e\u91cd\u542fHive\u670d\u52a1\u3002","title":"FAQ"},{"location":"Data_Integration/streamsets_3.16.1/","text":"streamsets\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Streamsets 3.16.1 \u2194 FusionInsight HD 6.5 (HDFS/Hive/HBase/Kafka) Streamsets 3.16.1 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/HBase/Kafka) MRS 8.0 \u5bf9\u63a5\u8bf4\u660e \u00b6 \u8bf4\u660e\uff1a 1: mrs8.0\u7684\u7ec4\u4ef6\u7248\u672c\u63a5\u8fd1streamsets\u91cc\u9762hdp3.1\u7248\u672c\uff0c\u652f\u6301hdfs,hive,hbase\u7684\u5bf9\u63a5 \u9700\u8981\u66f4\u6539\u4f9d\u8d56\u5982\u4e0b\uff1a /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib \u628ahadoop-plugins-8.0.0-301001-SNAPSHOT.jar\uff0czookeeper-3.5.6-hw-ei-301001-SNAPSHOT.jar\uff0czookeeper-jute-3.5.6-hw-ei-301001-SNAPSHOT.jar\u5bfc\u5165\u5230\u8fd9\u4e2a\u8def\u5f84\u4e0b\uff0c\u628a\u4e4b\u524d\u7684zookeeper-3.4.10.jar\u6ce8\u91ca\u6389(\u5177\u4f53Jar\u5305\u540d\u5b57\u53ef\u80fd\u6709\u66f4\u6539) 2\uff1a mrs8.0\u7684kafka\u7248\u672c\u9009\u7528streamsets\u91cc\u9762\u7684Apache kafka 2.0.0 \u9700\u8981\u66f4\u6539\u4f9d\u8d56\u5982\u4e0b\uff1a /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_2_0-lib/lib \u628akafka-clients-2.4.0-hw-ei-301001-SNAPSHOT.jar\uff0czookeeper-3.5.6-hw-ei-301001-SNAPSHOT.jar\uff0czookeeper-jute-3.5.6-hw-ei-301001-SNAPSHOT.jar\u4e0a\u8ff03\u4e2ajar\u5305\u62f7\u8d1d\u8fc7\u6765\uff0c\u5e76\u4e14\u628akafka-client\u548czookeeper\u539f\u6765\u5e26\u7684jar\u5305\u6ce8\u91ca\u6389\uff08\u5177\u4f53Jar\u5305\u540d\u5b57\u53ef\u80fd\u6709\u66f4\u6539\uff09 \u5b89\u88c5streamsets \u00b6 \u73af\u5883\uff1a172.16.2.121 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5streamsets 3.16.1 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u9646streamsets\u5b98\u7f51\u4e0b\u8f7d\u5b89\u88c5\u5305 https://streamsets.com/products/dataops-platform/data-collector/download/#download-sdc \u5c06\u4e0b\u8f7d\u597d\u7684\u5b89\u88c5\u5305\u653e\u5230 /opt/streamsets \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf streamsets-datacollector-all-3.16.1.tgz \u89e3\u538b\u5b89\u88c5\u5305 \u53c2\u8003\u5b98\u65b9\u4ecb\u7ecd https://streamsets.com/documentation/datacollector/3.16.x/help/datacollector/UserGuide/Installation/Installing_the_DC.html#task_bt1_zcp_kq \u914d\u7f6e $SDC_DIST/libexec/sdc-env.sh \u6587\u4ef6 \u6bd4\u5982\u914d\u7f6e /opt/streamsets/streamsets-datacollector-3.16.1/libexec/sdc-env.sh \u6587\u4ef6 \u5e76\u4e14\u5c06 /opt/streamsets/streamsets-datacollector-3.16.1/etc \u8def\u5f84\u4e0b\u7684\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/streamsets/sdc/conf \u8def\u5f84\u4e0b cp /opt/streamsets/streamsets-datacollector-3.16.1/etc/* /opt/streamsets/sdc/conf/ \u6ce8\u610f\uff1a\u9700\u63d0\u524d\u521b\u5efa\u597d\u5bf9\u5e94\u7684\u8def\u5f84\uff0c\u518d\u914d\u7f6e \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8streamsets ulimit -n 32768 bin/streamsets dc \u767b\u9646streamsets\u7684web ui\u8fdb\u884c\u767b\u9646\uff0c\u6bd4\u5982 http://172.16.2.121:18630/ \u6ce8\u610f\uff1a\u7b2c\u4e00\u6b21\u767b\u9646\u9700\u8981\u4f7f\u7528\u5de5\u4f5c\u90ae\u7bb1\u5728streamsets\u5b98\u7f51\u6ce8\u518c\uff0c\u83b7\u53d6\u6fc0\u6d3b\u7801\uff0c\u5b8c\u6210\u540e\u4f7f\u7528\u9ed8\u8ba4\u7528\u6237admin, \u9ed8\u8ba4\u5bc6\u7801admin\u8fdb\u884c\u767b\u9646 Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u914d\u7f6estreamsets\u4e0eFI HD\u96c6\u7fa4\u5bf9\u63a5\u76f8\u5173\u8ba4\u8bc1\u914d\u7f6e \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210streamsets\u5b89\u88c5 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u8bf4\u660e\uff1aFI\u4ea4\u4e92\u7ec4\u4ef6\u4e3aKafka, Hive, HDFS\uff0c\u914d\u7f6e\u65f6\u9700\u8981\u5728streamsets\u9009\u62e9\u76f8\u8fd1\u7684\u7248\u672c\uff0c\u4e0b\u8868\u4e3a\u76f8\u8fd1\u7248\u672c\u5bf9\u5e94\u5217\u8868\uff0c\u6839\u636e\u6b64\u8868\u627e\u5230streamsets\u5bf9\u5e94\u7248\u672c\u4f9d\u8d56\u5e93\u8def\u5f84\uff0c\u65b9\u4fbf\u4fee\u6539\u76f8\u5173\u7684jar\u5305 \u7ec4\u4ef6 \u5bf9\u5e94streamsets\u7248\u672c HDFS HDP 3.1.0 Hive HDP 3.1.0 HBase CDH 5.14.0 Kafka Apache Kafka 1.1.0 \u914d\u7f6e\u6b65\u9aa4\uff1a \u4eceFI Manager\u4e0b\u8f7d\u5bf9\u5e94\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6user.keytab,krb5.conf\u5e76\u4e0a\u4f20\u5230streamsets\u4e3b\u673a\u7684/opt\u8def\u5f84\u4e0b. \u5e76\u4e14\u5c06krb5.conf\u6587\u4ef6\u653e\u7f6e\u5230streamsets\u4e3b\u673a\u7684/etc\u8def\u5f84\u4e0b\uff0cstreamsets\u9ed8\u8ba4\u8bfb\u53d6/etc/krb5.conf\u6587\u4ef6\u505a\u8ba4\u8bc1\u670d\u52a1 \u4fee\u6539 $SDC_CONF/sdc.properties \u914d\u7f6e\u6587\u4ef6\uff0c\u6bd4\u5982 /opt/streamsets/sdc/conf/sdc.properties \u5176\u4e2d\u7b2c2\u6761\u548c\u7b2c3\u6761\u4e3aKerberos\u8ba4\u8bc1\u7528\u6237\u4ee5\u53causer.keytab\u6587\u4ef6\uff0c\u4eceFI Manager\u4e0a\u83b7\u53d6\uff0c\u5e76\u4fdd\u8bc1developuser\u6709kafka,hdfs,hive\u76f8\u5173\u6743\u9650 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 hadoop-plugins-1.0.jar \u5e76\u62f7\u8d1d\u5230 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib \u8def\u5f84\u4e0b cp /opt/125_651hdclient/hadoopclient/HDFS/hadoop/share/hadoop/common/lib/hadoop-plugins-1.0.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 zookeeper-3.5.1.jar \u62f7\u8d1d\u5230 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib \u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u628a\u539f\u6765\u7684 zookeeper-3.4.10.jar \u6ce8\u91ca\u6389 mv /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib/zookeeper-3.4.10.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib/zookeeper-3.4.10.jar.org cp /opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/zookeeper-3.5.1.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib/ \u5b8c\u6210\u540e\u68c0\u67e5\uff1a \u627e\u5230streamsets\u542f\u52a8jvm\u53c2\u6570\u914d\u7f6e\u6587\u4ef6 /opt/streamsets/streamsets-datacollector-3.16.1/libexec/sdc-env.sh \u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58\uff1a \u589e\u52a0\u53c2\u6570\u5185\u5bb9\u4e3a -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dsun.security.krb5.debug=false -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf \u5728/opt\u8def\u5f84\u4e0b\u521b\u5efajaas.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" storeKey=true useTicketCache=false principal=\"developuser@HADOOP.COM\"; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 kafka-clients-1.1.0.jar \u5e76\u62f7\u8d1d\u5230\u8def\u5f84 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib \u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u628a\u539f\u6765\u7684 kafka-clients-1.1.0.jar \u6ce8\u91ca\u6389 mv /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/kafka-clients-1.1.0.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/kafka-clients-1.1.0.jar.org cp /opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/ \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 zookeeper-3.5.1.jar \u62f7\u8d1d\u5230 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib \u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u628a\u539f\u6765\u7684 zookeeper-3.4.6.jar \u6ce8\u91ca\u6389 mv /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/zookeeper-3.4.6.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/zookeeper-3.4.6.jar.org cp /opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/zookeeper-3.5.1.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/ \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 zookeeper-3.5.1.jar \u62f7\u8d1d\u5230 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-cdh_5_14-lib/lib \u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u628a\u539f\u6765\u7684 zookeeper-3.4.5-cdh5.14.0.jar \u6ce8\u91ca\u6389 mv /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-cdh_5_14-lib/lib/zookeeper-3.4.5-cdh5.14.0.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-cdh_5_14-lib/lib/zookeeper-3.4.5-cdh5.14.0.jar.org cp /opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/zookeeper-3.5.1.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-cdh_5_14-lib/lib \u521b\u5efa\u8def\u5f84 /opt/streamsets/hdfsconf \uff0c\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 core-site.xml, hdfs-site.xml, hive-site.xml, hbase-site.xml \u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u505a\u5982\u4e0b\u4fee\u6539 core-site.xml\u914d\u7f6e\u6587\u4ef6\u4fee\u6539\uff1a \u5176\u4e2d172.16.4.123\u4e3ahdfs\u4e3bnamenode\u7684ip,\u7aef\u53e3\u4e3a25000 hdfs-site.xml\u914d\u7f6e\u6587\u4ef6\u4fee\u6539\uff1a \u627e\u5230\u5982\u4e0b\u914d\u7f6e\u9879\uff0c\u5e76\u4e14\u5220\u9664 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider</value> </property> \u5b8c\u6210\u540e\u91cd\u542fstreamsets \u5bf9\u63a5HDFS \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u914d\u7f6estreamsets\u5bf9\u63a5HDFS\uff0c\u8bfb\u3001\u5199\u6570\u636e \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u8bfb\u53d6HDFS\u6570\u636e\u7528\u4f8b \u00b6 \u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a \u6ce8\u610f\uff08\u91cd\u8981\uff09 \uff1a\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u6570\u636e\u6e90\u7aef\u9009\u62e9 Hadoop FS Standalone . \u5982\u679c\u6570\u636e\u6e90\u7aef\u9009\u62e9 Hadoop FS \u4f1a\u9047\u5230\u95ee\u9898 \u5728\u7a7a\u767d\u5904\u5355\u51fb\u9f20\u6807\u5de6\u952e\uff0c\u5728General\u9875\u7b7e\u914d\u7f6e\u6570\u636e\u6d41\u6267\u884c\u6a21\u5f0f\u4e3aStandalone Hadoop FS Standalone \u914d\u7f6e\u5982\u4e0b\uff1a General\u9875\u7b7e Connection\u9875\u7b7e \u6ce8\u610f\uff1a\u5176\u4e2d172.16.4.123\u4e3ahdfs\u4e3bnamenode\u8282\u70b9\u7684ip Files\u9875\u7b7e Post processing\u9875\u7b7e\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e Local FS \u914d\u7f6e\u5982\u4e0b\uff1a \u53ea\u4fee\u6539Output Files\u9875\u7b7e \u542f\u52a8\u4efb\u52a1\u524d\uff0c\u767b\u9646\u96c6\u7fa4hdfs\u8def\u5f84 /tmp/out/test/ ,\u5e76\u521b\u5efa\u6570\u636e\u6587\u4ef6 test.txt \u542f\u52a8\u4efb\u52a1\u6d41 \u6ce8\u610f\uff1a\u591a\u6b21\u542f\u52a8\u5982\u679c\u6570\u636e\u6ca1\u6709\u7ed3\u679c\u8ddf\u65b0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684\u542f\u52a8\u65b9\u5f0f \u767b\u9646\u540e\u53f0\u5bf9\u5e94\u8def\u5f84\u68c0\u67e5\u7ed3\u679c\uff1a \u5199\u5165HDFS\u6570\u636e\u7528\u4f8b \u00b6 \u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a \u5728\u7a7a\u767d\u5904\u5355\u51fb\u9f20\u6807\u5de6\u952e\uff0c\u5728General\u9875\u7b7e\u914d\u7f6e\u6570\u636e\u6d41\u6267\u884c\u6a21\u5f0f\u4e3aStandalone Directory \u914d\u7f6e General\u9875\u7b7e Files\u9875\u7b7e Post Processing\u9875\u7b7e\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e Hadoop FS \u914d\u7f6e General\u9875\u7b7e Connection\u9875\u7b7e \u6ce8\u610f\uff1a\u5176\u4e2d172.16.4.123\u4e3ahdfs\u4e3bnamenode\u8282\u70b9\u7684ip Output Files\u9875\u7b7e Late Records\u9875\u7b7e Data Format\u9875\u7b7e \u6d4b\u8bd5\u524d\u767b\u9646\u4e3b\u673a\u540e\u53f0\u51c6\u5907\u4e0a\u4f20\u6570\u636e\u6587\u4ef6\uff1a \u542f\u52a8\u6570\u636e\u6d41 \u767b\u9646hdfs\u5bf9\u5e94\u8def\u5f84\u67e5\u770b\u7ed3\u679c \u5bf9\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u914d\u7f6estreamsets\u5bf9\u63a5Hive\uff0c\u5199\u5165\u6570\u636e \u8bf4\u660e\uff1astreamsets\u4e0d\u63d0\u4f9bhive\u4f5c\u4e3a\u76f4\u63a5\u6570\u636e\u6e90\uff0c\u672c\u8282\u53ea\u63d0\u4f9bhive\u5199\u5165\u7528\u4f8b \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u5199\u5165Hive\u6570\u636e\u7528\u4f8b \u00b6 \u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a Dev Raw Data Source \u914d\u7f6e General\u9875\u7b7e Raw Data\u9875\u7b7e { \"firstname\": \"abc\", \"midname\": \"xyz\", \"lastname\": \"lmn\" } Event Data\u9875\u7b7e,\u672a\u505a\u914d\u7f6e Data Format\u9875\u7b7e Expression Evaluator \u914d\u7f6e General\u9875\u7b7e Expressions\u9875\u7b7e database = default table_name = sdc_drift_example Hive Metadata \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e\uff1a Hive\u9875\u7b7e\u914d\u7f6e\uff1a 1. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 2. org.apache.hive.jdbc.HiveDriver 3. /opt/streamsets/hdfsconf Table\u9875\u7b7e\u914d\u7f6e 1. ${record:attribute('database')} 2. ${record:attribute('table_name')} Advanced\u9875\u7b7e\u6309\u9ed8\u8ba4\u914d\u7f6e\uff0c\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e\u914d\u7f6e Hadoop FS \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e Connection\u9875\u7b7e\u914d\u7f6e Output Files\u9875\u7b7e\u914d\u7f6e Late Records\u9875\u7b7e\u914d\u7f6e Data Format\u9875\u7b7e\u914d\u7f6e Hive Metastore \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e Hive\u9875\u7b7e\u914d\u7f6e 1. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 2. org.apache.hive.jdbc.HiveDriver 3. /opt/streamsets/hdfsconf Advanced\u9875\u7b7e\u914d\u7f6e hive\u8868streamsets\u4f1a\u81ea\u52a8\u521b\u5efa\uff0c\u4e0d\u9700\u8981\u63d0\u524d\u521b\u5efa\u6539\u8868 \u542f\u52a8\u6570\u636e\u6d41 beeline\u767b\u9646Hive\u68c0\u67e5\u7ed3\u679c \u5bf9\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u914d\u7f6estreamsets\u5bf9\u63a5HBase \u6ce8\u610f\uff1astreamstes\u4e0d\u63d0\u4f9bHBase\u4f5c\u4e3a\u6e90\u7aef\u7684\u529f\u80fd\uff0c\u672c\u8282\u53ea\u63d0\u4f9bHBase\u5199\u5165\u7528\u4f8b \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e HBase\u76f8\u5173\u914d\u7f6e \u00b6 \u8bf4\u660e\uff1a FusionInisight \u7684HBase\u7248\u672c\uff081.3.1\uff09\u4e0eCDH 5.14.0 \u7684HBase\u7248\u672c\uff081.2.0\uff09\u76f8\u8fd1, \u5df2\u7ecf\u4fee\u6539\u4e86zookeeper-3.4.5-cdh5.14.0.jar \u4e3aFusionInsight\u7684 zookeeper-3.5.1.jar. \u4f46\u662f\u7248\u672c\u8fd8\u662f\u6709\u5dee\u5f02\uff0c\u9700\u8981\u5bfc\u5165Fusioninsight hbase\u76f8\u5173jar\u5305\uff0c\u907f\u514d\u53d1\u751f\u4f9d\u8d56\u9519\u8bef \u53c2\u8003\u5982\u4e0b\u94fe\u63a5\u5b89\u88c5\u5916\u90e8\u4f9d\u8d56\u5e93,\u5e76\u5bfc\u5165FusionInsgiht HBase\u4f9d\u8d56\uff1a https://streamsets.com/documentation/datacollector/latest/help/datacollector/UserGuide/Configuration/ExternalLibs.html#concept_amy_pzs_gz \u9996\u5148\u521b\u5efa\u8def\u5f84 /opt/streamsets/sdc/sdc-extras/streamsets-datacollector-cdh_5_14-lib/lib mkdir -p /opt/streamsets/sdc/sdc-extras/streamsets-datacollector-cdh_5_14-lib/lib \u5c06FusionInsight HBase\u76f8\u5173\u4f9d\u8d56jar\u5305\u62f7\u8d1d\u5230\u4e0a\u4e00\u6b65\u521b\u5efa\u7684\u8def\u5f84\u4e2d cp /opt/125_651hdclient/hadoopclient/HBase/hbase/lib/*.jar /opt/streamsets/sdc/sdc-extras/streamsets-datacollector-cdh_5_14-lib/lib \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 $SDC_CONF/sdc-security.policy \uff0c\u6bd4\u5982 /opt/streamsets/sdc/conf/sdc-security.policy ,\u589e\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a // user-defined external directory grant codebase \"file:///opt/streamsets/sdc/sdc-extras/-\" { permission java.security.AllPermission; }; (\u91cd\u8981)\u5173\u95ed\u6b63\u5728\u8fd0\u884c\u7684streamsets, \u5148\u7533\u660e\u4e4b\u524d\u914d\u7f6e\u7684\u73af\u5883\u53d8\u91cf STREAMSETS_LIBRARIES_EXTRA_DIR ,\u518d\u542f\u52a8streamsets. \u6bd4\u5982\uff1a export STREAMSETS_LIBRARIES_EXTRA_DIR=\"/opt/streamsets/sdc/sdc-extras\" bin/streamsets dc \u767b\u9646streamsets\u7684web\u754c\u9762\uff0c\u5728**Package Manager**\u5904\u7684**External Libraries**\u68c0\u67e5\u76f8\u5173\u4f9d\u8d56\u662f\u5426\u5bfc\u5165\u6210\u529f \u5199\u5165HBase\u6570\u636e\u7528\u4f8b \u00b6 \u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a Dev Raw Data Source 1 \u914d\u7f6e General\u9875\u7b7e Raw Data\u9875\u7b7e { \"firstname\": \"abc\", \"midname\": \"xyz\",\"lastname\": \"lmn\" } Event Data\u9875\u7b7e\u672a\u4f5c\u4fee\u6539 Data Format\u9875\u7b7e HBase \u914d\u7f6e General\u9875\u7b7e HBase\u9875\u7b7e 1. host-172-16-4-121,host-172-16-4-122,host-172-16-4-123 2. 24002 3. /hbase 4. streamsets1 5. /firstname 6. Text 7. /firstname - data:firstname - Text /midname - data:midname - Text /lastname - data:lastname - Text 8. /opt/streamsets/hdfsconf \u6d4b\u8bd5\u524d\u767b\u9646hbase\u5ba2\u6237\u7aef\uff0c\u521b\u5efa\u8868streamsets1 hbase shell create 'streamsets1','data' \u542f\u52a8\u6570\u636e\u6d41 hbase\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c scan 'streamsets1' \u5bf9\u63a5Kafka\u5b89\u5168\u6a21\u5f0f \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u914d\u7f6estreamsets\u5bf9\u63a5Kafka\uff0c\u751f\u4ea7\uff0c\u6d88\u8d39\u6570\u636e \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u5199\u5165Kafka\u6570\u636e\u7528\u4f8b \u00b6 \u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a Directory \u914d\u7f6e\uff1a General\u9875\u7b7e Files\u9875\u7b7e Post Processing\u9875\u7b7e\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e Kafka Producer \u914d\u7f6e\uff1a General\u9875\u7b7e Kafka\u9875\u7b7e 1. 172.16.4.121:21007 2. streamtests21007 3. security.protocol = SASL_PLAINTEXT 4. sasl.kerberos.service.name = kafka Data Format\u9875\u7b7e Response\u9875\u7b7e \u542f\u52a8\u6570\u636e\u6d41\u524d\uff0c\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/kafka --partitions 2 --replication-factor 2 --topic streamtests21007 \u521b\u5efatopic \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8kafka\u6d88\u8d39\u8005 bin/kafka-console-consumer.sh --zookeeper 172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/kafka --topic streamtests21007 \u542f\u52a8\u4efb\u52a1\u6d41 \u5bf9\u5df2\u542f\u52a8\u8fc7\u7684\u4efb\u52a1\uff0c\u7528\u5982\u4e0b\u65b9\u5f0f\u91cd\u65b0\u542f\u52a8 \u5728kafka\u6d88\u8d39\u8005\u68c0\u67e5\u7ed3\u679c \u8bfb\u53d6Kafka\u6570\u636e\u7528\u4f8b \u00b6 \u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a \u6ce8\u610f\uff08\u91cd\u8981\uff09 \uff1a\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u6570\u636e\u6e90\u7aef\u9009\u62e9 Kafka Multitopic Consumer . \u5982\u679c\u6570\u636e\u6e90\u7aef\u9009\u62e9 Kafka Consumer \u4f1a\u9047\u5230\u95ee\u9898 Kafka Multitopic Consumer \u914d\u7f6e\uff1a General\u9875\u7b7e Connection\u9875\u7b7e Data Format\u9875\u7b7e Local FS \u914d\u7f6e General\u9875\u7b7e Output Files\u9875\u7b7e Late Records\u9875\u7b7e Data Format\u9875\u7b7e \u542f\u52a8\u6570\u636e\u6d41\u524d\uff0c\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/kafka --partitions 2 --replication-factor 2 --topic streamtests21007test \u521b\u5efatopic \u9996\u5148\u4f7f\u7528 bin/kafka-console-producer.sh --broker-list 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 --topic streamtests21007test --producer.config config/producer.properties \u542f\u52a8kafka\u751f\u4ea7\u8005 \u542f\u52a8\u6570\u636e\u6d41\uff0c\u624b\u52a8\u5728\u524d\u4e00\u6b65\u751f\u4ea7\u8005\u4e2d\u63d2\u5165\u6570\u636e \u767b\u9646\u540e\u53f0\u5bf9\u5e94\u8def\u5f84\u68c0\u67e5\u7ed3\u679c\uff1a streamsets\u6700\u4f73\u5b9e\u8df5 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u914d\u7f6estreamsets ETL\u6570\u636e\u6d41\uff0c\u4eceFI kafka\u8bfb\u53d6\u6570\u636e\uff0c\u518d\u5199\u5165FI Hive\u4e2d \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\uff0c\u5b8c\u6210HDFS, Hive, Kafka\u7528\u4f8b\u6d4b\u8bd5 \u6d4b\u8bd5\u7528\u4f8b \u00b6 \u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a Kafka Multitopic Consumer \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e\uff1a Connection\u9875\u7b7e\u914d\u7f6e\uff1a 1. 172.16.4.121:21007 2. demo01json21007 3. security.protocol = SASL_PLAINTEXT 4. sasl.kerberos.service.name = kafka Data Format\u9875\u7b7e\u914d\u7f6e Expression Evaluator \u914d\u7f6e General\u9875\u7b7e\u6309\u9ed8\u8ba4\u914d\u7f6e\uff0c\u672a\u505a\u4fee\u6539 Expressions\u9875\u7b7e\u914d\u7f6e\uff1a 1. database = default 2. table_name = sdc_drift_example03 Hive Metadata \u914d\u7f6e - General\u9875\u7b7e\u914d\u7f6e\uff1a Hive\u9875\u7b7e\u914d\u7f6e\uff1a 1. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 2. org.apache.hive.jdbc.HiveDriver 3. /opt/streamsets/hdfsconf Table\u9875\u7b7e\u914d\u7f6e 1. ${record:attribute('database')} 2. ${record:attribute('table_name')} Advanced\u9875\u7b7e\u6309\u9ed8\u8ba4\u914d\u7f6e\uff0c\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e\u914d\u7f6e Hadoop FS \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e Connection\u9875\u7b7e\u914d\u7f6e Output Files\u9875\u7b7e\u914d\u7f6e Late Records\u9875\u7b7e\u914d\u7f6e Data Format\u9875\u7b7e\u914d\u7f6e Hive Metastore \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e Hive\u9875\u7b7e\u914d\u7f6e 1. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 2. org.apache.hive.jdbc.HiveDriver 3. /opt/streamsets/hdfsconf Advanced\u9875\u7b7e\u914d\u7f6e \u6d4b\u8bd5\u524d\u51c6\u5907 \u4f7f\u7528FI HD\u5ba2\u6237\u7aef\u521b\u5efakafka\u76f8\u5173topic demo01json21007 bin/kafka-topics.sh --create --zookeeper 172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/kafka --partitions 2 --replication-factor 2 --topic demo01json21007 hive\u8868streamsets\u4f1a\u81ea\u52a8\u521b\u5efa\uff0c\u4e0d\u9700\u8981\u63d0\u524d\u521b\u5efa\u6539\u8868 \u542f\u52a8streamsets\u6570\u636e\u6d41 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8kafka producer\u5e76\u4e14\u63d2\u5165\u6570\u636e bin/kafka-console-producer.sh --broker-list 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 --topic demo01json21007 --producer.config config/producer.properties \u6570\u636e\uff1a { \"firstname\": \"abc\", \"midname\": \"xyz\",\"lastname\": \"lmn\" } { \"firstname\": \"abc1\", \"midname\": \"xyz1\",\"lastname\": \"lmn1\" } { \"firstname\": \"abc2\", \"midname\": \"xyz2\",\"lastname\": \"lmn2\" } \u68c0\u67e5streamsets\u754c\u9762 \u540e\u53f0\u68c0\u67e5hive\u8868 FAQ \u00b6 \u95ee\u98981 \u5728\u505ahive\u5199\u5165\u76f8\u5173\u7528\u4f8b\u7684\u65f6\u5019\uff08hive\u8868\u5199\u5165\u7528\u4f8b\uff0c\u6700\u4f73\u5b9e\u8df5\u7528\u4f8b\uff09\uff0c\u5982\u679c\u9884\u5148\u5728hive\u8868\u4e2d\u5efa\u5bf9\u5e94\u7684\u8868\uff0c\u53d1\u73b0\u5de5\u4f5c\u6d41\u542f\u52a8\u4e4b\u540e\u6570\u636e\u4e0d\u80fd\u5199\u5165\u5230hive\u8868\u4e2d\uff0c\u4f46\u662f\u4e5f\u6ca1\u6709\u62a5\u9519 \u89e3\u51b3\u529e\u6cd5\uff1a \u4e0d\u8981\u9884\u5148\u5728hive\u4e2d\u9884\u5148\u5efa\u8868\uff0c\u7531streamset\u81ea\u52a8\u5efa\u5bf9\u5e94hive\u8868\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6570\u636e\u5199\u5165\u6210\u529f \u95ee\u98982 \u5728\u542f\u52a8\u6570\u636e\u6d41\u4e4b\u540e\uff0c\u4f7f\u7528kafka producer\u63d2\u5165\u6570\u636e\u65f6\uff0cstreamsets\u62a5\u9519 Reason:Error while compiling statement: FAILED: HiveAccessControlException Permission denied: Principal [name=developuser, type=USER] does not have following privileges for operation CREATETABLE [[OBJECT OWNERSHIP] on Object [type=DFS_URI, name=hdfs://hacluster/user/hive/warehouse/sdc_drift_example03]] \u95ee\u9898\u539f\u56e0\uff1astreamsets\u4f1a\u4f7f\u7528developuser\u521b\u5efa\u8868sdc_drift_example03\uff0c\u671f\u95f4\u8981\u5728hdfs\u4e0b\u521b\u5efa\u5bf9\u5e94\u8def\u5f84\uff0c\u5f53\u524d\u7528\u6237\u62e5\u6709\u7684\u89d2\u8272\u4e0d\u5177\u5907\u64cd\u4f5cHDFS\u6743\u9650\u3002 \u89e3\u51b3\u529e\u6cd5\uff1a\u5728OM\u7ba1\u7406\u754c\u9762System\u4e0bRole Management\u4e2d\u7ed9\u5bf9\u5e94\u7684\u89d2\u8272\u8d4b\u4e88\u76f8\u5e94\u7684HDFS\u64cd\u4f5c\u6743\u9650\u3002 \u6ce8\u610f \uff1a\u5982\u679c\u8be5\u89d2\u8272\u5df2\u7ecf\u914d\u7f6e\uff0c\u91cd\u542fstreamsets\u5373\u53ef","title":"3.16.1 <--> 8.0"},{"location":"Data_Integration/streamsets_3.16.1/#streamsetsfusioninsight","text":"","title":"streamsets\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/streamsets_3.16.1/#_1","text":"Streamsets 3.16.1 \u2194 FusionInsight HD 6.5 (HDFS/Hive/HBase/Kafka) Streamsets 3.16.1 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/HBase/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Data_Integration/streamsets_3.16.1/#mrs-80","text":"\u8bf4\u660e\uff1a 1: mrs8.0\u7684\u7ec4\u4ef6\u7248\u672c\u63a5\u8fd1streamsets\u91cc\u9762hdp3.1\u7248\u672c\uff0c\u652f\u6301hdfs,hive,hbase\u7684\u5bf9\u63a5 \u9700\u8981\u66f4\u6539\u4f9d\u8d56\u5982\u4e0b\uff1a /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib \u628ahadoop-plugins-8.0.0-301001-SNAPSHOT.jar\uff0czookeeper-3.5.6-hw-ei-301001-SNAPSHOT.jar\uff0czookeeper-jute-3.5.6-hw-ei-301001-SNAPSHOT.jar\u5bfc\u5165\u5230\u8fd9\u4e2a\u8def\u5f84\u4e0b\uff0c\u628a\u4e4b\u524d\u7684zookeeper-3.4.10.jar\u6ce8\u91ca\u6389(\u5177\u4f53Jar\u5305\u540d\u5b57\u53ef\u80fd\u6709\u66f4\u6539) 2\uff1a mrs8.0\u7684kafka\u7248\u672c\u9009\u7528streamsets\u91cc\u9762\u7684Apache kafka 2.0.0 \u9700\u8981\u66f4\u6539\u4f9d\u8d56\u5982\u4e0b\uff1a /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_2_0-lib/lib \u628akafka-clients-2.4.0-hw-ei-301001-SNAPSHOT.jar\uff0czookeeper-3.5.6-hw-ei-301001-SNAPSHOT.jar\uff0czookeeper-jute-3.5.6-hw-ei-301001-SNAPSHOT.jar\u4e0a\u8ff03\u4e2ajar\u5305\u62f7\u8d1d\u8fc7\u6765\uff0c\u5e76\u4e14\u628akafka-client\u548czookeeper\u539f\u6765\u5e26\u7684jar\u5305\u6ce8\u91ca\u6389\uff08\u5177\u4f53Jar\u5305\u540d\u5b57\u53ef\u80fd\u6709\u66f4\u6539\uff09","title":"MRS 8.0 \u5bf9\u63a5\u8bf4\u660e"},{"location":"Data_Integration/streamsets_3.16.1/#streamsets","text":"\u73af\u5883\uff1a172.16.2.121","title":"\u5b89\u88c5streamsets"},{"location":"Data_Integration/streamsets_3.16.1/#_2","text":"\u5b89\u88c5streamsets 3.16.1","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/streamsets_3.16.1/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/streamsets_3.16.1/#_4","text":"\u767b\u9646streamsets\u5b98\u7f51\u4e0b\u8f7d\u5b89\u88c5\u5305 https://streamsets.com/products/dataops-platform/data-collector/download/#download-sdc \u5c06\u4e0b\u8f7d\u597d\u7684\u5b89\u88c5\u5305\u653e\u5230 /opt/streamsets \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf streamsets-datacollector-all-3.16.1.tgz \u89e3\u538b\u5b89\u88c5\u5305 \u53c2\u8003\u5b98\u65b9\u4ecb\u7ecd https://streamsets.com/documentation/datacollector/3.16.x/help/datacollector/UserGuide/Installation/Installing_the_DC.html#task_bt1_zcp_kq \u914d\u7f6e $SDC_DIST/libexec/sdc-env.sh \u6587\u4ef6 \u6bd4\u5982\u914d\u7f6e /opt/streamsets/streamsets-datacollector-3.16.1/libexec/sdc-env.sh \u6587\u4ef6 \u5e76\u4e14\u5c06 /opt/streamsets/streamsets-datacollector-3.16.1/etc \u8def\u5f84\u4e0b\u7684\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/streamsets/sdc/conf \u8def\u5f84\u4e0b cp /opt/streamsets/streamsets-datacollector-3.16.1/etc/* /opt/streamsets/sdc/conf/ \u6ce8\u610f\uff1a\u9700\u63d0\u524d\u521b\u5efa\u597d\u5bf9\u5e94\u7684\u8def\u5f84\uff0c\u518d\u914d\u7f6e \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8streamsets ulimit -n 32768 bin/streamsets dc \u767b\u9646streamsets\u7684web ui\u8fdb\u884c\u767b\u9646\uff0c\u6bd4\u5982 http://172.16.2.121:18630/ \u6ce8\u610f\uff1a\u7b2c\u4e00\u6b21\u767b\u9646\u9700\u8981\u4f7f\u7528\u5de5\u4f5c\u90ae\u7bb1\u5728streamsets\u5b98\u7f51\u6ce8\u518c\uff0c\u83b7\u53d6\u6fc0\u6d3b\u7801\uff0c\u5b8c\u6210\u540e\u4f7f\u7528\u9ed8\u8ba4\u7528\u6237admin, \u9ed8\u8ba4\u5bc6\u7801admin\u8fdb\u884c\u767b\u9646","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/streamsets_3.16.1/#kerberos","text":"","title":"Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e"},{"location":"Data_Integration/streamsets_3.16.1/#_5","text":"\u914d\u7f6estreamsets\u4e0eFI HD\u96c6\u7fa4\u5bf9\u63a5\u76f8\u5173\u8ba4\u8bc1\u914d\u7f6e","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/streamsets_3.16.1/#_6","text":"\u5b8c\u6210streamsets\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/streamsets_3.16.1/#_7","text":"\u8bf4\u660e\uff1aFI\u4ea4\u4e92\u7ec4\u4ef6\u4e3aKafka, Hive, HDFS\uff0c\u914d\u7f6e\u65f6\u9700\u8981\u5728streamsets\u9009\u62e9\u76f8\u8fd1\u7684\u7248\u672c\uff0c\u4e0b\u8868\u4e3a\u76f8\u8fd1\u7248\u672c\u5bf9\u5e94\u5217\u8868\uff0c\u6839\u636e\u6b64\u8868\u627e\u5230streamsets\u5bf9\u5e94\u7248\u672c\u4f9d\u8d56\u5e93\u8def\u5f84\uff0c\u65b9\u4fbf\u4fee\u6539\u76f8\u5173\u7684jar\u5305 \u7ec4\u4ef6 \u5bf9\u5e94streamsets\u7248\u672c HDFS HDP 3.1.0 Hive HDP 3.1.0 HBase CDH 5.14.0 Kafka Apache Kafka 1.1.0 \u914d\u7f6e\u6b65\u9aa4\uff1a \u4eceFI Manager\u4e0b\u8f7d\u5bf9\u5e94\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6user.keytab,krb5.conf\u5e76\u4e0a\u4f20\u5230streamsets\u4e3b\u673a\u7684/opt\u8def\u5f84\u4e0b. \u5e76\u4e14\u5c06krb5.conf\u6587\u4ef6\u653e\u7f6e\u5230streamsets\u4e3b\u673a\u7684/etc\u8def\u5f84\u4e0b\uff0cstreamsets\u9ed8\u8ba4\u8bfb\u53d6/etc/krb5.conf\u6587\u4ef6\u505a\u8ba4\u8bc1\u670d\u52a1 \u4fee\u6539 $SDC_CONF/sdc.properties \u914d\u7f6e\u6587\u4ef6\uff0c\u6bd4\u5982 /opt/streamsets/sdc/conf/sdc.properties \u5176\u4e2d\u7b2c2\u6761\u548c\u7b2c3\u6761\u4e3aKerberos\u8ba4\u8bc1\u7528\u6237\u4ee5\u53causer.keytab\u6587\u4ef6\uff0c\u4eceFI Manager\u4e0a\u83b7\u53d6\uff0c\u5e76\u4fdd\u8bc1developuser\u6709kafka,hdfs,hive\u76f8\u5173\u6743\u9650 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 hadoop-plugins-1.0.jar \u5e76\u62f7\u8d1d\u5230 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib \u8def\u5f84\u4e0b cp /opt/125_651hdclient/hadoopclient/HDFS/hadoop/share/hadoop/common/lib/hadoop-plugins-1.0.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 zookeeper-3.5.1.jar \u62f7\u8d1d\u5230 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib \u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u628a\u539f\u6765\u7684 zookeeper-3.4.10.jar \u6ce8\u91ca\u6389 mv /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib/zookeeper-3.4.10.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib/zookeeper-3.4.10.jar.org cp /opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/zookeeper-3.5.1.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-hdp_3_1-lib/lib/ \u5b8c\u6210\u540e\u68c0\u67e5\uff1a \u627e\u5230streamsets\u542f\u52a8jvm\u53c2\u6570\u914d\u7f6e\u6587\u4ef6 /opt/streamsets/streamsets-datacollector-3.16.1/libexec/sdc-env.sh \u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58\uff1a \u589e\u52a0\u53c2\u6570\u5185\u5bb9\u4e3a -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dsun.security.krb5.debug=false -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf \u5728/opt\u8def\u5f84\u4e0b\u521b\u5efajaas.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" storeKey=true useTicketCache=false principal=\"developuser@HADOOP.COM\"; }; KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 kafka-clients-1.1.0.jar \u5e76\u62f7\u8d1d\u5230\u8def\u5f84 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib \u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u628a\u539f\u6765\u7684 kafka-clients-1.1.0.jar \u6ce8\u91ca\u6389 mv /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/kafka-clients-1.1.0.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/kafka-clients-1.1.0.jar.org cp /opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/ \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 zookeeper-3.5.1.jar \u62f7\u8d1d\u5230 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib \u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u628a\u539f\u6765\u7684 zookeeper-3.4.6.jar \u6ce8\u91ca\u6389 mv /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/zookeeper-3.4.6.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/zookeeper-3.4.6.jar.org cp /opt/125_651hdclient/hadoopclient/Kafka/kafka/libs/zookeeper-3.5.1.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-apache-kafka_1_1-lib/lib/ \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 zookeeper-3.5.1.jar \u62f7\u8d1d\u5230 /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-cdh_5_14-lib/lib \u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u628a\u539f\u6765\u7684 zookeeper-3.4.5-cdh5.14.0.jar \u6ce8\u91ca\u6389 mv /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-cdh_5_14-lib/lib/zookeeper-3.4.5-cdh5.14.0.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-cdh_5_14-lib/lib/zookeeper-3.4.5-cdh5.14.0.jar.org cp /opt/125_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/zookeeper-3.5.1.jar /opt/streamsets/streamsets-datacollector-3.16.1/streamsets-libs/streamsets-datacollector-cdh_5_14-lib/lib \u521b\u5efa\u8def\u5f84 /opt/streamsets/hdfsconf \uff0c\u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 core-site.xml, hdfs-site.xml, hive-site.xml, hbase-site.xml \u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u505a\u5982\u4e0b\u4fee\u6539 core-site.xml\u914d\u7f6e\u6587\u4ef6\u4fee\u6539\uff1a \u5176\u4e2d172.16.4.123\u4e3ahdfs\u4e3bnamenode\u7684ip,\u7aef\u53e3\u4e3a25000 hdfs-site.xml\u914d\u7f6e\u6587\u4ef6\u4fee\u6539\uff1a \u627e\u5230\u5982\u4e0b\u914d\u7f6e\u9879\uff0c\u5e76\u4e14\u5220\u9664 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.AdaptiveFailoverProxyProvider</value> </property> \u5b8c\u6210\u540e\u91cd\u542fstreamsets","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Data_Integration/streamsets_3.16.1/#hdfs","text":"","title":"\u5bf9\u63a5HDFS"},{"location":"Data_Integration/streamsets_3.16.1/#_8","text":"\u914d\u7f6estreamsets\u5bf9\u63a5HDFS\uff0c\u8bfb\u3001\u5199\u6570\u636e","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/streamsets_3.16.1/#_9","text":"\u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/streamsets_3.16.1/#hdfs_1","text":"\u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a \u6ce8\u610f\uff08\u91cd\u8981\uff09 \uff1a\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u6570\u636e\u6e90\u7aef\u9009\u62e9 Hadoop FS Standalone . \u5982\u679c\u6570\u636e\u6e90\u7aef\u9009\u62e9 Hadoop FS \u4f1a\u9047\u5230\u95ee\u9898 \u5728\u7a7a\u767d\u5904\u5355\u51fb\u9f20\u6807\u5de6\u952e\uff0c\u5728General\u9875\u7b7e\u914d\u7f6e\u6570\u636e\u6d41\u6267\u884c\u6a21\u5f0f\u4e3aStandalone Hadoop FS Standalone \u914d\u7f6e\u5982\u4e0b\uff1a General\u9875\u7b7e Connection\u9875\u7b7e \u6ce8\u610f\uff1a\u5176\u4e2d172.16.4.123\u4e3ahdfs\u4e3bnamenode\u8282\u70b9\u7684ip Files\u9875\u7b7e Post processing\u9875\u7b7e\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e Local FS \u914d\u7f6e\u5982\u4e0b\uff1a \u53ea\u4fee\u6539Output Files\u9875\u7b7e \u542f\u52a8\u4efb\u52a1\u524d\uff0c\u767b\u9646\u96c6\u7fa4hdfs\u8def\u5f84 /tmp/out/test/ ,\u5e76\u521b\u5efa\u6570\u636e\u6587\u4ef6 test.txt \u542f\u52a8\u4efb\u52a1\u6d41 \u6ce8\u610f\uff1a\u591a\u6b21\u542f\u52a8\u5982\u679c\u6570\u636e\u6ca1\u6709\u7ed3\u679c\u8ddf\u65b0\uff0c\u4f7f\u7528\u5982\u4e0b\u7684\u542f\u52a8\u65b9\u5f0f \u767b\u9646\u540e\u53f0\u5bf9\u5e94\u8def\u5f84\u68c0\u67e5\u7ed3\u679c\uff1a","title":"\u8bfb\u53d6HDFS\u6570\u636e\u7528\u4f8b"},{"location":"Data_Integration/streamsets_3.16.1/#hdfs_2","text":"\u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a \u5728\u7a7a\u767d\u5904\u5355\u51fb\u9f20\u6807\u5de6\u952e\uff0c\u5728General\u9875\u7b7e\u914d\u7f6e\u6570\u636e\u6d41\u6267\u884c\u6a21\u5f0f\u4e3aStandalone Directory \u914d\u7f6e General\u9875\u7b7e Files\u9875\u7b7e Post Processing\u9875\u7b7e\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e Hadoop FS \u914d\u7f6e General\u9875\u7b7e Connection\u9875\u7b7e \u6ce8\u610f\uff1a\u5176\u4e2d172.16.4.123\u4e3ahdfs\u4e3bnamenode\u8282\u70b9\u7684ip Output Files\u9875\u7b7e Late Records\u9875\u7b7e Data Format\u9875\u7b7e \u6d4b\u8bd5\u524d\u767b\u9646\u4e3b\u673a\u540e\u53f0\u51c6\u5907\u4e0a\u4f20\u6570\u636e\u6587\u4ef6\uff1a \u542f\u52a8\u6570\u636e\u6d41 \u767b\u9646hdfs\u5bf9\u5e94\u8def\u5f84\u67e5\u770b\u7ed3\u679c","title":"\u5199\u5165HDFS\u6570\u636e\u7528\u4f8b"},{"location":"Data_Integration/streamsets_3.16.1/#hive","text":"","title":"\u5bf9\u63a5Hive"},{"location":"Data_Integration/streamsets_3.16.1/#_10","text":"\u914d\u7f6estreamsets\u5bf9\u63a5Hive\uff0c\u5199\u5165\u6570\u636e \u8bf4\u660e\uff1astreamsets\u4e0d\u63d0\u4f9bhive\u4f5c\u4e3a\u76f4\u63a5\u6570\u636e\u6e90\uff0c\u672c\u8282\u53ea\u63d0\u4f9bhive\u5199\u5165\u7528\u4f8b","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/streamsets_3.16.1/#_11","text":"\u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/streamsets_3.16.1/#hive_1","text":"\u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a Dev Raw Data Source \u914d\u7f6e General\u9875\u7b7e Raw Data\u9875\u7b7e { \"firstname\": \"abc\", \"midname\": \"xyz\", \"lastname\": \"lmn\" } Event Data\u9875\u7b7e,\u672a\u505a\u914d\u7f6e Data Format\u9875\u7b7e Expression Evaluator \u914d\u7f6e General\u9875\u7b7e Expressions\u9875\u7b7e database = default table_name = sdc_drift_example Hive Metadata \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e\uff1a Hive\u9875\u7b7e\u914d\u7f6e\uff1a 1. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 2. org.apache.hive.jdbc.HiveDriver 3. /opt/streamsets/hdfsconf Table\u9875\u7b7e\u914d\u7f6e 1. ${record:attribute('database')} 2. ${record:attribute('table_name')} Advanced\u9875\u7b7e\u6309\u9ed8\u8ba4\u914d\u7f6e\uff0c\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e\u914d\u7f6e Hadoop FS \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e Connection\u9875\u7b7e\u914d\u7f6e Output Files\u9875\u7b7e\u914d\u7f6e Late Records\u9875\u7b7e\u914d\u7f6e Data Format\u9875\u7b7e\u914d\u7f6e Hive Metastore \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e Hive\u9875\u7b7e\u914d\u7f6e 1. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 2. org.apache.hive.jdbc.HiveDriver 3. /opt/streamsets/hdfsconf Advanced\u9875\u7b7e\u914d\u7f6e hive\u8868streamsets\u4f1a\u81ea\u52a8\u521b\u5efa\uff0c\u4e0d\u9700\u8981\u63d0\u524d\u521b\u5efa\u6539\u8868 \u542f\u52a8\u6570\u636e\u6d41 beeline\u767b\u9646Hive\u68c0\u67e5\u7ed3\u679c","title":"\u5199\u5165Hive\u6570\u636e\u7528\u4f8b"},{"location":"Data_Integration/streamsets_3.16.1/#hbase","text":"","title":"\u5bf9\u63a5HBase"},{"location":"Data_Integration/streamsets_3.16.1/#_12","text":"\u914d\u7f6estreamsets\u5bf9\u63a5HBase \u6ce8\u610f\uff1astreamstes\u4e0d\u63d0\u4f9bHBase\u4f5c\u4e3a\u6e90\u7aef\u7684\u529f\u80fd\uff0c\u672c\u8282\u53ea\u63d0\u4f9bHBase\u5199\u5165\u7528\u4f8b","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/streamsets_3.16.1/#_13","text":"\u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/streamsets_3.16.1/#hbase_1","text":"\u8bf4\u660e\uff1a FusionInisight \u7684HBase\u7248\u672c\uff081.3.1\uff09\u4e0eCDH 5.14.0 \u7684HBase\u7248\u672c\uff081.2.0\uff09\u76f8\u8fd1, \u5df2\u7ecf\u4fee\u6539\u4e86zookeeper-3.4.5-cdh5.14.0.jar \u4e3aFusionInsight\u7684 zookeeper-3.5.1.jar. \u4f46\u662f\u7248\u672c\u8fd8\u662f\u6709\u5dee\u5f02\uff0c\u9700\u8981\u5bfc\u5165Fusioninsight hbase\u76f8\u5173jar\u5305\uff0c\u907f\u514d\u53d1\u751f\u4f9d\u8d56\u9519\u8bef \u53c2\u8003\u5982\u4e0b\u94fe\u63a5\u5b89\u88c5\u5916\u90e8\u4f9d\u8d56\u5e93,\u5e76\u5bfc\u5165FusionInsgiht HBase\u4f9d\u8d56\uff1a https://streamsets.com/documentation/datacollector/latest/help/datacollector/UserGuide/Configuration/ExternalLibs.html#concept_amy_pzs_gz \u9996\u5148\u521b\u5efa\u8def\u5f84 /opt/streamsets/sdc/sdc-extras/streamsets-datacollector-cdh_5_14-lib/lib mkdir -p /opt/streamsets/sdc/sdc-extras/streamsets-datacollector-cdh_5_14-lib/lib \u5c06FusionInsight HBase\u76f8\u5173\u4f9d\u8d56jar\u5305\u62f7\u8d1d\u5230\u4e0a\u4e00\u6b65\u521b\u5efa\u7684\u8def\u5f84\u4e2d cp /opt/125_651hdclient/hadoopclient/HBase/hbase/lib/*.jar /opt/streamsets/sdc/sdc-extras/streamsets-datacollector-cdh_5_14-lib/lib \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 $SDC_CONF/sdc-security.policy \uff0c\u6bd4\u5982 /opt/streamsets/sdc/conf/sdc-security.policy ,\u589e\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a // user-defined external directory grant codebase \"file:///opt/streamsets/sdc/sdc-extras/-\" { permission java.security.AllPermission; }; (\u91cd\u8981)\u5173\u95ed\u6b63\u5728\u8fd0\u884c\u7684streamsets, \u5148\u7533\u660e\u4e4b\u524d\u914d\u7f6e\u7684\u73af\u5883\u53d8\u91cf STREAMSETS_LIBRARIES_EXTRA_DIR ,\u518d\u542f\u52a8streamsets. \u6bd4\u5982\uff1a export STREAMSETS_LIBRARIES_EXTRA_DIR=\"/opt/streamsets/sdc/sdc-extras\" bin/streamsets dc \u767b\u9646streamsets\u7684web\u754c\u9762\uff0c\u5728**Package Manager**\u5904\u7684**External Libraries**\u68c0\u67e5\u76f8\u5173\u4f9d\u8d56\u662f\u5426\u5bfc\u5165\u6210\u529f","title":"HBase\u76f8\u5173\u914d\u7f6e"},{"location":"Data_Integration/streamsets_3.16.1/#hbase_2","text":"\u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a Dev Raw Data Source 1 \u914d\u7f6e General\u9875\u7b7e Raw Data\u9875\u7b7e { \"firstname\": \"abc\", \"midname\": \"xyz\",\"lastname\": \"lmn\" } Event Data\u9875\u7b7e\u672a\u4f5c\u4fee\u6539 Data Format\u9875\u7b7e HBase \u914d\u7f6e General\u9875\u7b7e HBase\u9875\u7b7e 1. host-172-16-4-121,host-172-16-4-122,host-172-16-4-123 2. 24002 3. /hbase 4. streamsets1 5. /firstname 6. Text 7. /firstname - data:firstname - Text /midname - data:midname - Text /lastname - data:lastname - Text 8. /opt/streamsets/hdfsconf \u6d4b\u8bd5\u524d\u767b\u9646hbase\u5ba2\u6237\u7aef\uff0c\u521b\u5efa\u8868streamsets1 hbase shell create 'streamsets1','data' \u542f\u52a8\u6570\u636e\u6d41 hbase\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c scan 'streamsets1'","title":"\u5199\u5165HBase\u6570\u636e\u7528\u4f8b"},{"location":"Data_Integration/streamsets_3.16.1/#kafka","text":"","title":"\u5bf9\u63a5Kafka\u5b89\u5168\u6a21\u5f0f"},{"location":"Data_Integration/streamsets_3.16.1/#_14","text":"\u914d\u7f6estreamsets\u5bf9\u63a5Kafka\uff0c\u751f\u4ea7\uff0c\u6d88\u8d39\u6570\u636e","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/streamsets_3.16.1/#_15","text":"\u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/streamsets_3.16.1/#kafka_1","text":"\u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a Directory \u914d\u7f6e\uff1a General\u9875\u7b7e Files\u9875\u7b7e Post Processing\u9875\u7b7e\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e Kafka Producer \u914d\u7f6e\uff1a General\u9875\u7b7e Kafka\u9875\u7b7e 1. 172.16.4.121:21007 2. streamtests21007 3. security.protocol = SASL_PLAINTEXT 4. sasl.kerberos.service.name = kafka Data Format\u9875\u7b7e Response\u9875\u7b7e \u542f\u52a8\u6570\u636e\u6d41\u524d\uff0c\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/kafka --partitions 2 --replication-factor 2 --topic streamtests21007 \u521b\u5efatopic \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8kafka\u6d88\u8d39\u8005 bin/kafka-console-consumer.sh --zookeeper 172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/kafka --topic streamtests21007 \u542f\u52a8\u4efb\u52a1\u6d41 \u5bf9\u5df2\u542f\u52a8\u8fc7\u7684\u4efb\u52a1\uff0c\u7528\u5982\u4e0b\u65b9\u5f0f\u91cd\u65b0\u542f\u52a8 \u5728kafka\u6d88\u8d39\u8005\u68c0\u67e5\u7ed3\u679c","title":"\u5199\u5165Kafka\u6570\u636e\u7528\u4f8b"},{"location":"Data_Integration/streamsets_3.16.1/#kafka_2","text":"\u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a \u6ce8\u610f\uff08\u91cd\u8981\uff09 \uff1a\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u6570\u636e\u6e90\u7aef\u9009\u62e9 Kafka Multitopic Consumer . \u5982\u679c\u6570\u636e\u6e90\u7aef\u9009\u62e9 Kafka Consumer \u4f1a\u9047\u5230\u95ee\u9898 Kafka Multitopic Consumer \u914d\u7f6e\uff1a General\u9875\u7b7e Connection\u9875\u7b7e Data Format\u9875\u7b7e Local FS \u914d\u7f6e General\u9875\u7b7e Output Files\u9875\u7b7e Late Records\u9875\u7b7e Data Format\u9875\u7b7e \u542f\u52a8\u6570\u636e\u6d41\u524d\uff0c\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/kafka --partitions 2 --replication-factor 2 --topic streamtests21007test \u521b\u5efatopic \u9996\u5148\u4f7f\u7528 bin/kafka-console-producer.sh --broker-list 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 --topic streamtests21007test --producer.config config/producer.properties \u542f\u52a8kafka\u751f\u4ea7\u8005 \u542f\u52a8\u6570\u636e\u6d41\uff0c\u624b\u52a8\u5728\u524d\u4e00\u6b65\u751f\u4ea7\u8005\u4e2d\u63d2\u5165\u6570\u636e \u767b\u9646\u540e\u53f0\u5bf9\u5e94\u8def\u5f84\u68c0\u67e5\u7ed3\u679c\uff1a","title":"\u8bfb\u53d6Kafka\u6570\u636e\u7528\u4f8b"},{"location":"Data_Integration/streamsets_3.16.1/#streamsets_1","text":"","title":"streamsets\u6700\u4f73\u5b9e\u8df5"},{"location":"Data_Integration/streamsets_3.16.1/#_16","text":"\u914d\u7f6estreamsets ETL\u6570\u636e\u6d41\uff0c\u4eceFI kafka\u8bfb\u53d6\u6570\u636e\uff0c\u518d\u5199\u5165FI Hive\u4e2d","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Data_Integration/streamsets_3.16.1/#_17","text":"\u5b8c\u6210streamsets\u5b89\u88c5\uff0c\u5b8c\u6210Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\uff0c\u5b8c\u6210HDFS, Hive, Kafka\u7528\u4f8b\u6d4b\u8bd5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Data_Integration/streamsets_3.16.1/#_18","text":"\u65b0\u5efa\u4e00\u4e2a\u6570\u636e\u6d41\u4efb\u52a1 \u6574\u4e2a\u6570\u636e\u6d41\u5982\u4e0b\uff1a Kafka Multitopic Consumer \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e\uff1a Connection\u9875\u7b7e\u914d\u7f6e\uff1a 1. 172.16.4.121:21007 2. demo01json21007 3. security.protocol = SASL_PLAINTEXT 4. sasl.kerberos.service.name = kafka Data Format\u9875\u7b7e\u914d\u7f6e Expression Evaluator \u914d\u7f6e General\u9875\u7b7e\u6309\u9ed8\u8ba4\u914d\u7f6e\uff0c\u672a\u505a\u4fee\u6539 Expressions\u9875\u7b7e\u914d\u7f6e\uff1a 1. database = default 2. table_name = sdc_drift_example03 Hive Metadata \u914d\u7f6e - General\u9875\u7b7e\u914d\u7f6e\uff1a Hive\u9875\u7b7e\u914d\u7f6e\uff1a 1. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/default;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 2. org.apache.hive.jdbc.HiveDriver 3. /opt/streamsets/hdfsconf Table\u9875\u7b7e\u914d\u7f6e 1. ${record:attribute('database')} 2. ${record:attribute('table_name')} Advanced\u9875\u7b7e\u6309\u9ed8\u8ba4\u914d\u7f6e\uff0c\u672a\u505a\u4fee\u6539 Data Format\u9875\u7b7e\u914d\u7f6e Hadoop FS \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e Connection\u9875\u7b7e\u914d\u7f6e Output Files\u9875\u7b7e\u914d\u7f6e Late Records\u9875\u7b7e\u914d\u7f6e Data Format\u9875\u7b7e\u914d\u7f6e Hive Metastore \u914d\u7f6e General\u9875\u7b7e\u914d\u7f6e Hive\u9875\u7b7e\u914d\u7f6e 1. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 2. org.apache.hive.jdbc.HiveDriver 3. /opt/streamsets/hdfsconf Advanced\u9875\u7b7e\u914d\u7f6e \u6d4b\u8bd5\u524d\u51c6\u5907 \u4f7f\u7528FI HD\u5ba2\u6237\u7aef\u521b\u5efakafka\u76f8\u5173topic demo01json21007 bin/kafka-topics.sh --create --zookeeper 172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/kafka --partitions 2 --replication-factor 2 --topic demo01json21007 hive\u8868streamsets\u4f1a\u81ea\u52a8\u521b\u5efa\uff0c\u4e0d\u9700\u8981\u63d0\u524d\u521b\u5efa\u6539\u8868 \u542f\u52a8streamsets\u6570\u636e\u6d41 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8kafka producer\u5e76\u4e14\u63d2\u5165\u6570\u636e bin/kafka-console-producer.sh --broker-list 172.16.4.121:21007,172.16.4.122:21007,172.16.4.123:21007 --topic demo01json21007 --producer.config config/producer.properties \u6570\u636e\uff1a { \"firstname\": \"abc\", \"midname\": \"xyz\",\"lastname\": \"lmn\" } { \"firstname\": \"abc1\", \"midname\": \"xyz1\",\"lastname\": \"lmn1\" } { \"firstname\": \"abc2\", \"midname\": \"xyz2\",\"lastname\": \"lmn2\" } \u68c0\u67e5streamsets\u754c\u9762 \u540e\u53f0\u68c0\u67e5hive\u8868","title":"\u6d4b\u8bd5\u7528\u4f8b"},{"location":"Data_Integration/streamsets_3.16.1/#faq","text":"\u95ee\u98981 \u5728\u505ahive\u5199\u5165\u76f8\u5173\u7528\u4f8b\u7684\u65f6\u5019\uff08hive\u8868\u5199\u5165\u7528\u4f8b\uff0c\u6700\u4f73\u5b9e\u8df5\u7528\u4f8b\uff09\uff0c\u5982\u679c\u9884\u5148\u5728hive\u8868\u4e2d\u5efa\u5bf9\u5e94\u7684\u8868\uff0c\u53d1\u73b0\u5de5\u4f5c\u6d41\u542f\u52a8\u4e4b\u540e\u6570\u636e\u4e0d\u80fd\u5199\u5165\u5230hive\u8868\u4e2d\uff0c\u4f46\u662f\u4e5f\u6ca1\u6709\u62a5\u9519 \u89e3\u51b3\u529e\u6cd5\uff1a \u4e0d\u8981\u9884\u5148\u5728hive\u4e2d\u9884\u5148\u5efa\u8868\uff0c\u7531streamset\u81ea\u52a8\u5efa\u5bf9\u5e94hive\u8868\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6570\u636e\u5199\u5165\u6210\u529f \u95ee\u98982 \u5728\u542f\u52a8\u6570\u636e\u6d41\u4e4b\u540e\uff0c\u4f7f\u7528kafka producer\u63d2\u5165\u6570\u636e\u65f6\uff0cstreamsets\u62a5\u9519 Reason:Error while compiling statement: FAILED: HiveAccessControlException Permission denied: Principal [name=developuser, type=USER] does not have following privileges for operation CREATETABLE [[OBJECT OWNERSHIP] on Object [type=DFS_URI, name=hdfs://hacluster/user/hive/warehouse/sdc_drift_example03]] \u95ee\u9898\u539f\u56e0\uff1astreamsets\u4f1a\u4f7f\u7528developuser\u521b\u5efa\u8868sdc_drift_example03\uff0c\u671f\u95f4\u8981\u5728hdfs\u4e0b\u521b\u5efa\u5bf9\u5e94\u8def\u5f84\uff0c\u5f53\u524d\u7528\u6237\u62e5\u6709\u7684\u89d2\u8272\u4e0d\u5177\u5907\u64cd\u4f5cHDFS\u6743\u9650\u3002 \u89e3\u51b3\u529e\u6cd5\uff1a\u5728OM\u7ba1\u7406\u754c\u9762System\u4e0bRole Management\u4e2d\u7ed9\u5bf9\u5e94\u7684\u89d2\u8272\u8d4b\u4e88\u76f8\u5e94\u7684HDFS\u64cd\u4f5c\u6743\u9650\u3002 \u6ce8\u610f \uff1a\u5982\u679c\u8be5\u89d2\u8272\u5df2\u7ecf\u914d\u7f6e\uff0c\u91cd\u542fstreamsets\u5373\u53ef","title":"FAQ"},{"location":"Data_Integration/%E6%9D%AD%E5%B7%9E%E5%90%88%E4%BC%97UTL/","text":"\u676d\u5dde\u5408\u4f17UTL\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 \u676d\u5dde\u5408\u4f17UTL 5.1 \u2194 FusionInsight HD V100R002C50 (HDFS/HBase/Hive/Kafka)","title":"5.1 <--> C50"},{"location":"Data_Integration/%E6%9D%AD%E5%B7%9E%E5%90%88%E4%BC%97UTL/#utlfusioninsight","text":"","title":"\u676d\u5dde\u5408\u4f17UTL\u5bf9\u63a5FusionInsight"},{"location":"Data_Integration/%E6%9D%AD%E5%B7%9E%E5%90%88%E4%BC%97UTL/#_1","text":"\u676d\u5dde\u5408\u4f17UTL 5.1 \u2194 FusionInsight HD V100R002C50 (HDFS/HBase/Hive/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/","text":"\u6570\u636e\u5e93 \u00b6 Apache Druid 0.14.2 \u2194 C80 0.15.1 \u2194 C80 OpenTSDB 2.4.0 \u2194 6.5 2.4.0 \u2194 8.0 SAP VORA 2.0 \u2194 C70 2.1 \u2194 C70 \u676d\u5dde\u5408\u4f17UDB 6.1 \u2194 C50","title":"Index"},{"location":"Database/#_1","text":"Apache Druid 0.14.2 \u2194 C80 0.15.1 \u2194 C80 OpenTSDB 2.4.0 \u2194 6.5 2.4.0 \u2194 8.0 SAP VORA 2.0 \u2194 C70 2.1 \u2194 C70 \u676d\u5dde\u5408\u4f17UDB 6.1 \u2194 C50","title":"\u6570\u636e\u5e93"},{"location":"Database/OpenTSDB_2.4.0/","text":"OpenTSDB\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 OpenTSDB 2.4.0 \u2194 FusionInsight HD 6.5 (HBase) OpenTSDB 2.4.0 \u2194 FusionInsight MRS 8.0 (HBase) \u7b80\u4ecb \u00b6 OpenTSDB\u7528HBase\u5b58\u50a8\u6240\u6709\u7684\u65f6\u5e8f\uff08\u65e0\u987b\u91c7\u6837\uff09\u6765\u6784\u5efa\u4e00\u4e2a\u5206\u5e03\u5f0f\u3001\u53ef\u4f38\u7f29\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u3002\u5b83\u652f\u6301\u79d2\u7ea7\u6570\u636e\u91c7\u96c6\u6240\u6709metrics\uff0c\u652f\u6301\u6c38\u4e45\u5b58\u50a8\uff0c\u53ef\u4ee5\u505a\u5bb9\u91cf\u89c4\u5212\u3002OpenTSDB\u53ef\u4ee5\u4ece\u5927\u89c4\u6a21\u7684\u96c6\u7fa4\uff08\u5305\u62ec\u96c6\u7fa4\u4e2d\u7684\u7f51\u7edc\u8bbe\u5907\u3001\u64cd\u4f5c\u7cfb\u7edf\u3001\u5e94\u7528\u7a0b\u5e8f\uff09\u4e2d\u83b7\u53d6\u76f8\u5e94\u7684metrics\u5e76\u8fdb\u884c\u5b58\u50a8\u3001\u7d22\u5f15\u4ee5\u53ca\u670d\u52a1\uff0c\u4ece\u800c\u4f7f\u5f97\u8fd9\u4e9b\u6570\u636e\u66f4\u5bb9\u6613\u8ba9\u4eba\u7406\u89e3\uff0c\u5982web\u5316\u3001\u56fe\u5f62\u5316\u7b49\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Linux\u64cd\u4f5c\u7cfb\u7edf\uff0cOpenTSDB\u4f7f\u7528FusionInsight HD\u7684HBase\u7ec4\u4ef6\u5b58\u50a8\u6570\u636e\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002\u672c\u6587\u4f7f\u7528\u7684\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 /opt \u76ee\u5f55\u4e0b\u3002 \u5b89\u88c5\u90e8\u7f72OpenTSDB \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5728\u5df2\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\u7684\u8282\u70b9\u5b89\u88c5\u90e8\u7f72OpenTSDB\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4ece https://github.com/OpenTSDB/opentsdb/releases \u4e0b\u8f7d\u6700\u65b0\u7248\u672c\u7684OpenTSDB\uff0c\u672c\u6587\u6863\u4f7f\u7528\u7684\u662fOpenTSDB 2.4.0\u3002\u5c06\u4e0b\u8f7d\u7684opentsdb-2.4.0.tar.gz\u4e0a\u4f20\u81f3FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u5e76\u89e3\u538b\u3002 cd /opt tar -zxvf opentsdb-2.4.0.tar.gz \u7f16\u8bd1\u6e90\u4ee3\u7801\u3002 source /opt/hadoopclient/bigdata_env cd /opt/opentsdb-2.4.0 mkdir build cp -r third_party/ ./build ./build.sh \u5b89\u88c5OpenTSDB cd /opt/opentsdb-2.4.0/build make install \u8bf4\u660e\uff1a\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55\u4e3a/usr/local/share/opentsdb/\u3002 \u914d\u7f6eOpenTSDB \u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u5939 /etc/opentsdb \u3002 ln -s /usr/local/share/opentsdb/etc/opentsdb /etc/opentsdb \u590d\u5236\u7528\u6237\u51ed\u8bc1\u5230\u914d\u7f6e\u6587\u4ef6\u5939\u3002 cp /opt/user.keytab /etc/opentsdb/ cp /opt/krb5.conf /etc/opentsdb/ \u5728 /etc/opentsdb \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/etc/opentsdb/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539opentsdb\u7684\u914d\u7f6e\u6587\u4ef6\u3002 vi /etc/opentsdb/opentsdb.conf opentsdb.conf\u914d\u7f6e\u5982\u4e0b\uff1a # --------- NETWORK ---------- tsd.network.port = 4242 tsd.network.bind = 0.0.0.0 #tsd.network.tcp_no_delay = true #tsd.network.keep_alive = true #tsd.network.reuse_address = true #tsd.network.worker_threads = 8 #tsd.network.async_io = true # ----------- HTTP ----------- tsd.http.staticroot = /usr/local/share/opentsdb/static/ tsd.http.cachedir = /tmp/opentsdb # --------- CORE ---------- tsd.core.auto_create_metrics = true tsd.query.skip_unresolved_tagvs = true tsd.core.meta.enable_realtime_ts = true tsd.core.meta.enable_realtime_uid = true tsd.core.meta.enable_tsuid_incrementing = true tsd.core.meta.enable_tsuid_tracking = true tsd.core.tag.allow_specialchars=@( ) tsd.core.plugin_path = /usr/local/share/opentsdb/plugins # --------- STORAGE ---------- #tsd.storage.enable_compaction = true # tsd.storage.flush_interval = 1000 #tsd.storage.hbase.data_table = tsdb #tsd.storage.hbase.uid_table = tsdb-uid tsd.storage.hbase.zk_basedir = /hbase tsd.storage.fix_duplicates=true tsd.http.request.enable_chunked=true tsd.http.request.max_chunk=1073741824 tsd.http.query.allow_delete=true # hbase zookeeper\u96c6\u7fa4\u5730\u5740\uff0c\u8bf7\u4fee\u6539\u4e3a\u96c6\u7fa4\u7684zk\u5730\u5740 tsd.storage.hbase.zk_quorum = 172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002 # hbase kerberos\u8ba4\u8bc1\u4fe1\u606f hbase.security.auth.enable=true hbase.security.authentication=Kerberos # hadoop.com@HADOOP.COM\u9700\u8981\u6839\u636e\u5f53\u524d\u96c6\u7fa4 krbServer \u7684 realm \u8fdb\u884c\u4fee\u6539 hbase.kerberos.regionserver.principal=hbase/hadoop.hadoop.com@HADOOP.COM hbase.sasl.clientconfig=Client \u4fee\u6539opentsdb\u7684tsdb\u811a\u672c\u589e\u52a0\u8ba4\u8bc1\u914d\u7f6e\u3002 vi /usr/local/share/opentsdb/bin/tsdb \u5c06\u811a\u672c\u5012\u6570\u7b2c\u4e8c\u884c\"$CLASSPATH\"\u540e\u9762\uff0cnet.opentsdb.tools\u4e4b\u524d\u589e\u52a0\u5982\u4e0b\u5b89\u5168\u76f8\u5173\u7684\u73af\u5883\u53d8\u91cf\uff1a -Djava.security.krb5.conf=/etc/opentsdb/krb5.conf -Djava.security.auth.login.config=/etc/opentsdb/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com \u521b\u5efa\u7f3a\u5931\u7684\u76ee\u5f55 mkdir /usr/local/share/opentsdb/plugins \u66f4\u65b0\u4e3a\u4f7f\u7528FusionInsight HD\u7684Zookeeper jar\u5305\u3002 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /usr/local/share/opentsdb/lib/ rm -rf /usr/local/share/opentsdb/lib/zookeeper-3.4.6.jar \u62f7\u8d1d\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \u3001 core-site.xml \u3001 hbase-site.xml \u5230 /etc/opentsdb \u76ee\u5f55\u3002 cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /etc/opentsdb/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /etc/opentsdb/ cp /opt/hadoopclient/HBase/hbase/conf/hbase-site.xml /etc/opentsdb/ \u521b\u5efaOpentsdb\u76f8\u5173\u7684hbase\u8868\u7ed3\u6784 \u5220\u9664create_table.sh \u811a\u672c\u7684\u7b2c53\u884c\u7684\u201d , TTL => '$TSDB_TTL'\u201d\u3002 cp /opt/opentsdb-2.4.0/src/create_table.sh /usr/local/share/opentsdb/bin/ vi /usr/local/share/opentsdb/bin/create_table.sh \u589e\u52a0 create_table.sh \u811a\u672c\u7684\u6267\u884c\u6743\u9650\u3002 chmod +x /usr/local/share/opentsdb/bin/create_table.sh \u8fd0\u884c\u521b\u5efaHbase\u8868\u811a\u672c\u3002 source /opt/hadoopclient/bigdata_env kdestroy kinit -kt /etc/opentsdb/user.keytab developuser env COMPRESSION=snappy /usr/local/share/opentsdb/bin/create_table.sh \u4f7f\u7528hbase shell\u67e5\u8be2Opentsdb\u76f8\u5173\u8868 hbase shell list \u8bf4\u660e\uff1aOpentsdb\u8868\u4e00\u5171\u67094\u5f20\u8868 tsdb\uff1a\u5b58\u50a8\u6240\u6709\u6570\u636e tsdb-meta\uff1a\u5b58\u50a8\u989d\u5916\u4fe1\u606f tsdb-tree\uff1a\u6811\u72b6\u7ed3\u6784\uff0c\u7c7b\u4f3c\u4e8e\u6587\u4ef6\u7cfb\u7edf tsdb-uid\uff1a\u5b58\u50a8UID\u6620\u5c04 \u5b89\u88c5gnuplot yum install gnuplot \u542f\u52a8OpenTSDB source /opt/hadoopclient/bigdata_env /usr/local/share/opentsdb/bin/tsdb tsd \u4f7f\u7528\u6d4f\u89c8\u5668\u6253\u5f00 http://172.16.5.105:4242/ \u3002 \u8bf4\u660e\uff1a 172.16.5.105\u4e3a\u542f\u52a8OpenTSDB\u8282\u70b9\u7684IP\u3002 \u5fc5\u987b\u786e\u4fdd\u786e\u5b9a\u7684\u9632\u706b\u5899\u662f\u5173\u95ed\u72b6\u6001\uff0c\u6d4f\u89c8\u5668\u624d\u80fd\u8bbf\u95ee\u3002\u6267\u884c systemctl stop firewalld \u53ef\u5173\u95ed\u9632\u706b\u5899\u3002 \u4f7f\u7528OpenTSDB \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4f7f\u7528OpenTSDB\u521b\u5efa\u6570\u636e\u5b58\u5165HBase\u5e76\u67e5\u770b\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u5b89\u88c5\u90e8\u7f72OpenTSDB\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u51c6\u5907\u6570\u636e \u521b\u5efametric source /opt/hadoopclient/bigdata_env /usr/local/share/opentsdb/bin/tsdb mkmetric mymetric.data \u51c6\u5907\u5bfc\u5165\u6570\u636e vi /opt/opentsdb_mymetric.txt opentsdb_mymetrix.txt\u5185\u5bb9\u5982\u4e0b\uff1a mymetric.data 1574423500 0.841470984808 host=172-16-5-105 mymetric.data 1574423510 0.909297426826 host=172-16-5-105 mymetric.data 1574423520 0.14112000806 host=172-16-5-105 mymetric.data 1574423530 0.756802495308 host=172-16-5-105 mymetric.data 1574423540 0.958924274663 host=172-16-5-105 \u5411metric\u5bfc\u5165\u6570\u636e /usr/local/share/opentsdb/bin/tsdb import /opt/opentsdb_mymetric.txt \u542f\u52a8OpenTSDB\u5e76\u7528\u6d4f\u89c8\u5668\u8bbf\u95ee\uff0c\u4f8b\u5982 http://172.16.5.105:4242/ \u3002 /usr/local/share/opentsdb/bin/tsdb tsd \u8f93\u5165\u4ee5\u4e0b\u67e5\u8be2\u6761\u4ef6\uff1a From: UNIX timestamp=1574423500 To: UNIX timestamp=1574423540 Metrix: mymetric.data FAQ \u00b6 \u67e5\u8be2\u6570\u636e\u65f6\u8fd4\u56dejava.lang.NullPointerException: null \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5728\u754c\u9762\u8f93\u5165\u67e5\u8be2\u6761\u4ef6\u67e5\u8be2\u6570\u636e\u65f6\uff0c\u8fd4\u56de\u4ee5\u4e0b\u9519\u8bef\u3002 Request failed: Internal Server Error java.lang.NullPointerException: null at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012) ~[na:1.8.0_201] at net.opentsdb.tsd.GraphHandler.runGnuplot(GraphHandler.java:786) ~[tsdb-2.4.0.jar:] at net.opentsdb.tsd.GraphHandler$RunGnuplot.execute(GraphHandler.java:353) ~[tsdb-2.4.0.jar:] at net.opentsdb.tsd.GraphHandler$RunGnuplot.run(GraphHandler.java:340) ~[tsdb-2.4.0.jar:] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_201] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_201] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_201] \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u68c0\u67e5\u662f\u5426\u5df2\u5b89\u88c5\u63d2\u4ef6\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\u540e\u518d\u91cd\u65b0\u542f\u52a8OpenTSDB\u3002 yum install gnuplot /usr/local/share/opentsdb/bin/tsdb tsd","title":"2.4.0 <--> 8.0"},{"location":"Database/OpenTSDB_2.4.0/#opentsdbfusioninsight","text":"","title":"OpenTSDB\u5bf9\u63a5FusionInsight"},{"location":"Database/OpenTSDB_2.4.0/#_1","text":"OpenTSDB 2.4.0 \u2194 FusionInsight HD 6.5 (HBase) OpenTSDB 2.4.0 \u2194 FusionInsight MRS 8.0 (HBase)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/OpenTSDB_2.4.0/#_2","text":"OpenTSDB\u7528HBase\u5b58\u50a8\u6240\u6709\u7684\u65f6\u5e8f\uff08\u65e0\u987b\u91c7\u6837\uff09\u6765\u6784\u5efa\u4e00\u4e2a\u5206\u5e03\u5f0f\u3001\u53ef\u4f38\u7f29\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u3002\u5b83\u652f\u6301\u79d2\u7ea7\u6570\u636e\u91c7\u96c6\u6240\u6709metrics\uff0c\u652f\u6301\u6c38\u4e45\u5b58\u50a8\uff0c\u53ef\u4ee5\u505a\u5bb9\u91cf\u89c4\u5212\u3002OpenTSDB\u53ef\u4ee5\u4ece\u5927\u89c4\u6a21\u7684\u96c6\u7fa4\uff08\u5305\u62ec\u96c6\u7fa4\u4e2d\u7684\u7f51\u7edc\u8bbe\u5907\u3001\u64cd\u4f5c\u7cfb\u7edf\u3001\u5e94\u7528\u7a0b\u5e8f\uff09\u4e2d\u83b7\u53d6\u76f8\u5e94\u7684metrics\u5e76\u8fdb\u884c\u5b58\u50a8\u3001\u7d22\u5f15\u4ee5\u53ca\u670d\u52a1\uff0c\u4ece\u800c\u4f7f\u5f97\u8fd9\u4e9b\u6570\u636e\u66f4\u5bb9\u6613\u8ba9\u4eba\u7406\u89e3\uff0c\u5982web\u5316\u3001\u56fe\u5f62\u5316\u7b49\u3002 \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Linux\u64cd\u4f5c\u7cfb\u7edf\uff0cOpenTSDB\u4f7f\u7528FusionInsight HD\u7684HBase\u7ec4\u4ef6\u5b58\u50a8\u6570\u636e\u3002","title":"\u7b80\u4ecb"},{"location":"Database/OpenTSDB_2.4.0/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eHBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002\u672c\u6587\u4f7f\u7528\u7684\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 /opt \u76ee\u5f55\u4e0b\u3002","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Database/OpenTSDB_2.4.0/#opentsdb","text":"","title":"\u5b89\u88c5\u90e8\u7f72OpenTSDB"},{"location":"Database/OpenTSDB_2.4.0/#_4","text":"\u5728\u5df2\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\u7684\u8282\u70b9\u5b89\u88c5\u90e8\u7f72OpenTSDB\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Database/OpenTSDB_2.4.0/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Database/OpenTSDB_2.4.0/#_6","text":"\u4ece https://github.com/OpenTSDB/opentsdb/releases \u4e0b\u8f7d\u6700\u65b0\u7248\u672c\u7684OpenTSDB\uff0c\u672c\u6587\u6863\u4f7f\u7528\u7684\u662fOpenTSDB 2.4.0\u3002\u5c06\u4e0b\u8f7d\u7684opentsdb-2.4.0.tar.gz\u4e0a\u4f20\u81f3FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u8282\u70b9\u7684 /opt \u76ee\u5f55\u5e76\u89e3\u538b\u3002 cd /opt tar -zxvf opentsdb-2.4.0.tar.gz \u7f16\u8bd1\u6e90\u4ee3\u7801\u3002 source /opt/hadoopclient/bigdata_env cd /opt/opentsdb-2.4.0 mkdir build cp -r third_party/ ./build ./build.sh \u5b89\u88c5OpenTSDB cd /opt/opentsdb-2.4.0/build make install \u8bf4\u660e\uff1a\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55\u4e3a/usr/local/share/opentsdb/\u3002 \u914d\u7f6eOpenTSDB \u521b\u5efa\u914d\u7f6e\u6587\u4ef6\u5939 /etc/opentsdb \u3002 ln -s /usr/local/share/opentsdb/etc/opentsdb /etc/opentsdb \u590d\u5236\u7528\u6237\u51ed\u8bc1\u5230\u914d\u7f6e\u6587\u4ef6\u5939\u3002 cp /opt/user.keytab /etc/opentsdb/ cp /opt/krb5.conf /etc/opentsdb/ \u5728 /etc/opentsdb \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/etc/opentsdb/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539opentsdb\u7684\u914d\u7f6e\u6587\u4ef6\u3002 vi /etc/opentsdb/opentsdb.conf opentsdb.conf\u914d\u7f6e\u5982\u4e0b\uff1a # --------- NETWORK ---------- tsd.network.port = 4242 tsd.network.bind = 0.0.0.0 #tsd.network.tcp_no_delay = true #tsd.network.keep_alive = true #tsd.network.reuse_address = true #tsd.network.worker_threads = 8 #tsd.network.async_io = true # ----------- HTTP ----------- tsd.http.staticroot = /usr/local/share/opentsdb/static/ tsd.http.cachedir = /tmp/opentsdb # --------- CORE ---------- tsd.core.auto_create_metrics = true tsd.query.skip_unresolved_tagvs = true tsd.core.meta.enable_realtime_ts = true tsd.core.meta.enable_realtime_uid = true tsd.core.meta.enable_tsuid_incrementing = true tsd.core.meta.enable_tsuid_tracking = true tsd.core.tag.allow_specialchars=@( ) tsd.core.plugin_path = /usr/local/share/opentsdb/plugins # --------- STORAGE ---------- #tsd.storage.enable_compaction = true # tsd.storage.flush_interval = 1000 #tsd.storage.hbase.data_table = tsdb #tsd.storage.hbase.uid_table = tsdb-uid tsd.storage.hbase.zk_basedir = /hbase tsd.storage.fix_duplicates=true tsd.http.request.enable_chunked=true tsd.http.request.max_chunk=1073741824 tsd.http.query.allow_delete=true # hbase zookeeper\u96c6\u7fa4\u5730\u5740\uff0c\u8bf7\u4fee\u6539\u4e3a\u96c6\u7fa4\u7684zk\u5730\u5740 tsd.storage.hbase.zk_quorum = 172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002 # hbase kerberos\u8ba4\u8bc1\u4fe1\u606f hbase.security.auth.enable=true hbase.security.authentication=Kerberos # hadoop.com@HADOOP.COM\u9700\u8981\u6839\u636e\u5f53\u524d\u96c6\u7fa4 krbServer \u7684 realm \u8fdb\u884c\u4fee\u6539 hbase.kerberos.regionserver.principal=hbase/hadoop.hadoop.com@HADOOP.COM hbase.sasl.clientconfig=Client \u4fee\u6539opentsdb\u7684tsdb\u811a\u672c\u589e\u52a0\u8ba4\u8bc1\u914d\u7f6e\u3002 vi /usr/local/share/opentsdb/bin/tsdb \u5c06\u811a\u672c\u5012\u6570\u7b2c\u4e8c\u884c\"$CLASSPATH\"\u540e\u9762\uff0cnet.opentsdb.tools\u4e4b\u524d\u589e\u52a0\u5982\u4e0b\u5b89\u5168\u76f8\u5173\u7684\u73af\u5883\u53d8\u91cf\uff1a -Djava.security.krb5.conf=/etc/opentsdb/krb5.conf -Djava.security.auth.login.config=/etc/opentsdb/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com \u521b\u5efa\u7f3a\u5931\u7684\u76ee\u5f55 mkdir /usr/local/share/opentsdb/plugins \u66f4\u65b0\u4e3a\u4f7f\u7528FusionInsight HD\u7684Zookeeper jar\u5305\u3002 cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-3.5.1.jar /usr/local/share/opentsdb/lib/ rm -rf /usr/local/share/opentsdb/lib/zookeeper-3.4.6.jar \u62f7\u8d1d\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 hdfs-site.xml \u3001 core-site.xml \u3001 hbase-site.xml \u5230 /etc/opentsdb \u76ee\u5f55\u3002 cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /etc/opentsdb/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /etc/opentsdb/ cp /opt/hadoopclient/HBase/hbase/conf/hbase-site.xml /etc/opentsdb/ \u521b\u5efaOpentsdb\u76f8\u5173\u7684hbase\u8868\u7ed3\u6784 \u5220\u9664create_table.sh \u811a\u672c\u7684\u7b2c53\u884c\u7684\u201d , TTL => '$TSDB_TTL'\u201d\u3002 cp /opt/opentsdb-2.4.0/src/create_table.sh /usr/local/share/opentsdb/bin/ vi /usr/local/share/opentsdb/bin/create_table.sh \u589e\u52a0 create_table.sh \u811a\u672c\u7684\u6267\u884c\u6743\u9650\u3002 chmod +x /usr/local/share/opentsdb/bin/create_table.sh \u8fd0\u884c\u521b\u5efaHbase\u8868\u811a\u672c\u3002 source /opt/hadoopclient/bigdata_env kdestroy kinit -kt /etc/opentsdb/user.keytab developuser env COMPRESSION=snappy /usr/local/share/opentsdb/bin/create_table.sh \u4f7f\u7528hbase shell\u67e5\u8be2Opentsdb\u76f8\u5173\u8868 hbase shell list \u8bf4\u660e\uff1aOpentsdb\u8868\u4e00\u5171\u67094\u5f20\u8868 tsdb\uff1a\u5b58\u50a8\u6240\u6709\u6570\u636e tsdb-meta\uff1a\u5b58\u50a8\u989d\u5916\u4fe1\u606f tsdb-tree\uff1a\u6811\u72b6\u7ed3\u6784\uff0c\u7c7b\u4f3c\u4e8e\u6587\u4ef6\u7cfb\u7edf tsdb-uid\uff1a\u5b58\u50a8UID\u6620\u5c04 \u5b89\u88c5gnuplot yum install gnuplot \u542f\u52a8OpenTSDB source /opt/hadoopclient/bigdata_env /usr/local/share/opentsdb/bin/tsdb tsd \u4f7f\u7528\u6d4f\u89c8\u5668\u6253\u5f00 http://172.16.5.105:4242/ \u3002 \u8bf4\u660e\uff1a 172.16.5.105\u4e3a\u542f\u52a8OpenTSDB\u8282\u70b9\u7684IP\u3002 \u5fc5\u987b\u786e\u4fdd\u786e\u5b9a\u7684\u9632\u706b\u5899\u662f\u5173\u95ed\u72b6\u6001\uff0c\u6d4f\u89c8\u5668\u624d\u80fd\u8bbf\u95ee\u3002\u6267\u884c systemctl stop firewalld \u53ef\u5173\u95ed\u9632\u706b\u5899\u3002","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Database/OpenTSDB_2.4.0/#opentsdb_1","text":"","title":"\u4f7f\u7528OpenTSDB"},{"location":"Database/OpenTSDB_2.4.0/#_7","text":"\u4f7f\u7528OpenTSDB\u521b\u5efa\u6570\u636e\u5b58\u5165HBase\u5e76\u67e5\u770b\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Database/OpenTSDB_2.4.0/#_8","text":"\u5df2\u5b8c\u6210\u5b89\u88c5\u90e8\u7f72OpenTSDB\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Database/OpenTSDB_2.4.0/#_9","text":"\u51c6\u5907\u6570\u636e \u521b\u5efametric source /opt/hadoopclient/bigdata_env /usr/local/share/opentsdb/bin/tsdb mkmetric mymetric.data \u51c6\u5907\u5bfc\u5165\u6570\u636e vi /opt/opentsdb_mymetric.txt opentsdb_mymetrix.txt\u5185\u5bb9\u5982\u4e0b\uff1a mymetric.data 1574423500 0.841470984808 host=172-16-5-105 mymetric.data 1574423510 0.909297426826 host=172-16-5-105 mymetric.data 1574423520 0.14112000806 host=172-16-5-105 mymetric.data 1574423530 0.756802495308 host=172-16-5-105 mymetric.data 1574423540 0.958924274663 host=172-16-5-105 \u5411metric\u5bfc\u5165\u6570\u636e /usr/local/share/opentsdb/bin/tsdb import /opt/opentsdb_mymetric.txt \u542f\u52a8OpenTSDB\u5e76\u7528\u6d4f\u89c8\u5668\u8bbf\u95ee\uff0c\u4f8b\u5982 http://172.16.5.105:4242/ \u3002 /usr/local/share/opentsdb/bin/tsdb tsd \u8f93\u5165\u4ee5\u4e0b\u67e5\u8be2\u6761\u4ef6\uff1a From: UNIX timestamp=1574423500 To: UNIX timestamp=1574423540 Metrix: mymetric.data","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Database/OpenTSDB_2.4.0/#faq","text":"\u67e5\u8be2\u6570\u636e\u65f6\u8fd4\u56dejava.lang.NullPointerException: null \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5728\u754c\u9762\u8f93\u5165\u67e5\u8be2\u6761\u4ef6\u67e5\u8be2\u6570\u636e\u65f6\uff0c\u8fd4\u56de\u4ee5\u4e0b\u9519\u8bef\u3002 Request failed: Internal Server Error java.lang.NullPointerException: null at java.lang.ProcessBuilder.start(ProcessBuilder.java:1012) ~[na:1.8.0_201] at net.opentsdb.tsd.GraphHandler.runGnuplot(GraphHandler.java:786) ~[tsdb-2.4.0.jar:] at net.opentsdb.tsd.GraphHandler$RunGnuplot.execute(GraphHandler.java:353) ~[tsdb-2.4.0.jar:] at net.opentsdb.tsd.GraphHandler$RunGnuplot.run(GraphHandler.java:340) ~[tsdb-2.4.0.jar:] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_201] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_201] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_201] \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u68c0\u67e5\u662f\u5426\u5df2\u5b89\u88c5\u63d2\u4ef6\u3002\u5982\u679c\u672a\u5b89\u88c5\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\u540e\u518d\u91cd\u65b0\u542f\u52a8OpenTSDB\u3002 yum install gnuplot /usr/local/share/opentsdb/bin/tsdb tsd","title":"FAQ"},{"location":"Database/SAP_VORA/","text":"SAP VORA\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 SAP VORA 2.0 \u2194 FusionInsight HD V100R002C70SPC200 (Spark) SAP VORA 2.1 \u2194 FusionInsight HD V100R002C70SPC200 (Spark)","title":"2.1 <--> C70"},{"location":"Database/SAP_VORA/#sap-vorafusioninsight","text":"","title":"SAP VORA\u5bf9\u63a5FusionInsight"},{"location":"Database/SAP_VORA/#_1","text":"SAP VORA 2.0 \u2194 FusionInsight HD V100R002C70SPC200 (Spark) SAP VORA 2.1 \u2194 FusionInsight HD V100R002C70SPC200 (Spark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/apache_druid_0.14.2/","text":"Apache Druid 0.14.2 \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Druid 0.14.2 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD 2.8 \u73af\u5883\u63cf\u8ff0 \u00b6 FI HD\u4e3b\u673a\u4e09\u53f0\uff1a 172.16.6.10 - 12 Druid\u90e8\u7f72\u4e3b\u673a\uff1a 172.16.2.121 \u51c6\u5907\u5de5\u4f5c \u00b6 FI HD\u96c6\u7fa4\u76f8\u5173\u51c6\u5907 \u00b6 \u53c2\u8003\u4ea7\u54c1\u6587\u6863\u5b8c\u6210FI HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 \u4e0b\u8f7d\u51c6\u5907\u597d\u7684\u7528\u6237developuser\u76f8\u5173\u7684user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5MySQL \u00b6 Druid \u7684\u5143\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u672c\u6587\u9009\u7528\u81ea\u5df1\u642d\u5efa\u7684MySQL\u6570\u636e\u5e93\uff0c\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5MySQL\u6570\u636e\u5e93 \u767b\u5f55 https://downloads.mysql.com/archives/community/ \uff0c \u5728 Product Version \u4e2d\u9009\u62e9 5.7.27\uff0cOperating System\u8bf7\u9009\u62e9Linux-Generic\uff0c\u4e0b\u8f7d\u793e\u533a\u7248MySQL\u8f6f\u4ef6\u5305\u3002 \u4ee5root\u7528\u6237\u767b\u5f55\u5f85\u5b89\u88c5\u7684\u670d\u52a1\u5668 \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5e76\u89e3\u538b\u3002 \u4ee5root\u7528\u6237\u901a\u8fc7sftp/ftp\u5de5\u5177\u4e0a\u4f20\u201cmysql-5.7.24-linux-glibc2.12-x86_64.tar.gz\u201d\u8f6f\u4ef6\u5305\u5230\u201c/opt\u201d\u76ee\u5f55 \u3002 \u8fdb\u5165opt\u76ee\u5f55\uff0c\u5e76\u89e3\u538b\u7f29\u8f6f\u4ef6\u5305\u3002 cd /opt/ tar -xzvf mysql-5.7.27-linux-glibc2.12-x86_64.tar.gz \u5c06\u89e3\u538b\u540e\u76ee\u5f55\u6539\u540d\u4e3amysql\u3002 mv mysql-5.7.27-linux-glibc2.12-x86_64 mysql \u521b\u5efa\u7528\u6237\u548c\u7528\u6237\u7ec4\uff0c\u5e76\u8fdb\u884c\u6388\u6743\u3002 \u6dfb\u52a0mysql\u7ec4\u3002 groupadd mysql \u6dfb\u52a0mysql\u7528\u6237\u3002 useradd -d /home/mysql -s /bin/bash -g mysql -m mysql \u628amysql\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql \u5728\u6570\u636e\u76d8\u76ee\u5f55\u4e0b\uff08\u5982/data01\uff09\uff0c\u521b\u5efamysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55tmp\u3002 mkdir /opt/mysql-data mkdir /opt/mysql-data/tmp mkdir /opt/mysql-data/log \u628amysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7ec4\u4e2d\u7684mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql-data mysql-data\u76ee\u5f55\u7684\u6240\u5c5e\u7fa4\u7ec4\u4fee\u6539\u4e3amysql\u3002 chgrp -R mysql /opt/mysql-data \u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 \u5728mysql\u76ee\u5f55\u4e0b\u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 vi /opt/mysql/my.cnf \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u6309\u5982\u4e0b\u8981\u6c42\u4fee\u6539\u6587\u4ef6\u5185\u5bb9\uff0c\u4fee\u6539\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002\u5176\u4e2d\uff0c\u201cbind-address\u201d\u53c2\u6570\u8bf7\u4fee\u6539\u4e3aMySQL\u670d\u52a1\u5668\u7684\u5730\u5740\u3002 [mysqld] basedir = /opt/mysql bind-address = 172.16.2.121 datadir = /opt/mysql-data/workdbs tmpdir = /opt/mysql-data/tmp/ port = 3306 socket =/opt/mysql/lib/mysql.sock lower_case_table_names=1 character-set-server = utf8 max_allowed_packet = 150M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,STRICT_ALL_TABLES log-error=/opt/mysql-data/log/mysql_3306.log max_connections=1000 event_scheduler=ON [mysql] default-character-set = utf8 socket =/opt/mysql/lib/mysql.sock \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4fee\u6539my.cnf\u6587\u4ef6\u7684\u5c5e\u4e3b\u3002 chown mysql:mysql /opt/mysql/my.cnf \u62f7\u8d1dmy.cnf\u6587\u4ef6\u5230etc\u76ee\u5f55\u4e0b\u3002 cp -fr /opt/mysql/my.cnf /etc/my.cnf \u4fee\u6539\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6profile\u3002 \u7f16\u8f91etc\u76ee\u5f55\u4e0b\u7684\u201cprofile\u201d\u6587\u4ef6\u3002 vi /etc/profile \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u5728\u6587\u4ef6\u672b\u5c3e\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a export PATH=$PATH:/opt/mysql/bin export PATH=$PATH:/etc/init.d\u6dfb\u52a0\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002 \u91cd\u65b0\u52a0\u8f7detc\u76ee\u5f55\u4e0b\u7684profile\u6587\u4ef6\u3002 source /etc/profile \u5c06mysql.server\u590d\u5236\u5230/etc/init.d/ \u3002 cd /opt/mysql cp -a ./support-files/mysql.server /etc/init.d/mysql.server \u521d\u59cb\u5316mysql cd /opt/mysql ./bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql-data/workdbs \u547d\u4ee4\u6267\u884c\u540e\uff0c\u5982\u65e0\u9519\u8bef\uff0c\u4e0d\u4f1a\u6709\u663e\u793a\u4fe1\u606f\uff0c\u67e5\u770b\u65e5\u5fd7\u6587\u4ef6\u201c/opt/mysql-data/log/mysql_3306.log\u201d\uff0c\u83b7\u53d6\u4e34\u65f6\u5bc6\u7801\u3002 cat /data01/mysql-data/log/mysql_3306.log \u521b\u5efa\u8f6f\u8fde\u63a5\u3002 \u5c06mysql\u7684\u5b89\u88c5\u76ee\u5f55\u8f6f\u8fde\u63a5\u5230local\u4e0b\u9762\u3002 ln -s /opt/mysql /usr/local/mysql \u5c06mysql.sock\u6587\u4ef6\u8f6f\u8fde\u63a5\u5230tmp\u4e0b\u9762 ln -s /opt/mysql/lib/mysql.sock /tmp/mysql.sock \u6ce8\u518c\u5e76\u8bbe\u7f6emysql.server\u670d\u52a1\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u3002 systemctl enable mysql.server.service \u67e5\u770bMySQL\u72b6\u6001\u3002 mysql.server status \u5728 opt/mysql/bin \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u767b\u5f55MySQL\u3002 cd /opt/mysql/bin mysql -u root -p \u6309\u7167\u63d0\u793a\u4fe1\u606f\u8f93\u5165\u8bb0\u5f55\u7684\u4e34\u65f6\u5bc6\u7801\u3002 Enter Password\uff1a\u767b\u5f55\u6210\u529f\u540e\u7cfb\u7edf\u663e\u793a\u5982\u4e0b\u7c7b\u4f3c\u4fe1\u606f\uff1a \u4fee\u6539root\u7528\u6237\u5bc6\u7801\u3002 mysql> set password=password('Password'); \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u8d4b\u4e88\u4efb\u4f55\u4e3b\u673a\u8bbf\u95ee\u6570\u636e\u7684\u6743\u9650\u3002 mysql> grant all privileges on *.* to 'root'@'%' identified by 'Password' with grant option; \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u4f7f\u4fee\u6539\u751f\u6548\u5e76\u4f7f\u7528\u6570\u636e\u5e93\u3002 mysql> flush privileges; mysql> use mysql; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadruid\u5143\u6570\u636e\u5b58\u50a8\u7684database mysql -u root -e \"CREATE DATABASE druid CHARACTER SET utf8 COLLATE utf8_general_ci\" -p \u5b8c\u6210\u5b89\u88c5\uff0c\u90e8\u7f72\uff0c\u9000\u51faMySQL\u6570\u636e\u5e93\u3002 mysql> exit \u5b89\u88c5\uff0c\u90e8\u7f72Druid \u00b6 \u767b\u5f55\u5982\u4e0b\u7f51\u5740\u9009\u62e9\u76f8\u5173Druid\u7248\u672c\u4e0b\u8f7d\uff1a https://druid.apache.org/downloads.html \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5230/opt/druid\u76ee\u5f55\u4e0b tar -xvf apache-druid-0.14.2-incubating-bin.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 Druid \u9700\u8981\u4f7f\u7528zookeeper\u670d\u52a1\u4f5c\u4e3a\u81ea\u5df1\u672c\u8eabdistributed coordination\u670d\u52a1\u7684\u4f9d\u8d56\uff0c\u6240\u4ee5\u5728\u4f7f\u7528druid\u4e4b\u524d\u9700\u8981\u63d0\u524d\u90e8\u7f72zookeeper\u670d\u52a1\uff0c\u672c\u6587\u4f7f\u7528\u5f00\u6e90zookeeper\u670d\u52a1\u4f5c\u4e3adruid\u7684\u4f9d\u8d56\uff0c\u800c\u4e0d\u4f7f\u7528FI HD\u672c\u771f\u7684zookeeper\u670d\u52a1 \u767b\u5f55druid\u5b89\u88c5\u76ee\u5f55 cd /opt/druid/apache-druid-0.14.2-incubating \u4e0b\u8f7d\u5f00\u6e90\u7684zookeeper\uff0c\u5e76\u4e14\u6539\u540d\u4e3azk curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz tar -xzf zookeeper-3.4.11.tar.gz mv zookeeper-3.4.11 zk \u52a0\u8f7dFI HD\u5ba2\u6237\u7aef\u73af\u5883 source /opt/hadoopclient/bigdata_env \u68c0\u67e5druid\u5b89\u88c5\u4e3b\u673a\u540c\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8druid bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf \u5f85\u5168\u90e8\u670d\u52a1\u542f\u52a8\u540e\uff0c\u767b\u5f55172.16.2.121:8888 web\u754c\u9762\u67e5\u770bdruid \u5b8c\u6210\u540e Ctrl+C \u505c\u6b62druid \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4 \u00b6 \u8bf4\u660e\uff1a \u53c2\u8003Druid\u5b98\u65b9\u6587\u6863\uff0c\u914d\u7f6eFI HD\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u4e3aDruid\u7684Deep Storage,\u5e76\u4e14\u4f7f\u7528FI HD\u7684yarn\u670d\u52a1\u4ee5\u53caMapreduce\u670d\u52a1\u6765\u6279\u91cf\u5c06\u5b58\u50a8\u5728HDFS\u4e0a\u7684\u6570\u636e\u5bfc\u5165Druid\u6570\u636e\u5e93 Deep Storage\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/development/extensions-core/hdfs.html Hadoop\u6279\u5904\u7406\u6570\u636e\u5bfc\u5165\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/ingestion/hadoop.html \u4eceFI HD\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u5230\u914d\u7f6e\u6587\u4ef6core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a core-site.xml: \u5c06\u9ed8\u8ba4\u914d\u7f6e\u9879 <property> <name>fs.defaultFS</name> <value>hdfs://hacluster</value> </property> \u66f4\u6539\u4e3a\u4e3bNamenode\u8282\u70b9IP + \u7aef\u53e3\u5f62\u5f0f\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://172.16.6.12:25000</value> </property> hdfs-site.xml: \u5220\u9664\u5982\u4e0b\u8fd9\u4e2a\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u5c06\u4e0a\u9762\u6b65\u9aa4\u7684core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230druid\u5982\u4e0b\u4e24\u4e2a\u8def\u5f84\u4e0b\uff1a /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common \u4fee\u6539druid\u914d\u7f6e\u6587\u4ef6 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u589e\u52a0 druid.extensions.loadList=[\"druid-hdfs-storage\", \"mysql-metadata-storage\"] \u4fee\u6539zookeeper\u914d\u7f6e\u9879\u5982\u4e0b\uff1a druid.zk.service.host=localhost druid.zk.paths.base=/druid Druid\u5143\u6570\u636e\u5b58\u50a8\u6539\u4e3a\u4e4b\u524d\u914d\u7f6e\u597d\u7684MySQL\u6570\u636e\u5e93 \u914d\u7f6ehdfs Deep Storage\u76f8\u5173\u53c2\u6570\uff1a druid.storage.type=hdfs druid.storage.storageDirectory=hdfs://172.16.6.12:25000/druid121/segments \u914d\u7f6ehadoop indexer\u4ee5\u53cakerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a druid.indexer.logs.type=hdfs druid.indexer.logs.directory=hdfs://172.16.6.12:25000/druid121/indexing-logs druid.hadoop.security.kerberos.principal=developuser@HADOOP.COM druid.hadoop.security.kerberos.keytab=/opt/101hdclient/user.keytab \u5176\u4e2ddevelopuser\u4e3a\u96c6\u7fa4\u521b\u5efa\u7684\u7528\u6237\uff0cuser.keytab\u4e3a\u4e0b\u8f7d\u7684developuser\u8ba4\u8bc1\u6587\u4ef6 \u5c06\u4e0a\u8ff0\u6b65\u9aa4\u4fee\u6539\u540e\u7684common.runtime.properties\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common \u8def\u5f84\u4e0b \u5c06\u4e0b\u8f7d\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230druid\u670d\u52a1\u5668 /etc/ \u8def\u5f84\u4e0b\uff08\u9ed8\u8ba4\u5728\u6b64\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff09 cp /opt/user_keytabs/101keytab/krb5.conf /etc \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/extensions/mysql-metadata-storage \u8def\u5f84\uff0c\u5bfc\u5165mysql\u8fde\u63a5\u9a71\u52a8 mysql-connector-java-5.1.48.jar\uff0c \u9a71\u52a8jar\u5305\u53ef\u5728mysql\u5b98\u65b9\u7f51\u7ad9\u83b7\u53d6 \u767b\u5f55druid extension\u8def\u5f84\u4e0b\u627e\u5230druid-hdfs-storage\u4f9d\u8d56\u8def\u5f84\uff0c\u6539\u540d\u5907\u4efd cd /opt/druid/apache-druid-0.14.2-incubating/extensions mv druid-hdfs-storage/ druid-hdfs-storage-backup/ \u53e6\u884c\u521b\u5efadruid-hdfs-storage\u6587\u4ef6\u5939\uff0c\u4eceFI HD\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c druid-hdfs-storage\u81ea\u5e26\u7684jar\u5305\u4e2d\u6536\u96c6\u5e76\u5bfc\u5165\u5982\u4e0b\u4f9d\u8d56jar\u5305\uff1a \u6ce8\uff1ahdfs\u76f8\u5173jar\u5305\u4e00\u5b9a\u662f\u4eceFI HD\u4e0b\u8f7d\u7684\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar asm-3.2.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-collections-3.2.2.jar commons-compress-1.16.jar commons-configuration-1.6.jar commons-daemon-1.0.13.jar commons-digester-1.8.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar commons-net-3.1.jar core.jar curator-framework-4.1.0.jar curator-recipes-4.1.0.jar druid-hdfs-storage-0.14.2-incubating.jar dynalogger-V100R002C30.jar gson-2.2.4.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar htrace-core4-4.0.1-incubating.jar jackson-core-asl-1.9.13.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-xc-1.9.13.jar javaluator-3.0.1.jar jaxb-api-2.2.2.jar jcip-annotations-1.0.jar jersey-client-1.9.jar jetty-6.1.26.jar jetty-sslengine-6.1.26.jar jetty-util-6.1.26.jar json-smart-1.1.1.jar jsp-api-2.1.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar nimbus-jose-jwt-3.9.jar objenesis-2.6.jar okhttp-2.4.0.jar okio-1.4.0.jar protobuf-java-2.5.0.jar rt.jar servlet-api-2.5.jar stax-api-1.0-2.jar xercesImpl-2.9.1.jar xml-apis-1.3.04.jar xmlenc-0.52.jar \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client \u8def\u5f84\u4e0b\uff0c\u521b\u5efa\u8def\u5f84 2.7.2 cd /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client mkdir 2.7.2 \u6309\u7167\u5982\u4e0b\u5217\u8868\u51c6\u59072.7.2\u8def\u5f84\u4e0b\u7684\u4f9d\u8d56Jar\u5305 asm-3.2.jar avro-1.7.4.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-daemon-1.0.13.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar dynalogger-V100R002C30.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar javaluator-3.0.1.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-6.1.26.jar jetty-util-6.1.26.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar \u521b\u5efadruid\u9700\u8981\u7684spec\u6587\u4ef6\u5e76\u653e\u5230druid\u76ee\u5f55\u4e0b\uff1awikipedia-index-hadoop.json \u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\" : { \"format\" : \"auto\", \"column\" : \"time\" } } }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/data/wikiticker-2015-09-12-sampled.json.gz\" } }, \"tuningConfig\" : { \"type\" : \"hadoop\", \"partitionsSpec\" : { \"type\" : \"hashed\", \"targetPartitionSize\" : 5000000 }, \"forceExtendableShardSpecs\" : true, \"jobProperties\" : { \"fs.default.name\" : \"hdfs://172.16.6.12:25000\", \"fs.defaultFS\" : \"hdfs://172.16.6.12:25000\", \"dfs.datanode.address\" : \"HD03\", \"dfs.client.use.datanode.hostname\" : \"true\", \"dfs.datanode.use.datanode.hostname\" : \"true\", \"yarn.resourcemanager.hostname\" : \"HD03\", \"yarn.nodemanager.vmem-check-enabled\" : \"false\", \"mapreduce.map.java.opts\" : \"-Duser.timezone=UTC -Dfile.encoding=UTF-8\", \"mapreduce.job.user.classpath.first\" : \"true\", \"mapreduce.reduce.java.opts\" : \"-Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8\", \"mapreduce.map.memory.mb\" : 1024, \"mapreduce.reduce.memory.mb\" : 1024 } } }, \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.2\"] } \u767b\u5f55FI HD\u96c6\u7fa4,\u5728HDFS\u7684/data\u76ee\u5f55\u4e0b\u4f20\u5165\u6570\u636e\u6587\u4ef6 wikiticker-2015-09-12-sampled.json.gz\uff0c \u8be5\u6570\u636e\u6587\u4ef6\u53ef\u4ee5\u5728 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial \u4e0b\u83b7\u53d6 \u540c\u65f6\u68c0\u67e5HDFS\u662f\u5426\u5b58\u5728 /druid121/indexing-logs \u4ee5\u53ca /druid121/segments \uff0c\u82e5\u6ca1\u6709\u8981\u521b\u5efa\u597d \u4f7f\u7528\u547d\u4ee4 bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf \u542f\u52a8druid \u5f85druid\u6240\u6709\u670d\u52a1\u542f\u52a8\u540e\uff0c\u5f00\u542f\u53e6\u4e00\u7ec8\u7aef\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u63d0\u4ea4hadoop index\u4f5c\u4e1a\uff0c\u7b49\u5f85\u4f5c\u4e1a\u5b8c\u6210 bin/post-index-task -f /opt/druid/apache-druid-0.14.2-incubating/wikipedia-index-hadoop.json \u767b\u5f55\u5bf9\u63a5FI HD\u96c6\u7fa4yarn\u670d\u52a1\u67e5\u770b\u4efb\u52a1\uff1a \u6ce8\uff1a\u4e00\u6b21hadoop index\u4f5c\u4e1a\u4f1a\u5728yarn\u4e0a\u8d77\u4e24\u4e2amap reduce\u4efb\u52a1 \u767b\u5f55druid web\u754c\u9762\u5728Tasks\u9762\u677f\u4e0b\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff0c\u65e5\u5fd7\uff1a \u5728Datasources\u4e0b\u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e\uff1a SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u666e\u901a\u6a21\u5f0f \u00b6 \u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21005 \u521b\u5efatopic wikipedia21005 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646druid\u4e3b\u673a\uff0c\u53e6\u5916\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u5728\u4f9d\u8d56\u5e93\u91cc\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982kafka-clients-0.11.0.1.jar\uff0c\u5e76\u628a\u8be5jar\u5305\u4f20\u5230druid\u4e3b\u673a\u7684 %Druid Home%/extensions/druid-kafka-indexing-service \u4e0b\uff0c\u5e76\u4e14\u5c06\u8be5\u8def\u5f84\u4e0b\u5df2\u6709\u7684kafka client jar\u5305\u901a\u8fc7\u52a0\u540e\u7f00 .org \u7684\u65b9\u5f0f\u6ce8\u9500\u6389\uff1a \u5176\u4e2d%Druid Home%\u4e3adruid\u5b89\u88c5\u8def\u5f84 \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.121:8888/ \u70b9\u51fbTasks\u627e\u5230Supervisor\uff1a \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21005\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21005\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21005,172.16.6.12:21005,172.16.6.10:21005\" } } } \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef\uff0c\u628a\u6d4b\u8bd5\u6570\u636ewikiticker-2015-09-12-sampled.json\u4e0a\u4f20\u5230kafka\u5ba2\u6237\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f80topic wikipedia21005\u5199\u5165\u6570\u636e cd /opt/hadoopclient/Kafka/kafka ./bin/kafka-console-producer.sh --broker-list 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 --topic wikipedia21005 < /opt/wikiticker-2015-09-12-sampled.json --producer.config config/producer.properties \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c\uff1a \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT page, COUNT(*) AS Edits FROM wikipedia21005 WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u5207\u6362\u56de\u5bf9\u63a5kafka\u5ba2\u6237\u7aef\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770btopic wikipedia21005\u91cc\u9762\u7684\u6570\u636e\uff1a bin/kafka-console-consumer.sh --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --topic wikipedia21005 --from-beginning \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u5b89\u5168\u6a21\u5f0f \u00b6 \u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21007 \u521b\u5efatopic wikipedia21007 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646druid\u4e3b\u673a\uff0c\u53e6\u5916\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u5728\u4f9d\u8d56\u5e93\u91cc\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982kafka-clients-0.11.0.1.jar\uff0c\u5e76\u628a\u8be5jar\u5305\u4f20\u5230druid\u4e3b\u673a\u7684 %Druid Home%/extensions/druid-kafka-indexing-service \u4e0b\uff0c\u5e76\u4e14\u5c06\u8be5\u8def\u5f84\u4e0b\u5df2\u6709\u7684kafka client jar\u5305\u901a\u8fc7\u52a0\u540e\u7f00 .org \u7684\u65b9\u5f0f\u6ce8\u9500\u6389\uff1a \u5176\u4e2d%Druid Home%\u4e3adruid\u5b89\u88c5\u8def\u5f84 \u5728druid\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u4e3a\uff1a \u5c06\u8ba4\u8bc1\u4f7f\u7528\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5e76\u8986\u76d6\u5230druid\u4e3b\u673a\u7684/etc\u8def\u5f84\u4e0b\uff0cdruid\u9ed8\u8ba4\u4ece\u6b64\u8def\u5f84\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u767b\u9646druid\u7684\u914d\u7f6e\u8def\u5f84 /opt/druid/apache-druid-0.14.2-incubating/conf/druid \u5206\u522b\u5728broker,coordinator,historical,middleManager,overlord,router\u670d\u52a1\u8def\u5f84\u4e2d\u7684jvm.config\u6587\u4ef6\u4e2d\u52a0\u5165\u4e09\u6761\u914d\u7f6e\u9879 -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com \u767b\u9646druid\u7684\u914d\u7f6e\u8def\u5f84 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid , \u91cd\u590d\u4e0a\u8ff0\u6b65\u9aa4\u5206\u522b\u5728broker,coordinator,historical,middleManager,overlord,router\u670d\u52a1\u8def\u5f84\u4e2d\u7684jvm.config\u6587\u4ef6\u4e2d\u52a0\u5165\u4e09\u6761\u914d\u7f6e\u9879 -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com \u505c\u6b62\u4e4b\u524d\u8fd0\u884c\u4e2d\u7684druid \u4f7f\u7528\u547d\u4ee4 source /opt/hadoopclient/bigdata_env \u52a0\u8f7d\u96c6\u7fa4\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com\" \u52a0\u8f7d\u8fd0\u884cjava\u6574\u4f53jvm\u53c2\u6570\uff0c\u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u7ed3\u679c\uff1a \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.121:8888/ \u70b9\u51fbTasks\u627e\u5230Supervisor\uff1a \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21007\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21007\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21007,172.16.6.12:21007,172.16.6.10:21007\", \"kerberos.domain.name\": \"hadoop.hadoop.com\", \"security.protocol\": \"SASL_PLAINTEXT\", \"sasl.kerberos.service.name\": \"kafka\" } } } \u540e\u53f0\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4Kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8\u5b89\u5168\u6a21\u5f0fkafka producer bin/kafka-console-producer.sh --broker-list 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --topic wikipedia21007 --producer.config config/producer.properties \u63d2\u5165\u4e00\u6761\u6570\u636e\uff0c\u5185\u5bb9\u5982\u4e0b {\"time\":\"2015-09-12T05:22:32.338Z\",\"channel\":\"#zh.wikipedia\",\"cityName\":null,\"comment\":\"/* \u6210\u7acb */\",\"countryIsoCode\":null,\"countryName\":null,\"isAnonymous\":false,\"isMinor\":false,\"isNew\":false,\"isRobot\":false,\"isUnpatrolled\":false,\"metroCode\":null,\"namespace\":\"Main\",\"page\":\"\u8056\u4f2f\u591a\u797f\u53f8\u9438\u5144\u5f1f\u6703\",\"regionIsoCode\":null,\"regionName\":null,\"user\":\"\u91d1\u8085\",\"delta\":675,\"added\":675,\"deleted\":0} \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT * FROM wikipedia21007","title":"0.14.2 <--> C80"},{"location":"Database/apache_druid_0.14.2/#apache-druid-0142-fusioninsight","text":"","title":"Apache Druid 0.14.2 \u5bf9\u63a5FusionInsight"},{"location":"Database/apache_druid_0.14.2/#_1","text":"Apache Druid 0.14.2 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/apache_druid_0.14.2/#_2","text":"\u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD 2.8","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Database/apache_druid_0.14.2/#_3","text":"FI HD\u4e3b\u673a\u4e09\u53f0\uff1a 172.16.6.10 - 12 Druid\u90e8\u7f72\u4e3b\u673a\uff1a 172.16.2.121","title":"\u73af\u5883\u63cf\u8ff0"},{"location":"Database/apache_druid_0.14.2/#_4","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Database/apache_druid_0.14.2/#fi-hd","text":"\u53c2\u8003\u4ea7\u54c1\u6587\u6863\u5b8c\u6210FI HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 \u4e0b\u8f7d\u51c6\u5907\u597d\u7684\u7528\u6237developuser\u76f8\u5173\u7684user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6","title":"FI HD\u96c6\u7fa4\u76f8\u5173\u51c6\u5907"},{"location":"Database/apache_druid_0.14.2/#mysql","text":"Druid \u7684\u5143\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u672c\u6587\u9009\u7528\u81ea\u5df1\u642d\u5efa\u7684MySQL\u6570\u636e\u5e93\uff0c\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5MySQL\u6570\u636e\u5e93 \u767b\u5f55 https://downloads.mysql.com/archives/community/ \uff0c \u5728 Product Version \u4e2d\u9009\u62e9 5.7.27\uff0cOperating System\u8bf7\u9009\u62e9Linux-Generic\uff0c\u4e0b\u8f7d\u793e\u533a\u7248MySQL\u8f6f\u4ef6\u5305\u3002 \u4ee5root\u7528\u6237\u767b\u5f55\u5f85\u5b89\u88c5\u7684\u670d\u52a1\u5668 \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5e76\u89e3\u538b\u3002 \u4ee5root\u7528\u6237\u901a\u8fc7sftp/ftp\u5de5\u5177\u4e0a\u4f20\u201cmysql-5.7.24-linux-glibc2.12-x86_64.tar.gz\u201d\u8f6f\u4ef6\u5305\u5230\u201c/opt\u201d\u76ee\u5f55 \u3002 \u8fdb\u5165opt\u76ee\u5f55\uff0c\u5e76\u89e3\u538b\u7f29\u8f6f\u4ef6\u5305\u3002 cd /opt/ tar -xzvf mysql-5.7.27-linux-glibc2.12-x86_64.tar.gz \u5c06\u89e3\u538b\u540e\u76ee\u5f55\u6539\u540d\u4e3amysql\u3002 mv mysql-5.7.27-linux-glibc2.12-x86_64 mysql \u521b\u5efa\u7528\u6237\u548c\u7528\u6237\u7ec4\uff0c\u5e76\u8fdb\u884c\u6388\u6743\u3002 \u6dfb\u52a0mysql\u7ec4\u3002 groupadd mysql \u6dfb\u52a0mysql\u7528\u6237\u3002 useradd -d /home/mysql -s /bin/bash -g mysql -m mysql \u628amysql\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql \u5728\u6570\u636e\u76d8\u76ee\u5f55\u4e0b\uff08\u5982/data01\uff09\uff0c\u521b\u5efamysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55tmp\u3002 mkdir /opt/mysql-data mkdir /opt/mysql-data/tmp mkdir /opt/mysql-data/log \u628amysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7ec4\u4e2d\u7684mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql-data mysql-data\u76ee\u5f55\u7684\u6240\u5c5e\u7fa4\u7ec4\u4fee\u6539\u4e3amysql\u3002 chgrp -R mysql /opt/mysql-data \u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 \u5728mysql\u76ee\u5f55\u4e0b\u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 vi /opt/mysql/my.cnf \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u6309\u5982\u4e0b\u8981\u6c42\u4fee\u6539\u6587\u4ef6\u5185\u5bb9\uff0c\u4fee\u6539\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002\u5176\u4e2d\uff0c\u201cbind-address\u201d\u53c2\u6570\u8bf7\u4fee\u6539\u4e3aMySQL\u670d\u52a1\u5668\u7684\u5730\u5740\u3002 [mysqld] basedir = /opt/mysql bind-address = 172.16.2.121 datadir = /opt/mysql-data/workdbs tmpdir = /opt/mysql-data/tmp/ port = 3306 socket =/opt/mysql/lib/mysql.sock lower_case_table_names=1 character-set-server = utf8 max_allowed_packet = 150M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,STRICT_ALL_TABLES log-error=/opt/mysql-data/log/mysql_3306.log max_connections=1000 event_scheduler=ON [mysql] default-character-set = utf8 socket =/opt/mysql/lib/mysql.sock \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4fee\u6539my.cnf\u6587\u4ef6\u7684\u5c5e\u4e3b\u3002 chown mysql:mysql /opt/mysql/my.cnf \u62f7\u8d1dmy.cnf\u6587\u4ef6\u5230etc\u76ee\u5f55\u4e0b\u3002 cp -fr /opt/mysql/my.cnf /etc/my.cnf \u4fee\u6539\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6profile\u3002 \u7f16\u8f91etc\u76ee\u5f55\u4e0b\u7684\u201cprofile\u201d\u6587\u4ef6\u3002 vi /etc/profile \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u5728\u6587\u4ef6\u672b\u5c3e\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a export PATH=$PATH:/opt/mysql/bin export PATH=$PATH:/etc/init.d\u6dfb\u52a0\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002 \u91cd\u65b0\u52a0\u8f7detc\u76ee\u5f55\u4e0b\u7684profile\u6587\u4ef6\u3002 source /etc/profile \u5c06mysql.server\u590d\u5236\u5230/etc/init.d/ \u3002 cd /opt/mysql cp -a ./support-files/mysql.server /etc/init.d/mysql.server \u521d\u59cb\u5316mysql cd /opt/mysql ./bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql-data/workdbs \u547d\u4ee4\u6267\u884c\u540e\uff0c\u5982\u65e0\u9519\u8bef\uff0c\u4e0d\u4f1a\u6709\u663e\u793a\u4fe1\u606f\uff0c\u67e5\u770b\u65e5\u5fd7\u6587\u4ef6\u201c/opt/mysql-data/log/mysql_3306.log\u201d\uff0c\u83b7\u53d6\u4e34\u65f6\u5bc6\u7801\u3002 cat /data01/mysql-data/log/mysql_3306.log \u521b\u5efa\u8f6f\u8fde\u63a5\u3002 \u5c06mysql\u7684\u5b89\u88c5\u76ee\u5f55\u8f6f\u8fde\u63a5\u5230local\u4e0b\u9762\u3002 ln -s /opt/mysql /usr/local/mysql \u5c06mysql.sock\u6587\u4ef6\u8f6f\u8fde\u63a5\u5230tmp\u4e0b\u9762 ln -s /opt/mysql/lib/mysql.sock /tmp/mysql.sock \u6ce8\u518c\u5e76\u8bbe\u7f6emysql.server\u670d\u52a1\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u3002 systemctl enable mysql.server.service \u67e5\u770bMySQL\u72b6\u6001\u3002 mysql.server status \u5728 opt/mysql/bin \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u767b\u5f55MySQL\u3002 cd /opt/mysql/bin mysql -u root -p \u6309\u7167\u63d0\u793a\u4fe1\u606f\u8f93\u5165\u8bb0\u5f55\u7684\u4e34\u65f6\u5bc6\u7801\u3002 Enter Password\uff1a\u767b\u5f55\u6210\u529f\u540e\u7cfb\u7edf\u663e\u793a\u5982\u4e0b\u7c7b\u4f3c\u4fe1\u606f\uff1a \u4fee\u6539root\u7528\u6237\u5bc6\u7801\u3002 mysql> set password=password('Password'); \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u8d4b\u4e88\u4efb\u4f55\u4e3b\u673a\u8bbf\u95ee\u6570\u636e\u7684\u6743\u9650\u3002 mysql> grant all privileges on *.* to 'root'@'%' identified by 'Password' with grant option; \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u4f7f\u4fee\u6539\u751f\u6548\u5e76\u4f7f\u7528\u6570\u636e\u5e93\u3002 mysql> flush privileges; mysql> use mysql; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadruid\u5143\u6570\u636e\u5b58\u50a8\u7684database mysql -u root -e \"CREATE DATABASE druid CHARACTER SET utf8 COLLATE utf8_general_ci\" -p \u5b8c\u6210\u5b89\u88c5\uff0c\u90e8\u7f72\uff0c\u9000\u51faMySQL\u6570\u636e\u5e93\u3002 mysql> exit","title":"\u5b89\u88c5MySQL"},{"location":"Database/apache_druid_0.14.2/#druid","text":"\u767b\u5f55\u5982\u4e0b\u7f51\u5740\u9009\u62e9\u76f8\u5173Druid\u7248\u672c\u4e0b\u8f7d\uff1a https://druid.apache.org/downloads.html \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5230/opt/druid\u76ee\u5f55\u4e0b tar -xvf apache-druid-0.14.2-incubating-bin.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 Druid \u9700\u8981\u4f7f\u7528zookeeper\u670d\u52a1\u4f5c\u4e3a\u81ea\u5df1\u672c\u8eabdistributed coordination\u670d\u52a1\u7684\u4f9d\u8d56\uff0c\u6240\u4ee5\u5728\u4f7f\u7528druid\u4e4b\u524d\u9700\u8981\u63d0\u524d\u90e8\u7f72zookeeper\u670d\u52a1\uff0c\u672c\u6587\u4f7f\u7528\u5f00\u6e90zookeeper\u670d\u52a1\u4f5c\u4e3adruid\u7684\u4f9d\u8d56\uff0c\u800c\u4e0d\u4f7f\u7528FI HD\u672c\u771f\u7684zookeeper\u670d\u52a1 \u767b\u5f55druid\u5b89\u88c5\u76ee\u5f55 cd /opt/druid/apache-druid-0.14.2-incubating \u4e0b\u8f7d\u5f00\u6e90\u7684zookeeper\uff0c\u5e76\u4e14\u6539\u540d\u4e3azk curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz tar -xzf zookeeper-3.4.11.tar.gz mv zookeeper-3.4.11 zk \u52a0\u8f7dFI HD\u5ba2\u6237\u7aef\u73af\u5883 source /opt/hadoopclient/bigdata_env \u68c0\u67e5druid\u5b89\u88c5\u4e3b\u673a\u540c\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8druid bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf \u5f85\u5168\u90e8\u670d\u52a1\u542f\u52a8\u540e\uff0c\u767b\u5f55172.16.2.121:8888 web\u754c\u9762\u67e5\u770bdruid \u5b8c\u6210\u540e Ctrl+C \u505c\u6b62druid","title":"\u5b89\u88c5\uff0c\u90e8\u7f72Druid"},{"location":"Database/apache_druid_0.14.2/#druidfi-hd","text":"\u8bf4\u660e\uff1a \u53c2\u8003Druid\u5b98\u65b9\u6587\u6863\uff0c\u914d\u7f6eFI HD\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u4e3aDruid\u7684Deep Storage,\u5e76\u4e14\u4f7f\u7528FI HD\u7684yarn\u670d\u52a1\u4ee5\u53caMapreduce\u670d\u52a1\u6765\u6279\u91cf\u5c06\u5b58\u50a8\u5728HDFS\u4e0a\u7684\u6570\u636e\u5bfc\u5165Druid\u6570\u636e\u5e93 Deep Storage\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/development/extensions-core/hdfs.html Hadoop\u6279\u5904\u7406\u6570\u636e\u5bfc\u5165\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/ingestion/hadoop.html \u4eceFI HD\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u5230\u914d\u7f6e\u6587\u4ef6core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a core-site.xml: \u5c06\u9ed8\u8ba4\u914d\u7f6e\u9879 <property> <name>fs.defaultFS</name> <value>hdfs://hacluster</value> </property> \u66f4\u6539\u4e3a\u4e3bNamenode\u8282\u70b9IP + \u7aef\u53e3\u5f62\u5f0f\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://172.16.6.12:25000</value> </property> hdfs-site.xml: \u5220\u9664\u5982\u4e0b\u8fd9\u4e2a\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u5c06\u4e0a\u9762\u6b65\u9aa4\u7684core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230druid\u5982\u4e0b\u4e24\u4e2a\u8def\u5f84\u4e0b\uff1a /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common \u4fee\u6539druid\u914d\u7f6e\u6587\u4ef6 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u589e\u52a0 druid.extensions.loadList=[\"druid-hdfs-storage\", \"mysql-metadata-storage\"] \u4fee\u6539zookeeper\u914d\u7f6e\u9879\u5982\u4e0b\uff1a druid.zk.service.host=localhost druid.zk.paths.base=/druid Druid\u5143\u6570\u636e\u5b58\u50a8\u6539\u4e3a\u4e4b\u524d\u914d\u7f6e\u597d\u7684MySQL\u6570\u636e\u5e93 \u914d\u7f6ehdfs Deep Storage\u76f8\u5173\u53c2\u6570\uff1a druid.storage.type=hdfs druid.storage.storageDirectory=hdfs://172.16.6.12:25000/druid121/segments \u914d\u7f6ehadoop indexer\u4ee5\u53cakerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a druid.indexer.logs.type=hdfs druid.indexer.logs.directory=hdfs://172.16.6.12:25000/druid121/indexing-logs druid.hadoop.security.kerberos.principal=developuser@HADOOP.COM druid.hadoop.security.kerberos.keytab=/opt/101hdclient/user.keytab \u5176\u4e2ddevelopuser\u4e3a\u96c6\u7fa4\u521b\u5efa\u7684\u7528\u6237\uff0cuser.keytab\u4e3a\u4e0b\u8f7d\u7684developuser\u8ba4\u8bc1\u6587\u4ef6 \u5c06\u4e0a\u8ff0\u6b65\u9aa4\u4fee\u6539\u540e\u7684common.runtime.properties\u6587\u4ef6\u62f7\u8d1d\u5230 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common \u8def\u5f84\u4e0b \u5c06\u4e0b\u8f7d\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230druid\u670d\u52a1\u5668 /etc/ \u8def\u5f84\u4e0b\uff08\u9ed8\u8ba4\u5728\u6b64\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff09 cp /opt/user_keytabs/101keytab/krb5.conf /etc \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/extensions/mysql-metadata-storage \u8def\u5f84\uff0c\u5bfc\u5165mysql\u8fde\u63a5\u9a71\u52a8 mysql-connector-java-5.1.48.jar\uff0c \u9a71\u52a8jar\u5305\u53ef\u5728mysql\u5b98\u65b9\u7f51\u7ad9\u83b7\u53d6 \u767b\u5f55druid extension\u8def\u5f84\u4e0b\u627e\u5230druid-hdfs-storage\u4f9d\u8d56\u8def\u5f84\uff0c\u6539\u540d\u5907\u4efd cd /opt/druid/apache-druid-0.14.2-incubating/extensions mv druid-hdfs-storage/ druid-hdfs-storage-backup/ \u53e6\u884c\u521b\u5efadruid-hdfs-storage\u6587\u4ef6\u5939\uff0c\u4eceFI HD\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c druid-hdfs-storage\u81ea\u5e26\u7684jar\u5305\u4e2d\u6536\u96c6\u5e76\u5bfc\u5165\u5982\u4e0b\u4f9d\u8d56jar\u5305\uff1a \u6ce8\uff1ahdfs\u76f8\u5173jar\u5305\u4e00\u5b9a\u662f\u4eceFI HD\u4e0b\u8f7d\u7684\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar asm-3.2.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-collections-3.2.2.jar commons-compress-1.16.jar commons-configuration-1.6.jar commons-daemon-1.0.13.jar commons-digester-1.8.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar commons-net-3.1.jar core.jar curator-framework-4.1.0.jar curator-recipes-4.1.0.jar druid-hdfs-storage-0.14.2-incubating.jar dynalogger-V100R002C30.jar gson-2.2.4.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar htrace-core4-4.0.1-incubating.jar jackson-core-asl-1.9.13.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-xc-1.9.13.jar javaluator-3.0.1.jar jaxb-api-2.2.2.jar jcip-annotations-1.0.jar jersey-client-1.9.jar jetty-6.1.26.jar jetty-sslengine-6.1.26.jar jetty-util-6.1.26.jar json-smart-1.1.1.jar jsp-api-2.1.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar nimbus-jose-jwt-3.9.jar objenesis-2.6.jar okhttp-2.4.0.jar okio-1.4.0.jar protobuf-java-2.5.0.jar rt.jar servlet-api-2.5.jar stax-api-1.0-2.jar xercesImpl-2.9.1.jar xml-apis-1.3.04.jar xmlenc-0.52.jar \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client \u8def\u5f84\u4e0b\uff0c\u521b\u5efa\u8def\u5f84 2.7.2 cd /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client mkdir 2.7.2 \u6309\u7167\u5982\u4e0b\u5217\u8868\u51c6\u59072.7.2\u8def\u5f84\u4e0b\u7684\u4f9d\u8d56Jar\u5305 asm-3.2.jar avro-1.7.4.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-daemon-1.0.13.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar dynalogger-V100R002C30.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar javaluator-3.0.1.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-6.1.26.jar jetty-util-6.1.26.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar \u521b\u5efadruid\u9700\u8981\u7684spec\u6587\u4ef6\u5e76\u653e\u5230druid\u76ee\u5f55\u4e0b\uff1awikipedia-index-hadoop.json \u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\" : { \"format\" : \"auto\", \"column\" : \"time\" } } }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/data/wikiticker-2015-09-12-sampled.json.gz\" } }, \"tuningConfig\" : { \"type\" : \"hadoop\", \"partitionsSpec\" : { \"type\" : \"hashed\", \"targetPartitionSize\" : 5000000 }, \"forceExtendableShardSpecs\" : true, \"jobProperties\" : { \"fs.default.name\" : \"hdfs://172.16.6.12:25000\", \"fs.defaultFS\" : \"hdfs://172.16.6.12:25000\", \"dfs.datanode.address\" : \"HD03\", \"dfs.client.use.datanode.hostname\" : \"true\", \"dfs.datanode.use.datanode.hostname\" : \"true\", \"yarn.resourcemanager.hostname\" : \"HD03\", \"yarn.nodemanager.vmem-check-enabled\" : \"false\", \"mapreduce.map.java.opts\" : \"-Duser.timezone=UTC -Dfile.encoding=UTF-8\", \"mapreduce.job.user.classpath.first\" : \"true\", \"mapreduce.reduce.java.opts\" : \"-Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8\", \"mapreduce.map.memory.mb\" : 1024, \"mapreduce.reduce.memory.mb\" : 1024 } } }, \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.2\"] } \u767b\u5f55FI HD\u96c6\u7fa4,\u5728HDFS\u7684/data\u76ee\u5f55\u4e0b\u4f20\u5165\u6570\u636e\u6587\u4ef6 wikiticker-2015-09-12-sampled.json.gz\uff0c \u8be5\u6570\u636e\u6587\u4ef6\u53ef\u4ee5\u5728 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial \u4e0b\u83b7\u53d6 \u540c\u65f6\u68c0\u67e5HDFS\u662f\u5426\u5b58\u5728 /druid121/indexing-logs \u4ee5\u53ca /druid121/segments \uff0c\u82e5\u6ca1\u6709\u8981\u521b\u5efa\u597d \u4f7f\u7528\u547d\u4ee4 bin/supervise -c quickstart/tutorial/conf/tutorial-cluster.conf \u542f\u52a8druid \u5f85druid\u6240\u6709\u670d\u52a1\u542f\u52a8\u540e\uff0c\u5f00\u542f\u53e6\u4e00\u7ec8\u7aef\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u63d0\u4ea4hadoop index\u4f5c\u4e1a\uff0c\u7b49\u5f85\u4f5c\u4e1a\u5b8c\u6210 bin/post-index-task -f /opt/druid/apache-druid-0.14.2-incubating/wikipedia-index-hadoop.json \u767b\u5f55\u5bf9\u63a5FI HD\u96c6\u7fa4yarn\u670d\u52a1\u67e5\u770b\u4efb\u52a1\uff1a \u6ce8\uff1a\u4e00\u6b21hadoop index\u4f5c\u4e1a\u4f1a\u5728yarn\u4e0a\u8d77\u4e24\u4e2amap reduce\u4efb\u52a1 \u767b\u5f55druid web\u754c\u9762\u5728Tasks\u9762\u677f\u4e0b\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff0c\u65e5\u5fd7\uff1a \u5728Datasources\u4e0b\u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e\uff1a SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4"},{"location":"Database/apache_druid_0.14.2/#druidfi-hdkafka","text":"\u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21005 \u521b\u5efatopic wikipedia21005 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646druid\u4e3b\u673a\uff0c\u53e6\u5916\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u5728\u4f9d\u8d56\u5e93\u91cc\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982kafka-clients-0.11.0.1.jar\uff0c\u5e76\u628a\u8be5jar\u5305\u4f20\u5230druid\u4e3b\u673a\u7684 %Druid Home%/extensions/druid-kafka-indexing-service \u4e0b\uff0c\u5e76\u4e14\u5c06\u8be5\u8def\u5f84\u4e0b\u5df2\u6709\u7684kafka client jar\u5305\u901a\u8fc7\u52a0\u540e\u7f00 .org \u7684\u65b9\u5f0f\u6ce8\u9500\u6389\uff1a \u5176\u4e2d%Druid Home%\u4e3adruid\u5b89\u88c5\u8def\u5f84 \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.121:8888/ \u70b9\u51fbTasks\u627e\u5230Supervisor\uff1a \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21005\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21005\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21005,172.16.6.12:21005,172.16.6.10:21005\" } } } \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef\uff0c\u628a\u6d4b\u8bd5\u6570\u636ewikiticker-2015-09-12-sampled.json\u4e0a\u4f20\u5230kafka\u5ba2\u6237\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f80topic wikipedia21005\u5199\u5165\u6570\u636e cd /opt/hadoopclient/Kafka/kafka ./bin/kafka-console-producer.sh --broker-list 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 --topic wikipedia21005 < /opt/wikiticker-2015-09-12-sampled.json --producer.config config/producer.properties \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c\uff1a \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT page, COUNT(*) AS Edits FROM wikipedia21005 WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u5207\u6362\u56de\u5bf9\u63a5kafka\u5ba2\u6237\u7aef\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770btopic wikipedia21005\u91cc\u9762\u7684\u6570\u636e\uff1a bin/kafka-console-consumer.sh --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --topic wikipedia21005 --from-beginning","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u666e\u901a\u6a21\u5f0f"},{"location":"Database/apache_druid_0.14.2/#druidfi-hdkafka_1","text":"\u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21007 \u521b\u5efatopic wikipedia21007 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646druid\u4e3b\u673a\uff0c\u53e6\u5916\u4fee\u6539 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u5728\u4f9d\u8d56\u5e93\u91cc\u627e\u5230\u5bf9\u5e94\u7684kafka client jar\u5305\uff0c\u6bd4\u5982kafka-clients-0.11.0.1.jar\uff0c\u5e76\u628a\u8be5jar\u5305\u4f20\u5230druid\u4e3b\u673a\u7684 %Druid Home%/extensions/druid-kafka-indexing-service \u4e0b\uff0c\u5e76\u4e14\u5c06\u8be5\u8def\u5f84\u4e0b\u5df2\u6709\u7684kafka client jar\u5305\u901a\u8fc7\u52a0\u540e\u7f00 .org \u7684\u65b9\u5f0f\u6ce8\u9500\u6389\uff1a \u5176\u4e2d%Druid Home%\u4e3adruid\u5b89\u88c5\u8def\u5f84 \u5728druid\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u4e3a\uff1a \u5c06\u8ba4\u8bc1\u4f7f\u7528\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5e76\u8986\u76d6\u5230druid\u4e3b\u673a\u7684/etc\u8def\u5f84\u4e0b\uff0cdruid\u9ed8\u8ba4\u4ece\u6b64\u8def\u5f84\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u767b\u9646druid\u7684\u914d\u7f6e\u8def\u5f84 /opt/druid/apache-druid-0.14.2-incubating/conf/druid \u5206\u522b\u5728broker,coordinator,historical,middleManager,overlord,router\u670d\u52a1\u8def\u5f84\u4e2d\u7684jvm.config\u6587\u4ef6\u4e2d\u52a0\u5165\u4e09\u6761\u914d\u7f6e\u9879 -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com \u767b\u9646druid\u7684\u914d\u7f6e\u8def\u5f84 /opt/druid/apache-druid-0.14.2-incubating/quickstart/tutorial/conf/druid , \u91cd\u590d\u4e0a\u8ff0\u6b65\u9aa4\u5206\u522b\u5728broker,coordinator,historical,middleManager,overlord,router\u670d\u52a1\u8def\u5f84\u4e2d\u7684jvm.config\u6587\u4ef6\u4e2d\u52a0\u5165\u4e09\u6761\u914d\u7f6e\u9879 -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com \u505c\u6b62\u4e4b\u524d\u8fd0\u884c\u4e2d\u7684druid \u4f7f\u7528\u547d\u4ee4 source /opt/hadoopclient/bigdata_env \u52a0\u8f7d\u96c6\u7fa4\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u547d\u4ee4 export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dsun.security.krb5.debug=true -Dkerberos.domain.name=hadoop.hadoop.com\" \u52a0\u8f7d\u8fd0\u884cjava\u6574\u4f53jvm\u53c2\u6570\uff0c\u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u7ed3\u679c\uff1a \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.121:8888/ \u70b9\u51fbTasks\u627e\u5230Supervisor\uff1a \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21007\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21007\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21007,172.16.6.12:21007,172.16.6.10:21007\", \"kerberos.domain.name\": \"hadoop.hadoop.com\", \"security.protocol\": \"SASL_PLAINTEXT\", \"sasl.kerberos.service.name\": \"kafka\" } } } \u540e\u53f0\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4Kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8\u5b89\u5168\u6a21\u5f0fkafka producer bin/kafka-console-producer.sh --broker-list 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --topic wikipedia21007 --producer.config config/producer.properties \u63d2\u5165\u4e00\u6761\u6570\u636e\uff0c\u5185\u5bb9\u5982\u4e0b {\"time\":\"2015-09-12T05:22:32.338Z\",\"channel\":\"#zh.wikipedia\",\"cityName\":null,\"comment\":\"/* \u6210\u7acb */\",\"countryIsoCode\":null,\"countryName\":null,\"isAnonymous\":false,\"isMinor\":false,\"isNew\":false,\"isRobot\":false,\"isUnpatrolled\":false,\"metroCode\":null,\"namespace\":\"Main\",\"page\":\"\u8056\u4f2f\u591a\u797f\u53f8\u9438\u5144\u5f1f\u6703\",\"regionIsoCode\":null,\"regionName\":null,\"user\":\"\u91d1\u8085\",\"delta\":675,\"added\":675,\"deleted\":0} \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT * FROM wikipedia21007","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u5b89\u5168\u6a21\u5f0f"},{"location":"Database/apache_druid_0.15.1/","text":"Apache Druid 0.15.1 \u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Druid 0.15.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD 2.8 \u73af\u5883\u63cf\u8ff0 \u00b6 FI HD\u4e3b\u673a\u4e09\u53f0\uff1a 172.16.6.10 - 12 Druid\u90e8\u7f72\u4e3b\u673a\uff1a 172.16.2.120 \u51c6\u5907\u5de5\u4f5c \u00b6 FI HD\u96c6\u7fa4\u76f8\u5173\u51c6\u5907 \u00b6 \u53c2\u8003\u4ea7\u54c1\u6587\u6863\u5b8c\u6210FI HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 \u4e0b\u8f7d\u51c6\u5907\u597d\u7684\u7528\u6237developuser\u76f8\u5173\u7684user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5MySQL \u00b6 Druid \u7684\u5143\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u672c\u6587\u9009\u7528\u81ea\u5df1\u642d\u5efa\u7684MySQL\u6570\u636e\u5e93\uff0c\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5MySQL\u6570\u636e\u5e93 \u767b\u5f55 https://downloads.mysql.com/archives/community/ \uff0c \u5728 Product Version \u4e2d\u9009\u62e9 5.7.27\uff0cOperating System\u8bf7\u9009\u62e9Linux-Generic\uff0c\u4e0b\u8f7d\u793e\u533a\u7248MySQL\u8f6f\u4ef6\u5305\u3002 \u4ee5root\u7528\u6237\u767b\u5f55\u5f85\u5b89\u88c5\u7684\u670d\u52a1\u5668 \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5e76\u89e3\u538b\u3002 \u4ee5root\u7528\u6237\u901a\u8fc7sftp/ftp\u5de5\u5177\u4e0a\u4f20\u201cmysql-5.7.24-linux-glibc2.12-x86_64.tar.gz\u201d\u8f6f\u4ef6\u5305\u5230\u201c/opt\u201d\u76ee\u5f55 \u3002 \u8fdb\u5165opt\u76ee\u5f55\uff0c\u5e76\u89e3\u538b\u7f29\u8f6f\u4ef6\u5305\u3002 cd /opt/ tar -xzvf mysql-5.7.27-linux-glibc2.12-x86_64.tar.gz \u5c06\u89e3\u538b\u540e\u76ee\u5f55\u6539\u540d\u4e3amysql\u3002 mv mysql-5.7.27-linux-glibc2.12-x86_64 mysql \u521b\u5efa\u7528\u6237\u548c\u7528\u6237\u7ec4\uff0c\u5e76\u8fdb\u884c\u6388\u6743\u3002 \u6dfb\u52a0mysql\u7ec4\u3002 groupadd mysql \u6dfb\u52a0mysql\u7528\u6237\u3002 useradd -d /home/mysql -s /bin/bash -g mysql -m mysql \u628amysql\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql \u5728\u6570\u636e\u76d8\u76ee\u5f55\u4e0b\uff08\u5982/data01\uff09\uff0c\u521b\u5efamysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55tmp\u3002 mkdir /opt/mysql-data mkdir /opt/mysql-data/tmp mkdir /opt/mysql-data/log \u628amysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7ec4\u4e2d\u7684mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql-data mysql-data\u76ee\u5f55\u7684\u6240\u5c5e\u7fa4\u7ec4\u4fee\u6539\u4e3amysql\u3002 chgrp -R mysql /opt/mysql-data \u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 \u5728mysql\u76ee\u5f55\u4e0b\u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 vi /opt/mysql/my.cnf \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u6309\u5982\u4e0b\u8981\u6c42\u4fee\u6539\u6587\u4ef6\u5185\u5bb9\uff0c\u4fee\u6539\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002\u5176\u4e2d\uff0c\u201cbind-address\u201d\u53c2\u6570\u8bf7\u4fee\u6539\u4e3aMySQL\u670d\u52a1\u5668\u7684\u5730\u5740\u3002 [mysqld] basedir = /opt/mysql bind-address = 172.16.2.120 datadir = /opt/mysql-data/workdbs tmpdir = /opt/mysql-data/tmp/ port = 3306 socket =/opt/mysql/lib/mysql.sock lower_case_table_names=1 character-set-server = utf8 max_allowed_packet = 150M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,STRICT_ALL_TABLES log-error=/opt/mysql-data/log/mysql_3306.log max_connections=1000 event_scheduler=ON [mysql] default-character-set = utf8 socket =/opt/mysql/lib/mysql.sock \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4fee\u6539my.cnf\u6587\u4ef6\u7684\u5c5e\u4e3b\u3002 chown mysql:mysql /opt/mysql/my.cnf \u62f7\u8d1dmy.cnf\u6587\u4ef6\u5230etc\u76ee\u5f55\u4e0b\u3002 cp -fr /opt/mysql/my.cnf /etc/my.cnf \u4fee\u6539\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6profile\u3002 \u7f16\u8f91etc\u76ee\u5f55\u4e0b\u7684\u201cprofile\u201d\u6587\u4ef6\u3002 vi /etc/profile \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u5728\u6587\u4ef6\u672b\u5c3e\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a export PATH=$PATH:/opt/mysql/bin export PATH=$PATH:/etc/init.d\u6dfb\u52a0\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002 \u91cd\u65b0\u52a0\u8f7detc\u76ee\u5f55\u4e0b\u7684profile\u6587\u4ef6\u3002 source /etc/profile \u5c06mysql.server\u590d\u5236\u5230/etc/init.d/ \u3002 cd /opt/mysql cp -a ./support-files/mysql.server /etc/init.d/mysql.server \u521d\u59cb\u5316mysql cd /opt/mysql ./bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql-data/workdbs \u547d\u4ee4\u6267\u884c\u540e\uff0c\u5982\u65e0\u9519\u8bef\uff0c\u4e0d\u4f1a\u6709\u663e\u793a\u4fe1\u606f\uff0c\u67e5\u770b\u65e5\u5fd7\u6587\u4ef6\u201c/opt/mysql-data/log/mysql_3306.log\u201d\uff0c\u83b7\u53d6\u4e34\u65f6\u5bc6\u7801\u3002 cat /data01/mysql-data/log/mysql_3306.log \u521b\u5efa\u8f6f\u8fde\u63a5\u3002 \u5c06mysql\u7684\u5b89\u88c5\u76ee\u5f55\u8f6f\u8fde\u63a5\u5230local\u4e0b\u9762\u3002 ln -s /opt/mysql /usr/local/mysql \u5c06mysql.sock\u6587\u4ef6\u8f6f\u8fde\u63a5\u5230tmp\u4e0b\u9762 ln -s /opt/mysql/lib/mysql.sock /tmp/mysql.sock \u6ce8\u518c\u5e76\u8bbe\u7f6emysql.server\u670d\u52a1\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u3002 systemctl enable mysql.server.service \u67e5\u770bMySQL\u72b6\u6001\u3002 mysql.server status \u5728 opt/mysql/bin \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u767b\u5f55MySQL\u3002 cd /opt/mysql/bin mysql -u root -p \u6309\u7167\u63d0\u793a\u4fe1\u606f\u8f93\u5165\u8bb0\u5f55\u7684\u4e34\u65f6\u5bc6\u7801\u3002 Enter Password\uff1a\u767b\u5f55\u6210\u529f\u540e\u7cfb\u7edf\u663e\u793a\u5982\u4e0b\u7c7b\u4f3c\u4fe1\u606f\uff1a \u4fee\u6539root\u7528\u6237\u5bc6\u7801\u3002 mysql> set password=password('Password'); \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u8d4b\u4e88\u4efb\u4f55\u4e3b\u673a\u8bbf\u95ee\u6570\u636e\u7684\u6743\u9650\u3002 mysql> grant all privileges on *.* to 'root'@'%' identified by 'Password' with grant option; \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u4f7f\u4fee\u6539\u751f\u6548\u5e76\u4f7f\u7528\u6570\u636e\u5e93\u3002 mysql> flush privileges; mysql> use mysql; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadruid\u5143\u6570\u636e\u5b58\u50a8\u7684database mysql -u root -e \"CREATE DATABASE druid CHARACTER SET utf8 COLLATE utf8_general_ci\" -p \u5b8c\u6210\u5b89\u88c5\uff0c\u90e8\u7f72\uff0c\u9000\u51faMySQL\u6570\u636e\u5e93\u3002 mysql> exit \u5b89\u88c5\uff0c\u90e8\u7f72Druid \u00b6 \u767b\u5f55\u5982\u4e0b\u7f51\u5740\u9009\u62e9\u76f8\u5173Druid\u7248\u672c\u4e0b\u8f7d\uff1a https://druid.apache.org/downloads.html \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5230/opt/druid\u76ee\u5f55\u4e0b tar -xvf apache-druid-0.15.1-incubating-bin.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 Druid \u9700\u8981\u4f7f\u7528zookeeper\u670d\u52a1\u4f5c\u4e3a\u81ea\u5df1\u672c\u8eabdistributed coordination\u670d\u52a1\u7684\u4f9d\u8d56\uff0c\u6240\u4ee5\u5728\u4f7f\u7528druid\u4e4b\u524d\u9700\u8981\u63d0\u524d\u90e8\u7f72zookeeper\u670d\u52a1\uff0c\u672c\u6587\u4f7f\u7528\u5f00\u6e90zookeeper\u670d\u52a1\u4f5c\u4e3adruid\u7684\u4f9d\u8d56\uff0c\u800c\u4e0d\u4f7f\u7528FI HD\u672c\u771f\u7684zookeeper\u670d\u52a1 \u767b\u5f55druid\u5b89\u88c5\u76ee\u5f55 cd /opt/druid/apache-druid-0.15.1-incubating \u4e0b\u8f7d\u5f00\u6e90\u7684zookeeper\uff0c\u5e76\u4e14\u6539\u540d\u4e3azk curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz tar -xzf zookeeper-3.4.11.tar.gz mv zookeeper-3.4.11 zk \u52a0\u8f7dFI HD\u5ba2\u6237\u7aef\u73af\u5883 source /opt/hadoopclient/bigdata_env \u68c0\u67e5druid\u5b89\u88c5\u4e3b\u673a\u540c\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8druid bin/start-micro-quickstart \u5f85\u5168\u90e8\u670d\u52a1\u542f\u52a8\u540e\uff0c\u767b\u5f55172.16.2.120:8888 web\u754c\u9762\u67e5\u770bdruid \u5b8c\u6210\u540e Ctrl+C \u505c\u6b62druid \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4HDFS \u00b6 \u8bf4\u660e\uff1a \u53c2\u8003Druid\u5b98\u65b9\u6587\u6863\uff0c\u914d\u7f6eFI HD\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u4e3aDruid\u7684Deep Storage,\u5e76\u4e14\u4f7f\u7528FI HD\u7684yarn\u670d\u52a1\u4ee5\u53caMapreduce\u670d\u52a1\u6765\u6279\u91cf\u5c06\u5b58\u50a8\u5728HDFS\u4e0a\u7684\u6570\u636e\u5bfc\u5165Druid\u6570\u636e\u5e93 Deep Storage\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/development/extensions-core/hdfs.html Hadoop\u6279\u5904\u7406\u6570\u636e\u5bfc\u5165\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/ingestion/hadoop.html \u4eceFI HD\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u5230\u914d\u7f6e\u6587\u4ef6core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a core-site.xml: \u5c06\u9ed8\u8ba4\u914d\u7f6e\u9879 <property> <name>fs.defaultFS</name> <value>hdfs://hacluster</value> </property> \u66f4\u6539\u4e3a\u4e3bNamenode\u8282\u70b9IP + \u7aef\u53e3\u5f62\u5f0f\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://172.16.6.12:25000</value> </property> hdfs-site.xml: \u5220\u9664\u5982\u4e0b\u8fd9\u4e2a\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u5c06\u4e0a\u9762\u6b65\u9aa4\u7684core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230druid\u5982\u4e0b\u4e24\u4e2a\u8def\u5f84\u4e0b\uff1a /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common \u4fee\u6539druid\u914d\u7f6e\u6587\u4ef6 /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties \u589e\u52a0 druid.extensions.loadList=[\"druid-hdfs-storage\", \"mysql-metadata-storage\"] \u4fee\u6539zookeeper\u914d\u7f6e\u9879\u5982\u4e0b\uff1a druid.zk.service.host=localhost druid.zk.paths.base=/druid Druid\u5143\u6570\u636e\u5b58\u50a8\u6539\u4e3a\u4e4b\u524d\u914d\u7f6e\u597d\u7684MySQL\u6570\u636e\u5e93 \u914d\u7f6ehdfs Deep Storage\u76f8\u5173\u53c2\u6570\uff1a druid.storage.type=hdfs druid.storage.storageDirectory=hdfs://172.16.6.12:25000/druid120/segments \u914d\u7f6ehadoop indexer\u4ee5\u53cakerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a druid.indexer.logs.type=hdfs druid.indexer.logs.directory=hdfs://172.16.6.12:25000/druid120/indexing-logs druid.hadoop.security.kerberos.principal=developuser@HADOOP.COM druid.hadoop.security.kerberos.keytab=/opt/101hdclient/user.keytab \u5176\u4e2ddevelopuser\u4e3a\u96c6\u7fa4\u521b\u5efa\u7684\u7528\u6237\uff0cuser.keytab\u4e3a\u4e0b\u8f7d\u7684developuser\u8ba4\u8bc1\u6587\u4ef6 \u5c06\u4e0b\u8f7d\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230druid\u670d\u52a1\u5668 /etc/ \u8def\u5f84\u4e0b\uff08\u9ed8\u8ba4\u5728\u6b64\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff09 cp /opt/user_keytabs/101keytab/krb5.conf /etc \u767b\u5f55 /opt/druid/apache-druid-0.15.1-incubating/extensions/mysql-metadata-storage \u8def\u5f84\uff0c\u5bfc\u5165mysql\u8fde\u63a5\u9a71\u52a8 mysql-connector-java-5.1.48.jar\uff0c \u9a71\u52a8jar\u5305\u53ef\u5728mysql\u5b98\u65b9\u7f51\u7ad9\u83b7\u53d6 \u767b\u5f55druid extension\u8def\u5f84\u4e0b\u627e\u5230druid-hdfs-storage\u4f9d\u8d56\u8def\u5f84\uff0c\u6539\u540d\u5907\u4efd cd /opt/druid/apache-druid-0.15.1-incubating/extensions mv druid-hdfs-storage/ druid-hdfs-storage-backup/ \u53e6\u884c\u521b\u5efadruid-hdfs-storage\u6587\u4ef6\u5939\uff0c\u4eceFI HD\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c druid-hdfs-storage\u81ea\u5e26\u7684jar\u5305\u4e2d\u6536\u96c6\u5e76\u5bfc\u5165\u5982\u4e0b\u4f9d\u8d56jar\u5305\uff1a \u6ce8\uff1ahdfs\u76f8\u5173jar\u5305\u4e00\u5b9a\u662f\u4eceFI HD\u4e0b\u8f7d\u7684\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar asm-3.2.jar avro-1.7.4.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-collections-3.2.2.jar commons-compress-1.16.jar commons-configuration-1.6.jar commons-daemon-1.0.13.jar commons-digester-1.8.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar commons-net-3.1.jar curator-framework-4.1.0.jar curator-recipes-4.1.0.jar druid-hdfs-storage-0.15.1-incubating.jar dynalogger-V100R002C30.jar gson-2.2.4.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client \u8def\u5f84\u4e0b\uff0c\u521b\u5efa\u8def\u5f84 2.7.2 cd /opt/druid/apache-druid-0.15.1-incubating/hadoop-dependencies/hadoop-client mkdir 2.7.2 \u6309\u7167\u5982\u4e0b\u5217\u8868\u51c6\u59072.7.2\u8def\u5f84\u4e0b\u7684\u4f9d\u8d56Jar\u5305 asm-3.2.jar avro-1.7.4.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-daemon-1.0.13.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar dynalogger-V100R002C30.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar javaluator-3.0.1.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-6.1.26.jar jetty-util-6.1.26.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar \u521b\u5efadruid\u9700\u8981\u7684spec\u6587\u4ef6\u5e76\u653e\u5230druid\u76ee\u5f55\u4e0b\uff1awikipedia-index-hadoop.json \u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\" : { \"format\" : \"auto\", \"column\" : \"time\" } } }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/data/wikiticker-2015-09-12-sampled.json.gz\" } }, \"tuningConfig\" : { \"type\" : \"hadoop\", \"partitionsSpec\" : { \"type\" : \"hashed\", \"targetPartitionSize\" : 5000000 }, \"forceExtendableShardSpecs\" : true, \"jobProperties\" : { \"fs.default.name\" : \"hdfs://172.16.6.12:25000\", \"fs.defaultFS\" : \"hdfs://172.16.6.12:25000\", \"dfs.datanode.address\" : \"HD03\", \"dfs.client.use.datanode.hostname\" : \"true\", \"dfs.datanode.use.datanode.hostname\" : \"true\", \"yarn.resourcemanager.hostname\" : \"HD03\", \"yarn.nodemanager.vmem-check-enabled\" : \"false\", \"mapreduce.map.java.opts\" : \"-Duser.timezone=UTC -Dfile.encoding=UTF-8\", \"mapreduce.job.user.classpath.first\" : \"true\", \"mapreduce.reduce.java.opts\" : \"-Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8\", \"mapreduce.map.memory.mb\" : 1024, \"mapreduce.reduce.memory.mb\" : 1024 } } }, \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.2\"] } \u767b\u5f55FI HD\u96c6\u7fa4,\u5728HDFS\u7684/data\u76ee\u5f55\u4e0b\u4f20\u5165\u6570\u636e\u6587\u4ef6 wikiticker-2015-09-12-sampled.json.gz\uff0c \u8be5\u6570\u636e\u6587\u4ef6\u53ef\u4ee5\u5728 /opt/druid/apache-druid-0.15.1-incubating/quickstart/tutorial \u4e0b\u83b7\u53d6 \u540c\u65f6\u68c0\u67e5HDFS\u662f\u5426\u5b58\u5728 /druid120/indexing-logs \u4ee5\u53ca /druid120/segments \uff0c\u82e5\u6ca1\u6709\u8981\u521b\u5efa\u597d \u4f7f\u7528\u547d\u4ee4 bin/start-micro-quickstart \u542f\u52a8druid \u5f85druid\u6240\u6709\u670d\u52a1\u542f\u52a8\u540e\uff0c\u5f00\u542f\u53e6\u4e00\u7ec8\u7aef\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u63d0\u4ea4hadoop index\u4f5c\u4e1a\uff0c\u7b49\u5f85\u4f5c\u4e1a\u5b8c\u6210 bin/post-index-task --file /opt/druid/apache-druid-0.15.1-incubating/wikipedia-index-hadoop.json --url http://172.16.2.120:8081 \u767b\u5f55\u5bf9\u63a5FI HD\u96c6\u7fa4yarn\u670d\u52a1\u67e5\u770b\u4efb\u52a1\uff1a \u6ce8\uff1a\u4e00\u6b21hadoop index\u4f5c\u4e1a\u4f1a\u5728yarn\u4e0a\u8d77\u4e24\u4e2amap reduce\u4efb\u52a1 \u767b\u5f55druid web\u754c\u9762\u5728Tasks\u9762\u677f\u4e0b\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff0c\u65e5\u5fd7\uff1a \u5728Datasources\u4e0b\u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e\uff1a SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u666e\u901a\u6a21\u5f0f \u00b6 \u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21005 \u521b\u5efatopic wikipedia21005 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.120:8888/ \u70b9\u51fbsupervisor \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21005\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21005\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21005,172.16.6.12:21005,172.16.6.10:21005\" } } } \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef\uff0c\u628a\u6d4b\u8bd5\u6570\u636ewikiticker-2015-09-12-sampled.json\u4e0a\u4f20\u5230kafka\u5ba2\u6237\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f80topic wikipedia21005\u5199\u5165\u6570\u636e cd /opt/hadoopclient/Kafka/kafka ./bin/kafka-console-producer.sh --broker-list 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 --topic wikipedia21005 < /opt/wikiticker-2015-09-12-sampled.json --producer.config config/producer.properties \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c\uff1a \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT page, COUNT(*) AS Edits FROM wikipedia21005 WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u5207\u6362\u56de\u5bf9\u63a5kafka\u5ba2\u6237\u7aef\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770btopic wikipedia21005\u91cc\u9762\u7684\u6570\u636e\uff1a bin/kafka-console-consumer.sh --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --topic wikipedia21005 --from-beginning \u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u5b89\u5168\u6a21\u5f0f \u00b6 \u8bf4\u660e\uff1aDruid Kafka\u7248\u672c\u4e3a2.1.0\uff0c FI HD\u7248\u672c\u6700\u9ad8\u4e3a1.1.0\uff0c\u7248\u672c\u4e0d\u9002\u914d\u6240\u4ee5\u6682\u4e0d\u652f\u6301kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5","title":"0.15.1 <--> C80"},{"location":"Database/apache_druid_0.15.1/#apache-druid-0151-fusioninsight","text":"","title":"Apache Druid 0.15.1 \u5bf9\u63a5FusionInsight"},{"location":"Database/apache_druid_0.15.1/#_1","text":"Apache Druid 0.15.1 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Database/apache_druid_0.15.1/#_2","text":"\u5df2\u5b8c\u6210FusionInsight\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u7248\u672cFusionInsight HD 2.8","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Database/apache_druid_0.15.1/#_3","text":"FI HD\u4e3b\u673a\u4e09\u53f0\uff1a 172.16.6.10 - 12 Druid\u90e8\u7f72\u4e3b\u673a\uff1a 172.16.2.120","title":"\u73af\u5883\u63cf\u8ff0"},{"location":"Database/apache_druid_0.15.1/#_4","text":"","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Database/apache_druid_0.15.1/#fi-hd","text":"\u53c2\u8003\u4ea7\u54c1\u6587\u6863\u5b8c\u6210FI HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 \u4e0b\u8f7d\u51c6\u5907\u597d\u7684\u7528\u6237developuser\u76f8\u5173\u7684user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6","title":"FI HD\u96c6\u7fa4\u76f8\u5173\u51c6\u5907"},{"location":"Database/apache_druid_0.15.1/#mysql","text":"Druid \u7684\u5143\u6570\u636e\u9700\u8981\u5b58\u50a8\uff0c\u672c\u6587\u9009\u7528\u81ea\u5df1\u642d\u5efa\u7684MySQL\u6570\u636e\u5e93\uff0c\u4e0b\u9762\u4ecb\u7ecd\u5982\u4f55\u5b89\u88c5MySQL\u6570\u636e\u5e93 \u767b\u5f55 https://downloads.mysql.com/archives/community/ \uff0c \u5728 Product Version \u4e2d\u9009\u62e9 5.7.27\uff0cOperating System\u8bf7\u9009\u62e9Linux-Generic\uff0c\u4e0b\u8f7d\u793e\u533a\u7248MySQL\u8f6f\u4ef6\u5305\u3002 \u4ee5root\u7528\u6237\u767b\u5f55\u5f85\u5b89\u88c5\u7684\u670d\u52a1\u5668 \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5e76\u89e3\u538b\u3002 \u4ee5root\u7528\u6237\u901a\u8fc7sftp/ftp\u5de5\u5177\u4e0a\u4f20\u201cmysql-5.7.24-linux-glibc2.12-x86_64.tar.gz\u201d\u8f6f\u4ef6\u5305\u5230\u201c/opt\u201d\u76ee\u5f55 \u3002 \u8fdb\u5165opt\u76ee\u5f55\uff0c\u5e76\u89e3\u538b\u7f29\u8f6f\u4ef6\u5305\u3002 cd /opt/ tar -xzvf mysql-5.7.27-linux-glibc2.12-x86_64.tar.gz \u5c06\u89e3\u538b\u540e\u76ee\u5f55\u6539\u540d\u4e3amysql\u3002 mv mysql-5.7.27-linux-glibc2.12-x86_64 mysql \u521b\u5efa\u7528\u6237\u548c\u7528\u6237\u7ec4\uff0c\u5e76\u8fdb\u884c\u6388\u6743\u3002 \u6dfb\u52a0mysql\u7ec4\u3002 groupadd mysql \u6dfb\u52a0mysql\u7528\u6237\u3002 useradd -d /home/mysql -s /bin/bash -g mysql -m mysql \u628amysql\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql \u5728\u6570\u636e\u76d8\u76ee\u5f55\u4e0b\uff08\u5982/data01\uff09\uff0c\u521b\u5efamysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55tmp\u3002 mkdir /opt/mysql-data mkdir /opt/mysql-data/tmp mkdir /opt/mysql-data/log \u628amysql-data\u76ee\u5f55\u53ca\u5b50\u76ee\u5f55\u6388\u6743\u7ed9mysql\u7ec4\u4e2d\u7684mysql\u7528\u6237\u3002 chown -R mysql:mysql /opt/mysql-data mysql-data\u76ee\u5f55\u7684\u6240\u5c5e\u7fa4\u7ec4\u4fee\u6539\u4e3amysql\u3002 chgrp -R mysql /opt/mysql-data \u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 \u5728mysql\u76ee\u5f55\u4e0b\u65b0\u5efa\u5e76\u7f16\u8f91my.cnf\u6587\u4ef6\u3002 vi /opt/mysql/my.cnf \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u6309\u5982\u4e0b\u8981\u6c42\u4fee\u6539\u6587\u4ef6\u5185\u5bb9\uff0c\u4fee\u6539\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002\u5176\u4e2d\uff0c\u201cbind-address\u201d\u53c2\u6570\u8bf7\u4fee\u6539\u4e3aMySQL\u670d\u52a1\u5668\u7684\u5730\u5740\u3002 [mysqld] basedir = /opt/mysql bind-address = 172.16.2.120 datadir = /opt/mysql-data/workdbs tmpdir = /opt/mysql-data/tmp/ port = 3306 socket =/opt/mysql/lib/mysql.sock lower_case_table_names=1 character-set-server = utf8 max_allowed_packet = 150M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,STRICT_ALL_TABLES log-error=/opt/mysql-data/log/mysql_3306.log max_connections=1000 event_scheduler=ON [mysql] default-character-set = utf8 socket =/opt/mysql/lib/mysql.sock \u6267\u884c\u5982\u4e0b\u547d\u4ee4\uff0c\u4fee\u6539my.cnf\u6587\u4ef6\u7684\u5c5e\u4e3b\u3002 chown mysql:mysql /opt/mysql/my.cnf \u62f7\u8d1dmy.cnf\u6587\u4ef6\u5230etc\u76ee\u5f55\u4e0b\u3002 cp -fr /opt/mysql/my.cnf /etc/my.cnf \u4fee\u6539\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6profile\u3002 \u7f16\u8f91etc\u76ee\u5f55\u4e0b\u7684\u201cprofile\u201d\u6587\u4ef6\u3002 vi /etc/profile \u8f93\u5165i\u8fdb\u5165\u7f16\u8f91\u6a21\u5f0f\uff0c\u5728\u6587\u4ef6\u672b\u5c3e\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9\uff1a export PATH=$PATH:/opt/mysql/bin export PATH=$PATH:/etc/init.d\u6dfb\u52a0\u5b8c\u6210\u540e\u6309Esc\u9000\u51fa\u7f16\u8f91\u6a21\u5f0f\uff0c\u6267\u884c:wq!\u4fdd\u5b58\u5e76\u9000\u51fa\u3002 \u91cd\u65b0\u52a0\u8f7detc\u76ee\u5f55\u4e0b\u7684profile\u6587\u4ef6\u3002 source /etc/profile \u5c06mysql.server\u590d\u5236\u5230/etc/init.d/ \u3002 cd /opt/mysql cp -a ./support-files/mysql.server /etc/init.d/mysql.server \u521d\u59cb\u5316mysql cd /opt/mysql ./bin/mysqld --initialize --user=mysql --basedir=/opt/mysql/ --datadir=/opt/mysql-data/workdbs \u547d\u4ee4\u6267\u884c\u540e\uff0c\u5982\u65e0\u9519\u8bef\uff0c\u4e0d\u4f1a\u6709\u663e\u793a\u4fe1\u606f\uff0c\u67e5\u770b\u65e5\u5fd7\u6587\u4ef6\u201c/opt/mysql-data/log/mysql_3306.log\u201d\uff0c\u83b7\u53d6\u4e34\u65f6\u5bc6\u7801\u3002 cat /data01/mysql-data/log/mysql_3306.log \u521b\u5efa\u8f6f\u8fde\u63a5\u3002 \u5c06mysql\u7684\u5b89\u88c5\u76ee\u5f55\u8f6f\u8fde\u63a5\u5230local\u4e0b\u9762\u3002 ln -s /opt/mysql /usr/local/mysql \u5c06mysql.sock\u6587\u4ef6\u8f6f\u8fde\u63a5\u5230tmp\u4e0b\u9762 ln -s /opt/mysql/lib/mysql.sock /tmp/mysql.sock \u6ce8\u518c\u5e76\u8bbe\u7f6emysql.server\u670d\u52a1\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u3002 systemctl enable mysql.server.service \u67e5\u770bMySQL\u72b6\u6001\u3002 mysql.server status \u5728 opt/mysql/bin \u76ee\u5f55\u4e0b\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u767b\u5f55MySQL\u3002 cd /opt/mysql/bin mysql -u root -p \u6309\u7167\u63d0\u793a\u4fe1\u606f\u8f93\u5165\u8bb0\u5f55\u7684\u4e34\u65f6\u5bc6\u7801\u3002 Enter Password\uff1a\u767b\u5f55\u6210\u529f\u540e\u7cfb\u7edf\u663e\u793a\u5982\u4e0b\u7c7b\u4f3c\u4fe1\u606f\uff1a \u4fee\u6539root\u7528\u6237\u5bc6\u7801\u3002 mysql> set password=password('Password'); \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u8d4b\u4e88\u4efb\u4f55\u4e3b\u673a\u8bbf\u95ee\u6570\u636e\u7684\u6743\u9650\u3002 mysql> grant all privileges on *.* to 'root'@'%' identified by 'Password' with grant option; \u5176\u4e2d\uff0c\u5355\u5f15\u53f7\u4e2d\u7684Password\u7531\u7528\u6237\u81ea\u5b9a\u4e49\u3002 \u4f7f\u4fee\u6539\u751f\u6548\u5e76\u4f7f\u7528\u6570\u636e\u5e93\u3002 mysql> flush privileges; mysql> use mysql; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efadruid\u5143\u6570\u636e\u5b58\u50a8\u7684database mysql -u root -e \"CREATE DATABASE druid CHARACTER SET utf8 COLLATE utf8_general_ci\" -p \u5b8c\u6210\u5b89\u88c5\uff0c\u90e8\u7f72\uff0c\u9000\u51faMySQL\u6570\u636e\u5e93\u3002 mysql> exit","title":"\u5b89\u88c5MySQL"},{"location":"Database/apache_druid_0.15.1/#druid","text":"\u767b\u5f55\u5982\u4e0b\u7f51\u5740\u9009\u62e9\u76f8\u5173Druid\u7248\u672c\u4e0b\u8f7d\uff1a https://druid.apache.org/downloads.html \u4e0a\u4f20\u8f6f\u4ef6\u5305\u5230/opt/druid\u76ee\u5f55\u4e0b tar -xvf apache-druid-0.15.1-incubating-bin.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 Druid \u9700\u8981\u4f7f\u7528zookeeper\u670d\u52a1\u4f5c\u4e3a\u81ea\u5df1\u672c\u8eabdistributed coordination\u670d\u52a1\u7684\u4f9d\u8d56\uff0c\u6240\u4ee5\u5728\u4f7f\u7528druid\u4e4b\u524d\u9700\u8981\u63d0\u524d\u90e8\u7f72zookeeper\u670d\u52a1\uff0c\u672c\u6587\u4f7f\u7528\u5f00\u6e90zookeeper\u670d\u52a1\u4f5c\u4e3adruid\u7684\u4f9d\u8d56\uff0c\u800c\u4e0d\u4f7f\u7528FI HD\u672c\u771f\u7684zookeeper\u670d\u52a1 \u767b\u5f55druid\u5b89\u88c5\u76ee\u5f55 cd /opt/druid/apache-druid-0.15.1-incubating \u4e0b\u8f7d\u5f00\u6e90\u7684zookeeper\uff0c\u5e76\u4e14\u6539\u540d\u4e3azk curl https://archive.apache.org/dist/zookeeper/zookeeper-3.4.11/zookeeper-3.4.11.tar.gz -o zookeeper-3.4.11.tar.gz tar -xzf zookeeper-3.4.11.tar.gz mv zookeeper-3.4.11 zk \u52a0\u8f7dFI HD\u5ba2\u6237\u7aef\u73af\u5883 source /opt/hadoopclient/bigdata_env \u68c0\u67e5druid\u5b89\u88c5\u4e3b\u673a\u540c\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8druid bin/start-micro-quickstart \u5f85\u5168\u90e8\u670d\u52a1\u542f\u52a8\u540e\uff0c\u767b\u5f55172.16.2.120:8888 web\u754c\u9762\u67e5\u770bdruid \u5b8c\u6210\u540e Ctrl+C \u505c\u6b62druid","title":"\u5b89\u88c5\uff0c\u90e8\u7f72Druid"},{"location":"Database/apache_druid_0.15.1/#druidfi-hdhdfs","text":"\u8bf4\u660e\uff1a \u53c2\u8003Druid\u5b98\u65b9\u6587\u6863\uff0c\u914d\u7f6eFI HD\u96c6\u7fa4\u7684HDFS\u670d\u52a1\u4e3aDruid\u7684Deep Storage,\u5e76\u4e14\u4f7f\u7528FI HD\u7684yarn\u670d\u52a1\u4ee5\u53caMapreduce\u670d\u52a1\u6765\u6279\u91cf\u5c06\u5b58\u50a8\u5728HDFS\u4e0a\u7684\u6570\u636e\u5bfc\u5165Druid\u6570\u636e\u5e93 Deep Storage\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/development/extensions-core/hdfs.html Hadoop\u6279\u5904\u7406\u6570\u636e\u5bfc\u5165\u76f8\u5173\u6587\u6863\u8fde\u63a5\uff1a https://druid.apache.org/docs/latest/ingestion/hadoop.html \u4eceFI HD\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6\u5230\u914d\u7f6e\u6587\u4ef6core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a core-site.xml: \u5c06\u9ed8\u8ba4\u914d\u7f6e\u9879 <property> <name>fs.defaultFS</name> <value>hdfs://hacluster</value> </property> \u66f4\u6539\u4e3a\u4e3bNamenode\u8282\u70b9IP + \u7aef\u53e3\u5f62\u5f0f\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://172.16.6.12:25000</value> </property> hdfs-site.xml: \u5220\u9664\u5982\u4e0b\u8fd9\u4e2a\u914d\u7f6e\u9879 <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.BlackListingFailoverProxyProvider</value> </property> \u5c06\u4e0a\u9762\u6b65\u9aa4\u7684core-site.xml\uff0c hdfs-site.xml\uff0c mapred-site.xml\uff0c yarn-site.xml\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230druid\u5982\u4e0b\u4e24\u4e2a\u8def\u5f84\u4e0b\uff1a /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common \u4fee\u6539druid\u914d\u7f6e\u6587\u4ef6 /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties \u589e\u52a0 druid.extensions.loadList=[\"druid-hdfs-storage\", \"mysql-metadata-storage\"] \u4fee\u6539zookeeper\u914d\u7f6e\u9879\u5982\u4e0b\uff1a druid.zk.service.host=localhost druid.zk.paths.base=/druid Druid\u5143\u6570\u636e\u5b58\u50a8\u6539\u4e3a\u4e4b\u524d\u914d\u7f6e\u597d\u7684MySQL\u6570\u636e\u5e93 \u914d\u7f6ehdfs Deep Storage\u76f8\u5173\u53c2\u6570\uff1a druid.storage.type=hdfs druid.storage.storageDirectory=hdfs://172.16.6.12:25000/druid120/segments \u914d\u7f6ehadoop indexer\u4ee5\u53cakerberos\u8ba4\u8bc1\u76f8\u5173\u53c2\u6570\uff1a druid.indexer.logs.type=hdfs druid.indexer.logs.directory=hdfs://172.16.6.12:25000/druid120/indexing-logs druid.hadoop.security.kerberos.principal=developuser@HADOOP.COM druid.hadoop.security.kerberos.keytab=/opt/101hdclient/user.keytab \u5176\u4e2ddevelopuser\u4e3a\u96c6\u7fa4\u521b\u5efa\u7684\u7528\u6237\uff0cuser.keytab\u4e3a\u4e0b\u8f7d\u7684developuser\u8ba4\u8bc1\u6587\u4ef6 \u5c06\u4e0b\u8f7d\u7684krb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230druid\u670d\u52a1\u5668 /etc/ \u8def\u5f84\u4e0b\uff08\u9ed8\u8ba4\u5728\u6b64\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff09 cp /opt/user_keytabs/101keytab/krb5.conf /etc \u767b\u5f55 /opt/druid/apache-druid-0.15.1-incubating/extensions/mysql-metadata-storage \u8def\u5f84\uff0c\u5bfc\u5165mysql\u8fde\u63a5\u9a71\u52a8 mysql-connector-java-5.1.48.jar\uff0c \u9a71\u52a8jar\u5305\u53ef\u5728mysql\u5b98\u65b9\u7f51\u7ad9\u83b7\u53d6 \u767b\u5f55druid extension\u8def\u5f84\u4e0b\u627e\u5230druid-hdfs-storage\u4f9d\u8d56\u8def\u5f84\uff0c\u6539\u540d\u5907\u4efd cd /opt/druid/apache-druid-0.15.1-incubating/extensions mv druid-hdfs-storage/ druid-hdfs-storage-backup/ \u53e6\u884c\u521b\u5efadruid-hdfs-storage\u6587\u4ef6\u5939\uff0c\u4eceFI HD\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c druid-hdfs-storage\u81ea\u5e26\u7684jar\u5305\u4e2d\u6536\u96c6\u5e76\u5bfc\u5165\u5982\u4e0b\u4f9d\u8d56jar\u5305\uff1a \u6ce8\uff1ahdfs\u76f8\u5173jar\u5305\u4e00\u5b9a\u662f\u4eceFI HD\u4e0b\u8f7d\u7684\u5ba2\u6237\u7aef\u4e2d\u83b7\u53d6 apacheds-i18n-2.0.0-M15.jar apacheds-kerberos-codec-2.0.0-M15.jar api-asn1-api-1.0.0-M20.jar api-util-1.0.0-M20.jar asm-3.2.jar avro-1.7.4.jar commons-beanutils-1.7.0.jar commons-beanutils-core-1.8.0.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-collections-3.2.2.jar commons-compress-1.16.jar commons-configuration-1.6.jar commons-daemon-1.0.13.jar commons-digester-1.8.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar commons-net-3.1.jar curator-framework-4.1.0.jar curator-recipes-4.1.0.jar druid-hdfs-storage-0.15.1-incubating.jar dynalogger-V100R002C30.jar gson-2.2.4.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar \u767b\u5f55 /opt/druid/apache-druid-0.14.2-incubating/hadoop-dependencies/hadoop-client \u8def\u5f84\u4e0b\uff0c\u521b\u5efa\u8def\u5f84 2.7.2 cd /opt/druid/apache-druid-0.15.1-incubating/hadoop-dependencies/hadoop-client mkdir 2.7.2 \u6309\u7167\u5982\u4e0b\u5217\u8868\u51c6\u59072.7.2\u8def\u5f84\u4e0b\u7684\u4f9d\u8d56Jar\u5305 asm-3.2.jar avro-1.7.4.jar commons-cli-1.2.jar commons-codec-1.4.jar commons-daemon-1.0.13.jar commons-io-2.4.jar commons-lang-2.6.jar commons-logging-1.1.3.jar dynalogger-V100R002C30.jar guava-11.0.2.jar hadoop-annotations-2.7.2.jar hadoop-auth-2.7.2.jar hadoop-client-2.7.2.jar hadoop-common-2.7.2.jar hadoop-hdfs-2.7.2.jar hadoop-hdfs-client-2.7.2.jar hadoop-hdfs-colocation-2.7.2.jar hadoop-hdfs-datamovement-2.7.2.jar hadoop-hdfs-nfs-2.7.2.jar hadoop-mapreduce-client-app-2.7.2.jar hadoop-mapreduce-client-common-2.7.2.jar hadoop-mapreduce-client-core-2.7.2.jar hadoop-mapreduce-client-jobclient-2.7.2.jar hadoop-mapreduce-client-shuffle-2.7.2.jar hadoop-yarn-api-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar hadoop-yarn-server-common-2.7.2.jar hdfs-inode-provider-2.7.2.jar hdfs-nodelabel-provider-2.7.2.jar htrace-core-3.1.0-incubating.jar jackson-core-asl-1.9.13.jar jackson-mapper-asl-1.9.13.jar javaluator-3.0.1.jar jersey-core-1.9.jar jersey-server-1.9.jar jetty-6.1.26.jar jetty-util-6.1.26.jar jsr305-3.0.0.jar leveldbjni-all-1.8.jar log4j-1.2.17.jar netty-3.6.2.Final.jar netty-all-4.0.23.Final.jar \u521b\u5efadruid\u9700\u8981\u7684spec\u6587\u4ef6\u5e76\u653e\u5230druid\u76ee\u5f55\u4e0b\uff1awikipedia-index-hadoop.json \u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\" : \"index_hadoop\", \"spec\" : { \"dataSchema\" : { \"dataSource\" : \"wikipedia\", \"parser\" : { \"type\" : \"hadoopyString\", \"parseSpec\" : { \"format\" : \"json\", \"dimensionsSpec\" : { \"dimensions\" : [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] }, \"timestampSpec\" : { \"format\" : \"auto\", \"column\" : \"time\" } } }, \"metricsSpec\" : [], \"granularitySpec\" : { \"type\" : \"uniform\", \"segmentGranularity\" : \"day\", \"queryGranularity\" : \"none\", \"intervals\" : [\"2015-09-12/2015-09-13\"], \"rollup\" : false } }, \"ioConfig\" : { \"type\" : \"hadoop\", \"inputSpec\" : { \"type\" : \"static\", \"paths\" : \"/data/wikiticker-2015-09-12-sampled.json.gz\" } }, \"tuningConfig\" : { \"type\" : \"hadoop\", \"partitionsSpec\" : { \"type\" : \"hashed\", \"targetPartitionSize\" : 5000000 }, \"forceExtendableShardSpecs\" : true, \"jobProperties\" : { \"fs.default.name\" : \"hdfs://172.16.6.12:25000\", \"fs.defaultFS\" : \"hdfs://172.16.6.12:25000\", \"dfs.datanode.address\" : \"HD03\", \"dfs.client.use.datanode.hostname\" : \"true\", \"dfs.datanode.use.datanode.hostname\" : \"true\", \"yarn.resourcemanager.hostname\" : \"HD03\", \"yarn.nodemanager.vmem-check-enabled\" : \"false\", \"mapreduce.map.java.opts\" : \"-Duser.timezone=UTC -Dfile.encoding=UTF-8\", \"mapreduce.job.user.classpath.first\" : \"true\", \"mapreduce.reduce.java.opts\" : \"-Duser.timezone=UTC+0800 -Dfile.encoding=UTF-8\", \"mapreduce.map.memory.mb\" : 1024, \"mapreduce.reduce.memory.mb\" : 1024 } } }, \"hadoopDependencyCoordinates\": [\"org.apache.hadoop:hadoop-client:2.7.2\"] } \u767b\u5f55FI HD\u96c6\u7fa4,\u5728HDFS\u7684/data\u76ee\u5f55\u4e0b\u4f20\u5165\u6570\u636e\u6587\u4ef6 wikiticker-2015-09-12-sampled.json.gz\uff0c \u8be5\u6570\u636e\u6587\u4ef6\u53ef\u4ee5\u5728 /opt/druid/apache-druid-0.15.1-incubating/quickstart/tutorial \u4e0b\u83b7\u53d6 \u540c\u65f6\u68c0\u67e5HDFS\u662f\u5426\u5b58\u5728 /druid120/indexing-logs \u4ee5\u53ca /druid120/segments \uff0c\u82e5\u6ca1\u6709\u8981\u521b\u5efa\u597d \u4f7f\u7528\u547d\u4ee4 bin/start-micro-quickstart \u542f\u52a8druid \u5f85druid\u6240\u6709\u670d\u52a1\u542f\u52a8\u540e\uff0c\u5f00\u542f\u53e6\u4e00\u7ec8\u7aef\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u63d0\u4ea4hadoop index\u4f5c\u4e1a\uff0c\u7b49\u5f85\u4f5c\u4e1a\u5b8c\u6210 bin/post-index-task --file /opt/druid/apache-druid-0.15.1-incubating/wikipedia-index-hadoop.json --url http://172.16.2.120:8081 \u767b\u5f55\u5bf9\u63a5FI HD\u96c6\u7fa4yarn\u670d\u52a1\u67e5\u770b\u4efb\u52a1\uff1a \u6ce8\uff1a\u4e00\u6b21hadoop index\u4f5c\u4e1a\u4f1a\u5728yarn\u4e0a\u8d77\u4e24\u4e2amap reduce\u4efb\u52a1 \u767b\u5f55druid web\u754c\u9762\u5728Tasks\u9762\u677f\u4e0b\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff0c\u65e5\u5fd7\uff1a \u5728Datasources\u4e0b\u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e\uff1a SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4HDFS"},{"location":"Database/apache_druid_0.15.1/#druidfi-hdkafka","text":"\u8bf4\u660e\uff1a\u53c2\u8003Druid\u5b98\u65b9\u6587\u6863,\u4f7f\u7528druid\u7684kafka index\u670d\u52a1\u4ecekafka topic\u4e2d\u5bfc\u5165\u6d41\u6570\u636e\u5230druid\u4e2d \u53c2\u8003\u6587\u6863\uff1a https://druid.apache.org/docs/latest/tutorials/tutorial-kafka.html \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef,\u4f7f\u7528\u547d\u4ee4 bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic wikipedia21005 \u521b\u5efatopic wikipedia21005 \u767b\u9646druid\u4e3b\u673a\uff0c\u4fee\u6539 /opt/druid/apache-druid-0.15.1-incubating/conf/druid/single-server/micro-quickstart/_common/common.runtime.properties \u914d\u7f6e\u6587\u4ef6,\u589e\u52a0druid-kafka-indexing-service \u91cd\u542fdruid \u767b\u9646druid\u7684web ui\u754c\u9762 http://172.16.2.120:8888/ \u70b9\u51fbsupervisor \u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684supervisor \u8f93\u5165\u7684\u5185\u5bb9\u4e3a { \"type\": \"kafka\", \"dataSchema\": { \"dataSource\": \"wikipedia21005\", \"parser\": { \"type\": \"string\", \"parseSpec\": { \"format\": \"json\", \"timestampSpec\": { \"column\": \"time\", \"format\": \"auto\" }, \"dimensionsSpec\": { \"dimensions\": [ \"channel\", \"cityName\", \"comment\", \"countryIsoCode\", \"countryName\", \"isAnonymous\", \"isMinor\", \"isNew\", \"isRobot\", \"isUnpatrolled\", \"metroCode\", \"namespace\", \"page\", \"regionIsoCode\", \"regionName\", \"user\", { \"name\": \"added\", \"type\": \"long\" }, { \"name\": \"deleted\", \"type\": \"long\" }, { \"name\": \"delta\", \"type\": \"long\" } ] } } }, \"metricsSpec\" : [], \"granularitySpec\": { \"type\": \"uniform\", \"segmentGranularity\": \"DAY\", \"queryGranularity\": \"NONE\", \"rollup\": false } }, \"tuningConfig\": { \"type\": \"kafka\", \"reportParseExceptions\": false }, \"ioConfig\": { \"topic\": \"wikipedia21005\", \"replicas\": 2, \"taskDuration\": \"PT10M\", \"completionTimeout\": \"PT20M\", \"consumerProperties\": { \"bootstrap.servers\": \"172.16.6.11:21005,172.16.6.12:21005,172.16.6.10:21005\" } } } \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u7684kafka\u5ba2\u6237\u7aef\uff0c\u628a\u6d4b\u8bd5\u6570\u636ewikiticker-2015-09-12-sampled.json\u4e0a\u4f20\u5230kafka\u5ba2\u6237\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f80topic wikipedia21005\u5199\u5165\u6570\u636e cd /opt/hadoopclient/Kafka/kafka ./bin/kafka-console-producer.sh --broker-list 172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005 --topic wikipedia21005 < /opt/wikiticker-2015-09-12-sampled.json --producer.config config/producer.properties \u56de\u5230druid web ui \u5230druid datasource\u4e0b\u68c0\u67e5\u7ed3\u679c\uff1a \u70b9\u51fbGo to SQL\u4f7f\u7528\u547d\u4ee4\u67e5\u8be2\u5bfc\u5165\u7684\u6570\u636e SELECT page, COUNT(*) AS Edits FROM wikipedia21005 WHERE \"__time\" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10 \u5207\u6362\u56de\u5bf9\u63a5kafka\u5ba2\u6237\u7aef\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770btopic wikipedia21005\u91cc\u9762\u7684\u6570\u636e\uff1a bin/kafka-console-consumer.sh --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --topic wikipedia21005 --from-beginning","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u666e\u901a\u6a21\u5f0f"},{"location":"Database/apache_druid_0.15.1/#druidfi-hdkafka_1","text":"\u8bf4\u660e\uff1aDruid Kafka\u7248\u672c\u4e3a2.1.0\uff0c FI HD\u7248\u672c\u6700\u9ad8\u4e3a1.1.0\uff0c\u7248\u672c\u4e0d\u9002\u914d\u6240\u4ee5\u6682\u4e0d\u652f\u6301kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5","title":"\u914d\u7f6edruid\u5bf9\u63a5FI HD\u96c6\u7fa4Kafka\u5b89\u5168\u6a21\u5f0f"},{"location":"Database/%E6%9D%AD%E5%B7%9E%E5%90%88%E4%BC%97UDB/","text":"\u676d\u5dde\u5408\u4f17UDB\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 \u676d\u5dde\u5408\u4f17UDB 6.1 \u2194 FusionInsight HD V100R002C50 (GaussDB)","title":"6.1 <--> C50"},{"location":"Database/%E6%9D%AD%E5%B7%9E%E5%90%88%E4%BC%97UDB/#udbfusioninsight","text":"","title":"\u676d\u5dde\u5408\u4f17UDB\u5bf9\u63a5FusionInsight"},{"location":"Database/%E6%9D%AD%E5%B7%9E%E5%90%88%E4%BC%97UDB/#_1","text":"\u676d\u5dde\u5408\u4f17UDB 6.1 \u2194 FusionInsight HD V100R002C50 (GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/","text":"\u96c6\u6210\u5f00\u53d1\u73af\u5883 \u00b6 Anaconda 2-2019.03-Linux-x86_64 \u2194 6.5 2-2019.03-Linux-x86_64 \u2194 8.0 DBeaver 4.0.8 \u2194 C60 4.2.1 \u2194 C70 6.1.4 \u2194 6.5 6.3.4 \u2194 6.5 6.3.4 \u2194 8.0 DbVisualizer 10.0.1 \u2194 C70 10.0.21 \u2194 6.5 10.0.21 \u2194 8.0 9.5.7 \u2194 C60 HUE 4.0.1 \u2194 C70 Jupyter Notebook 2.4.4.0 \u2194 C70 2.7.16 \u2194 C80 2.7.16 \u2194 6.5 2.7.16 \u2194 8.0 JupyterHub 1.0.0 \u2194 C80 1.0.0 \u2194 8.0 RStudio 3.4.1 \u2194 C60 3.4.1 \u2194 C70 3.4.1 \u2194 8.0 Squirrel 3.7.1 \u2194 C60 3.8.0 \u2194 C70 3.9.1 \u2194 6.5 3.9.1 \u2194 8.0 Zeppelin 0.7.2 \u2194 C60 0.7.3 \u2194 C70 0.7.3 \u2194 C80 0.8.0 \u2194 C80 0.8.1 \u2194 6.5 0.9.0 \u2194 8.0","title":"Index"},{"location":"Development/#_1","text":"Anaconda 2-2019.03-Linux-x86_64 \u2194 6.5 2-2019.03-Linux-x86_64 \u2194 8.0 DBeaver 4.0.8 \u2194 C60 4.2.1 \u2194 C70 6.1.4 \u2194 6.5 6.3.4 \u2194 6.5 6.3.4 \u2194 8.0 DbVisualizer 10.0.1 \u2194 C70 10.0.21 \u2194 6.5 10.0.21 \u2194 8.0 9.5.7 \u2194 C60 HUE 4.0.1 \u2194 C70 Jupyter Notebook 2.4.4.0 \u2194 C70 2.7.16 \u2194 C80 2.7.16 \u2194 6.5 2.7.16 \u2194 8.0 JupyterHub 1.0.0 \u2194 C80 1.0.0 \u2194 8.0 RStudio 3.4.1 \u2194 C60 3.4.1 \u2194 C70 3.4.1 \u2194 8.0 Squirrel 3.7.1 \u2194 C60 3.8.0 \u2194 C70 3.9.1 \u2194 6.5 3.9.1 \u2194 8.0 Zeppelin 0.7.2 \u2194 C60 0.7.3 \u2194 C70 0.7.3 \u2194 C80 0.8.0 \u2194 C80 0.8.1 \u2194 6.5 0.9.0 \u2194 8.0","title":"\u96c6\u6210\u5f00\u53d1\u73af\u5883"},{"location":"Development/Anaconda/","text":"Aanconda\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Anaconda 2-2019.03-Linux-x86_64 \u2194 FusionInsight HD 6.5 (Spark2x) Anaconda 2-2019.03-Linux-x86_64 \u2194 FusionInsight HD 8.0 (Spark2x) \u4ea7\u54c1\u4ecb\u7ecd \u00b6 Anaconda Anaconda \u662f\u4e00\u4e2a\u514d\u8d39\uff0c\u6613\u4e8e\u5b89\u88c5\u7684\u8f6f\u4ef6\u5305\u7ba1\u7406\u5668\uff0c\u73af\u5883\u7ba1\u7406\u5668\u548cpython\u73af\u5883\u7ba1\u7406\u8f6f\u4ef6\uff0c\u5176\u4e2d\u5305\u542b1,000\u591a\u4e2a\u5e26\u6709\u514d\u8d39\u793e\u533a\u652f\u6301\u7684\u5f00\u6e90\u8f6f\u4ef6\u5305\u3002 Anaconda\u53ef\u4ee5\u90e8\u7f72\u5728Windows\uff0cmacOS\u4ee5\u53caLinux\u4e0a\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u767b\u5f55\u5b98\u7f51\u4e86\u89e3: Anaconda \u6d4b\u8bd5\u73af\u5883\u7269\u7406\u62d3\u6251\u7ed3\u6784\u56fe \u00b6 \u8bf4\u660e\uff1a\u5982\u679c\u76f4\u63a5\u5728\u5e26\u6709\u56fe\u5f62\u754c\u9762\u7684\u5ba2\u6237\u7aef\u4e3b\u673a\u5de5\u4f5c\uff0c\u53ef\u4ee5\u4e0d\u7528\u5b89\u88c5vnc\u5ba2\u6237\u7aef\u4ee5\u53cavnc Server \u6d4b\u8bd5\u73af\u5883\u76f8\u5173\u4ea7\u54c1\u7248\u672c \u00b6 Anaconda2-2019.03-Linux-x86_64 FusionInsight HD 6.5.1 Spark 2.3.2 R version 3.5.1 (2018-07-02) RStudio 1.1.456 Python 2.7.16 |Anaconda, Inc.| (default, Mar 14 2019, 21:00:58) Jypyter Notebook 5.7.8 Anaconda\u540cFusionInsight HD\u4ea4\u4e92\u7684\u591a\u79cd\u65b9\u5f0f \u00b6 (1) \u4f7f\u7528R\u8bed\u8a00 RStudio \u65b9\u68481\uff1a\u4f7f\u7528sparklyr\u5728RStudio\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u8bf4\u660e\uff1a\u901a\u8fc7\u4f7f\u7528sparklyr\u6765\u5bf9\u822a\u7a7a\u516c\u53f8\u822a\u73ed\u8fdb\u884c\u6570\u636e\u5206\u6790\uff0c\u53ef\u67e5\u770b\u5728\u7ebfdemo http://rpubs.com/jinbnie/513252 \u65b9\u68482: \u4f7f\u7528SparkR\u5728RStudio\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u8bf4\u660e:\u4f7f\u7528SparkR\u8bfb\u53d6\u5bf9\u63a5\u96c6\u7fa4hive\u8868\u91cc\u7684\u6570\u636e Jupyter Notebook \u65b9\u68483\uff1a\u4f7f\u7528sparklyr\u5728jupyter notebook\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u8bf4\u660e: \u4e0e\u65b9\u68481\u7684R\u4ee3\u7801\u4e00\u6837\uff0c\u552f\u4e00\u7684\u533a\u522b\u662f\u4ea4\u4e92\u73af\u5883\u6362\u6210\u4e86jupyter notebook (2) \u4f7f\u7528python\u8bed\u8a00 Jupyter Notebook \u65b9\u68484\uff1a\u5728jupyter notebook\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06python\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u8bf4\u660e: \u4f7f\u7528python\u63a5\u53e3\uff0c\u8bfb\u53d6\u5bf9\u63a5\u96c6\u7fa4hdfs\u4e2d\u7684\u6570\u636e\uff0c\u5b8c\u6210\u4e00\u6b21\u7ecf\u5178\u7684spark word count\u6f14\u793a\u6837\u4f8b \u524d\u63d0\u6761\u4ef6 \u00b6 FusionInsight HD 6.5 \u73af\u5883\u5b89\u88c5\u5b8c\u6210 linux \u5ba2\u6237\u7aef\u4e3b\u673aroot\u6743\u9650 \u6d4b\u8bd5\u73af\u5883\u76f8\u5173windows\u8df3\u677f\u673a\uff0clinux\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u5bf9\u63a5FI HD\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a \u6d4b\u8bd5\u73af\u5883\u51c6\u5907 \u00b6 1. \u5728Linux\u5ba2\u6237\u7aef\u8282\u70b9\u4e0a\u5b89\u88c5\u548c\u914d\u7f6eVNC\u670d\u52a1\u5668\uff08\u5982\u679c\u53ef\u76f4\u63a5\u5728GUI\u5ba2\u6237\u7aef\u73af\u5883\u4e2d\u5de5\u4f5c\u6b64\u6b65\u9aa4\u53ef\u7701\u7565\uff09 \u8bf7\u53c2\u8003\uff1a How to Install and Configure VNC Server on CentOS 7 2.\u5b89\u88c5 Anaconda \u53c2\u8003Anaconda\u5b98\u65b9\u6587\u6863\uff1a https://docs.anaconda.com/anaconda/install/linux/ \u4e0b\u8f7dAnaconda\u5b89\u88c5\u5305 wget https://repo.anaconda.com/archive/Anaconda2-2019.03-Linux-x86_64.sh Enter the following to install Anaconda for Python 2.7: \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5Anaconda \u8bf4\u660e ~/.bashrc \u6587\u4ef6\u4f1a\u5728anaconda\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u521d\u59cb\u5316\uff0c\u8bf7\u5728\u5b89\u88c5\u524d\u5907\u4efd\u6b64\u914d\u7f6e\u6587\u4ef6 cp ~/.bashrc ~/.bashrc.bak bash Anaconda2-2019.03-Linux-x86_64.sh \u5b89\u88c5\u7a0b\u5e8f\u5c06\u63d0\u793a\u201cIn order to continue the installation process, please review the license agreement.\u201d\u5355\u51fbEnter\u4ee5\u67e5\u770b\u8bb8\u53ef\u6761\u6b3e\u3002 \u6eda\u52a8\u5230\u8bb8\u53ef\u6761\u6b3e\u7684\u5e95\u90e8\uff0c\u7136\u540e\u8f93\u5165\u201cyes\u201d\u4ee5\u8868\u793a\u540c\u610f\u3002 \u5b89\u88c5\u7a0b\u5e8f\u63d0\u793a\u60a8\u5355\u51fbEnter\u63a5\u53d7\u9ed8\u8ba4\u5b89\u88c5\u4f4d\u7f6e\uff0c\u6309CTRL-C\u53d6\u6d88\u5b89\u88c5\uff0c\u6216\u6307\u5b9a\u5907\u7528\u5b89\u88c5\u76ee\u5f55\u3002\u5982\u679c\u60a8\u63a5\u53d7\u9ed8\u8ba4\u5b89\u88c5\u4f4d\u7f6e\uff0c\u5219\u5b89\u88c5\u7a0b\u5e8f\u5c06\u663e\u793a\u201cPREFIX=/home/{username}/anaconda<2 or 3>\u201d\u5e76\u7ee7\u7eed\u5b89\u88c5\u3002\u53ef\u80fd\u9700\u8981\u51e0\u5206\u949f\u624d\u80fd\u5b8c\u6210\u3002 \u6ce8\u610f\uff1a \u63a8\u8350\u4f7f\u7528\u9ed8\u8ba4\u5b89\u88c5\u4f4d\u7f6e\uff0c\u5982\u679c\u4f7f\u7528\u975eroot\u7528\u6237\u8fdb\u884c\u4ea4\u4e92\uff0c\u8bf7\u5b89\u88c5\u5728\u5176\u4ed6\u53ef\u8bbf\u95ee\u7684\u8def\u5f84\uff0c\u6bd4\u5982/opt\uff0c\u5426\u5219\u4e4b\u540e\u4f1a\u62a5\u5173\u4e8e\u7528\u6237\u6743\u9650\u7684\u95ee\u9898 \u5b89\u88c5\u7a0b\u5e8f\u63d0\u793a\u201cDo you wish the installer to initialize Anaconda2 by running conda init?\u201d\u63a8\u8350\u586b\u5199 \u201cyes\u201d\u3002 \u5b89\u88c5\u5b8c\u6210\uff0c\u4f1a\u663e\u793a \u201cThank you for installing Anaconda<2 or 3>!\u201d \u5b8c\u6210\u540e\u4f1a\u63d0\u4f9b\u4e00\u4e2aPyCharm\u5173\u4e8eAnaconda\u7684\u5b89\u88c5\u5305\u94fe\u63a5 https://www.anaconda.com/pycharm . \u5173\u95ed\u5e76\u6253\u5f00\u7ec8\u7aef\u7a97\u53e3\uff0c\u4ee5\u4f7f\u5b89\u88c5\u751f\u6548\uff0c mv ~/.bashrc ~/.bashrc.anaconda cp ~/.bashrc.bak ~/.bashrc source ~/.bashrc.anaconda 3.\u521b\u5efaR\u4ea4\u4e92\u73af\u5883\u5e76\u4e14\u5b89\u88c5\u3001\u8fd0\u884cRStudio \u53c2\u8003\uff1a http://docs.anaconda.com/anaconda/navigator/tutorials/create-r-environment/ vi ~/.condarc \u53ef\u4ee5\u5c06\u5982\u4e0b\u5185\u5bb9\u6dfb\u52a0\u5230 ~/.condarc \u6587\u4ef6\u4e2d\u6765\u589e\u52a0conda\u76f8\u5173\u5305\u7684\u4e0b\u8f7d\u901f\u5ea6 channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaults show_channel_urls: true \u542f\u52a8 anaconda navigator anaconda-navigator & \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7f16\u8bd1\u73af\u5883\uff0c\u6bd4\u5982\u8d77\u540d\u4e3arenv2 Python :2.7 R:r \u56de\u5230Home, \u5b89\u88c5 rstudio \u542f\u52a8 RStudio conda env list conda activate renv2 rstudio & 4.\u5b89\u88c5 FusionInsight HD \u5ba2\u6237\u7aef \u6253\u5f00\u6d4f\u89c8\u5668\uff0c\u767b\u5f55FusionInsight Mananger\uff0c\u7136\u540e\u4e0b\u8f7d\u5ba2\u6237\u7aef \u5728Linux\u5ba2\u6237\u7aef\u8282\u70b9\u4e0a\u4ee5root\u8eab\u4efd\u767b\u5f55\uff0c\u8fd0\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u4ee5\u5b89\u88c5HD\u5ba2\u6237\u7aef su \u2013 root scp root@172.16.4.32:/tmp/FusionInsight-Client/FusionInsight_Services_Client.tar /opt/FusionInsight_Services_Client172.16.4.35.tar tar xvf FusionInsight_Services_Client172.16.4.35.tar tar xvf FusionInsight_Services_ClientConfig.tar cd FusionInsight_Services_ClientConfig/ ./install.sh /opt/client172.16.4.35/ \u5c06\u5ba2\u6237\u7aef\u8282\u70b9ip\uff0c\u4e3b\u673a\u540d\u52a0\u5165\u5bf9\u63a5\u96c6\u7fa4\u6240\u6709\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\u4e0b\uff0c\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\u8d77spark\u4efb\u52a1\u7684\u65f6\u5019\uff0cworker\u8282\u70b9\uff08\u96c6\u7fa4\u8282\u70b9\uff09\u80fd\u591f\u8bc6\u522b\u5230driver\u4e3b\u673a\uff08\u5ba2\u6237\u7aef\u8282\u70b9\uff09\uff1a \u542f\u52a8 RStudio su - huawei cd /opt/client172.16.4.35/ source bigdata_env kinit developuser klist source ~/.bashrc.anaconda conda activate renv2 rstudio & \u65b9\u68481\uff1a\u4f7f\u7528sparklyr\u5728RStudio\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u00b6 \u8bf4\u660e\uff1a\u901a\u8fc7\u4f7f\u7528sparklyr\u6765\u5bf9\u822a\u7a7a\u516c\u53f8\u822a\u73ed\u8fdb\u884c\u6570\u636e\u5206\u6790\uff0c\u53ef\u67e5\u770b\u5728\u7ebfdemo http://rpubs.com/jinbnie/513252 \u6ce8\u610f\uff1a \u4ee5\u4e0b 7 \u6b65\u7684\u6e90\u7801\u53ef\u4ee5\u5728\u5982\u4e0b\u94fe\u63a5\u4e2d\u83b7\u53d6\uff1a RStudio \u7b2c5,7\u6b65\u8fdb\u884c\u76f8\u5173\u9002\u914d\u53ef\u4ee5\u5bf9\u63a5 FI HD 6.5(Spark 2.3.2). Step 1 \u5b89\u88c5\u76f8\u5173\u5305 install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") Step 2 \u8fde\u63a5spark library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/opt/client172.16.4.35/JDK/jdk-8u201\") Sys.setenv(SPARK_HOME=\"/opt/client172.16.4.35/Spark2x/spark\") spark_version_from_home(Sys.getenv(\"SPARK_HOME\")) Sys.setenv(SPARK_HOME_VERSION=\"2.3.2\") sc <- spark_connect(master = \"yarn-client\", version = \"2.3.2\", spark_home = \"/opt/client172.16.4.35/Spark2x/spark\") Step 3 \u5c06\u63d0\u524d\u5efa\u597d\u7684hive\u8868\u5b58\u5165\u7f13\u5b58 \u8bf4\u660e\uff1a\u5177\u4f53\u6570\u636e\u51c6\u5907\u7684\u65b9\u6cd5\u8bf7\u53c2\u8003\u751f\u6001\u5730\u56fe->Rstudio->\u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e #Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl <- tbl(sc, 'flights') # Cache airlines Hive table into Spark tbl_cache(sc, 'airlines') airlines_tbl <- tbl(sc, 'airlines') # Cache airports Hive table into Spark tbl_cache(sc, 'airports') airports_tbl <- tbl(sc, 'airports') Step 4 \u5efa\u6a21 #Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year >= 2003 & year <= 2007) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Step 5 \u8bad\u7ec3\u6a21\u578b #Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition <- model_data %>% sdf_random_split(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 <- model_partition$train %>% ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) Step 6 \u8bc4\u4f30\u6a21\u578b\u8868\u73b0 #Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles <- lapply(model_partition, function(x) { ml_predict(ml1, x) %>% mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles <- rbind( data.frame(data = 'train', model_deciles$train), data.frame(data = 'valid', model_deciles$valid), make.row.names = FALSE ) # Plot average gains by predicted decile deciles %>% ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Step 7 \u5c06\u9884\u6d4b\u53ef\u89c6\u5316 #Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier <- ml_predict(ml1, data_2008) %>% group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') \u65b9\u68482: \u4f7f\u7528SparkR\u5728RStudio\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u00b6 \u8bf4\u660e\uff1a\u4f7f\u7528SparkR\u8bfb\u53d6\u9884\u5b58\u7684hive\u8868\u6570\u636e \u542f\u52a8 RStudio cd /opt/client172.16.4.35/ source ~/.bashrc.anaconda conda activate renv2 source bigdata_env kinit developuser klist rstudio & \u5728RStudio\u4e2d\u63d0\u4ea4\u5982\u4e0bR\u4ee3\u7801 if (nchar(Sys.getenv(\"SPARK_HOME\")) < 1) { Sys.setenv(SPARK_HOME=\"/opt/client172.16.4.35/Spark2x/spark\") } library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"))) sparkR.session(master = \"yarn\", sparkConfig = list(spark.driver.memory = \"2g\"),(spark.submit.deployMode=\"client\")) results <- sql(\"FROM airports SELECT id, name \") head(results) \u65b9\u68483\uff1a\u4f7f\u7528sparklyr\u5728jupyter notebook\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u00b6 step 1:\u542f\u52a8 Jupyter Notebook cd /opt/client172.16.4.35/ source bigdata_env kinit developuser source ~/.bashrc.anaconda conda env list conda activate renv2 sudo chmod -R 777 /opt/client172.16.4.35 jupyter-notebook & step 2:\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 R Notebook step 3:\u5728Jupyter Notebook\u63d0\u4ea4\u540c\u65b9\u68481\u4e2d\u76f8\u540c\u7684R\u4ee3\u7801\uff0c\u67e5\u770b\u7ed3\u679c \u65b9\u68484\uff1a\u5728jupyter notebook\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06python\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u00b6 \u8bf4\u660e: \u4f7f\u7528python\u63a5\u53e3\uff0c\u8bfb\u53d6\u5bf9\u63a5\u96c6\u7fa4hdfs\u4e2d\u7684\u6570\u636e\uff0c\u5b8c\u6210\u4e00\u6b21\u7ecf\u5178\u7684spark word count\u6f14\u793a\u6837\u4f8b step 1:\u542f\u52a8 PySpark cd /opt/client172.16.4.35/ source bigdata_env kinit developuser klist source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" pyspark --master yarn --deploy-mode client & step 2:\u521b\u5efa\u4e00\u4e2a\u65b0\u7684Python Notebook step3:\u5728Jupyter Notebook\u4e2d\u63d0\u4ea4\u5982\u4e0b\u7684python\u4ee3\u7801 # spark-wordcount.py from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() conf.setAppName('spark-wordcount_from172.16.5.103') sc = SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/user/developuser/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print 'Nonempty lines', nonempty_lines.count() words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print 'Top 100 words:' print wordcounts.take(100) \u67e5\u770b\u7ed3\u679c:","title":"2-2019.03-Linux-x86_64 <--> 8.0"},{"location":"Development/Anaconda/#aancondafusioninsight-hd","text":"","title":"Aanconda\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Anaconda/#_1","text":"Anaconda 2-2019.03-Linux-x86_64 \u2194 FusionInsight HD 6.5 (Spark2x) Anaconda 2-2019.03-Linux-x86_64 \u2194 FusionInsight HD 8.0 (Spark2x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Anaconda/#_2","text":"Anaconda Anaconda \u662f\u4e00\u4e2a\u514d\u8d39\uff0c\u6613\u4e8e\u5b89\u88c5\u7684\u8f6f\u4ef6\u5305\u7ba1\u7406\u5668\uff0c\u73af\u5883\u7ba1\u7406\u5668\u548cpython\u73af\u5883\u7ba1\u7406\u8f6f\u4ef6\uff0c\u5176\u4e2d\u5305\u542b1,000\u591a\u4e2a\u5e26\u6709\u514d\u8d39\u793e\u533a\u652f\u6301\u7684\u5f00\u6e90\u8f6f\u4ef6\u5305\u3002 Anaconda\u53ef\u4ee5\u90e8\u7f72\u5728Windows\uff0cmacOS\u4ee5\u53caLinux\u4e0a\u3002 \u66f4\u591a\u4fe1\u606f\u8bf7\u767b\u5f55\u5b98\u7f51\u4e86\u89e3: Anaconda","title":"\u4ea7\u54c1\u4ecb\u7ecd"},{"location":"Development/Anaconda/#_3","text":"\u8bf4\u660e\uff1a\u5982\u679c\u76f4\u63a5\u5728\u5e26\u6709\u56fe\u5f62\u754c\u9762\u7684\u5ba2\u6237\u7aef\u4e3b\u673a\u5de5\u4f5c\uff0c\u53ef\u4ee5\u4e0d\u7528\u5b89\u88c5vnc\u5ba2\u6237\u7aef\u4ee5\u53cavnc Server","title":"\u6d4b\u8bd5\u73af\u5883\u7269\u7406\u62d3\u6251\u7ed3\u6784\u56fe"},{"location":"Development/Anaconda/#_4","text":"Anaconda2-2019.03-Linux-x86_64 FusionInsight HD 6.5.1 Spark 2.3.2 R version 3.5.1 (2018-07-02) RStudio 1.1.456 Python 2.7.16 |Anaconda, Inc.| (default, Mar 14 2019, 21:00:58) Jypyter Notebook 5.7.8","title":"\u6d4b\u8bd5\u73af\u5883\u76f8\u5173\u4ea7\u54c1\u7248\u672c"},{"location":"Development/Anaconda/#anacondafusioninsight-hd","text":"(1) \u4f7f\u7528R\u8bed\u8a00 RStudio \u65b9\u68481\uff1a\u4f7f\u7528sparklyr\u5728RStudio\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u8bf4\u660e\uff1a\u901a\u8fc7\u4f7f\u7528sparklyr\u6765\u5bf9\u822a\u7a7a\u516c\u53f8\u822a\u73ed\u8fdb\u884c\u6570\u636e\u5206\u6790\uff0c\u53ef\u67e5\u770b\u5728\u7ebfdemo http://rpubs.com/jinbnie/513252 \u65b9\u68482: \u4f7f\u7528SparkR\u5728RStudio\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u8bf4\u660e:\u4f7f\u7528SparkR\u8bfb\u53d6\u5bf9\u63a5\u96c6\u7fa4hive\u8868\u91cc\u7684\u6570\u636e Jupyter Notebook \u65b9\u68483\uff1a\u4f7f\u7528sparklyr\u5728jupyter notebook\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u8bf4\u660e: \u4e0e\u65b9\u68481\u7684R\u4ee3\u7801\u4e00\u6837\uff0c\u552f\u4e00\u7684\u533a\u522b\u662f\u4ea4\u4e92\u73af\u5883\u6362\u6210\u4e86jupyter notebook (2) \u4f7f\u7528python\u8bed\u8a00 Jupyter Notebook \u65b9\u68484\uff1a\u5728jupyter notebook\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06python\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4 \u8bf4\u660e: \u4f7f\u7528python\u63a5\u53e3\uff0c\u8bfb\u53d6\u5bf9\u63a5\u96c6\u7fa4hdfs\u4e2d\u7684\u6570\u636e\uff0c\u5b8c\u6210\u4e00\u6b21\u7ecf\u5178\u7684spark word count\u6f14\u793a\u6837\u4f8b","title":"Anaconda\u540cFusionInsight HD\u4ea4\u4e92\u7684\u591a\u79cd\u65b9\u5f0f"},{"location":"Development/Anaconda/#_5","text":"FusionInsight HD 6.5 \u73af\u5883\u5b89\u88c5\u5b8c\u6210 linux \u5ba2\u6237\u7aef\u4e3b\u673aroot\u6743\u9650 \u6d4b\u8bd5\u73af\u5883\u76f8\u5173windows\u8df3\u677f\u673a\uff0clinux\u5ba2\u6237\u7aef\u4e3b\u673a\u4ee5\u53ca\u5bf9\u63a5FI HD\u96c6\u7fa4\u7f51\u7edc\u4e92\u901a","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Anaconda/#_6","text":"1. \u5728Linux\u5ba2\u6237\u7aef\u8282\u70b9\u4e0a\u5b89\u88c5\u548c\u914d\u7f6eVNC\u670d\u52a1\u5668\uff08\u5982\u679c\u53ef\u76f4\u63a5\u5728GUI\u5ba2\u6237\u7aef\u73af\u5883\u4e2d\u5de5\u4f5c\u6b64\u6b65\u9aa4\u53ef\u7701\u7565\uff09 \u8bf7\u53c2\u8003\uff1a How to Install and Configure VNC Server on CentOS 7 2.\u5b89\u88c5 Anaconda \u53c2\u8003Anaconda\u5b98\u65b9\u6587\u6863\uff1a https://docs.anaconda.com/anaconda/install/linux/ \u4e0b\u8f7dAnaconda\u5b89\u88c5\u5305 wget https://repo.anaconda.com/archive/Anaconda2-2019.03-Linux-x86_64.sh Enter the following to install Anaconda for Python 2.7: \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5Anaconda \u8bf4\u660e ~/.bashrc \u6587\u4ef6\u4f1a\u5728anaconda\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u521d\u59cb\u5316\uff0c\u8bf7\u5728\u5b89\u88c5\u524d\u5907\u4efd\u6b64\u914d\u7f6e\u6587\u4ef6 cp ~/.bashrc ~/.bashrc.bak bash Anaconda2-2019.03-Linux-x86_64.sh \u5b89\u88c5\u7a0b\u5e8f\u5c06\u63d0\u793a\u201cIn order to continue the installation process, please review the license agreement.\u201d\u5355\u51fbEnter\u4ee5\u67e5\u770b\u8bb8\u53ef\u6761\u6b3e\u3002 \u6eda\u52a8\u5230\u8bb8\u53ef\u6761\u6b3e\u7684\u5e95\u90e8\uff0c\u7136\u540e\u8f93\u5165\u201cyes\u201d\u4ee5\u8868\u793a\u540c\u610f\u3002 \u5b89\u88c5\u7a0b\u5e8f\u63d0\u793a\u60a8\u5355\u51fbEnter\u63a5\u53d7\u9ed8\u8ba4\u5b89\u88c5\u4f4d\u7f6e\uff0c\u6309CTRL-C\u53d6\u6d88\u5b89\u88c5\uff0c\u6216\u6307\u5b9a\u5907\u7528\u5b89\u88c5\u76ee\u5f55\u3002\u5982\u679c\u60a8\u63a5\u53d7\u9ed8\u8ba4\u5b89\u88c5\u4f4d\u7f6e\uff0c\u5219\u5b89\u88c5\u7a0b\u5e8f\u5c06\u663e\u793a\u201cPREFIX=/home/{username}/anaconda<2 or 3>\u201d\u5e76\u7ee7\u7eed\u5b89\u88c5\u3002\u53ef\u80fd\u9700\u8981\u51e0\u5206\u949f\u624d\u80fd\u5b8c\u6210\u3002 \u6ce8\u610f\uff1a \u63a8\u8350\u4f7f\u7528\u9ed8\u8ba4\u5b89\u88c5\u4f4d\u7f6e\uff0c\u5982\u679c\u4f7f\u7528\u975eroot\u7528\u6237\u8fdb\u884c\u4ea4\u4e92\uff0c\u8bf7\u5b89\u88c5\u5728\u5176\u4ed6\u53ef\u8bbf\u95ee\u7684\u8def\u5f84\uff0c\u6bd4\u5982/opt\uff0c\u5426\u5219\u4e4b\u540e\u4f1a\u62a5\u5173\u4e8e\u7528\u6237\u6743\u9650\u7684\u95ee\u9898 \u5b89\u88c5\u7a0b\u5e8f\u63d0\u793a\u201cDo you wish the installer to initialize Anaconda2 by running conda init?\u201d\u63a8\u8350\u586b\u5199 \u201cyes\u201d\u3002 \u5b89\u88c5\u5b8c\u6210\uff0c\u4f1a\u663e\u793a \u201cThank you for installing Anaconda<2 or 3>!\u201d \u5b8c\u6210\u540e\u4f1a\u63d0\u4f9b\u4e00\u4e2aPyCharm\u5173\u4e8eAnaconda\u7684\u5b89\u88c5\u5305\u94fe\u63a5 https://www.anaconda.com/pycharm . \u5173\u95ed\u5e76\u6253\u5f00\u7ec8\u7aef\u7a97\u53e3\uff0c\u4ee5\u4f7f\u5b89\u88c5\u751f\u6548\uff0c mv ~/.bashrc ~/.bashrc.anaconda cp ~/.bashrc.bak ~/.bashrc source ~/.bashrc.anaconda 3.\u521b\u5efaR\u4ea4\u4e92\u73af\u5883\u5e76\u4e14\u5b89\u88c5\u3001\u8fd0\u884cRStudio \u53c2\u8003\uff1a http://docs.anaconda.com/anaconda/navigator/tutorials/create-r-environment/ vi ~/.condarc \u53ef\u4ee5\u5c06\u5982\u4e0b\u5185\u5bb9\u6dfb\u52a0\u5230 ~/.condarc \u6587\u4ef6\u4e2d\u6765\u589e\u52a0conda\u76f8\u5173\u5305\u7684\u4e0b\u8f7d\u901f\u5ea6 channels: - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/ - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaults show_channel_urls: true \u542f\u52a8 anaconda navigator anaconda-navigator & \u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u7f16\u8bd1\u73af\u5883\uff0c\u6bd4\u5982\u8d77\u540d\u4e3arenv2 Python :2.7 R:r \u56de\u5230Home, \u5b89\u88c5 rstudio \u542f\u52a8 RStudio conda env list conda activate renv2 rstudio & 4.\u5b89\u88c5 FusionInsight HD \u5ba2\u6237\u7aef \u6253\u5f00\u6d4f\u89c8\u5668\uff0c\u767b\u5f55FusionInsight Mananger\uff0c\u7136\u540e\u4e0b\u8f7d\u5ba2\u6237\u7aef \u5728Linux\u5ba2\u6237\u7aef\u8282\u70b9\u4e0a\u4ee5root\u8eab\u4efd\u767b\u5f55\uff0c\u8fd0\u884c\u4e0b\u9762\u7684\u547d\u4ee4\u4ee5\u5b89\u88c5HD\u5ba2\u6237\u7aef su \u2013 root scp root@172.16.4.32:/tmp/FusionInsight-Client/FusionInsight_Services_Client.tar /opt/FusionInsight_Services_Client172.16.4.35.tar tar xvf FusionInsight_Services_Client172.16.4.35.tar tar xvf FusionInsight_Services_ClientConfig.tar cd FusionInsight_Services_ClientConfig/ ./install.sh /opt/client172.16.4.35/ \u5c06\u5ba2\u6237\u7aef\u8282\u70b9ip\uff0c\u4e3b\u673a\u540d\u52a0\u5165\u5bf9\u63a5\u96c6\u7fa4\u6240\u6709\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\u4e0b\uff0c\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\u8d77spark\u4efb\u52a1\u7684\u65f6\u5019\uff0cworker\u8282\u70b9\uff08\u96c6\u7fa4\u8282\u70b9\uff09\u80fd\u591f\u8bc6\u522b\u5230driver\u4e3b\u673a\uff08\u5ba2\u6237\u7aef\u8282\u70b9\uff09\uff1a \u542f\u52a8 RStudio su - huawei cd /opt/client172.16.4.35/ source bigdata_env kinit developuser klist source ~/.bashrc.anaconda conda activate renv2 rstudio &","title":"\u6d4b\u8bd5\u73af\u5883\u51c6\u5907"},{"location":"Development/Anaconda/#1sparklyrrstudiorhd-spark","text":"\u8bf4\u660e\uff1a\u901a\u8fc7\u4f7f\u7528sparklyr\u6765\u5bf9\u822a\u7a7a\u516c\u53f8\u822a\u73ed\u8fdb\u884c\u6570\u636e\u5206\u6790\uff0c\u53ef\u67e5\u770b\u5728\u7ebfdemo http://rpubs.com/jinbnie/513252 \u6ce8\u610f\uff1a \u4ee5\u4e0b 7 \u6b65\u7684\u6e90\u7801\u53ef\u4ee5\u5728\u5982\u4e0b\u94fe\u63a5\u4e2d\u83b7\u53d6\uff1a RStudio \u7b2c5,7\u6b65\u8fdb\u884c\u76f8\u5173\u9002\u914d\u53ef\u4ee5\u5bf9\u63a5 FI HD 6.5(Spark 2.3.2). Step 1 \u5b89\u88c5\u76f8\u5173\u5305 install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") Step 2 \u8fde\u63a5spark library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/opt/client172.16.4.35/JDK/jdk-8u201\") Sys.setenv(SPARK_HOME=\"/opt/client172.16.4.35/Spark2x/spark\") spark_version_from_home(Sys.getenv(\"SPARK_HOME\")) Sys.setenv(SPARK_HOME_VERSION=\"2.3.2\") sc <- spark_connect(master = \"yarn-client\", version = \"2.3.2\", spark_home = \"/opt/client172.16.4.35/Spark2x/spark\") Step 3 \u5c06\u63d0\u524d\u5efa\u597d\u7684hive\u8868\u5b58\u5165\u7f13\u5b58 \u8bf4\u660e\uff1a\u5177\u4f53\u6570\u636e\u51c6\u5907\u7684\u65b9\u6cd5\u8bf7\u53c2\u8003\u751f\u6001\u5730\u56fe->Rstudio->\u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e #Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl <- tbl(sc, 'flights') # Cache airlines Hive table into Spark tbl_cache(sc, 'airlines') airlines_tbl <- tbl(sc, 'airlines') # Cache airports Hive table into Spark tbl_cache(sc, 'airports') airports_tbl <- tbl(sc, 'airports') Step 4 \u5efa\u6a21 #Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year >= 2003 & year <= 2007) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Step 5 \u8bad\u7ec3\u6a21\u578b #Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition <- model_data %>% sdf_random_split(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 <- model_partition$train %>% ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) Step 6 \u8bc4\u4f30\u6a21\u578b\u8868\u73b0 #Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles <- lapply(model_partition, function(x) { ml_predict(ml1, x) %>% mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles <- rbind( data.frame(data = 'train', model_deciles$train), data.frame(data = 'valid', model_deciles$valid), make.row.names = FALSE ) # Plot average gains by predicted decile deciles %>% ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Step 7 \u5c06\u9884\u6d4b\u53ef\u89c6\u5316 #Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier <- ml_predict(ml1, data_2008) %>% group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted')","title":"\u65b9\u68481\uff1a\u4f7f\u7528sparklyr\u5728RStudio\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4"},{"location":"Development/Anaconda/#2-sparkrrstudiorhd-spark","text":"\u8bf4\u660e\uff1a\u4f7f\u7528SparkR\u8bfb\u53d6\u9884\u5b58\u7684hive\u8868\u6570\u636e \u542f\u52a8 RStudio cd /opt/client172.16.4.35/ source ~/.bashrc.anaconda conda activate renv2 source bigdata_env kinit developuser klist rstudio & \u5728RStudio\u4e2d\u63d0\u4ea4\u5982\u4e0bR\u4ee3\u7801 if (nchar(Sys.getenv(\"SPARK_HOME\")) < 1) { Sys.setenv(SPARK_HOME=\"/opt/client172.16.4.35/Spark2x/spark\") } library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"))) sparkR.session(master = \"yarn\", sparkConfig = list(spark.driver.memory = \"2g\"),(spark.submit.deployMode=\"client\")) results <- sql(\"FROM airports SELECT id, name \") head(results)","title":"\u65b9\u68482: \u4f7f\u7528SparkR\u5728RStudio\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4"},{"location":"Development/Anaconda/#3sparklyrjupyter-notebookrhd-spark","text":"step 1:\u542f\u52a8 Jupyter Notebook cd /opt/client172.16.4.35/ source bigdata_env kinit developuser source ~/.bashrc.anaconda conda env list conda activate renv2 sudo chmod -R 777 /opt/client172.16.4.35 jupyter-notebook & step 2:\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 R Notebook step 3:\u5728Jupyter Notebook\u63d0\u4ea4\u540c\u65b9\u68481\u4e2d\u76f8\u540c\u7684R\u4ee3\u7801\uff0c\u67e5\u770b\u7ed3\u679c","title":"\u65b9\u68483\uff1a\u4f7f\u7528sparklyr\u5728jupyter notebook\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06R\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4"},{"location":"Development/Anaconda/#4jupyter-notebookpythonhd-spark","text":"\u8bf4\u660e: \u4f7f\u7528python\u63a5\u53e3\uff0c\u8bfb\u53d6\u5bf9\u63a5\u96c6\u7fa4hdfs\u4e2d\u7684\u6570\u636e\uff0c\u5b8c\u6210\u4e00\u6b21\u7ecf\u5178\u7684spark word count\u6f14\u793a\u6837\u4f8b step 1:\u542f\u52a8 PySpark cd /opt/client172.16.4.35/ source bigdata_env kinit developuser klist source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" pyspark --master yarn --deploy-mode client & step 2:\u521b\u5efa\u4e00\u4e2a\u65b0\u7684Python Notebook step3:\u5728Jupyter Notebook\u4e2d\u63d0\u4ea4\u5982\u4e0b\u7684python\u4ee3\u7801 # spark-wordcount.py from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() conf.setAppName('spark-wordcount_from172.16.5.103') sc = SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/user/developuser/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print 'Nonempty lines', nonempty_lines.count() words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print 'Top 100 words:' print wordcounts.take(100) \u67e5\u770b\u7ed3\u679c:","title":"\u65b9\u68484\uff1a\u5728jupyter notebook\u4e2d\u4ee5\u4ea4\u4e92\u65b9\u5f0f\u5c06python\u4ee3\u7801\u63d0\u4ea4\u5230HD Spark\u96c6\u7fa4"},{"location":"Development/DBeaver_4.2.1/","text":"DBeaver\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DBeaver 4.0.8 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) DBeaver 4.2.1 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL) \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DBeaver\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 Linux\u4e0bDBeaver\u8fde\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b89\u88c5\u597dLinux\uff08Redhat Linux Enterprise 6.5 64bit\uff09Desktop\u64cd\u4f5c\u7cfb\u7edf\uff1b \u5df2\u7ecf\u5b89\u88c5\u597d\u7684Linux\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5jdk1.8\uff0cDBeaver4.0.8\u9700\u8981jdk1.8\u4ee5\u4e0a\u7248\u672c tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf/etc/profile\uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf #configure java export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH \u4e0b\u8f7d\u5730\u5740\uff1a http://dbeaver.jkiss.org/download/ , \u8f6f\u4ef6 dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \uff0c\u5b89\u88c5DBeaver tar -xvf dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u4ea7\u54c1\u6587\u6863\u300b\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \uff0c\u5176\u4e2dFiber\u5ba2\u6237\u7aef\u76ee\u5f55 /opt/hadoopclient/Fiber/ \u3002 \u4fee\u6539Fiber\u7684\u914d\u7f6e\u6587\u4ef6 /opt/hadoopclient/Fiber/conf/fiber.xml \uff0c\u5c06\u5176\u4e2dhive\u3001spark\u3001phoenix\u7684\u8ba4\u8bc1\u65b9\u5f0f\u6539\u4e3a\u5b89\u5168\u6a21\u5f0fkeytab\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5177\u4f53\u914d\u7f6e\u65b9\u6cd5\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 Hive JDBC\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Hive/config:/opt/hadoopclient/Hive/Beeline/lib:/opt/hadoopclient/Hive/Beeline/conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>{HIVE_CLIENT_ZK_PRINCIPAL}</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Spark\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Spark/spark/conf:/opt/hadoopclient/Spark/spark/lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Phoenix\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/HBase/hbase/lib:/opt/hadoopclient/HBase/hbase/conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase:test:/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> jaas.conf\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u6253\u5f00DBeaver\uff0c\u8fdb\u5165DBeaver\u7684\u5b89\u88c5\u76ee\u5f55\u6267\u884c ./dbeaver \uff0c\u542f\u52a8dbeaver \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef /opt/hadoopclient/Fiber/lib/ \u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 commons-cli-1.2.jar commons-logging-1.1.3.jar fiber-jdbc-1.0.jar hadoop-common-2.7.2.jar hive-beeline-1.2.1.spark.jar hive-common-1.2.1.spark.jar jline-2.12.jar log4j-1.2.17.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar super-csv-2.2.0.jar \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027\uff1a \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection , \u7c7b\u578b\u9009\u62e9Fiber User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u914d\u7f6eDriver properties\u91cc\u9762\u7684defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\uff0c\u70b9\u51fbnext Network\u9875\u9762\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u8f93\u5165\u81ea\u5b9a\u4e49Connection name\u540e\uff0c\u70b9\u51fb finish , \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u94fe\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u94fe\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e Windows\u4e0bDBeaver\u8fde\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix \u524d\u63d0\u6761\u4ef6 \u00b6 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5b8c\u6210windows\u4e0a\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u64cd\u4f5c\u6b65\u9aa4 \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u53ef\u4ee5\u7528kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\uff0c\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002kinit\u8ba4\u8bc1\u7684\u6709\u6548\u671f\u662f24\u5c0f\u65f6\uff0ckeytab\u8ba4\u8bc1\u65b9\u5f0f\u957f\u671f\u6709\u6548 - \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e - \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\uff0c\u5e76\u5b89\u88c5 http://web.mit.edu/kerberos/dist/#kfw-4.0 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u89d2\u8272\u548c\u4eba\u673a\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u7cfb\u7edf\u8bbe\u7f6e -> \u6743\u9650\u7ba1\u7406 -> \u7528\u6237\u7ba1\u7406 -> \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\uff0c\u521b\u5efa\u7528\u6237\u201ctest\u201d \u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6 user.keytab \u4ee5\u53ca krb5.conf \u6587\u4ef6\uff0c\u628a krb5.conf \u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d(\u5982\uff1a test@HADOOP.COM )\uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; \u4fee\u6539 fiber.xml \u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5\u524d\u786e\u8ba4kerberos\u8ba4\u8bc1\u6709\u6548 \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199 Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab\u548chbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5Fiber \u00b6 \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef\uff08 /opt/hadoopclient/Fiber/lib/ \uff09\u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027 \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u786e\u8ba4defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\u3002 Network\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u81ea\u5b9a\u4e49Connection name\uff0c\u70b9\u51fbfinish \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u8fde\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u8fde\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e DBeaver\u5bf9\u63a5Fiber\u529f\u80fd\u9a8c\u8bc1 \u00b6 Hive\u589e\u52a0\u67e5\u770b\u6570\u636e \u00b6 \u5c06JDBC\u7684defaultDrive\u5207\u6362\u81f3Hive Hive\u67e5\u8be2\u6570\u636e\uff1a\u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 \u67e5\u770b\u66f4\u65b0\u540e\u6570\u636e\uff1a Spark\u589e\u52a0\u67e5\u770b\u6570\u636e \u00b6 \u5c06JDBC \u7684defaultDriver\u5207\u6362\u81f3Spark Spark\u67e5\u8be2\u6570\u636e\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Spark\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3Spark\u7684JDBCServer(\u4e3b)\u5b9e\u4f8b\u6240\u5728\u7684\u8282\u70b9\u7684/opt/\u76ee\u5f55\u4e0b \u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 LOAD DATA LOCAL INPATH '/opt/data_input.txt' OVERWRITE INTO TABLE workers_info \u67e5\u770b\u7ed3\u679c\uff1a Phoenix\u589e\u5220\u6539\u67e5\u6570\u636e \u00b6 \u5c06JDBC \u7684defaultDrive\u5207\u6362\u81f3Phoenix Phoenix\u589e\u52a0\u6570\u636e \u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2 \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (104,'phoenix_user4','company4') \u67e5\u770b\u589e\u52a0\u7684\u6570\u636e\uff1a Phoenix\u5220\u9664\u6570\u636e \u9875\u9762\u4e0a\u5220\u9664\uff1a\u9009\u62e9\u5f85\u5220\u9664\u7684\u5217\uff0c\u7136\u540e\u70b9\u51fb\u4e0b\u65b9 \u5220\u9664 \u6309\u94ae\uff0c\u7136\u540e\u70b9\u51fb save \u6309\u94ae\uff1a \u811a\u672c\u5220\u9664\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae delete from TB_PHOENIX where ID=104; \u67e5\u770b\u8f93\u51fa\u540e\u7684\u6570\u636e Phoenix\u66f4\u65b0\u6570\u636e, \u7f16\u8f91\u66f4\u65b0\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae UPSERT INTO TB_PHOENIX(Id, Name,Company) values (103,'phoenix_user3_up','company3_up') \u67e5\u770b\u66f4\u65b0\u540e\u7684\u6570\u636e\uff1a \u67e5\u770b\u6570\u636e\uff1a\u7f16\u8f91\u67e5\u8be2\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae\u3002 SELECT * FROM TB_PHOENIX","title":"4.2.1 <--> C70"},{"location":"Development/DBeaver_4.2.1/#dbeaverfusioninsight","text":"","title":"DBeaver\u5bf9\u63a5FusionInsight"},{"location":"Development/DBeaver_4.2.1/#_1","text":"DBeaver 4.0.8 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) DBeaver 4.2.1 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DBeaver_4.2.1/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DBeaver\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/DBeaver_4.2.1/#linuxdbeaverfiber","text":"","title":"Linux\u4e0bDBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver_4.2.1/#_3","text":"\u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_4.2.1/#_4","text":"\u5df2\u7ecf\u5b89\u88c5\u597dLinux\uff08Redhat Linux Enterprise 6.5 64bit\uff09Desktop\u64cd\u4f5c\u7cfb\u7edf\uff1b \u5df2\u7ecf\u5b89\u88c5\u597d\u7684Linux\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_4.2.1/#_5","text":"\u5b89\u88c5jdk1.8\uff0cDBeaver4.0.8\u9700\u8981jdk1.8\u4ee5\u4e0a\u7248\u672c tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf/etc/profile\uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf #configure java export JAVA_HOME=/opt/jdk1.8.0_112 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$JAVA_HOME/bin:$PATH \u4e0b\u8f7d\u5730\u5740\uff1a http://dbeaver.jkiss.org/download/ , \u8f6f\u4ef6 dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \uff0c\u5b89\u88c5DBeaver tar -xvf dbeaver-ce-4.0.8-linux.gtk.x86_64.tar.gz \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u4ea7\u54c1\u6587\u6863\u300b\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \uff0c\u5176\u4e2dFiber\u5ba2\u6237\u7aef\u76ee\u5f55 /opt/hadoopclient/Fiber/ \u3002 \u4fee\u6539Fiber\u7684\u914d\u7f6e\u6587\u4ef6 /opt/hadoopclient/Fiber/conf/fiber.xml \uff0c\u5c06\u5176\u4e2dhive\u3001spark\u3001phoenix\u7684\u8ba4\u8bc1\u65b9\u5f0f\u6539\u4e3a\u5b89\u5168\u6a21\u5f0fkeytab\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5177\u4f53\u914d\u7f6e\u65b9\u6cd5\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 Hive JDBC\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Hive/config:/opt/hadoopclient/Hive/Beeline/lib:/opt/hadoopclient/Hive/Beeline/conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>{HIVE_CLIENT_ZK_PRINCIPAL}</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/Hive/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Spark\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/Spark/spark/conf:/opt/hadoopclient/Spark/spark/lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> Phoenix\u8fde\u63a5\u914d\u7f6e <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>/opt/hadoopclient/HBase/hbase/lib:/opt/hadoopclient/HBase/hbase/conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase:test:/opt/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/var/krb5kdc/krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>/opt/jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>/opt/hadoopclient/HBase/../KrbClient/kerberos/bin/kinit</value> </property> </properties> </jdbc> jaas.conf\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u6253\u5f00DBeaver\uff0c\u8fdb\u5165DBeaver\u7684\u5b89\u88c5\u76ee\u5f55\u6267\u884c ./dbeaver \uff0c\u542f\u52a8dbeaver \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef /opt/hadoopclient/Fiber/lib/ \u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 commons-cli-1.2.jar commons-logging-1.1.3.jar fiber-jdbc-1.0.jar hadoop-common-2.7.2.jar hive-beeline-1.2.1.spark.jar hive-common-1.2.1.spark.jar jline-2.12.jar log4j-1.2.17.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar super-csv-2.2.0.jar \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027\uff1a \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection , \u7c7b\u578b\u9009\u62e9Fiber User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u914d\u7f6eDriver properties\u91cc\u9762\u7684defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\uff0c\u70b9\u51fbnext Network\u9875\u9762\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u8f93\u5165\u81ea\u5b9a\u4e49Connection name\u540e\uff0c\u70b9\u51fb finish , \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u94fe\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u94fe\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_4.2.1/#windowsdbeaverfiber","text":"","title":"Windows\u4e0bDBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver_4.2.1/#_6","text":"\u4ee5\u5b89\u5168\u6a21\u5f0f\u4e3a\u4f8b\uff0c\u4f7f\u7528DBeaver\u901a\u8fc7Fiber\u8bbf\u95eeHive\u3001Spark\u3001Phoenix","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_4.2.1/#_7","text":"Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5b8c\u6210windows\u4e0a\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u5b89\u5168\u96c6\u7fa4\u7684\u5b89\u88c5\uff0c\u5df2\u5b89\u88c5\u597dFiber\u5ba2\u6237\u7aef\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_4.2.1/#_8","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u53ef\u4ee5\u7528kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\uff0c\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357 -> \u7edf\u4e00SQL(Fiber) -> \u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002kinit\u8ba4\u8bc1\u7684\u6709\u6548\u671f\u662f24\u5c0f\u65f6\uff0ckeytab\u8ba4\u8bc1\u65b9\u5f0f\u957f\u671f\u6709\u6548 - \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e - \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_4.2.1/#kinit","text":"\u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\uff0c\u5e76\u5b89\u88c5 http://web.mit.edu/kerberos/dist/#kfw-4.0 \u786e\u8ba4\u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u8981\u5c0f\u4e8e5\u5206\u949f \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6 \u5728FusionInsight Manager\u521b\u5efa\u89d2\u8272\u548c\u4eba\u673a\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1 \u4ea7\u54c1\u6587\u6863 -> \u7ba1\u7406\u5458\u6307\u5357 -> \u7cfb\u7edf\u8bbe\u7f6e -> \u6743\u9650\u7ba1\u7406 -> \u7528\u6237\u7ba1\u7406 -> \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u89d2\u8272\u9700\u8981\u6839\u636e\u4e1a\u52a1\u9700\u8981\u6388\u4e88Hive\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u5e76\u5c06\u7528\u6237\u52a0\u5165\u89d2\u8272\uff0c\u521b\u5efa\u7528\u6237\u201ctest\u201d \u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6 user.keytab \u4ee5\u53ca krb5.conf \u6587\u4ef6\uff0c\u628a krb5.conf \u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \uff0c\u5e76\u653e\u5230 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e2d \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d(\u5982\uff1a test@HADOOP.COM )\uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; \u4fee\u6539 fiber.xml \u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5 <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> DBeaver\u8fde\u63a5\u524d\u786e\u8ba4kerberos\u8ba4\u8bc1\u6709\u6548","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_4.2.1/#keytab","text":"\u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b jaas.conf \u6587\u4ef6\u548c krb5.conf \u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199 Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027hbase.myclient.keytab\u548chbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_4.2.1/#dbeaverfiber","text":"\u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database -> \u65b0\u5efaDriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u586b\u5199\u5982\u4e0b\u4fe1\u606f\uff0c\u70b9\u51fb OK Driver Name\uff1a Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1a com.huawei.fiber.FiberDriver URL Template\uff1a jdbc:fiber:// Default Port\uff1a 2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category\uff1a Hadoop \u70b9\u51fb Add File \u6309\u94ae\uff0c\u5c06Fiber\u5ba2\u6237\u7aef\uff08 /opt/hadoopclient/Fiber/lib/ \uff09\u4e0b\u7684jar\u5305\u6dfb\u52a0\u8fdb\u6765 \u5728Connection Properties\u4e2d\u52a0\u5165\u4ee5\u4e0b\u5c5e\u6027 \u83dc\u5355\u680f\u9009\u62e9 File -> New -> Database Connection User name\u548cPassword\u53ef\u4e0d\u586b\u5199 \u786e\u8ba4defaultDirver\uff0c\u53ef\u6309\u9700\u6c42\u586b\u5199hive\u6216spark\u6216phoenix\u3002 Network\u4fdd\u6301\u9ed8\u8ba4\uff0c\u70b9\u51fb next \u81ea\u5b9a\u4e49Connection name\uff0c\u70b9\u51fbfinish \u8fde\u63a5\u5efa\u7acb\u5b8c\u6210 \u6d4b\u8bd5hive\u8fde\u63a5 \u67e5\u770bHive\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5spark\u8fde\u63a5, \u628adriver\u5207\u6362\u4e3aspark\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u4f7f\u7528spark driver\u67e5\u770b\u8868\u4e2d\u6570\u636e \u6d4b\u8bd5phoenix\u8fde\u63a5\uff0c\u628adriver\u5207\u6362\u4e3aphoenix\uff0c\u8fde\u63a5\u53f3\u952e\u9009\u62e9 Edit Connection \u67e5\u770bphoenix\u8868\u4e2d\u6570\u636e","title":"DBeaver\u8fde\u63a5Fiber"},{"location":"Development/DBeaver_4.2.1/#dbeaverfiber_1","text":"","title":"DBeaver\u5bf9\u63a5Fiber\u529f\u80fd\u9a8c\u8bc1"},{"location":"Development/DBeaver_4.2.1/#hive","text":"\u5c06JDBC\u7684defaultDrive\u5207\u6362\u81f3Hive Hive\u67e5\u8be2\u6570\u636e\uff1a\u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 \u67e5\u770b\u66f4\u65b0\u540e\u6570\u636e\uff1a","title":"Hive\u589e\u52a0\u67e5\u770b\u6570\u636e"},{"location":"Development/DBeaver_4.2.1/#spark","text":"\u5c06JDBC \u7684defaultDriver\u5207\u6362\u81f3Spark Spark\u67e5\u8be2\u6570\u636e\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 SELECT * FROM workers_info Spark\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3Spark\u7684JDBCServer(\u4e3b)\u5b9e\u4f8b\u6240\u5728\u7684\u8282\u70b9\u7684/opt/\u76ee\u5f55\u4e0b \u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2\u6267\u884c\u6309\u94ae\u3002 LOAD DATA LOCAL INPATH '/opt/data_input.txt' OVERWRITE INTO TABLE workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"Spark\u589e\u52a0\u67e5\u770b\u6570\u636e"},{"location":"Development/DBeaver_4.2.1/#phoenix","text":"\u5c06JDBC \u7684defaultDrive\u5207\u6362\u81f3Phoenix Phoenix\u589e\u52a0\u6570\u636e \u83dc\u5355\u680f\u9009\u62e9 SQL Editor -> New SQL Editor \uff0c\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u89d2 \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (104,'phoenix_user4','company4') \u67e5\u770b\u589e\u52a0\u7684\u6570\u636e\uff1a Phoenix\u5220\u9664\u6570\u636e \u9875\u9762\u4e0a\u5220\u9664\uff1a\u9009\u62e9\u5f85\u5220\u9664\u7684\u5217\uff0c\u7136\u540e\u70b9\u51fb\u4e0b\u65b9 \u5220\u9664 \u6309\u94ae\uff0c\u7136\u540e\u70b9\u51fb save \u6309\u94ae\uff1a \u811a\u672c\u5220\u9664\uff1a\u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae delete from TB_PHOENIX where ID=104; \u67e5\u770b\u8f93\u51fa\u540e\u7684\u6570\u636e Phoenix\u66f4\u65b0\u6570\u636e, \u7f16\u8f91\u66f4\u65b0\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae UPSERT INTO TB_PHOENIX(Id, Name,Company) values (103,'phoenix_user3_up','company3_up') \u67e5\u770b\u66f4\u65b0\u540e\u7684\u6570\u636e\uff1a \u67e5\u770b\u6570\u636e\uff1a\u7f16\u8f91\u67e5\u8be2\u811a\u672c\uff0c\u70b9\u51fb\u5de6\u4e0a\u65b9 \u6267\u884c \u6309\u94ae\u3002 SELECT * FROM TB_PHOENIX","title":"Phoenix\u589e\u5220\u6539\u67e5\u6570\u636e"},{"location":"Development/DBeaver_6.1.4/","text":"DBeaver\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DBeaver 6.1.4 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL) \u7b80\u4ecb \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \u3002 <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> DBeaver\u5bf9\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 DBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002 \u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a\u3002\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin \u91cd\u542fDBeaver\u3002\u4fee\u6539dbeaver.ini\u540e\u9700\u8981\u91cd\u542fDBeaver\u624d\u751f\u6548\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Hive \u00b6 \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u3002 \u586b\u5199\u57fa\u672c\u4fe1\u606f\u5982\u4e0b\uff1a Driver Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1acom.huawei.fiber.FiberDriver URL Template\uff1ajdbc:fiber:// Default Port\uff1a2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category: Hadoop * \u70b9\u51fb Add File \uff0c\u589e\u52a0 C:\\ecotesting\\Fiber\\lib \u6240\u6709\u7684jar\u5305\u3002 \u70b9\u51fb Connection properties \uff0c\u589e\u52a0\u4e24\u4e2a\u5c5e\u6027\u3002\u70b9\u51fb OK \u3002 defaultDriver = hive fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection \u3002\u70b9\u51fb Next \u3002 \u9009\u62e9 Fiber \uff0c\u70b9\u51fb Next \u3002 \u201cUser name\u201d\u548c\u201cPassword\u201d\u53ef\u4ee5\u4e0d\u586b\u5199\uff0c\u70b9\u51fb Connection details (name,type,...) \u3002 \u201cConnection name\u201d\u8f93\u5165 Hadoop - Fiber \u3002\u70b9\u51fb back \u3002 \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \u3002\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002\u70b9\u51fb Finish \u3002 \u6d4b\u8bd5hive\u8fde\u63a5\u3002 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\u3002 \u53cc\u51fb Database Navigator->Hadoop - Fiber \uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u67e5\u770bHive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \u3002 \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 SELECT * FROM student; \u5411Hive\u8868test\u63d2\u5165\u6570\u636e \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 test \u3002\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS test (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test\u3002 SELECT * FROM test; DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x \u00b6 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a spark2x \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3\u3002 \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u67e5\u770b\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \u3002 \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 SELECT * FROM student; \u5411\u8868test\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test\u3002 SELECT * FROM test; DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix \u00b6 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a phoenix \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3\u3002 \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \uff0cSQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u8868\u548c\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c\u811a\u672c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.TEST VALUES(1,'John'); UPSERT INTO MY_NS.TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.TEST VALUES(4,'Aurora'); \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Refresh \u5219\u53ef\u770b\u5230\u65b0\u5efa\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 \u67e5\u770b\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb MY_NS->tables->TEST \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770btest\u8868\u6570\u636e\u3002 SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u4fee\u6539\u811a\u672c\u5e76\u6267\u884c\u3002 UPSERT INTO MY_NS.TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u5220\u9664\u811a\u672c\u5e76\u6267\u884c\u3002 DELETE FROM MY_NS.TEST WHERE ID=4; SQL\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u67e5\u8be2\u811a\u672c\u5e76\u6267\u884c\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 SELECT * FROM MY_NS.TEST; FAQ \u00b6 \u5bf9\u63a5Phoenix\u65f6\u8fd4\u56deDriver: Fiber? \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5Phoenix\u65f6\uff0c\u70b9\u51fb Test Connection \uff0c\u6ca1\u6709\u6b63\u786e\u8fd4\u56deServer\u548cDriver\u7684\u7248\u672c\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u786e\u8ba4\u662f\u5426\u5df2\u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a\u3002\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin","title":"6.1.4 <--> 6.5"},{"location":"Development/DBeaver_6.1.4/#dbeaverfusioninsight","text":"","title":"DBeaver\u5bf9\u63a5FusionInsight"},{"location":"Development/DBeaver_6.1.4/#_1","text":"DBeaver 6.1.4 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DBeaver_6.1.4/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Development/DBeaver_6.1.4/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1);","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Development/DBeaver_6.1.4/#fiber","text":"","title":"Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_6.1.4/#_4","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_6.1.4/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_6.1.4/#_6","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_6.1.4/#kinit","text":"\u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_6.1.4/#keytab","text":"\u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \u3002 <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_6.1.4/#dbeaverfiber","text":"","title":"DBeaver\u5bf9\u63a5Fiber"},{"location":"Development/DBeaver_6.1.4/#_7","text":"DBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_6.1.4/#_8","text":"\u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002 \u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a\u3002\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin \u91cd\u542fDBeaver\u3002\u4fee\u6539dbeaver.ini\u540e\u9700\u8981\u91cd\u542fDBeaver\u624d\u751f\u6548\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_6.1.4/#_9","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_6.1.4/#dbeaverfiberhive","text":"\u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New \u3002 \u586b\u5199\u57fa\u672c\u4fe1\u606f\u5982\u4e0b\uff1a Driver Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1acom.huawei.fiber.FiberDriver URL Template\uff1ajdbc:fiber:// Default Port\uff1a2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category: Hadoop * \u70b9\u51fb Add File \uff0c\u589e\u52a0 C:\\ecotesting\\Fiber\\lib \u6240\u6709\u7684jar\u5305\u3002 \u70b9\u51fb Connection properties \uff0c\u589e\u52a0\u4e24\u4e2a\u5c5e\u6027\u3002\u70b9\u51fb OK \u3002 defaultDriver = hive fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection \u3002\u70b9\u51fb Next \u3002 \u9009\u62e9 Fiber \uff0c\u70b9\u51fb Next \u3002 \u201cUser name\u201d\u548c\u201cPassword\u201d\u53ef\u4ee5\u4e0d\u586b\u5199\uff0c\u70b9\u51fb Connection details (name,type,...) \u3002 \u201cConnection name\u201d\u8f93\u5165 Hadoop - Fiber \u3002\u70b9\u51fb back \u3002 \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \u3002\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4\u3002\u70b9\u51fb Finish \u3002 \u6d4b\u8bd5hive\u8fde\u63a5\u3002 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\u3002 \u53cc\u51fb Database Navigator->Hadoop - Fiber \uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u67e5\u770bHive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \u3002 \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 SELECT * FROM student; \u5411Hive\u8868test\u63d2\u5165\u6570\u636e \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 test \u3002\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS test (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test\u3002 SELECT * FROM test;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Hive"},{"location":"Development/DBeaver_6.1.4/#dbeaverfiberspark2x","text":"\u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a spark2x \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3\u3002 \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u67e5\u770b\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \u3002 \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002 SELECT * FROM student; \u5411\u8868test\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test\u3002 SELECT * FROM test;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x"},{"location":"Development/DBeaver_6.1.4/#dbeaverfiberphoenix","text":"\u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection \u3002 \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a phoenix \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f\u3002\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3\u3002 \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \uff0cSQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u8868\u548c\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c\u811a\u672c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.TEST VALUES(1,'John'); UPSERT INTO MY_NS.TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.TEST VALUES(4,'Aurora'); \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Refresh \u5219\u53ef\u770b\u5230\u65b0\u5efa\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 \u67e5\u770b\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb MY_NS->tables->TEST \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770btest\u8868\u6570\u636e\u3002 SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u4fee\u6539\u811a\u672c\u5e76\u6267\u884c\u3002 UPSERT INTO MY_NS.TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u5220\u9664\u811a\u672c\u5e76\u6267\u884c\u3002 DELETE FROM MY_NS.TEST WHERE ID=4; SQL\u67e5\u8be2\u8868\u7684\u6570\u636e\u3002\u5728SQL Editor\u8f93\u5165\u67e5\u8be2\u811a\u672c\u5e76\u6267\u884c\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 SELECT * FROM MY_NS.TEST;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix"},{"location":"Development/DBeaver_6.1.4/#faq","text":"\u5bf9\u63a5Phoenix\u65f6\u8fd4\u56deDriver: Fiber? \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5Phoenix\u65f6\uff0c\u70b9\u51fb Test Connection \uff0c\u6ca1\u6709\u6b63\u786e\u8fd4\u56deServer\u548cDriver\u7684\u7248\u672c\u3002 \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u786e\u8ba4\u662f\u5426\u5df2\u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a\u3002\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\u3002 \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin","title":"FAQ"},{"location":"Development/DBeaver_6.3.4/","text":"DBeaver\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DBeaver 6.3.4 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL/Hetu) DBeaver 6.3.4 \u2194 FusionInsight MRS 8.0 (Hive/Phoenix/SparkSQL/Hetu) \u6587\u6863\u8bf4\u660e \u00b6 \u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u65b9\u5f0f\u5bf9\u63a5dbeaver\u548cfusioninsight. \u4f7f\u7528\u901a\u7528jdbc\u63a5\u53e3\u7684\u65b9\u5f0f\u5bf9\u63a5hive, hetu \u4f7f\u7528Fiber\u5bf9\u63a5hive, spark2x, phoenix(HBase) \u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u9009\u53d6\u5408\u9002\u7684\u5bf9\u63a5\u65b9\u5f0f\uff0c\u5982\u679c\u53ea\u662f\u5bf9\u63a5hive, hetu\u4f7f\u7528\u901a\u7528jdbc\u7684\u65b9\u5f0f\u914d\u7f6e\u66f4\u5bb9\u6613\uff0c\u5982\u679c\u9700\u8981\u5bf9\u63a5spark2x\u6216\u8005\u4f7f\u7528phoenix\u7684\u65b9\u5f0f\u5bf9\u63a5HBase\uff0c\u8bf7\u53c2\u8003Fiber\u76f8\u5173\u7ae0\u8282\u8fdb\u884c\u914d\u7f6e \u51c6\u5907\u5de5\u4f5c \u00b6 \uff08\u91cd\u8981\uff09\u68c0\u67e5windows\u73af\u5883jdk\u7248\u672c\u4e3a1.8.0_251 \u6ce8\u610f\uff1a\u5982\u679c\u4f7f\u7528java 1.8.0_112\u7248\u672c\u8fdb\u884c\u914d\u7f6e,\u4f1a\u51fa\u73b0\u4e0d\u80fd\u540c\u65f6\u8bbf\u95eepheonix,hive\u548cspark2x\u7684\u95ee\u9898,\u5efa\u8bae\u66f4\u6362\u9ad8\u7248\u672cjdk \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282.\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase. \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282. \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts . \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f. Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e. Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); \u5b89\u5168\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u00b6 \u53c2\u8003\u4ea7\u54c1\u6587\u6863MIT\u914d\u7f6e\u76f8\u5173\u90e8\u5206(\u7b2c1\u6b65\u5230\u7b2c4\u6b65) \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5. \u4eceManager\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6\uff1auser.keytab\u4ee5\u53cakrb5.conf \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6.\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\Windows \u76ee\u5f55\u4e0b. \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6.\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b. \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp . \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache . \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5_CONFIG \uff0c\u53d8\u91cf\u503c\u4e3a C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini . \u91cd\u542f\u8df3\u677f\u673a \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK . \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6ce8\u610f\uff1a \u5176\u4e2dkeyTab\u53c2\u6570\u4ee5\u53caprincipal\u53c2\u6570\u4e3a\u5bf9\u5e94\u7684\u8ba4\u8bc1\u7528\u6237\u540d\u4ee5\u53ca\u8ba4\u8bc1\u6587\u4ef6\u8def\u5f84 \u5b89\u88c5DBeaver \u00b6 \u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5. \u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a.\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c. \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\java64\\jdk\\bin \u914d\u7f6e\u6587\u4ef6\u6700\u540e\u52a0\u4e0a\uff1a -Djava.security.auth.login.config=C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf -Dzookeeper.sasl.clientconfig=Client -Dzookeeper.auth.type=kerberos -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com \u91cd\u542fDBeaver. \u4fee\u6539dbeaver.ini\u540e\u9700\u8981\u91cd\u542fDBeaver\u624d\u751f\u6548. \u4f7f\u7528\u81ea\u5b9a\u4e49JDBC\u5bf9\u63a5Hive \u00b6 \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New . \u65b0\u5efa\u7684\u8fde\u63a5\u540d\u5b57\u4e3aFI-hive-test\uff0c\u8fde\u63a5\u4fe1\u606f\u5982\u4e0b,\u5b8c\u6210\u540e\u70b9\u51fbOK 1. org.apache.hive.jdbc.HiveDriver 2. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 3. Hadoop 4. \u70b9Add File\u5728\u4e0b\u8f7d\u597d\u7684hive\u5ba2\u6237\u7aeflib\u4e2d\u628a\u6240\u6709jar\u5305\u52a0\u8fdb\u53bb \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection .\u70b9\u51fb Next . \u9009\u62e9FI-hive-test\u70b9\u51fbNEXT \u70b9\u51fbFinish \u53f3\u952e\u9009\u62e9FI-hive-test\u70b9\u51fbEdit Connection \u70b9\u51fbTest connection \u67e5\u770b\u7ed3\u679c\u6570\u636e \u4f7f\u7528\u81ea\u5b9a\u4e49JDBC\u5bf9\u63a5Spark2x \u00b6 \uff08\u91cd\u8981 fi6.5.1\uff09\u51c6\u5907spark2x jdbc\u8fde\u63a5\u9a71\u52a8jar\u5305 \u767b\u9646linux\u7aefspark2x\u5ba2\u6237\u7aef\u627e\u5230jdbc\u76f8\u5173\u4f9d\u8d56\uff0c\u6bd4\u5982\uff1a /opt/145_651hdclient/hadoopclient/Spark2x/spark/jars/jdbc \u5c06\u8be5\u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u5230windows\u672c\u5730\u76ee\u5f55\uff0c\u6bd4\u5982 E:\\145config\\spark2xjars , \u6ce8\u610f\u91cc\u9762\u542b\u6709\u4e00\u4e2ajdbc_pom.xml\u6587\u4ef6\uff0c\u9700\u8981\u5220\u9664 \u7ee7\u7eed\u767b\u9646linux\u7aefspark2x\u5ba2\u6237\u7aef\u8def\u5f84 /opt/145_651hdclient/hadoopclient/Spark2x/spark/jars ,\u5206\u522b\u627e\u5230\u5982\u4e0b4\u4e2ajar\u5305\uff0c\u62f7\u8d1d\u5230windows\u672c\u5730\u76ee\u5f55\uff0c\u6bd4\u5982 E:\\145config\\spark2xjars log4j-1.2.17.jar woodstox-core-5.0.3.jar stax2-api-3.1.4.jar commons-configuration2-2.1.1.jar \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New . \u65b0\u5efa\u7684\u8fde\u63a5\u540d\u5b57\u4e3aFI-spark2x-651-direct\uff0c\u8fde\u63a5\u4fe1\u606f\u5982\u4e0b,\u5b8c\u6210\u540e\u70b9\u51fbOK 1. org.apache.hive.jdbc.HiveDriver 2. jdbc:hive2://172.16.4.141:24002,172.16.4.142:24002,172.16.4.143:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=E:/145config/user.keytab 3. Hadoop 4. \u70b9Add File\u5728\u4e0a\u8ff0\u6b65\u9aa4\u914d\u7f6e\u597d\u7684spark2x\u8fde\u63a5\u6240\u6709jar\u5305\u52a0\u8fdb\u53bb \u6ce8\u610f\uff1a\u8fde\u63a5url\u7684user.principal=developuser;user.keytab=E:/145config/user.keytab\u8fd9\u4e24\u4e2a\u53c2\u6570\u5fc5\u987b\u52a0\u4e0a\uff0c\u5e76\u4e14\u4fdd\u8bc1\u6b63\u786e \u8fde\u63a5url\u4e32\uff0c\u53ef\u5728Linux\u5ba2\u6237\u7aef\u4f7f\u7528 spark-beeline \u547d\u4ee4\u83b7\u53d6\u53c2\u8003\uff1a \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection .\u70b9\u51fb Next . \u9009\u62e9FI-spark2x-651-direct\u70b9\u51fbNEXT \u70b9\u51fbFinish \u53f3\u952e\u9009\u62e9FI-spark2x-651-direct\u70b9\u51fbEdit Connection \u70b9\u51fbTest connection \u67e5\u770b\u7ed3\u679c\u6570\u636e \u4f7f\u7528\u81ea\u5b9a\u4e49JDBC\u5bf9\u63a5Hetu \u00b6 \u521b\u5efahetu\u5bf9\u63a5\u914d\u7f6e\u8def\u5f84 C:\\ecotesting\\hetu-config ,\u51c6\u5907\u4e00\u4e0b\u8ba4\u8bc1\u6587\u4ef6 \u5176\u4e2d presto-jdbc-316.jar \u4e3aHetu\u5bf9\u63a5jdbc\u9a71\u52a8jar\u5305\uff0c\u5728\u5ba2\u6237\u7aef\u4e2d\u83b7\u5f97 hetuserver.jks\u6587\u4ef6\u5728\u670d\u52a1\u7aefHSBroker\u4e0b\u83b7\u5f97 user.keytab\u548ckrb5.conf\u6587\u4ef6\u4e3a\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6\uff0c\u5728manager\u4e0a\u83b7\u5f97 \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New . \u65b0\u5efa\u7684\u8fde\u63a5\u540d\u5b57\u4e3aFI-hetu-test\uff0c\u8fde\u63a5\u4fe1\u606f\u5982\u4e0b,\u5b8c\u6210\u540e\u70b9\u51fbOK 1. io.prestosql.jdbc.PrestoDriver 2. jdbc:presto://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002?serviceDiscoveryMode=zooKeeper&zooKeeperNamespace=hsbroker&deploymentMode=on_yarn&user=developuser&SSL=true&SSLTrustStorePath=C:/ecotesting/hetu-config/hetuserver.jks&KerberosConfigPath=C:/ecotesting/hetu-config/krb5.conf&KerberosPrincipal=developuser&KerberosKeytabPath=C:/ecotesting/hetu-config/user.keytab&KerberosRemoteServiceName=HTTP&KerberosServicePrincipalPattern=%24%7BSERVICE%7D%40%24%7BHOST%7D 3. Hadoop 4. \u70b9Add File\u5728\u4e0b\u8f7d\u597d\u7684hive\u5ba2\u6237\u7aeflib\u4e2d\u628a\u6240\u6709jar\u5305\u52a0\u8fdb\u53bb \u6ce8\u610f\uff1a jdbc\u8fde\u63a5\u4e32\u4e2d\u542b\u6709\u7528\u6237\u540d\u4fe1\u606f\u4ee5\u53ca\u76f8\u5173\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u914d\u7f6e \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection .\u70b9\u51fb Next . \u9009\u62e9FI-hetu-test\u70b9\u51fbNEXT \u70b9\u51fbFinish \u53f3\u952e\u9009\u62e9FI-hetu-test\u70b9\u51fbEdit Connection \u70b9\u51fbTest connection \u67e5\u770b\u7ed3\u679c\u6570\u636e Fiber \u7b80\u4ecb \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2. Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6. Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f.\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282. \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c. \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting .\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55. \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55. \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55. \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55. \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1.\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b. \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml . Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\java64\\\\jdk\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\java64\\\\jdk\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\java64\\\\jdk\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u5728\u672b\u5c3e\u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal . <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff08\u5982\u679c\u5df2\u914d\u7f6e\u53ef\u4ee5\u7565\u8fc7\u6b64\u6b65\uff09\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a.\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\uff08\u5982\u679c\u5df2\u914d\u7f6e\u53ef\u4ee5\u7565\u8fc7\u6b64\u6b65\uff09. \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\java64\\jdk\\bin \u914d\u7f6e\u6587\u4ef6\u6700\u540e\u52a0\u4e0a\uff1a -Djava.security.auth.login.config=C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf -Dzookeeper.sasl.clientconfig=Client -Dzookeeper.auth.type=kerberos -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com \u5b8c\u6210\u540e\u91cd\u542fdbeaver\u914d\u7f6e\u751f\u6548 DBeaver\u5bf9\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 DBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6. \u64cd\u4f5c\u6b65\u9aa4 \u00b6 DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Hive \u00b6 \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New . \u586b\u5199\u57fa\u672c\u4fe1\u606f\u5982\u4e0b\uff1a Driver Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1acom.huawei.fiber.FiberDriver URL Template\uff1ajdbc:fiber:// Default Port\uff1a2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category: Hadoop * \u70b9\u51fb Add File \uff0c\u589e\u52a0 C:\\ecotesting\\Fiber\\lib \u6240\u6709\u7684jar\u5305. \u70b9\u51fb Connection properties \uff0c\u589e\u52a0\u4e24\u4e2a\u5c5e\u6027.\u70b9\u51fb OK . defaultDriver = hive fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection .\u70b9\u51fb Next . \u9009\u62e9 Fiber \uff0c\u70b9\u51fb Next . \u201cUser name\u201d\u548c\u201cPassword\u201d\u53ef\u4ee5\u4e0d\u586b\u5199\uff0c\u70b9\u51fb Connection details (name,type,...) . \u201cConnection name\u201d\u8f93\u5165 Hadoop - Fiber .\u70b9\u51fb back . \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive .\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4.\u70b9\u51fb Finish . \u6d4b\u8bd5hive\u8fde\u63a5. \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection . \u70b9\u51fb Driver properties \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f.\u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3. \u53cc\u51fb Database Navigator->Hadoop - Fiber \uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f. \u67e5\u770bHive\u8868\u7684\u6570\u636e.\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e. SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e. \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor . \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e. SELECT * FROM student; \u5411Hive\u8868test\u63d2\u5165\u6570\u636e \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 test .\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e. CREATE TABLE IF NOT EXISTS test (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b. data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test. LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test. SELECT * FROM test; DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x \u00b6 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection . \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a spark2x \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f.\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3. \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f . \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f. \u67e5\u770b\u8868\u7684\u6570\u636e.\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e. SQL\u67e5\u8be2\u8868\u7684\u6570\u636e. \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor . \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e. SELECT * FROM student; \u5411\u8868test\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b. data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test. LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test. SELECT * FROM test; DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix \u00b6 \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection . \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a phoenix \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f.\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3. \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f . \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f. \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS . create_namespace 'MY_NS' \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \uff0cSQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u8868\u548c\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c\u811a\u672c. CREATE TABLE IF NOT EXISTS MY_NS.TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.TEST VALUES(1,'John'); UPSERT INTO MY_NS.TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.TEST VALUES(4,'Aurora'); \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Refresh \u5219\u53ef\u770b\u5230\u65b0\u5efa\u7684\u547d\u540d\u7a7a\u95f4 MY_NS . \u67e5\u770b\u8868\u7684\u6570\u636e.\u70b9\u51fb MY_NS->tables->TEST \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770btest\u8868\u6570\u636e. SQL\u4fee\u6539\u8868\u7684\u6570\u636e.\u5728SQL Editor\u8f93\u5165\u4fee\u6539\u811a\u672c\u5e76\u6267\u884c. UPSERT INTO MY_NS.TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e.\u5728SQL Editor\u8f93\u5165\u5220\u9664\u811a\u672c\u5e76\u6267\u884c. DELETE FROM MY_NS.TEST WHERE ID=4; SQL\u67e5\u8be2\u8868\u7684\u6570\u636e.\u5728SQL Editor\u8f93\u5165\u67e5\u8be2\u811a\u672c\u5e76\u6267\u884c.\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664. SELECT * FROM MY_NS.TEST; FAQ \u00b6 \u5bf9\u63a5Phoenix\u65f6\u8fd4\u56deDriver: Fiber? \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5Phoenix\u65f6\uff0c\u70b9\u51fb Test Connection \uff0c\u6ca1\u6709\u6b63\u786e\u8fd4\u56deServer\u548cDriver\u7684\u7248\u672c. \u3010\u67e5\u770b\u95ee\u9898\u539f\u56e0\u3011 \u9996\u5148\u767b\u9646\u540e\u53f0\u67e5\u770b\u65e5\u5fd7 C:\\Users\\haoxi\\AppData\\Roaming\\DBeaverData\\workspace6\\.metadata\\dbeaver-debug.log \u67e5\u770b\u65e5\u5fd7\uff1a java.sql.SQLException: Can not connect to driver: org.apache.phoenix.jdbc.PhoenixDriver. Message: ERROR 103 (08004): Unable to establish connection. at com.huawei.fiber.FiberConnection.getConnection(FiberConnection.java:881) at com.huawei.fiber.FiberConnection.getConnection(FiberConnection.java:650) \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u786e\u8ba4\u662f\u5426\u5df2\u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a.\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c. \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin","title":"6.3.4 <--> 8.0"},{"location":"Development/DBeaver_6.3.4/#dbeaverfusioninsight","text":"","title":"DBeaver\u5bf9\u63a5FusionInsight"},{"location":"Development/DBeaver_6.3.4/#_1","text":"DBeaver 6.3.4 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL/Hetu) DBeaver 6.3.4 \u2194 FusionInsight MRS 8.0 (Hive/Phoenix/SparkSQL/Hetu)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DBeaver_6.3.4/#_2","text":"\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u65b9\u5f0f\u5bf9\u63a5dbeaver\u548cfusioninsight. \u4f7f\u7528\u901a\u7528jdbc\u63a5\u53e3\u7684\u65b9\u5f0f\u5bf9\u63a5hive, hetu \u4f7f\u7528Fiber\u5bf9\u63a5hive, spark2x, phoenix(HBase) \u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u9009\u53d6\u5408\u9002\u7684\u5bf9\u63a5\u65b9\u5f0f\uff0c\u5982\u679c\u53ea\u662f\u5bf9\u63a5hive, hetu\u4f7f\u7528\u901a\u7528jdbc\u7684\u65b9\u5f0f\u914d\u7f6e\u66f4\u5bb9\u6613\uff0c\u5982\u679c\u9700\u8981\u5bf9\u63a5spark2x\u6216\u8005\u4f7f\u7528phoenix\u7684\u65b9\u5f0f\u5bf9\u63a5HBase\uff0c\u8bf7\u53c2\u8003Fiber\u76f8\u5173\u7ae0\u8282\u8fdb\u884c\u914d\u7f6e","title":"\u6587\u6863\u8bf4\u660e"},{"location":"Development/DBeaver_6.3.4/#_3","text":"\uff08\u91cd\u8981\uff09\u68c0\u67e5windows\u73af\u5883jdk\u7248\u672c\u4e3a1.8.0_251 \u6ce8\u610f\uff1a\u5982\u679c\u4f7f\u7528java 1.8.0_112\u7248\u672c\u8fdb\u884c\u914d\u7f6e,\u4f1a\u51fa\u73b0\u4e0d\u80fd\u540c\u65f6\u8bbf\u95eepheonix,hive\u548cspark2x\u7684\u95ee\u9898,\u5efa\u8bae\u66f4\u6362\u9ad8\u7248\u672cjdk \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282.\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase. \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282. \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts . \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f. Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e. Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1);","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Development/DBeaver_6.3.4/#_4","text":"\u53c2\u8003\u4ea7\u54c1\u6587\u6863MIT\u914d\u7f6e\u76f8\u5173\u90e8\u5206(\u7b2c1\u6b65\u5230\u7b2c4\u6b65) \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5. \u4eceManager\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6\uff1auser.keytab\u4ee5\u53cakrb5.conf \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6.\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\Windows \u76ee\u5f55\u4e0b. \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6.\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b. \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp . \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache . \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5_CONFIG \uff0c\u53d8\u91cf\u503c\u4e3a C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini . \u91cd\u542f\u8df3\u677f\u673a \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK . \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6ce8\u610f\uff1a \u5176\u4e2dkeyTab\u53c2\u6570\u4ee5\u53caprincipal\u53c2\u6570\u4e3a\u5bf9\u5e94\u7684\u8ba4\u8bc1\u7528\u6237\u540d\u4ee5\u53ca\u8ba4\u8bc1\u6587\u4ef6\u8def\u5f84","title":"\u5b89\u5168\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e"},{"location":"Development/DBeaver_6.3.4/#dbeaver","text":"\u4ece http://dbeaver.jkiss.org/download/ \u4e0b\u8f7dDBeaver\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5. \u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a.\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c. \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\java64\\jdk\\bin \u914d\u7f6e\u6587\u4ef6\u6700\u540e\u52a0\u4e0a\uff1a -Djava.security.auth.login.config=C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf -Dzookeeper.sasl.clientconfig=Client -Dzookeeper.auth.type=kerberos -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com \u91cd\u542fDBeaver. \u4fee\u6539dbeaver.ini\u540e\u9700\u8981\u91cd\u542fDBeaver\u624d\u751f\u6548.","title":"\u5b89\u88c5DBeaver"},{"location":"Development/DBeaver_6.3.4/#jdbchive","text":"\u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New . \u65b0\u5efa\u7684\u8fde\u63a5\u540d\u5b57\u4e3aFI-hive-test\uff0c\u8fde\u63a5\u4fe1\u606f\u5982\u4e0b,\u5b8c\u6210\u540e\u70b9\u51fbOK 1. org.apache.hive.jdbc.HiveDriver 2. jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM 3. Hadoop 4. \u70b9Add File\u5728\u4e0b\u8f7d\u597d\u7684hive\u5ba2\u6237\u7aeflib\u4e2d\u628a\u6240\u6709jar\u5305\u52a0\u8fdb\u53bb \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection .\u70b9\u51fb Next . \u9009\u62e9FI-hive-test\u70b9\u51fbNEXT \u70b9\u51fbFinish \u53f3\u952e\u9009\u62e9FI-hive-test\u70b9\u51fbEdit Connection \u70b9\u51fbTest connection \u67e5\u770b\u7ed3\u679c\u6570\u636e","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49JDBC\u5bf9\u63a5Hive"},{"location":"Development/DBeaver_6.3.4/#jdbcspark2x","text":"\uff08\u91cd\u8981 fi6.5.1\uff09\u51c6\u5907spark2x jdbc\u8fde\u63a5\u9a71\u52a8jar\u5305 \u767b\u9646linux\u7aefspark2x\u5ba2\u6237\u7aef\u627e\u5230jdbc\u76f8\u5173\u4f9d\u8d56\uff0c\u6bd4\u5982\uff1a /opt/145_651hdclient/hadoopclient/Spark2x/spark/jars/jdbc \u5c06\u8be5\u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u5230windows\u672c\u5730\u76ee\u5f55\uff0c\u6bd4\u5982 E:\\145config\\spark2xjars , \u6ce8\u610f\u91cc\u9762\u542b\u6709\u4e00\u4e2ajdbc_pom.xml\u6587\u4ef6\uff0c\u9700\u8981\u5220\u9664 \u7ee7\u7eed\u767b\u9646linux\u7aefspark2x\u5ba2\u6237\u7aef\u8def\u5f84 /opt/145_651hdclient/hadoopclient/Spark2x/spark/jars ,\u5206\u522b\u627e\u5230\u5982\u4e0b4\u4e2ajar\u5305\uff0c\u62f7\u8d1d\u5230windows\u672c\u5730\u76ee\u5f55\uff0c\u6bd4\u5982 E:\\145config\\spark2xjars log4j-1.2.17.jar woodstox-core-5.0.3.jar stax2-api-3.1.4.jar commons-configuration2-2.1.1.jar \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New . \u65b0\u5efa\u7684\u8fde\u63a5\u540d\u5b57\u4e3aFI-spark2x-651-direct\uff0c\u8fde\u63a5\u4fe1\u606f\u5982\u4e0b,\u5b8c\u6210\u540e\u70b9\u51fbOK 1. org.apache.hive.jdbc.HiveDriver 2. jdbc:hive2://172.16.4.141:24002,172.16.4.142:24002,172.16.4.143:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=E:/145config/user.keytab 3. Hadoop 4. \u70b9Add File\u5728\u4e0a\u8ff0\u6b65\u9aa4\u914d\u7f6e\u597d\u7684spark2x\u8fde\u63a5\u6240\u6709jar\u5305\u52a0\u8fdb\u53bb \u6ce8\u610f\uff1a\u8fde\u63a5url\u7684user.principal=developuser;user.keytab=E:/145config/user.keytab\u8fd9\u4e24\u4e2a\u53c2\u6570\u5fc5\u987b\u52a0\u4e0a\uff0c\u5e76\u4e14\u4fdd\u8bc1\u6b63\u786e \u8fde\u63a5url\u4e32\uff0c\u53ef\u5728Linux\u5ba2\u6237\u7aef\u4f7f\u7528 spark-beeline \u547d\u4ee4\u83b7\u53d6\u53c2\u8003\uff1a \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection .\u70b9\u51fb Next . \u9009\u62e9FI-spark2x-651-direct\u70b9\u51fbNEXT \u70b9\u51fbFinish \u53f3\u952e\u9009\u62e9FI-spark2x-651-direct\u70b9\u51fbEdit Connection \u70b9\u51fbTest connection \u67e5\u770b\u7ed3\u679c\u6570\u636e","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49JDBC\u5bf9\u63a5Spark2x"},{"location":"Development/DBeaver_6.3.4/#jdbchetu","text":"\u521b\u5efahetu\u5bf9\u63a5\u914d\u7f6e\u8def\u5f84 C:\\ecotesting\\hetu-config ,\u51c6\u5907\u4e00\u4e0b\u8ba4\u8bc1\u6587\u4ef6 \u5176\u4e2d presto-jdbc-316.jar \u4e3aHetu\u5bf9\u63a5jdbc\u9a71\u52a8jar\u5305\uff0c\u5728\u5ba2\u6237\u7aef\u4e2d\u83b7\u5f97 hetuserver.jks\u6587\u4ef6\u5728\u670d\u52a1\u7aefHSBroker\u4e0b\u83b7\u5f97 user.keytab\u548ckrb5.conf\u6587\u4ef6\u4e3a\u7528\u6237\u7684\u8ba4\u8bc1\u6587\u4ef6\uff0c\u5728manager\u4e0a\u83b7\u5f97 \u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New . \u65b0\u5efa\u7684\u8fde\u63a5\u540d\u5b57\u4e3aFI-hetu-test\uff0c\u8fde\u63a5\u4fe1\u606f\u5982\u4e0b,\u5b8c\u6210\u540e\u70b9\u51fbOK 1. io.prestosql.jdbc.PrestoDriver 2. jdbc:presto://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002?serviceDiscoveryMode=zooKeeper&zooKeeperNamespace=hsbroker&deploymentMode=on_yarn&user=developuser&SSL=true&SSLTrustStorePath=C:/ecotesting/hetu-config/hetuserver.jks&KerberosConfigPath=C:/ecotesting/hetu-config/krb5.conf&KerberosPrincipal=developuser&KerberosKeytabPath=C:/ecotesting/hetu-config/user.keytab&KerberosRemoteServiceName=HTTP&KerberosServicePrincipalPattern=%24%7BSERVICE%7D%40%24%7BHOST%7D 3. Hadoop 4. \u70b9Add File\u5728\u4e0b\u8f7d\u597d\u7684hive\u5ba2\u6237\u7aeflib\u4e2d\u628a\u6240\u6709jar\u5305\u52a0\u8fdb\u53bb \u6ce8\u610f\uff1a jdbc\u8fde\u63a5\u4e32\u4e2d\u542b\u6709\u7528\u6237\u540d\u4fe1\u606f\u4ee5\u53ca\u76f8\u5173\u914d\u7f6e\u6587\u4ef6\u7684\u8def\u5f84\uff0c\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u914d\u7f6e \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection .\u70b9\u51fb Next . \u9009\u62e9FI-hetu-test\u70b9\u51fbNEXT \u70b9\u51fbFinish \u53f3\u952e\u9009\u62e9FI-hetu-test\u70b9\u51fbEdit Connection \u70b9\u51fbTest connection \u67e5\u770b\u7ed3\u679c\u6570\u636e","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49JDBC\u5bf9\u63a5Hetu"},{"location":"Development/DBeaver_6.3.4/#fiber","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2. Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6.","title":"Fiber \u7b80\u4ecb"},{"location":"Development/DBeaver_6.3.4/#fiber_1","text":"","title":"Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DBeaver_6.3.4/#_5","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f.\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282.","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_6.3.4/#_6","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c. \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting .\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55. \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55. \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55. \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55. \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1.\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b.","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DBeaver_6.3.4/#_7","text":"\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml . Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\java64\\\\jdk\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.121:24002,172.16.4.122:24002,172.16.4.123:24002;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\java64\\\\jdk\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\java64\\\\jdk\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u5728\u672b\u5c3e\u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal . <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff08\u5982\u679c\u5df2\u914d\u7f6e\u53ef\u4ee5\u7565\u8fc7\u6b64\u6b65\uff09\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a.\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c\uff08\u5982\u679c\u5df2\u914d\u7f6e\u53ef\u4ee5\u7565\u8fc7\u6b64\u6b65\uff09. \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\java64\\jdk\\bin \u914d\u7f6e\u6587\u4ef6\u6700\u540e\u52a0\u4e0a\uff1a -Djava.security.auth.login.config=C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf -Dzookeeper.sasl.clientconfig=Client -Dzookeeper.auth.type=kerberos -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com \u5b8c\u6210\u540e\u91cd\u542fdbeaver\u914d\u7f6e\u751f\u6548","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_6.3.4/#dbeaverfiber","text":"","title":"DBeaver\u5bf9\u63a5Fiber"},{"location":"Development/DBeaver_6.3.4/#_8","text":"DBeaver\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6.","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DBeaver_6.3.4/#_9","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DBeaver_6.3.4/#dbeaverfiberhive","text":"\u8fdb\u5165DBeaver\u754c\u9762\uff0c\u83dc\u5355\u9009\u62e9 Database->DriverManager \uff0c\u5728\u5f39\u51fa\u7684\u5bf9\u8bdd\u6846\u4e2d\u70b9\u51fb New . \u586b\u5199\u57fa\u672c\u4fe1\u606f\u5982\u4e0b\uff1a Driver Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Class Name\uff1acom.huawei.fiber.FiberDriver URL Template\uff1ajdbc:fiber:// Default Port\uff1a2345\uff08\u53ef\u968f\u4fbf\u5199\uff09 Category: Hadoop * \u70b9\u51fb Add File \uff0c\u589e\u52a0 C:\\ecotesting\\Fiber\\lib \u6240\u6709\u7684jar\u5305. \u70b9\u51fb Connection properties \uff0c\u589e\u52a0\u4e24\u4e2a\u5c5e\u6027.\u70b9\u51fb OK . defaultDriver = hive fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \u83dc\u5355\u680f\u9009\u62e9 File->New->Database Connection .\u70b9\u51fb Next . \u9009\u62e9 Fiber \uff0c\u70b9\u51fb Next . \u201cUser name\u201d\u548c\u201cPassword\u201d\u53ef\u4ee5\u4e0d\u586b\u5199\uff0c\u70b9\u51fb Connection details (name,type,...) . \u201cConnection name\u201d\u8f93\u5165 Hadoop - Fiber .\u70b9\u51fb back . \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive .\u5176\u4f59\u9009\u9879\u4fdd\u6301\u9ed8\u8ba4.\u70b9\u51fb Finish . \u6d4b\u8bd5hive\u8fde\u63a5. \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection . \u70b9\u51fb Driver properties \u786e\u8ba4\u201cdefaultDirver\u201d\u4e3a hive \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f.\u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3. \u53cc\u51fb Database Navigator->Hadoop - Fiber \uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f. \u67e5\u770bHive\u8868\u7684\u6570\u636e.\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e. SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e. \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor . \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e. SELECT * FROM student; \u5411Hive\u8868test\u63d2\u5165\u6570\u636e \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 test .\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e. CREATE TABLE IF NOT EXISTS test (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b. data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test. LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test. SELECT * FROM test;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Hive"},{"location":"Development/DBeaver_6.3.4/#dbeaverfiberspark2x","text":"\u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection . \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a spark2x \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f.\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3. \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f . \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f. \u67e5\u770b\u8868\u7684\u6570\u636e.\u70b9\u51fb default->tables->student \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e. SQL\u67e5\u8be2\u8868\u7684\u6570\u636e. \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor . \u8f93\u5165\u67e5\u8be2\u8bed\u53e5\uff0c\u70b9\u51fb\u53f3\u4e09\u89d2\u6267\u884c\u811a\u672c\uff0c\u8fd4\u56de\u67e5\u8be2\u8868\u7684\u6570\u636e. SELECT * FROM student; \u5411\u8868test\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b. data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u5728SQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868test. LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE test; \u5728SQL Editor\u4e2d\u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868test. SELECT * FROM test;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x"},{"location":"Development/DBeaver_6.3.4/#dbeaverfiberphoenix","text":"\u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Edit Connection . \u70b9\u51fb Driver properties \u4fee\u6539\u201cdefaultDirver\u201d\u4e3a phoenix \uff0c\u70b9\u51fb Test Connection ... \uff0c\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u5219\u8fde\u63a5\u6210\u529f.\u70b9\u51fb OK \u5173\u95ed\u63d0\u793a\u7a97\u53e3. \u70b9\u51fb OK \u5173\u95ed\u914d\u7f6e\u7a97\u53e3\uff0c\u5f39\u51fa\u201cConnection \"Hadoop - Fiber\" has changed. Do you want to reconnect?\u201d\uff0c\u70b9\u51fb \u662f . \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f. \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS . create_namespace 'MY_NS' \u5728\u83dc\u5355\u680f\u9009\u62e9 SQL Editor->New SQL Editor \uff0cSQL Editor\u4e2d\u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u8868\u548c\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c\u811a\u672c. CREATE TABLE IF NOT EXISTS MY_NS.TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.TEST VALUES(1,'John'); UPSERT INTO MY_NS.TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.TEST VALUES(4,'Aurora'); \u53f3\u952e Database Navigator->Hadoop - Fiber \u9009\u62e9 Refresh \u5219\u53ef\u770b\u5230\u65b0\u5efa\u7684\u547d\u540d\u7a7a\u95f4 MY_NS . \u67e5\u770b\u8868\u7684\u6570\u636e.\u70b9\u51fb MY_NS->tables->TEST \uff0c\u5728 Data \u9875\u9762\u53ef\u67e5\u770btest\u8868\u6570\u636e. SQL\u4fee\u6539\u8868\u7684\u6570\u636e.\u5728SQL Editor\u8f93\u5165\u4fee\u6539\u811a\u672c\u5e76\u6267\u884c. UPSERT INTO MY_NS.TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e.\u5728SQL Editor\u8f93\u5165\u5220\u9664\u811a\u672c\u5e76\u6267\u884c. DELETE FROM MY_NS.TEST WHERE ID=4; SQL\u67e5\u8be2\u8868\u7684\u6570\u636e.\u5728SQL Editor\u8f93\u5165\u67e5\u8be2\u811a\u672c\u5e76\u6267\u884c.\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664. SELECT * FROM MY_NS.TEST;","title":"DBeaver\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix"},{"location":"Development/DBeaver_6.3.4/#faq","text":"\u5bf9\u63a5Phoenix\u65f6\u8fd4\u56deDriver: Fiber? \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u5bf9\u63a5Phoenix\u65f6\uff0c\u70b9\u51fb Test Connection \uff0c\u6ca1\u6709\u6b63\u786e\u8fd4\u56deServer\u548cDriver\u7684\u7248\u672c. \u3010\u67e5\u770b\u95ee\u9898\u539f\u56e0\u3011 \u9996\u5148\u767b\u9646\u540e\u53f0\u67e5\u770b\u65e5\u5fd7 C:\\Users\\haoxi\\AppData\\Roaming\\DBeaverData\\workspace6\\.metadata\\dbeaver-debug.log \u67e5\u770b\u65e5\u5fd7\uff1a java.sql.SQLException: Can not connect to driver: org.apache.phoenix.jdbc.PhoenixDriver. Message: ERROR 103 (08004): Unable to establish connection. at com.huawei.fiber.FiberConnection.getConnection(FiberConnection.java:881) at com.huawei.fiber.FiberConnection.getConnection(FiberConnection.java:650) \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u786e\u8ba4\u662f\u5426\u5df2\u6307\u5b9aDBeaver\u7684JDK\u865a\u62df\u673a.\u5728DBeaver\u5b89\u88c5\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00dbeaver.ini\u8bbe\u7f6e -vm \u53c2\u6570\u7684\u503c\uff0c\u53c2\u6570\u548c\u503c\u4e4b\u95f4\u9700\u8981\u6362\u884c. \u793a\u4f8b\u5982\u4e0b\uff1a -vm C:\\Program Files\\Java\\jdk1.8.0_202\\bin","title":"FAQ"},{"location":"Development/DbVisualizer_10.0.1/","text":"DbVisualizer\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DbVisualizer 9.5.7 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) DbVisualizer 10.0.1 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL) \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DbVisualizer\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 DbVisualizer\u5b89\u88c5 \u00b6 DbVisualizer9.5.7\u9700\u8981jdk1.8\uff0c\u4e0b\u8f7d\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u4fee\u6539C:\\Windows\\System32\\drivers\\etc\\hosts\u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982C:\\Fiber\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dDbVisualizer\uff0c\u5730\u5740\uff1a http://www.dbvis.com/download/ \uff0c\u4e0b\u8f7d\u8f6f\u4ef6dbvis_windows-x64_9_5_7_jre.exe \u53cc\u51fbdbvis_windows-x64_9_5_7_jre.exe\u5b89\u88c5 DbVisualizer\u8fde\u63a5Fiber \u00b6 \u914d\u7f6eDbVisualizer\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001Spark\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00DbVisualizer9.5.7\uff0c\u70b9\u51fb Cancel \u83dc\u5355\u680f\u9009\u62e9 ToolsDriver Manager \u65b0\u5efadriver Name\uff1aFiber(\u81ea\u5b9a\u4e49) URL Format\uff1ajdbc:fiber:// User Specified\uff1a\u5c06C:\\Fiber\\lib\\\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Driver Class\uff1a\u52a0\u5165jar\u5305\u540e\u9009\u62e9com.huawei.fiber.FiberDriver \u83dc\u5355\u680f Database -> Create Database Connection \u9009\u62e9 Use Wizard {width=\"4.2in\" height=\"1.4in\"} \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982Fiber \u9009\u62e9Driver Fiber \u586b\u5199URL\uff1ajdbc:fiber:// \u70b9\u51fb Finish \u67e5\u8be2Hive\u8868\u6570\u636e \u00b6 \u6253\u5f00 Properties \u9762\u677f\uff0c\u586b\u5199defaultDriver\u548cfiberconfig\u5c5e\u6027\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\uff0c\u53ef\u4ee5\u5728\u5de6\u4fa7\u770b\u5230hive\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 \u67e5\u8be2SparkSQL\u4e2d\u7684\u6570\u636e \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aspark\uff1a\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aspark\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00Connection\u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230SparkSQL\u4e2d\u7684\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 \u67e5\u8be2Phoenix\u4e2d\u7684\u6570\u636e \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aphoenix\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230phoenix\u6570\u636e\u8868 \u67e5\u770bphoenix\u8868TB_PHOENIX\u4e2d\u7684\u6570\u636e\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e \u00b6 Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e\uff0c\u9700\u8981\u5728Fiber\u4e2dhbase\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u4e2d\u52a0\u5165\u5982\u4e0b\u53c2\u6570\uff0c\u5426\u5219\u4e0d\u4f1a\u81ea\u52a8Commit \u4fee\u6539Hbase-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \uff0c\u7136\u540e\u91cd\u542fDbVisualizer\u3002 <property> <name>phoenix.connection.autoCommit</name> <value>true</value> </property> Phoenix\u8868\u589e\u52a0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (104,'phoenix_user4','company4'); select * from tb_phoenix; Phoenix\u8868\u5220\u9664\u6570\u636e delete from tb_phoenix where id=104; select * from tb_phoenix; Phoenix\u8868\u66f4\u65b0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (102,'phoenix_user2_up','company2_up'); select * from tb_phoenix;","title":"9.5.7 <--> C60"},{"location":"Development/DbVisualizer_10.0.1/#dbvisualizerfusioninsight","text":"","title":"DbVisualizer\u5bf9\u63a5FusionInsight"},{"location":"Development/DbVisualizer_10.0.1/#_1","text":"DbVisualizer 9.5.7 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) DbVisualizer 10.0.1 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DbVisualizer_10.0.1/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86DbVisualizer\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/DbVisualizer_10.0.1/#dbvisualizer","text":"DbVisualizer9.5.7\u9700\u8981jdk1.8\uff0c\u4e0b\u8f7d\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u4fee\u6539C:\\Windows\\System32\\drivers\\etc\\hosts\u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982C:\\Fiber\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml Hive\u7684JDBC\u8fde\u63a5 <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5 <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dDbVisualizer\uff0c\u5730\u5740\uff1a http://www.dbvis.com/download/ \uff0c\u4e0b\u8f7d\u8f6f\u4ef6dbvis_windows-x64_9_5_7_jre.exe \u53cc\u51fbdbvis_windows-x64_9_5_7_jre.exe\u5b89\u88c5","title":"DbVisualizer\u5b89\u88c5"},{"location":"Development/DbVisualizer_10.0.1/#dbvisualizerfiber","text":"\u914d\u7f6eDbVisualizer\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001Spark\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00DbVisualizer9.5.7\uff0c\u70b9\u51fb Cancel \u83dc\u5355\u680f\u9009\u62e9 ToolsDriver Manager \u65b0\u5efadriver Name\uff1aFiber(\u81ea\u5b9a\u4e49) URL Format\uff1ajdbc:fiber:// User Specified\uff1a\u5c06C:\\Fiber\\lib\\\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Driver Class\uff1a\u52a0\u5165jar\u5305\u540e\u9009\u62e9com.huawei.fiber.FiberDriver \u83dc\u5355\u680f Database -> Create Database Connection \u9009\u62e9 Use Wizard {width=\"4.2in\" height=\"1.4in\"} \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982Fiber \u9009\u62e9Driver Fiber \u586b\u5199URL\uff1ajdbc:fiber:// \u70b9\u51fb Finish","title":"DbVisualizer\u8fde\u63a5Fiber"},{"location":"Development/DbVisualizer_10.0.1/#hive","text":"\u6253\u5f00 Properties \u9762\u677f\uff0c\u586b\u5199defaultDriver\u548cfiberconfig\u5c5e\u6027\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\uff0c\u53ef\u4ee5\u5728\u5de6\u4fa7\u770b\u5230hive\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2Hive\u8868\u6570\u636e"},{"location":"Development/DbVisualizer_10.0.1/#sparksql","text":"\u5c06defaultDriver\u5207\u6362\u4e3aspark\uff1a\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aspark\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00Connection\u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230SparkSQL\u4e2d\u7684\u6570\u636e\u8868\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2SparkSQL\u4e2d\u7684\u6570\u636e"},{"location":"Development/DbVisualizer_10.0.1/#phoenix","text":"\u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u5c06 Properties \u4e2d\u7684defaultDriver\u503c\u6539\u4e3aphoenix\uff0c\u70b9\u51fb Apply \u3002 \u6253\u5f00 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \uff0c\u8fde\u63a5\u6210\u529f\uff0c\u53ef\u4ee5\u770b\u5230phoenix\u6570\u636e\u8868 \u67e5\u770bphoenix\u8868TB_PHOENIX\u4e2d\u7684\u6570\u636e\u3002 \u83dc\u5355\u680f\u9009\u62e9 File -> New SQL Commander \uff0c\u7f16\u8f91SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002","title":"\u67e5\u8be2Phoenix\u4e2d\u7684\u6570\u636e"},{"location":"Development/DbVisualizer_10.0.1/#phoenix_1","text":"Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e\uff0c\u9700\u8981\u5728Fiber\u4e2dhbase\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u4e2d\u52a0\u5165\u5982\u4e0b\u53c2\u6570\uff0c\u5426\u5219\u4e0d\u4f1a\u81ea\u52a8Commit \u4fee\u6539Hbase-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e C:\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \uff0c\u7136\u540e\u91cd\u542fDbVisualizer\u3002 <property> <name>phoenix.connection.autoCommit</name> <value>true</value> </property> Phoenix\u8868\u589e\u52a0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (104,'phoenix_user4','company4'); select * from tb_phoenix; Phoenix\u8868\u5220\u9664\u6570\u636e delete from tb_phoenix where id=104; select * from tb_phoenix; Phoenix\u8868\u66f4\u65b0\u6570\u636e UPSERT into tb_phoenix(Id, Name,Company) values (102,'phoenix_user2_up','company2_up'); select * from tb_phoenix;","title":"Phoenix\u7684\u589e\u52a0\u5220\u9664\u66f4\u65b0\u6570\u636e"},{"location":"Development/DbVisualizer_10.0.21/","text":"DbVisualizer\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 DbVisualizer 10.0.21 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL) DbVisualizer 10.0.21 \u2194 FusionInsight MRS 8.0 (Hive/Phoenix/SparkSQL) \u7b80\u4ecb \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDbVisualizer\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 C:\\ecotesting\\Fiber\\HBase\\hbase\\lib\\phoenix-core-4.13.1-HBase-1.3.jar \u62f7\u8d1d\u81f3 C:\\ecotesting\\Fiber\\lib \u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \u3002 <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> DbVisualizer\u5bf9\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 DbVisualizer\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u4ece http://www.dbvis.com/download/ \u4e0b\u8f7d\u4e0e\u672c\u5730\u7cfb\u7edf\u76f8\u5bf9\u5e94\u7684DbVisualizer\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002\u53ef\u6309\u7167\u9ed8\u8ba4\u9009\u9879\u5b8c\u6210\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Hive \u00b6 \u6253\u5f00DbVisualizer\uff0c\u70b9\u51fb Cancel \u3002 \u83dc\u5355\u680f\u9009\u62e9 Tools->Driver Manager \u3002 \u70b9\u51fb \u65b0\u5efadriver\u3002\u4fe1\u606f\u586b\u5199\u5b8c\u6bd5\u540e\uff0c\u5173\u95ed\u8be5\u7a97\u53e3\u3002 \u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 URL Format\uff1ajdbc:fiber:// Dirver jar Files: \u5c06C:\\ecotesting\\Fiber\\lib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Class Name\uff1acom.huawei.fiber.FiberDriver\uff08\u6dfb\u52a0jar\u5305\u540e\u4ece\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\uff09 \u83dc\u5355\u680f\u9009\u62e9 Database-> Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982 Fiber \uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9\u6570\u636e\u5e93Driver\u3002\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9 Fiber \u3002 \u586b\u5199 Database URL = jdbc:fiber:// \uff0c\u5176\u4f59\u7684\u53ef\u4e0d\u586b\u5199\u3002\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 \u9009\u62e9 Databases->Connections->Fiber \uff0c\u9009\u62e9 Properties \u9762\u677f\uff0c\u586b\u5199 defaultDriver = hive \u548c fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\u3002 Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91\u67e5\u8be2SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 select * from student; DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x \u00b6 \u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Driver Properties \uff0c\u5c06 \u201cdefaultDriver\u201d\u4fee\u6539\u4e3a spark2x \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \u6309\u94ae\u3002Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91\u67e5\u8be2SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 select * from student; DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix \u00b6 \u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Driver Properties \uff0c\u5c06 \u201cdefaultDriver\u201d\u4fee\u6539\u4e3a phoenix \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \u6309\u94ae\u3002Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 SQL\u64cd\u4f5c\u8868\u6570\u636e\u3002 \u5982\u679c\u4e0d\u5b58\u5728\u81ea\u5b9a\u4e49\u7684\u547d\u540d\u7a7a\u95f4\u7a7a\u95f4\u201cMY_NS\u201d\uff0c\u5219\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u521b\u5efa\u8868\u548c\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.SQL_TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.SQL_TEST VALUES(1,'John'); UPSERT INTO MY_NS.SQL_TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.SQL_TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.SQL_TEST VALUES(4,'Aurora'); SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 select * from MY_NS.SQL_TEST; SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO MY_NS.SQL_TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 DELETE FROM MY_NS.SQL_TEST WHERE ID=4; \u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 select * from MY_NS.SQL_TEST; \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb MY_NS->TABLE->SQL_TEST->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 FAQ \u00b6 \u754c\u9762\u67e5\u770b\u8868\u6570\u636e\u65f6\uff0c\u8fd4\u56deParseException line 1:14 cannot recognize input near '\"default\"' '.' '\"student\"' in join source \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u8fde\u63a5\u4e0aHive\u540e\uff0c\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u65f6\uff0c\u8fd4\u56de\u4ee5\u4e0b\u9519\u8bef\u3002 An error occurred while performing the operation: Error while compiling statement: FAILED: ParseException line 1:14 cannot recognize input near '\"default\"' '.' '\"student\"' in join source \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Generic->Delimited Identifiers \u3002\u5c06 Begin Identifier \u548c End Identifier \u7684\u5185\u5bb9\uff08\u4f8b\u5982\u53cc\u5f15\u53f7 \" \uff09\u6e05\u7a7a\u540e\uff0c\u70b9\u51fb Apply \u3002","title":"10.0.21 <--> 8.0"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfusioninsight","text":"","title":"DbVisualizer\u5bf9\u63a5FusionInsight"},{"location":"Development/DbVisualizer_10.0.21/#_1","text":"DbVisualizer 10.0.21 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL) DbVisualizer 10.0.21 \u2194 FusionInsight MRS 8.0 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/DbVisualizer_10.0.21/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cDbVisualizer\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Development/DbVisualizer_10.0.21/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1);","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Development/DbVisualizer_10.0.21/#fiber","text":"","title":"Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DbVisualizer_10.0.21/#_4","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DbVisualizer_10.0.21/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 C:\\ecotesting\\Fiber\\HBase\\hbase\\lib\\phoenix-core-4.13.1-HBase-1.3.jar \u62f7\u8d1d\u81f3 C:\\ecotesting\\Fiber\\lib \u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DbVisualizer_10.0.21/#_6","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DbVisualizer_10.0.21/#kinit","text":"\u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DbVisualizer_10.0.21/#keytab","text":"\u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \u3002 <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfiber","text":"","title":"DbVisualizer\u5bf9\u63a5Fiber"},{"location":"Development/DbVisualizer_10.0.21/#_7","text":"DbVisualizer\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/DbVisualizer_10.0.21/#_8","text":"\u4ece http://www.dbvis.com/download/ \u4e0b\u8f7d\u4e0e\u672c\u5730\u7cfb\u7edf\u76f8\u5bf9\u5e94\u7684DbVisualizer\u8f6f\u4ef6\uff0c\u5e76\u5b8c\u6210\u5b89\u88c5\u3002\u53ef\u6309\u7167\u9ed8\u8ba4\u9009\u9879\u5b8c\u6210\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/DbVisualizer_10.0.21/#_9","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfiberhive","text":"\u6253\u5f00DbVisualizer\uff0c\u70b9\u51fb Cancel \u3002 \u83dc\u5355\u680f\u9009\u62e9 Tools->Driver Manager \u3002 \u70b9\u51fb \u65b0\u5efadriver\u3002\u4fe1\u606f\u586b\u5199\u5b8c\u6bd5\u540e\uff0c\u5173\u95ed\u8be5\u7a97\u53e3\u3002 \u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 URL Format\uff1ajdbc:fiber:// Dirver jar Files: \u5c06C:\\ecotesting\\Fiber\\lib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Class Name\uff1acom.huawei.fiber.FiberDriver\uff08\u6dfb\u52a0jar\u5305\u540e\u4ece\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\uff09 \u83dc\u5355\u680f\u9009\u62e9 Database-> Create Database Connection \u3002 \u9009\u62e9 Use Wizard \u3002 \u81ea\u5b9a\u4e49\u8fde\u63a5\u540d\u79f0\uff0c\u4f8b\u5982 Fiber \uff0c\u70b9\u51fb Next \u3002 \u9009\u62e9\u6570\u636e\u5e93Driver\u3002\u5728\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9 Fiber \u3002 \u586b\u5199 Database URL = jdbc:fiber:// \uff0c\u5176\u4f59\u7684\u53ef\u4e0d\u586b\u5199\u3002\u70b9\u51fb Finish \u5b8c\u6210\u914d\u7f6e\u3002 \u9009\u62e9 Databases->Connections->Fiber \uff0c\u9009\u62e9 Properties \u9762\u677f\uff0c\u586b\u5199 defaultDriver = hive \u548c fiberconfig = C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Connect \u6309\u94ae\u3002 Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91\u67e5\u8be2SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 select * from student;","title":"DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Hive"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfiberspark2x","text":"\u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Driver Properties \uff0c\u5c06 \u201cdefaultDriver\u201d\u4fee\u6539\u4e3a spark2x \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \u6309\u94ae\u3002Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91\u67e5\u8be2SQL\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 select * from student;","title":"DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x"},{"location":"Development/DbVisualizer_10.0.21/#dbvisualizerfiberphoenix","text":"\u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Driver Properties \uff0c\u5c06 \u201cdefaultDriver\u201d\u4fee\u6539\u4e3a phoenix \uff0c\u70b9\u51fb Apply \u3002 \u9009\u62e9 Connection \u9762\u677f\uff0c\u70b9\u51fb Reconnect \u6309\u94ae\u3002Connection Message\u8fd4\u56de\u4f7f\u7528\u7684Server\u548cDriver\u7248\u672c\u4fe1\u606f\uff0c\u4e14\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 SQL\u64cd\u4f5c\u8868\u6570\u636e\u3002 \u5982\u679c\u4e0d\u5b58\u5728\u81ea\u5b9a\u4e49\u7684\u547d\u540d\u7a7a\u95f4\u7a7a\u95f4\u201cMY_NS\u201d\uff0c\u5219\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u521b\u5efa\u8868\u548c\u6570\u636e\u3002\u83dc\u5355\u680f\u9009\u62e9 File->New SQL Commander \uff0c\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u67e5\u770b\u67e5\u8be2\u7ed3\u679c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.SQL_TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.SQL_TEST VALUES(1,'John'); UPSERT INTO MY_NS.SQL_TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.SQL_TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.SQL_TEST VALUES(4,'Aurora'); SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 select * from MY_NS.SQL_TEST; SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO MY_NS.SQL_TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 DELETE FROM MY_NS.SQL_TEST WHERE ID=4; \u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 select * from MY_NS.SQL_TEST; \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb MY_NS->TABLE->SQL_TEST->Data \u67e5\u770bstudent\u8868\u6570\u636e\u3002","title":"DbVisualizer\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix"},{"location":"Development/DbVisualizer_10.0.21/#faq","text":"\u754c\u9762\u67e5\u770b\u8868\u6570\u636e\u65f6\uff0c\u8fd4\u56deParseException line 1:14 cannot recognize input near '\"default\"' '.' '\"student\"' in join source \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u8fde\u63a5\u4e0aHive\u540e\uff0c\u70b9\u51fb default->TABLE->student->Data \u67e5\u770bstudent\u8868\u6570\u636e\u65f6\uff0c\u8fd4\u56de\u4ee5\u4e0b\u9519\u8bef\u3002 An error occurred while performing the operation: Error while compiling statement: FAILED: ParseException line 1:14 cannot recognize input near '\"default\"' '.' '\"student\"' in join source \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u53cc\u51fb\u6570\u636e\u8fde\u63a5 Fiber \u6253\u5f00DataBase Connection\u914d\u7f6e\u9762\u677f\u3002\u9009\u62e9 Properties->Generic->Delimited Identifiers \u3002\u5c06 Begin Identifier \u548c End Identifier \u7684\u5185\u5bb9\uff08\u4f8b\u5982\u53cc\u5f15\u53f7 \" \uff09\u6e05\u7a7a\u540e\uff0c\u70b9\u51fb Apply \u3002","title":"FAQ"},{"location":"Development/HUE/","text":"HUE\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 HUE 4.0.1 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Hive/Spark)","title":"4.0.1 <--> C70"},{"location":"Development/HUE/#huefusioninsight","text":"","title":"HUE\u5bf9\u63a5FusionInsight"},{"location":"Development/HUE/#_1","text":"HUE 4.0.1 \u2194 FusionInsight HD V100R002C70SPC200 (HDFS/HBase/Hive/Spark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/JupyterHub/","text":"JupyterHub\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 JupyterHub 1.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x) JupyterHub 1.0.0 \u2194 FusionInsight HD 6.5 (Spark2x) JupyterHub 1.0.0 \u2194 FusionInsight MRS 8.0 (Spark2x) \u8bf4\u660e\uff1a 1. \u76f8\u8f83\u4e8eJupyter Notebook, JupyterHub\u80fd\u591f\u652f\u6301\u591a\u7528\u6237\u8bbf\u95ee\uff0c\u53ef\u7528\u4e8e\u521b\u5efa\u3001\u7ba1\u7406\u3001\u4ee3\u7406\u591a\u4e2aJupyter Notebook \u5b9e\u4f8b\u3002\u5177\u6709\u6269\u5c55\u6027\u548c\u53ef\u5b9a\u5236\u6027\u3002JupyterHub\u9ed8\u8ba4\u4f7f\u7528python3\u5185\u6838\uff0c\u6240\u4ee5\u5728\u5b89\u88c5JupyterHub\u4e4b\u524d\u8981\u5148\u5b89\u88c5Anaconda3 FI HD\u96c6\u7fa4\u9ed8\u8ba4\u5b89\u88c5\u7684python\u7248\u672c\u4e3a2.x\uff0c\u4f7f\u7528pyspark\u65f6\u4f1a\u5728worker\u8282\u70b9\u8d77python\u8fdb\u7a0b\uff0c\u6240\u4ee5\u5982\u679c\u8981\u4f7f\u7528python3\u5185\u6838\uff0c\u9700\u8981\u5728\u8282\u70b9\u4e0a\u5b89\u88c5python3\u73af\u5883\u3002 JuypterHub\u4e3b\u673a\uff1a172.16.2.119 \u5bf9\u63a5\u96c6\u7fa4\uff1a172.16.6.10-12 \u5b89\u88c5Anaconda3 \u00b6 \u51c6\u5907Anaconda3\u5b89\u88c5\u5305\u5230/opt\u8def\u5f84\u4e0b: \u5b89\u88c5Anaconda3 \u4f7f\u7528\u547d\u4ee4\uff1abash Anaconda3-2019.07-Linux-x86_64.sh \u5b89\u88c5 \u56de\u8f66\u67e5\u770bLicense Agreement \u8f93\u5165yes \u9009\u62e9\u5b89\u88c5\u4f4d\u7f6e\u4e3a/opt/anaconda3 \u56de\u8f66\u5b89\u88c5(\u540e\u7eedjuypterhub \u591a\u7528\u6237\u4e0d\u652f\u6301root\u7528\u6237\u767b\u5f55\uff0c\u6240\u4ee5\u5b89\u88c5\u8def\u5f84\u4e0d\u8981\u653e\u5728/root\u8def\u5f84\u4e0b) \u5b8c\u6210\u5b89\u88c5\u540e\u5982\u679c\u9009yes\u8fdb\u884c\u521d\u59cb\u5316 \u4f1a\u81ea\u52a8\u5728 ~/.bashrc\u6587\u4ef6\u4e0b\u5199\u5165anaconda3\u7684\u521d\u59cb\u5316\u914d\u7f6e \u5c06\u66f4\u6539\u540e\u7684.bashrc\u91cd\u547d\u540d\u4e3a.bashrc.anaconda\uff0c \u8fd8\u662f\u4fdd\u6301.bashrc\u6587\u4ef6\u4e3a\u4e4b\u524d\u7684\u5185\u5bb9 \u91cd\u590d\u4e0a\u8ff0\u6b65\u9aa4\u300a\u5b89\u88c5Anaconda3\u300b\u5728\u5bf9\u63a5FI HD\u96c6\u7fa4\u4e09\u4e2a\u8282\u70b9172.16.6.10-12\u90fd\u90e8\u7f72\u597danaconda3\uff0c\u5b89\u88c5\u8def\u5f84\u90fd\u4e3a /opt/anaconda3 \u5b89\u88c5JupyterHub \u00b6 \u5b89\u88c5\u76f8\u5173\u4f9d\u8d56 yum install gcc yum install openssl openssl-devel yum install sqlite-devel JupyterHub\u5b89\u88c5\u6b65\u9aa4\uff1a source ~/.bashrc.anaconda \u53ef\u4ee5\u4f7f\u7528\u6e05\u534e\u7684conda\u6e90\u52a0\u901f conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5JupyterHub conda install -c conda-forge jupyterhub \u5b89\u88c5JupyterHub\u76f8\u5173\u5305 npm install -g configurable-http-proxy \u589e\u52a0\u7528\u6237abc,\u7528\u4e8e\u767b\u5f55: useradd abc passwd abc \u8bbe\u7f6e\u7528\u6237abc\u7684\u767b\u9646\u5bc6\u7801\u4e3aHuawei@123 \u751f\u6210JupyterHub\u914d\u7f6e\u6587\u4ef6 mkdir /etc/jupyterhub jupyterhub --generate-config -f /etc/jupyterhub/jupyterhub_config.py vi /etc/jupyterhub/jupyterhub_config.py \u4fee\u6539\u5982\u4e0b\u53c2\u6570\uff1a c.JupyterHub.ip = '172.16.2.119' c.JupyterHub.port = 8001 c.Authenticator.whitelist = {'root','abc'} c.Authenticator.admin_users = {'root','abc'} c.JupyterHub.statsd_prefix = 'jupyterhub' - \u4fee\u6539/opt/anaconda3\u7684\u6240\u5c5e\u7528\u6237\uff0c\u4f7f\u5f97\u521b\u5efa\u7528\u6237abc\u6709\u6743\u9650 chown abc:abc -R /opt/anaconda3/ \u4f7f\u7528abc\u7528\u6237\u767b\u5f55\uff0c\u5c06root\u7528\u6237\u7684~/.bashrc.anaconda\u6587\u4ef6\u5185\u5bb9\u62f7\u8d1d\u5230abc\u7528\u6237\u7684 ~ \u8def\u5f84\u4e0b \u5b8c\u6210\u540e su - abc vi ~/.bashrc.anaconda \u5185\u5bb9\u5e94\u4e0eroot\u7528\u6237\u7684\u4e00\u6837\uff1a \u4ee5\u7528\u6237abc\u767b\u9646,\u4e4b\u540e\u542f\u52a8JupyterHub su - abc source ~/.bashrc.anaconda source /opt/101hdclient/hadoopclient/bigdata_env kinit jupyterhub --config=/etc/jupyterhub/jupyterhub_config.py --ip=172.16.2.119 --port=8001 --no-ssl \u6d4f\u89c8\u5668\u6253\u5f00JupyterHub\u7684Web UI\uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801(abc/Huawei@123) \u5bf9\u63a5Spark2x \u00b6 \u5230\u5982\u4e0b\u94fe\u63a5\u83b7\u53d6\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6airlines.csv\uff0c\u5e76\u5c06\u6570\u636e\u6587\u4ef6\u4e0a\u4f20\u5230\u5bf9\u63a5\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\uff1a https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u4e0b\u9762\u4e3a\u4ee3\u7801\u7247\u6bb5 import sys sys.path.insert(0, '/opt/101hdclient/hadoopclient/Spark2x/spark/python/') sys.path.insert(0, '/opt/101hdclient/hadoopclient/Spark2x/spark/python/lib/py4j-0.10.4-src.zip') import os os.environ[\"PYSPARK_PYTHON\"]=\"/opt/anaconda3/bin/python3\" import pyspark from pyspark import SparkConf from pyspark import SparkContext # use this conf = pyspark.SparkConf().setMaster('yarn').setAppName('spark-wordcount_from172.16.2.119') sc = pyspark.SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print ('Nonempty lines', nonempty_lines.count()) words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print ('Top 100 words:') print (wordcounts.take(100)) sc.stop() \u8bf4\u660e\uff1a 1. /opt/101hdclient/hadoopclient/Spark2x/spark/python/ \u4e3aJupyterHub\u4e3b\u673a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4Spark2x\u5ba2\u6237\u7aef\u4e2d\u7684python\u8def\u5f84 2. /opt/101hdclient/hadoopclient/Spark2x/spark/python/lib/py4j-0.10.4-src.zip \u4e3a\u5bf9\u63a5\u96c6\u7fa4Spark2x\u5ba2\u6237\u7aef\u4e2d\u7684python\u7684lib\u8def\u5f84\uff0c\u5177\u4f53\u7684zip\u5305\u4e0e\u5b9e\u9645\u60c5\u51b5\u4e00\u81f4 3. /opt/anaconda3/bin/python3 \u4e3apython\u73af\u5883\u5b89\u88c5\u8def\u5f84 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u540e\u53f0yarn\u67e5\u770b\uff1a","title":"1.0.0 <--> 8.0"},{"location":"Development/JupyterHub/#jupyterhubfusioninsight","text":"","title":"JupyterHub\u5bf9\u63a5FusionInsight"},{"location":"Development/JupyterHub/#_1","text":"JupyterHub 1.0.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x) JupyterHub 1.0.0 \u2194 FusionInsight HD 6.5 (Spark2x) JupyterHub 1.0.0 \u2194 FusionInsight MRS 8.0 (Spark2x) \u8bf4\u660e\uff1a 1. \u76f8\u8f83\u4e8eJupyter Notebook, JupyterHub\u80fd\u591f\u652f\u6301\u591a\u7528\u6237\u8bbf\u95ee\uff0c\u53ef\u7528\u4e8e\u521b\u5efa\u3001\u7ba1\u7406\u3001\u4ee3\u7406\u591a\u4e2aJupyter Notebook \u5b9e\u4f8b\u3002\u5177\u6709\u6269\u5c55\u6027\u548c\u53ef\u5b9a\u5236\u6027\u3002JupyterHub\u9ed8\u8ba4\u4f7f\u7528python3\u5185\u6838\uff0c\u6240\u4ee5\u5728\u5b89\u88c5JupyterHub\u4e4b\u524d\u8981\u5148\u5b89\u88c5Anaconda3 FI HD\u96c6\u7fa4\u9ed8\u8ba4\u5b89\u88c5\u7684python\u7248\u672c\u4e3a2.x\uff0c\u4f7f\u7528pyspark\u65f6\u4f1a\u5728worker\u8282\u70b9\u8d77python\u8fdb\u7a0b\uff0c\u6240\u4ee5\u5982\u679c\u8981\u4f7f\u7528python3\u5185\u6838\uff0c\u9700\u8981\u5728\u8282\u70b9\u4e0a\u5b89\u88c5python3\u73af\u5883\u3002 JuypterHub\u4e3b\u673a\uff1a172.16.2.119 \u5bf9\u63a5\u96c6\u7fa4\uff1a172.16.6.10-12","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/JupyterHub/#anaconda3","text":"\u51c6\u5907Anaconda3\u5b89\u88c5\u5305\u5230/opt\u8def\u5f84\u4e0b: \u5b89\u88c5Anaconda3 \u4f7f\u7528\u547d\u4ee4\uff1abash Anaconda3-2019.07-Linux-x86_64.sh \u5b89\u88c5 \u56de\u8f66\u67e5\u770bLicense Agreement \u8f93\u5165yes \u9009\u62e9\u5b89\u88c5\u4f4d\u7f6e\u4e3a/opt/anaconda3 \u56de\u8f66\u5b89\u88c5(\u540e\u7eedjuypterhub \u591a\u7528\u6237\u4e0d\u652f\u6301root\u7528\u6237\u767b\u5f55\uff0c\u6240\u4ee5\u5b89\u88c5\u8def\u5f84\u4e0d\u8981\u653e\u5728/root\u8def\u5f84\u4e0b) \u5b8c\u6210\u5b89\u88c5\u540e\u5982\u679c\u9009yes\u8fdb\u884c\u521d\u59cb\u5316 \u4f1a\u81ea\u52a8\u5728 ~/.bashrc\u6587\u4ef6\u4e0b\u5199\u5165anaconda3\u7684\u521d\u59cb\u5316\u914d\u7f6e \u5c06\u66f4\u6539\u540e\u7684.bashrc\u91cd\u547d\u540d\u4e3a.bashrc.anaconda\uff0c \u8fd8\u662f\u4fdd\u6301.bashrc\u6587\u4ef6\u4e3a\u4e4b\u524d\u7684\u5185\u5bb9 \u91cd\u590d\u4e0a\u8ff0\u6b65\u9aa4\u300a\u5b89\u88c5Anaconda3\u300b\u5728\u5bf9\u63a5FI HD\u96c6\u7fa4\u4e09\u4e2a\u8282\u70b9172.16.6.10-12\u90fd\u90e8\u7f72\u597danaconda3\uff0c\u5b89\u88c5\u8def\u5f84\u90fd\u4e3a /opt/anaconda3","title":"\u5b89\u88c5Anaconda3"},{"location":"Development/JupyterHub/#jupyterhub","text":"\u5b89\u88c5\u76f8\u5173\u4f9d\u8d56 yum install gcc yum install openssl openssl-devel yum install sqlite-devel JupyterHub\u5b89\u88c5\u6b65\u9aa4\uff1a source ~/.bashrc.anaconda \u53ef\u4ee5\u4f7f\u7528\u6e05\u534e\u7684conda\u6e90\u52a0\u901f conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config --set show_channel_urls yes \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5JupyterHub conda install -c conda-forge jupyterhub \u5b89\u88c5JupyterHub\u76f8\u5173\u5305 npm install -g configurable-http-proxy \u589e\u52a0\u7528\u6237abc,\u7528\u4e8e\u767b\u5f55: useradd abc passwd abc \u8bbe\u7f6e\u7528\u6237abc\u7684\u767b\u9646\u5bc6\u7801\u4e3aHuawei@123 \u751f\u6210JupyterHub\u914d\u7f6e\u6587\u4ef6 mkdir /etc/jupyterhub jupyterhub --generate-config -f /etc/jupyterhub/jupyterhub_config.py vi /etc/jupyterhub/jupyterhub_config.py \u4fee\u6539\u5982\u4e0b\u53c2\u6570\uff1a c.JupyterHub.ip = '172.16.2.119' c.JupyterHub.port = 8001 c.Authenticator.whitelist = {'root','abc'} c.Authenticator.admin_users = {'root','abc'} c.JupyterHub.statsd_prefix = 'jupyterhub' - \u4fee\u6539/opt/anaconda3\u7684\u6240\u5c5e\u7528\u6237\uff0c\u4f7f\u5f97\u521b\u5efa\u7528\u6237abc\u6709\u6743\u9650 chown abc:abc -R /opt/anaconda3/ \u4f7f\u7528abc\u7528\u6237\u767b\u5f55\uff0c\u5c06root\u7528\u6237\u7684~/.bashrc.anaconda\u6587\u4ef6\u5185\u5bb9\u62f7\u8d1d\u5230abc\u7528\u6237\u7684 ~ \u8def\u5f84\u4e0b \u5b8c\u6210\u540e su - abc vi ~/.bashrc.anaconda \u5185\u5bb9\u5e94\u4e0eroot\u7528\u6237\u7684\u4e00\u6837\uff1a \u4ee5\u7528\u6237abc\u767b\u9646,\u4e4b\u540e\u542f\u52a8JupyterHub su - abc source ~/.bashrc.anaconda source /opt/101hdclient/hadoopclient/bigdata_env kinit jupyterhub --config=/etc/jupyterhub/jupyterhub_config.py --ip=172.16.2.119 --port=8001 --no-ssl \u6d4f\u89c8\u5668\u6253\u5f00JupyterHub\u7684Web UI\uff0c\u8f93\u5165\u7528\u6237\u540d\u5bc6\u7801(abc/Huawei@123)","title":"\u5b89\u88c5JupyterHub"},{"location":"Development/JupyterHub/#spark2x","text":"\u5230\u5982\u4e0b\u94fe\u63a5\u83b7\u53d6\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6airlines.csv\uff0c\u5e76\u5c06\u6570\u636e\u6587\u4ef6\u4e0a\u4f20\u5230\u5bf9\u63a5\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\uff1a https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u4e0b\u9762\u4e3a\u4ee3\u7801\u7247\u6bb5 import sys sys.path.insert(0, '/opt/101hdclient/hadoopclient/Spark2x/spark/python/') sys.path.insert(0, '/opt/101hdclient/hadoopclient/Spark2x/spark/python/lib/py4j-0.10.4-src.zip') import os os.environ[\"PYSPARK_PYTHON\"]=\"/opt/anaconda3/bin/python3\" import pyspark from pyspark import SparkConf from pyspark import SparkContext # use this conf = pyspark.SparkConf().setMaster('yarn').setAppName('spark-wordcount_from172.16.2.119') sc = pyspark.SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print ('Nonempty lines', nonempty_lines.count()) words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print ('Top 100 words:') print (wordcounts.take(100)) sc.stop() \u8bf4\u660e\uff1a 1. /opt/101hdclient/hadoopclient/Spark2x/spark/python/ \u4e3aJupyterHub\u4e3b\u673a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4Spark2x\u5ba2\u6237\u7aef\u4e2d\u7684python\u8def\u5f84 2. /opt/101hdclient/hadoopclient/Spark2x/spark/python/lib/py4j-0.10.4-src.zip \u4e3a\u5bf9\u63a5\u96c6\u7fa4Spark2x\u5ba2\u6237\u7aef\u4e2d\u7684python\u7684lib\u8def\u5f84\uff0c\u5177\u4f53\u7684zip\u5305\u4e0e\u5b9e\u9645\u60c5\u51b5\u4e00\u81f4 3. /opt/anaconda3/bin/python3 \u4e3apython\u73af\u5883\u5b89\u88c5\u8def\u5f84 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u540e\u53f0yarn\u67e5\u770b\uff1a","title":"\u5bf9\u63a5Spark2x"},{"location":"Development/JupyterNotebook/","text":"Jupyter Notebook\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Jupyter Notebook 2.7.16 \u2194 FusionInsight HD V100R002C80SPC200 (Hive/Elk/Spark2x) Jupyter Notebook 2.7.16 \u2194 FusionInsight HD 6.5 (HDFS/Hive/Elk/Spark2x) Jupyter Notebook 2.7.16 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/Elk/Hetu/Spark2x) \u8bf4\u660e\uff1aJupyter Notebook\u7248\u672c \u57fa\u4e8eAnaconda Python\u5185\u6838\u7248\u672c \u5b89\u88c5Anaconda \u00b6 \u53c2\u8003Anaconda\u5b98\u65b9\u6587\u6863\u5b89\u88c5Linux\u5bf9\u5e94\u7684Anaconda\uff1a https://docs.anaconda.com/anaconda/install/linux/ \u4f7f\u7528\u547d\u4ee4 wget https://repo.anaconda.com/archive/Anaconda2-2019.03-Linux-x86_64.sh \u4e0b\u8f7dlinux\u76f8\u5173\u7684\u5b89\u88c5\u5305 \u4f7f\u7528\u547d\u4ee4 bash Anaconda2-2019.03-Linux-x86_64.sh \u5f00\u59cb\u5b89\u88c5 \u56de\u8f66\u67e5\u770bLicense Agreement \u8f93\u5165yes \u9009\u62e9\u5b89\u88c5\u4f4d\u7f6e\u4e0d\u8981\u9009\u62e9\u9ed8\u8ba4\u4f4d\u7f6e\uff0c\u800c\u8bbe\u7f6e\u4e3a /opt/anaconda2 \u5b8c\u6210\u5b89\u88c5\u540e\u9009yes\u8fdb\u884c\u521d\u59cb\u5316, \u4f1a\u5c06\u521d\u59cb\u5316\u8bbe\u7f6e\u5199\u5165 ~/.bashrc \u6587\u4ef6\u4e2d \u4f7f\u7528\u547d\u4ee4 cp ~/.bashrc ~/.bashrc.anaconda \u5c06\u521d\u59cb\u5316\u4e4b\u540e\u7684 .basrc \u6587\u4ef6\u590d\u5236\uff0c\u91cd\u547d\u540d\u4e3a .bashrc.anaconda \uff0c \u5185\u5bb9\u5982\u4e0b\uff1a \u7ea2\u6846\u90e8\u5206\u4e3a\u5b89\u88c5anaconda\u540e\u52a0\u4e0a\u7684\u521d\u59cb\u5316\u914d\u7f6e \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \u7f16\u8f91 .bashrc \u6587\u4ef6\uff0c\u5c06conda\u521d\u59cb\u5316\u90e8\u5206\u5220\u6389\uff1a \u4f7f\u7528\u547d\u4ee4 source ~/.bashrc.anaconda \u52a0\u8f7d\u73af\u5883 \u4f7f\u7528\u547d\u4ee4 jupyter notebook --generate-config --allow-root \u751f\u6210jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 \u4f7f\u7528\u547d\u4ee4 vi /root/.jupyter/jupyter_notebook_config.py \u4fee\u6539jupyter notebook\u7684\u914d\u7f6e\uff0c\u5177\u4f53\u5982\u4e0b\uff1a \u6539Ip\u4e3a\u672c\u673aIp \u6539\u7aef\u53e3\uff08\u5982\u679c\u88ab\u5360\u7528\uff09 \u4fdd\u5b58 \u5728jupyter notebook\u4e3b\u673a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\uff0c\u5982\u679c\u5b8c\u6210\u53ef\u4ee5\u4e0d\u505a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u76f4\u63a5\u590d\u5236\u7c98\u8d34\u5bf9\u5e94\u7684\u5730\u5740\u8bbf\u95eejupyter notebook web UI\uff1a \u5bf9\u63a5Spark2x \u00b6 \u8bf4\u660e\uff1a\u4f7f\u7528pySpark\u63a5\u53e3\u5bf9\u63a5FI HD\u96c6\u7fa4Spark2x\u7ec4\u4ef6 \u4f7f\u7528\u4e0a\u4e00\u8282\u547d\u4ee4\u542f\u52a8jupyter notebook\u5e76\u8fdb\u5165weibUI \u5230\u5982\u4e0b\u94fe\u63a5\u83b7\u53d6\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6airlines.csv\uff0c\u5e76\u5c06\u6570\u636e\u6587\u4ef6\u4e0a\u4f20\u5230\u5bf9\u63a5\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\uff1a https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165python\u4ee3\u7801 from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() conf.setAppName('spark-wordcount_from172.16.2.118') sc = SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print 'Nonempty lines', nonempty_lines.count() words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print 'Top 100 words:' print wordcounts.take(100) \u5e76\u4e14\u5728\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u4e0a\u67e5\u770b\u4efb\u52a1\uff1a \u5bf9\u63a5Hive \u00b6 \u8bf4\u660e\uff1a\u914d\u7f6ejdbc\u63a5\u53e3\uff0c\u5bf9\u63a5\u96c6\u7fa4Hive \u5982\u679cjupyter notebook\u5df2\u7ecf\u542f\u52a8\uff0c\u5148\u505c\u6b62 \u627e\u5230anaconda\u5b89\u88c5\u76ee\u5f55/bin/pip\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5jdbc\u76f8\u5173\u7684\u4e24\u4e2apython\u5305\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall \u6ce8\u610f\uff1aJPype1\u5df2\u7ecfJayDeBeApi\u7248\u672c\u5fc5\u987b\u540c\u4e0a\u8ff0\u4e00\u81f4\uff0c\u4e0d\u7136\u4f1a\u62a5\u7248\u672c\u5339\u914d\u9519\u8bef\uff0c\u5df2\u7ecf\u5b89\u88c5\u8fd9\u4e24\u4e2a\u5305\u7684\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u7248\u672c\uff1a ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi \u5c06\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u6587\u4ef6user.keytab\u653e\u5230jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u7528\u4e8e\u8fde\u63a5Hive\u8ba4\u8bc1\u4f7f\u7528,\u5c06\u8ba4\u8bc1\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\u653e\u5230/etc/\u8def\u5f84\u4e0b \u5728jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dJVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env kinit developuser export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype import os # this worked conn = jaydebeapi.connect( \"org.apache.hive.jdbc.HiveDriver\", [\"jdbc:hive2://172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/default;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/user.keytab\" , \"developuser\", \"Huawei@123\"], [ '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-collections-3.2.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-configuration-1.6.jar', '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-lang-2.6.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-logging-1.1.3.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'curator-client-2.11.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'curator-framework-2.11.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'guava-14.0.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-auth-2.7.2.jar', '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-common-2.7.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-mapreduce-client-core-2.7.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-common-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-exec-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-jdbc-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-metastore-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-serde-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-service-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-shims-0.23-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-shims-common-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'httpclient-4.4.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'httpcore-4.4.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'libthrift-0.9.3.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'log4j-1.2.17.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'slf4j-api-1.7.5.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'slf4j-log4j12-1.7.5.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'zookeeper-3.5.1.jar'] ) import pandas as pd sql = \"Select * From drill_iris\" df_hive = pd.read_sql(sql, conn) df_hive conn.close() \u8bf4\u660e\uff1ajaydebeapi.connect()\u4e3ajdbc\u8fde\u63a5\u65b9\u6cd5\uff0cjaydebeapi.connect(\"Driver Main Class\", [\"Connecting URL\", \"User\", \"Password\"], \"Path to JDBC driver\")\uff0c\u5bf9\u63a5hive\u9700\u8981\u5c06\u5ba2\u6237\u7aefhive jdbc\u6837\u4f8b\u4e2d\u6240\u6709\u7684jar\u5305\u90fd\u5bfc\u8fdb\u53bb \u5bf9\u63a5Hetu \u00b6 \u5c06\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u6587\u4ef6user.keytab\u653e\u5230jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u7528\u4e8e\u8fde\u63a5Hive\u8ba4\u8bc1\u4f7f\u7528,\u5c06\u8ba4\u8bc1\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\u653e\u5230/etc/\u8def\u5f84\u4e0b \u5728jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dJVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env kinit developuser export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype import os conn3 = jaydebeapi.connect( \"io.prestosql.jdbc.PrestoDriver\", [\"jdbc:presto://172.16.10.131:24002,172.16.10.132:24002,172.16.10.133:24002/hive/default?serviceDiscoveryMode=zooKeeper&zooKeeperNamespace=hsbroker&deploymentMode=on_yarn&user=developuser&SSL=true&SSLTrustStorePath=/opt/mrs_hetu_config/hetuserver.jks&KerberosConfigPath=/opt/mrs_hetu_config/krb5.conf&KerberosPrincipal=developuser&KerberosKeytabPath=/opt/mrs_hetu_config/user.keytab&KerberosRemoteServiceName=HTTP&KerberosServicePrincipalPattern=%24%7BSERVICE%7D%40%24%7BHOST%7D\" ], '/opt/mrs_hetu_config/presto-jdbc-316-hw-ei-301001-SNAPSHOT.jar' ) import pandas as pd sql = \"Select * From iris\" df_hive = pd.read_sql(sql, conn3) df_hive \u8bf4\u660e\uff1a\u8fde\u63a5hetu\u5df2\u7ecf\u5728url\u91cc\u9762\u914d\u7f6e\u4e86\u7528\u6237\u540d\u548c\u5bc6\u7801\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u518d\u6b21\u5728\u4ee3\u7801\u91cc\u9762\u914d\u7f6e \u5bf9\u63a5ELK \u00b6 \u8bf4\u660e\uff1a\u914d\u7f6ejdbc\u63a5\u53e3\uff0c\u5bf9\u63a5\u96c6\u7fa4ELK ELK\u914d\u7f6e \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237joe, \u5bc6\u7801\u4e3aBigdata@123\uff0c \u5e76\u8d4b\u4e88\u7528\u6237joe\u6240\u6709\u6743\u9650 \u521b\u5efaHDFS\u8868\u7a7a\u95f4 \u521b\u5efa\u6570\u636e\u5e93db_tpcds \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\uff0c\u63d2\u5165\u6570\u636e \u53c2\u8003ELK\u4ea7\u54c1\u6587\u6863\u300a\u8fdc\u7a0b\u8fde\u63a5\u6570\u636e\u5e93\u300b\u914d\u7f6eELK\u767d\u540d\u5355\uff0c\u80fd\u591f\u8bbf\u95eejupyter notebook\u4e3b\u673a \u5982\u679cjupyter notebook\u5df2\u7ecf\u542f\u52a8\uff0c\u5148\u505c\u6b62 \u627e\u5230anaconda\u5b89\u88c5\u76ee\u5f55/bin/pip\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5jdbc\u76f8\u5173\u7684\u4e24\u4e2apython\u5305\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall \u6ce8\u610f\uff1aJPype1\u5df2\u7ecfJayDeBeApi\u7248\u672c\u5fc5\u987b\u540c\u4e0a\u8ff0\u4e00\u81f4\uff0c\u4e0d\u7136\u4f1a\u62a5\u7248\u672c\u5339\u914d\u9519\u8bef\uff0c\u5df2\u7ecf\u5b89\u88c5\u8fd9\u4e24\u4e2a\u5305\u7684\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u7248\u672c\uff1a ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u5c06ELK JDBC\u9a71\u52a8jar\u5305 gsjdbc4.jar \u653e\u5230jupyter notebook\u4e3b\u673a /opt \u8def\u5f84\u4e0b \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype jar = \"/opt/gsjdbc4.jar\" # location of the jdbc driver jar args='-Djava.class.path=%s' % jar jvm = jpype.getDefaultJVMPath() jpype.startJVM(jvm, args) # this worked conn = jaydebeapi.connect( 'org.postgresql.Driver', [\"jdbc:postgresql://172.16.6.10:25108/db_tpcds\" , \"joe\", \"Bigdata@123\"], \"/opt/gsjdbc4.jar\" ) import pandas as pd sql = \"Select * From hdfs_001\" df = pd.read_sql(sql, conn) df conn.close() \u5bf9\u63a5HDFS \u00b6 \u9009\u7528python\u7684hdfs\u5305\u7528\u4f5c\u6d4b\u8bd5 https://pypi.org/project/hdfs/ \u539f\u56e0\uff1a\u6839\u636eanaconda\u6587\u6863\uff1a https://enterprise-docs.anaconda.com/en/docs-site-5.1.2/user-guide/projects/connect-hive-impala-hdfs.html \u53ef\u4ee5\u5f97\u77e5\u4f7f\u7528\u4e0a\u8ff0\u5de5\u5177\u5305\u5bf9\u63a5hdfs \u56e0\u4e3a\u6b64\u6b21\u4f7f\u7528webhdfs\u7684http\u8fde\u63a5\u65b9\u5f0f\u5bf9\u63a5\u96c6\u7fa4\uff0c\u9996\u5148\u5148\u68c0\u67e5\u96c6\u7fa4\u914d\u7f6e\u9879\u662f\u5426\u7b26\u5408\u8981\u6c42\uff1a \u82e5\u914d\u7f6e\u9879\u53ea\u652f\u6301HTTPS\uff0c\u53c2\u8003\u5982\u4e0b\u6b65\u9aa4\u4fee\u6539 \u767b\u5f55FusionInsight Manager\u9875\u9762\uff0c\u5355\u51fb\u201c\u96c6\u7fa4 > \u5f85\u64cd\u4f5c\u96c6\u7fa4\u7684\u540d\u79f0 > \u670d\u52a1 > HDFS > \u914d\u7f6e >\u5168\u90e8\u914d\u7f6e\u201d\uff0c\u5728\u201c\u641c\u7d22\u201d\u6846\u91cc\u641c\u7d22\u201cdfs.http.policy\u201d\uff0c\u7136\u540e\u52fe\u9009\u201cHTTP_AND_HTTPS\u201d\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u5355\u51fb\u201c\u66f4\u591a > \u91cd\u542f\u201d\u91cd\u542fHDFS\u670d\u52a1 \u767b\u9646\u8def\u5f84 /opt/anaconda3/bin , \u4f7f\u7528\u547d\u4ee4 ./pip install hdfs \u5b89\u88c5\u63d2\u4ef6\u5305 \u518d\u4f7f\u7528 ./pip install hdfs[avro,dataframe,kerberos] \u5b89\u88c5\u989d\u5916\u5de5\u5177\u5305 \u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\u4f1a\u5728bin\u76ee\u5f55\u4e0b\u751f\u6210hdfscli\u7684\u53ef\u6267\u884c\u6587\u4ef6 \u4f7f\u7528\u547d\u4ee4 vi ~/.hdfscli.cfg \u7f16\u8f91\u8fde\u63a5\u53c2\u6570\uff1a [global] default.alias = dev autoload.modules = hdfs.ext.kerberos [dev.alias] client = KerberosClient url = http://172.16.4.122:25002 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u767b\u9646\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u5e76\u505akerberos\u8ba4\u8bc1 source /opt/125_651hdclient/hadoopclient/bigdata_env kinit developuser \u4f7f\u7528\u547d\u4ee4 hdfscli \u767b\u9646\u547d\u4ee4\u884c\u7ec8\u7aef \u8f93\u5165\u547d\u4ee4 CLIENT.list('/') \u68c0\u67e5\u662f\u5426\u5bf9\u63a5\u6210\u529f\uff1a \u547d\u4ee4\u884c\u6d4b\u8bd5\u6210\u529f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda3 jupyter notebook --allow-root \u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u8fdb\u884chdfs\u5bf9\u63a5 import hdfs from hdfs.ext.kerberos import KerberosClient client = KerberosClient('http://172.16.4.122:25002', root='/tmp') print(client.list('/')) \u7ee7\u7eed\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u5f80hdfs\u91cc\u5199\u4e00\u4e2a\u6587\u4ef6\uff1a client.write('hello2.md', 'Hello, world!') \u53bbhdfs\u5bf9\u5e94\u8def\u5f84\u68c0\u67e5\u6587\u4ef6\u662f\u5426\u5199\u5165\u6210\u529f\uff1a \u7ee7\u7eed\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u4ecehdfs\u8def\u5f84\u8bfb\u53d6\u4e00\u4e2a\u6587\u4ef6\uff1a with client.read('iris.csv', encoding='utf-8') as reader: for row in reader: print(row) F&Q \u00b6 \u5728\u4f7f\u7528pySpark\u7684\u65f6\u5019\u9047\u5230\u5982\u4e0b\u95ee\u9898\uff1a ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /opt/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 \u89e3\u51b3\u529e\u6cd5\uff1a \u8fd0\u884c sc.stop() \u8fde\u63a5ELK\u65f6\u5019\u62a5\u9519\uff1a \u89e3\u51b3\u529e\u6cd5\uff1a\u914d\u7f6eELK\u767d\u540d\u5355 \u8fde\u63a5hdfs\u7684\u65f6\u5019\u9047\u5230\u62a5\u9519\uff1a \u89e3\u51b3\u529e\u6cd5\uff1akinit\u5b8c\u6210kerberos\u8ba4\u8bc1","title":"2.7.16 <--> 6.5"},{"location":"Development/JupyterNotebook/#jupyter-notebookfusioninsight","text":"","title":"Jupyter Notebook\u5bf9\u63a5FusionInsight"},{"location":"Development/JupyterNotebook/#_1","text":"Jupyter Notebook 2.7.16 \u2194 FusionInsight HD V100R002C80SPC200 (Hive/Elk/Spark2x) Jupyter Notebook 2.7.16 \u2194 FusionInsight HD 6.5 (HDFS/Hive/Elk/Spark2x) Jupyter Notebook 2.7.16 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/Elk/Hetu/Spark2x) \u8bf4\u660e\uff1aJupyter Notebook\u7248\u672c \u57fa\u4e8eAnaconda Python\u5185\u6838\u7248\u672c","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/JupyterNotebook/#anaconda","text":"\u53c2\u8003Anaconda\u5b98\u65b9\u6587\u6863\u5b89\u88c5Linux\u5bf9\u5e94\u7684Anaconda\uff1a https://docs.anaconda.com/anaconda/install/linux/ \u4f7f\u7528\u547d\u4ee4 wget https://repo.anaconda.com/archive/Anaconda2-2019.03-Linux-x86_64.sh \u4e0b\u8f7dlinux\u76f8\u5173\u7684\u5b89\u88c5\u5305 \u4f7f\u7528\u547d\u4ee4 bash Anaconda2-2019.03-Linux-x86_64.sh \u5f00\u59cb\u5b89\u88c5 \u56de\u8f66\u67e5\u770bLicense Agreement \u8f93\u5165yes \u9009\u62e9\u5b89\u88c5\u4f4d\u7f6e\u4e0d\u8981\u9009\u62e9\u9ed8\u8ba4\u4f4d\u7f6e\uff0c\u800c\u8bbe\u7f6e\u4e3a /opt/anaconda2 \u5b8c\u6210\u5b89\u88c5\u540e\u9009yes\u8fdb\u884c\u521d\u59cb\u5316, \u4f1a\u5c06\u521d\u59cb\u5316\u8bbe\u7f6e\u5199\u5165 ~/.bashrc \u6587\u4ef6\u4e2d \u4f7f\u7528\u547d\u4ee4 cp ~/.bashrc ~/.bashrc.anaconda \u5c06\u521d\u59cb\u5316\u4e4b\u540e\u7684 .basrc \u6587\u4ef6\u590d\u5236\uff0c\u91cd\u547d\u540d\u4e3a .bashrc.anaconda \uff0c \u5185\u5bb9\u5982\u4e0b\uff1a \u7ea2\u6846\u90e8\u5206\u4e3a\u5b89\u88c5anaconda\u540e\u52a0\u4e0a\u7684\u521d\u59cb\u5316\u914d\u7f6e \u4f7f\u7528\u547d\u4ee4 vi ~/.bashrc \u7f16\u8f91 .bashrc \u6587\u4ef6\uff0c\u5c06conda\u521d\u59cb\u5316\u90e8\u5206\u5220\u6389\uff1a \u4f7f\u7528\u547d\u4ee4 source ~/.bashrc.anaconda \u52a0\u8f7d\u73af\u5883 \u4f7f\u7528\u547d\u4ee4 jupyter notebook --generate-config --allow-root \u751f\u6210jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 \u4f7f\u7528\u547d\u4ee4 vi /root/.jupyter/jupyter_notebook_config.py \u4fee\u6539jupyter notebook\u7684\u914d\u7f6e\uff0c\u5177\u4f53\u5982\u4e0b\uff1a \u6539Ip\u4e3a\u672c\u673aIp \u6539\u7aef\u53e3\uff08\u5982\u679c\u88ab\u5360\u7528\uff09 \u4fdd\u5b58 \u5728jupyter notebook\u4e3b\u673a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\uff0c\u5982\u679c\u5b8c\u6210\u53ef\u4ee5\u4e0d\u505a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u76f4\u63a5\u590d\u5236\u7c98\u8d34\u5bf9\u5e94\u7684\u5730\u5740\u8bbf\u95eejupyter notebook web UI\uff1a","title":"\u5b89\u88c5Anaconda"},{"location":"Development/JupyterNotebook/#spark2x","text":"\u8bf4\u660e\uff1a\u4f7f\u7528pySpark\u63a5\u53e3\u5bf9\u63a5FI HD\u96c6\u7fa4Spark2x\u7ec4\u4ef6 \u4f7f\u7528\u4e0a\u4e00\u8282\u547d\u4ee4\u542f\u52a8jupyter notebook\u5e76\u8fdb\u5165weibUI \u5230\u5982\u4e0b\u94fe\u63a5\u83b7\u53d6\u9700\u8981\u7684\u6570\u636e\u6587\u4ef6airlines.csv\uff0c\u5e76\u5c06\u6570\u636e\u6587\u4ef6\u4e0a\u4f20\u5230\u5bf9\u63a5\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\uff1a https://github.com/beanumber/airlines/blob/master/data-raw/airlines.csv \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165python\u4ee3\u7801 from pyspark import SparkConf from pyspark import SparkContext conf = SparkConf() conf.setAppName('spark-wordcount_from172.16.2.118') sc = SparkContext(conf=conf) distFile = sc.textFile('hdfs://hacluster/tmp/airlines.csv') nonempty_lines = distFile.filter(lambda x: len(x) > 0) print 'Nonempty lines', nonempty_lines.count() words = nonempty_lines.flatMap(lambda x: x.split(' ')) wordcounts = words.map(lambda x: (x, 1)) \\ .reduceByKey(lambda x, y: x+y) \\ .map(lambda x: (x[1], x[0])).sortByKey(False) print 'Top 100 words:' print wordcounts.take(100) \u5e76\u4e14\u5728\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u4e0a\u67e5\u770b\u4efb\u52a1\uff1a","title":"\u5bf9\u63a5Spark2x"},{"location":"Development/JupyterNotebook/#hive","text":"\u8bf4\u660e\uff1a\u914d\u7f6ejdbc\u63a5\u53e3\uff0c\u5bf9\u63a5\u96c6\u7fa4Hive \u5982\u679cjupyter notebook\u5df2\u7ecf\u542f\u52a8\uff0c\u5148\u505c\u6b62 \u627e\u5230anaconda\u5b89\u88c5\u76ee\u5f55/bin/pip\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5jdbc\u76f8\u5173\u7684\u4e24\u4e2apython\u5305\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall \u6ce8\u610f\uff1aJPype1\u5df2\u7ecfJayDeBeApi\u7248\u672c\u5fc5\u987b\u540c\u4e0a\u8ff0\u4e00\u81f4\uff0c\u4e0d\u7136\u4f1a\u62a5\u7248\u672c\u5339\u914d\u9519\u8bef\uff0c\u5df2\u7ecf\u5b89\u88c5\u8fd9\u4e24\u4e2a\u5305\u7684\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u7248\u672c\uff1a ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi \u5c06\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u6587\u4ef6user.keytab\u653e\u5230jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u7528\u4e8e\u8fde\u63a5Hive\u8ba4\u8bc1\u4f7f\u7528,\u5c06\u8ba4\u8bc1\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\u653e\u5230/etc/\u8def\u5f84\u4e0b \u5728jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dJVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env kinit developuser export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype import os # this worked conn = jaydebeapi.connect( \"org.apache.hive.jdbc.HiveDriver\", [\"jdbc:hive2://172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/default;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/user.keytab\" , \"developuser\", \"Huawei@123\"], [ '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-collections-3.2.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-configuration-1.6.jar', '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-lang-2.6.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'commons-logging-1.1.3.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'curator-client-2.11.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'curator-framework-2.11.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'guava-14.0.1.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-auth-2.7.2.jar', '/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-common-2.7.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hadoop-mapreduce-client-core-2.7.2.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-common-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-exec-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-jdbc-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-metastore-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-serde-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-service-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-shims-0.23-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'hive-shims-common-1.3.0.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'httpclient-4.4.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'httpcore-4.4.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'libthrift-0.9.3.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'log4j-1.2.17.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'slf4j-api-1.7.5.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'slf4j-log4j12-1.7.5.jar','/opt/hadoopclient/Hive/Beeline/lib/jdbc/' + 'zookeeper-3.5.1.jar'] ) import pandas as pd sql = \"Select * From drill_iris\" df_hive = pd.read_sql(sql, conn) df_hive conn.close() \u8bf4\u660e\uff1ajaydebeapi.connect()\u4e3ajdbc\u8fde\u63a5\u65b9\u6cd5\uff0cjaydebeapi.connect(\"Driver Main Class\", [\"Connecting URL\", \"User\", \"Password\"], \"Path to JDBC driver\")\uff0c\u5bf9\u63a5hive\u9700\u8981\u5c06\u5ba2\u6237\u7aefhive jdbc\u6837\u4f8b\u4e2d\u6240\u6709\u7684jar\u5305\u90fd\u5bfc\u8fdb\u53bb","title":"\u5bf9\u63a5Hive"},{"location":"Development/JupyterNotebook/#hetu","text":"\u5c06\u5bf9\u63a5\u96c6\u7fa4\u8ba4\u8bc1\u6587\u4ef6user.keytab\u653e\u5230jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u7528\u4e8e\u8fde\u63a5Hive\u8ba4\u8bc1\u4f7f\u7528,\u5c06\u8ba4\u8bc1\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\u653e\u5230/etc/\u8def\u5f84\u4e0b \u5728jupyter notebook\u4e3b\u673a/opt\u8def\u5f84\u4e0b\u65b0\u5efajaas.conf\u914d\u7f6e\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7dJVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env kinit developuser export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u67e5\u770b\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype import os conn3 = jaydebeapi.connect( \"io.prestosql.jdbc.PrestoDriver\", [\"jdbc:presto://172.16.10.131:24002,172.16.10.132:24002,172.16.10.133:24002/hive/default?serviceDiscoveryMode=zooKeeper&zooKeeperNamespace=hsbroker&deploymentMode=on_yarn&user=developuser&SSL=true&SSLTrustStorePath=/opt/mrs_hetu_config/hetuserver.jks&KerberosConfigPath=/opt/mrs_hetu_config/krb5.conf&KerberosPrincipal=developuser&KerberosKeytabPath=/opt/mrs_hetu_config/user.keytab&KerberosRemoteServiceName=HTTP&KerberosServicePrincipalPattern=%24%7BSERVICE%7D%40%24%7BHOST%7D\" ], '/opt/mrs_hetu_config/presto-jdbc-316-hw-ei-301001-SNAPSHOT.jar' ) import pandas as pd sql = \"Select * From iris\" df_hive = pd.read_sql(sql, conn3) df_hive \u8bf4\u660e\uff1a\u8fde\u63a5hetu\u5df2\u7ecf\u5728url\u91cc\u9762\u914d\u7f6e\u4e86\u7528\u6237\u540d\u548c\u5bc6\u7801\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u518d\u6b21\u5728\u4ee3\u7801\u91cc\u9762\u914d\u7f6e","title":"\u5bf9\u63a5Hetu"},{"location":"Development/JupyterNotebook/#elk","text":"\u8bf4\u660e\uff1a\u914d\u7f6ejdbc\u63a5\u53e3\uff0c\u5bf9\u63a5\u96c6\u7fa4ELK ELK\u914d\u7f6e \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237joe, \u5bc6\u7801\u4e3aBigdata@123\uff0c \u5e76\u8d4b\u4e88\u7528\u6237joe\u6240\u6709\u6743\u9650 \u521b\u5efaHDFS\u8868\u7a7a\u95f4 \u521b\u5efa\u6570\u636e\u5e93db_tpcds \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\uff0c\u63d2\u5165\u6570\u636e \u53c2\u8003ELK\u4ea7\u54c1\u6587\u6863\u300a\u8fdc\u7a0b\u8fde\u63a5\u6570\u636e\u5e93\u300b\u914d\u7f6eELK\u767d\u540d\u5355\uff0c\u80fd\u591f\u8bbf\u95eejupyter notebook\u4e3b\u673a \u5982\u679cjupyter notebook\u5df2\u7ecf\u542f\u52a8\uff0c\u5148\u505c\u6b62 \u627e\u5230anaconda\u5b89\u88c5\u76ee\u5f55/bin/pip\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u9700\u8981\u5b89\u88c5jdbc\u76f8\u5173\u7684\u4e24\u4e2apython\u5305\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5: ./pip install JPype1==0.6.3 --force-reinstall ./pip install JayDeBeApi==0.2 --force-reinstall \u6ce8\u610f\uff1aJPype1\u5df2\u7ecfJayDeBeApi\u7248\u672c\u5fc5\u987b\u540c\u4e0a\u8ff0\u4e00\u81f4\uff0c\u4e0d\u7136\u4f1a\u62a5\u7248\u672c\u5339\u914d\u9519\u8bef\uff0c\u5df2\u7ecf\u5b89\u88c5\u8fd9\u4e24\u4e2a\u5305\u7684\u53ef\u4ee5\u901a\u8fc7\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u7248\u672c\uff1a ./pip freeze | grep JPype1 ./pip freeze | grep JayDeBeApi \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" pyspark --master yarn --deploy-mode client & \u8bf4\u660e\uff1a \u5982\u679c\u4e0d\u9700\u8981\u540cSpark2x\u7ec4\u4ef6\u4ea4\u4e92\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u547d\u4ee4 jupyter notebook --allow-root \u76f4\u63a5\u542f\u52a8jupyter notebook \u5c06ELK JDBC\u9a71\u52a8jar\u5305 gsjdbc4.jar \u653e\u5230jupyter notebook\u4e3b\u673a /opt \u8def\u5f84\u4e0b \u65b0\u5efa\u4e00\u4e2anotebook\uff0c\u8f93\u5165\u5982\u4e0b\u4ee3\u7801: import jaydebeapi import jpype jar = \"/opt/gsjdbc4.jar\" # location of the jdbc driver jar args='-Djava.class.path=%s' % jar jvm = jpype.getDefaultJVMPath() jpype.startJVM(jvm, args) # this worked conn = jaydebeapi.connect( 'org.postgresql.Driver', [\"jdbc:postgresql://172.16.6.10:25108/db_tpcds\" , \"joe\", \"Bigdata@123\"], \"/opt/gsjdbc4.jar\" ) import pandas as pd sql = \"Select * From hdfs_001\" df = pd.read_sql(sql, conn) df conn.close()","title":"\u5bf9\u63a5ELK"},{"location":"Development/JupyterNotebook/#hdfs","text":"\u9009\u7528python\u7684hdfs\u5305\u7528\u4f5c\u6d4b\u8bd5 https://pypi.org/project/hdfs/ \u539f\u56e0\uff1a\u6839\u636eanaconda\u6587\u6863\uff1a https://enterprise-docs.anaconda.com/en/docs-site-5.1.2/user-guide/projects/connect-hive-impala-hdfs.html \u53ef\u4ee5\u5f97\u77e5\u4f7f\u7528\u4e0a\u8ff0\u5de5\u5177\u5305\u5bf9\u63a5hdfs \u56e0\u4e3a\u6b64\u6b21\u4f7f\u7528webhdfs\u7684http\u8fde\u63a5\u65b9\u5f0f\u5bf9\u63a5\u96c6\u7fa4\uff0c\u9996\u5148\u5148\u68c0\u67e5\u96c6\u7fa4\u914d\u7f6e\u9879\u662f\u5426\u7b26\u5408\u8981\u6c42\uff1a \u82e5\u914d\u7f6e\u9879\u53ea\u652f\u6301HTTPS\uff0c\u53c2\u8003\u5982\u4e0b\u6b65\u9aa4\u4fee\u6539 \u767b\u5f55FusionInsight Manager\u9875\u9762\uff0c\u5355\u51fb\u201c\u96c6\u7fa4 > \u5f85\u64cd\u4f5c\u96c6\u7fa4\u7684\u540d\u79f0 > \u670d\u52a1 > HDFS > \u914d\u7f6e >\u5168\u90e8\u914d\u7f6e\u201d\uff0c\u5728\u201c\u641c\u7d22\u201d\u6846\u91cc\u641c\u7d22\u201cdfs.http.policy\u201d\uff0c\u7136\u540e\u52fe\u9009\u201cHTTP_AND_HTTPS\u201d\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u5355\u51fb\u201c\u66f4\u591a > \u91cd\u542f\u201d\u91cd\u542fHDFS\u670d\u52a1 \u767b\u9646\u8def\u5f84 /opt/anaconda3/bin , \u4f7f\u7528\u547d\u4ee4 ./pip install hdfs \u5b89\u88c5\u63d2\u4ef6\u5305 \u518d\u4f7f\u7528 ./pip install hdfs[avro,dataframe,kerberos] \u5b89\u88c5\u989d\u5916\u5de5\u5177\u5305 \u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\u4f1a\u5728bin\u76ee\u5f55\u4e0b\u751f\u6210hdfscli\u7684\u53ef\u6267\u884c\u6587\u4ef6 \u4f7f\u7528\u547d\u4ee4 vi ~/.hdfscli.cfg \u7f16\u8f91\u8fde\u63a5\u53c2\u6570\uff1a [global] default.alias = dev autoload.modules = hdfs.ext.kerberos [dev.alias] client = KerberosClient url = http://172.16.4.122:25002 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u767b\u9646\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u5e76\u505akerberos\u8ba4\u8bc1 source /opt/125_651hdclient/hadoopclient/bigdata_env kinit developuser \u4f7f\u7528\u547d\u4ee4 hdfscli \u767b\u9646\u547d\u4ee4\u884c\u7ec8\u7aef \u8f93\u5165\u547d\u4ee4 CLIENT.list('/') \u68c0\u67e5\u662f\u5426\u5bf9\u63a5\u6210\u529f\uff1a \u547d\u4ee4\u884c\u6d4b\u8bd5\u6210\u529f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8jupyter notebook source /opt/hadoopclient/bigdata_env kinit developuser source ~/.bashrc.anaconda3 jupyter notebook --allow-root \u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u8fdb\u884chdfs\u5bf9\u63a5 import hdfs from hdfs.ext.kerberos import KerberosClient client = KerberosClient('http://172.16.4.122:25002', root='/tmp') print(client.list('/')) \u7ee7\u7eed\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u5f80hdfs\u91cc\u5199\u4e00\u4e2a\u6587\u4ef6\uff1a client.write('hello2.md', 'Hello, world!') \u53bbhdfs\u5bf9\u5e94\u8def\u5f84\u68c0\u67e5\u6587\u4ef6\u662f\u5426\u5199\u5165\u6210\u529f\uff1a \u7ee7\u7eed\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\u4ecehdfs\u8def\u5f84\u8bfb\u53d6\u4e00\u4e2a\u6587\u4ef6\uff1a with client.read('iris.csv', encoding='utf-8') as reader: for row in reader: print(row)","title":"\u5bf9\u63a5HDFS"},{"location":"Development/JupyterNotebook/#fq","text":"\u5728\u4f7f\u7528pySpark\u7684\u65f6\u5019\u9047\u5230\u5982\u4e0b\u95ee\u9898\uff1a ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /opt/anaconda2/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 \u89e3\u51b3\u529e\u6cd5\uff1a \u8fd0\u884c sc.stop() \u8fde\u63a5ELK\u65f6\u5019\u62a5\u9519\uff1a \u89e3\u51b3\u529e\u6cd5\uff1a\u914d\u7f6eELK\u767d\u540d\u5355 \u8fde\u63a5hdfs\u7684\u65f6\u5019\u9047\u5230\u62a5\u9519\uff1a \u89e3\u51b3\u529e\u6cd5\uff1akinit\u5b8c\u6210kerberos\u8ba4\u8bc1","title":"F&amp;Q"},{"location":"Development/Jupyter_Notebook/","text":"Jupyter_Notebook\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Jupyter Notebook 2.4.4.0 \u2194 FusionInsight HD V100R002C70SPC200 (pySpark) \u5b89\u88c5Jupyter notebook \u00b6 Jupyter notebook\u7684\u5b89\u88c5\u4f9d\u8d56\u4e8ePython\uff0c\u4e14\u6d89\u53ca\u5230\u8bb8\u591a\u5de5\u5177\u7684\u4f9d\u8d56\u5305\uff0c\u76f8\u4e92\u4e4b\u95f4\u8fd8\u5b58\u5728\u7248\u672c\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bd4\u8f83\u9ebb\u70e6\uff0c\u901a\u5e38\u53ef\u4ee5\u76f4\u63a5\u5b89\u88c5Anaconda\u5305\uff0c\u91cc\u9762\u5305\u542b\u4e86Python\u3001Jupyter Notebook\uff0c\u4ee5\u53ca\u4f17\u591a\u7684\u79d1\u5b66\u5de5\u5177\u5305\uff0c\u8fd9\u91cc\u6211\u4eec\u76f4\u63a5\u5b89\u88c5Anaconda \u4eceAnaconda\u5b98\u7f51\u4e0b\u8f7d\u5e76\u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh bash Anaconda2-4.4.0-Linux-x86_64.sh \u751f\u6210Jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 jupyter notebook --generate-config --allow-root \u4fee\u6539Jupyter notebook\u7684\u914d\u7f6eIPc.NotebookApp.ip\u4e3a\u672c\u673aIP\u5730\u5740 vi /root/.jupyter/jupyter_notebook_config.py \u542f\u52a8Jupyter notebook:: jupyter notebook --allow-root \u51fa\u73b0\u5982\u4e0b\u63d0\u793a\u8868\u793aJupyter notebook\u542f\u52a8\u6210\u529f [I 15:53:46.918 NotebookApp] Serving notebooks from local directory: /opt [I 15:53:46.918 NotebookApp] 0 active kernels [I 15:53:46.918 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 [I 15:53:46.918 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 15:53:46.919 NotebookApp] No web browser found: could not locate runnable browser. [C 15:53:46.919 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 \u4f7f\u7528 Ctrl+C \u53ef\u4ee5\u9000\u51faJupyter notebook \u5b89\u88c5FusionInsight Client \u00b6 \u53c2\u8003FusionInsight\u7684\u4ea7\u54c1\u6587\u6863\u5b8c\u6210Linux\u4e0b\u7684FusionInsight\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5b89\u88c5\u5230 /opt/hadoopclient \u76ee\u5f55 \u5b8c\u6210Kerberos\u8ba4\u8bc1 \u00b6 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /opt/hadoopclient/ source bigdata_env kinit sparkuser \u5bfc\u5165ipython\u76f8\u5173\u73af\u5883\u53d8\u91cf \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165\u73af\u5883\u53d8\u91cf\uff0c\u6216\u8005\u5c06\u4e0b\u9762\u4e24\u884c\u6dfb\u52a0\u5230 /opt/hadoopclient/bigdata_env\u6587\u4ef6 \uff0c\u540e\u7eedsource bigdata_env\u65f6\u53ef\u4ee5\u81ea\u52a8\u5c06\u73af\u5883\u53d8\u91cf\u5bfc\u5165 export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\" Jupyter notebook\u4e2d\u4f7f\u7528pyspark\u8fdb\u884c\u5206\u6790 \u00b6 \u6267\u884cpyspark\u4f1a\u81ea\u52a8\u542f\u52a8Jupyter notebook [root@test01 opt]# pyspark [TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions. [TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future [I 16:24:20.802 NotebookApp] The port 8888 is already in use, trying another port. [I 16:24:20.809 NotebookApp] Serving notebooks from local directory: /opt [I 16:24:20.809 NotebookApp] 0 active kernels [I 16:24:20.809 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 [I 16:24:20.809 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 16:24:20.810 NotebookApp] No web browser found: could not locate runnable browser. [C 16:24:20.810 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 \u6253\u5f00\u4e0a\u8ff0\u94fe\u63a5\uff0c\u53ef\u4ee5\u8fdb\u884c\u6570\u636e\u5206\u6790 wget http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark/spark\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) library(magrittr) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10-1.2.0\") sqlContext <- sparkRSQL.init(sc) flightsDF <- read.df(sqlContext, \"/user/sparkuser/flights.csv\", source = \"com.databricks.spark.csv\", header = \"true\") destDF <- select(flightsDF, \"dest\", \"cancelled\") groupBy(flightsDF, flightsDF$date) %>% summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF head(dailyDelayDF) wget http://files.grouplens.org/datasets/movielens/ml-100k/u.user %pylab inline user_data = sc.textFile(\"ml-100k/u.user\") user_fields = user_data.map(lambda line: line.split(\"|\")) num_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes) ages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10) Jupyter notebook\u4e2d\u4f7f\u7528R\u8bed\u8a00\u8fdb\u884c\u5206\u6790 \u00b6 TBD","title":"2.4.4.0 <--> C70"},{"location":"Development/Jupyter_Notebook/#jupyter_notebookfusioninsight","text":"","title":"Jupyter_Notebook\u5bf9\u63a5FusionInsight"},{"location":"Development/Jupyter_Notebook/#_1","text":"Jupyter Notebook 2.4.4.0 \u2194 FusionInsight HD V100R002C70SPC200 (pySpark)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Jupyter_Notebook/#jupyter-notebook","text":"Jupyter notebook\u7684\u5b89\u88c5\u4f9d\u8d56\u4e8ePython\uff0c\u4e14\u6d89\u53ca\u5230\u8bb8\u591a\u5de5\u5177\u7684\u4f9d\u8d56\u5305\uff0c\u76f8\u4e92\u4e4b\u95f4\u8fd8\u5b58\u5728\u7248\u672c\u4f9d\u8d56\u5173\u7cfb\uff0c\u6bd4\u8f83\u9ebb\u70e6\uff0c\u901a\u5e38\u53ef\u4ee5\u76f4\u63a5\u5b89\u88c5Anaconda\u5305\uff0c\u91cc\u9762\u5305\u542b\u4e86Python\u3001Jupyter Notebook\uff0c\u4ee5\u53ca\u4f17\u591a\u7684\u79d1\u5b66\u5de5\u5177\u5305\uff0c\u8fd9\u91cc\u6211\u4eec\u76f4\u63a5\u5b89\u88c5Anaconda \u4eceAnaconda\u5b98\u7f51\u4e0b\u8f7d\u5e76\u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh bash Anaconda2-4.4.0-Linux-x86_64.sh \u751f\u6210Jupyter notebook\u7684\u914d\u7f6e\u6587\u4ef6 jupyter notebook --generate-config --allow-root \u4fee\u6539Jupyter notebook\u7684\u914d\u7f6eIPc.NotebookApp.ip\u4e3a\u672c\u673aIP\u5730\u5740 vi /root/.jupyter/jupyter_notebook_config.py \u542f\u52a8Jupyter notebook:: jupyter notebook --allow-root \u51fa\u73b0\u5982\u4e0b\u63d0\u793a\u8868\u793aJupyter notebook\u542f\u52a8\u6210\u529f [I 15:53:46.918 NotebookApp] Serving notebooks from local directory: /opt [I 15:53:46.918 NotebookApp] 0 active kernels [I 15:53:46.918 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 [I 15:53:46.918 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 15:53:46.919 NotebookApp] No web browser found: could not locate runnable browser. [C 15:53:46.919 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.122:8888/?token=f0494a2274cba1a6098ef21c417af2f3c49df872c6b34938 \u4f7f\u7528 Ctrl+C \u53ef\u4ee5\u9000\u51faJupyter notebook","title":"\u5b89\u88c5Jupyter notebook"},{"location":"Development/Jupyter_Notebook/#fusioninsight-client","text":"\u53c2\u8003FusionInsight\u7684\u4ea7\u54c1\u6587\u6863\u5b8c\u6210Linux\u4e0b\u7684FusionInsight\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5b89\u88c5\u5230 /opt/hadoopclient \u76ee\u5f55","title":"\u5b89\u88c5FusionInsight Client"},{"location":"Development/Jupyter_Notebook/#kerberos","text":"\u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /opt/hadoopclient/ source bigdata_env kinit sparkuser","title":"\u5b8c\u6210Kerberos\u8ba4\u8bc1"},{"location":"Development/Jupyter_Notebook/#ipython","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165\u73af\u5883\u53d8\u91cf\uff0c\u6216\u8005\u5c06\u4e0b\u9762\u4e24\u884c\u6dfb\u52a0\u5230 /opt/hadoopclient/bigdata_env\u6587\u4ef6 \uff0c\u540e\u7eedsource bigdata_env\u65f6\u53ef\u4ee5\u81ea\u52a8\u5c06\u73af\u5883\u53d8\u91cf\u5bfc\u5165 export PYSPARK_DRIVER_PYTHON=\"ipython\" export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --allow-root\"","title":"\u5bfc\u5165ipython\u76f8\u5173\u73af\u5883\u53d8\u91cf"},{"location":"Development/Jupyter_Notebook/#jupyter-notebookpyspark","text":"\u6267\u884cpyspark\u4f1a\u81ea\u52a8\u542f\u52a8Jupyter notebook [root@test01 opt]# pyspark [TerminalIPythonApp] WARNING | Subcommand `ipython notebook` is deprecated and will be removed in future versions. [TerminalIPythonApp] WARNING | You likely want to use `jupyter notebook` in the future [I 16:24:20.802 NotebookApp] The port 8888 is already in use, trying another port. [I 16:24:20.809 NotebookApp] Serving notebooks from local directory: /opt [I 16:24:20.809 NotebookApp] 0 active kernels [I 16:24:20.809 NotebookApp] The Jupyter Notebook is running at: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 [I 16:24:20.809 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [W 16:24:20.810 NotebookApp] No web browser found: could not locate runnable browser. [C 16:24:20.810 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://172.21.33.121:8889/?token=a951f440e47d932b1782fd97383c3dc935d468799a3c36c6 \u6253\u5f00\u4e0a\u8ff0\u94fe\u63a5\uff0c\u53ef\u4ee5\u8fdb\u884c\u6570\u636e\u5206\u6790 wget http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark/spark\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) library(magrittr) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10-1.2.0\") sqlContext <- sparkRSQL.init(sc) flightsDF <- read.df(sqlContext, \"/user/sparkuser/flights.csv\", source = \"com.databricks.spark.csv\", header = \"true\") destDF <- select(flightsDF, \"dest\", \"cancelled\") groupBy(flightsDF, flightsDF$date) %>% summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF head(dailyDelayDF) wget http://files.grouplens.org/datasets/movielens/ml-100k/u.user %pylab inline user_data = sc.textFile(\"ml-100k/u.user\") user_fields = user_data.map(lambda line: line.split(\"|\")) num_users = user_fields.map(lambda fields: fields[0]).count() num_genders = user_fields.map(lambda fields: fields[2]).distinct().count() num_occupations = user_fields.map(lambda fields: fields[3]).distinct().count() num_zipcodes = user_fields.map(lambda fields: fields[4]).distinct().count() print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes) ages = user_fields.map(lambda x: int(x[1])).collect() hist(ages, bins=20, color='lightblue', normed=True) fig = matplotlib.pyplot.gcf() fig.set_size_inches(16, 10)","title":"Jupyter notebook\u4e2d\u4f7f\u7528pyspark\u8fdb\u884c\u5206\u6790"},{"location":"Development/Jupyter_Notebook/#jupyter-notebookr","text":"TBD","title":"Jupyter notebook\u4e2d\u4f7f\u7528R\u8bed\u8a00\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/","text":"RSutdio\u5bf9\u63a5FusionInsight Spark \u00b6 \u9002\u7528\u573a\u666f \u00b6 RStudio 3.4.1 \u2194 FusionInsight HD V100R002C60U10 (SparkR) RStudio 3.4.1 \u2194 FusionInsight HD V100R002C70SPC100 (SparkR) RStudio 3.4.1 \u2194 FusionInsight MRS 8.0 (SparkR) \u5bf9\u63a5\u65b9\u5f0f \u00b6 RStudio\u4e0eSpark\u96c6\u6210\u6709\u4e24\u79cd\u65b9\u5f0f\uff1a \u901a\u8fc7RStudio\u5b98\u65b9\u53d1\u5e03\u7684sparklyr\u4e0eSpark\u8fdb\u884c\u96c6\u6210 \u901a\u8fc7Apache Spark\u793e\u533a\u53d1\u5e03\u7684SparkR\u8fdb\u884c\u96c6\u6210 \u672c\u6587\u6863\u5305\u542b\u4e86\u4e24\u79cd\u65b9\u5f0f\u5bf9\u63a5\u7684\u6b65\u9aa4, \u76f8\u5173\u5bf9\u63a5\u6b65\u9aa4\u5982\u4e0b\uff1a \u5b89\u88c5R \u5b89\u88c5RStudio Server \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x) \u5b89\u88c5R \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728RStudio\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u672c\u6587\u4f7f\u7528\u7684RStudio\u8282\u70b9\u4e3aRedhat7.1\uff0cFusionInsight\u96c6\u7fa4\u8282\u70b9\u4e3aRedhat6.6 \u914d\u7f6eredhat\u7684yum\u6e90\uff0c\u56fd\u5185\u53ef\u4ee5\u914d\u7f6e aliyun\u7684\u6e90 \u6216\u8005 163\u7684\u6e90 \u914d\u7f6eEPEL\u7684\u6e90 \u5b89\u88c5R-3.4.1 \u914d\u7f6ealiyun\u7684\u6e90 \u00b6 \u914d\u7f6e\u597dRedhat7.1\u7684yum\u6e90 cd ~ rpm -qa|grep yum|xargs rpm -e --nodeps rpm -qa|grep python-urlgrabber|xargs rpm -e --nodeps wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-3.4.3-150.el7.centos.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-rhn-plugin-2.0.1-6.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-40.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-8.el7.noarch.rpm rpm -ivh *.rpm cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/$releasever/7/g' /etc/yum.repos.d/Centos-7.repo yum clean yum makecache \u914d\u7f6e163\u7684\u6e90 \u00b6 \u914d\u7f6e\u597dRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 \u00b6 \u5b89\u88c5EPEL\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm Redhat 7.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//7/x86_64/e/epel-release-7-10.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u5b89\u88c5R-3.4.1 \u00b6 \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u5b89\u88c5RStudio Server \u00b6 \u4e0b\u8f7d\u5e76\u5b89\u88c5RStudio Server wget https://download2.rstudio.org/rstudio-server-rhel-1.0.153-x86_64.rpm yum install --nogpgcheck rstudio-server-rhel-1.0.153-x86_64.rpm \u4f7f\u7528 vi /etc/rstudio/rserver.conf \u4fee\u6539RStudio\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u6307\u5b9aRStudio Server\u4f7f\u7528\u7684R\u7684\u8def\u5f84 rsession-which-r=/usr/bin/R \u91cd\u542frstudio-server\u540e\uff0c\u67e5\u770b\u670d\u52a1\u662f\u5426\u6b63\u5e38 sudo systemctl restart rstudio-server sudo systemctl status rstudio-server \u670d\u52a1\u6b63\u5e38\u542f\u52a8\u5982\u4e0b \u7531\u4e8eRStudio Server\u4e0d\u5141\u8bb8\u4f7f\u7528root\u7528\u6237\u767b\u9646\uff0c\u9700\u8981\u65b0\u5efa\u4e00\u4e2a\u666e\u901a\u7528\u6237\u7528\u4e8eWeb\u754c\u9762\u7684\u767b\u9646 useradd -d /home/test -m test passwd test \u7528\u6237\u65b0\u5efa\u5b8c\u6210\u540e\uff0c\u5173\u95ed\u9632\u706b\u5899\uff0c\u7136\u540e\u4f7f\u7528\u672c\u673aip:8787\u7aef\u53e3\u8bbf\u95eeRStudio Server\uff0c\u4f7f\u7528\u65b0\u5efa\u7684test\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 sudo systemctl stop firewalld \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u00b6 \u767b\u5f55FusionInsight Manager\u7cfb\u7edf\uff0c\u5355\u51fb \u670d\u52a1\u7ba1\u7406 \uff0c\u5728\u83dc\u5355\u680f\u4e2d\u5355\u51fb \u4e0b\u8f7d\u5ba2\u6237\u7aef , \u5ba2\u6237\u7aef\u7c7b\u578b\u52fe\u9009 \u5b8c\u6574\u5ba2\u6237\u7aef , \u662f\u5426\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e2d\u751f\u6210\u5ba2\u6237\u7aef\u6587\u4ef6\u9009\u62e9 \u5426 \u4f7f\u7528WinSCP\u5de5\u5177\u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684\u8f6f\u4ef6\u5305\u4e0a\u4f20\u5230Linux\u670d\u52a1\u5668\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u5207\u6362\u5230\u65b0\u5efa\u7684test\u7528\u6237 su test \u89e3\u538b\u8f6f\u4ef6\u5305\u3002\u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u3002\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u89e3\u538b\u5b89\u88c5\u5305\u5230\u672c\u5730\u76ee\u5f55 cd /tmp/client tar -xvf FusionInsight_V100R002C60U20_Services_Client.tar tar -xvf FusionInsight_V100R002C60U20_Services_ClientConfig.tar \u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\u5ba2\u6237\u7aef\u5230\u6307\u5b9a\u76ee\u5f55\uff08\u7edd\u5bf9\u8def\u5f84\uff09\uff0c\u4f8b\u5982\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55 cd /opt/tmp/FusionInsight_V100R002C60U20_Services_ClientConfig ./install.sh /home/test/hadoopclient \u5ba2\u6237\u7aef\u5c06\u88ab\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55\u4e2d \u68c0\u67e5\u5ba2\u6237\u7aef\u8282\u70b9\u4e0eFusionInsight\u96c6\u7fa4\u65f6\u95f4\u540c\u6b65\uff08\u5dee\u8ddd\u4e0d\u80fd\u8d85\u8fc75\u5206\u949f\uff09 \u68c0\u67e5SparkR\u662f\u5426\u53ef\u7528 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u6267\u884c sparkR \u542f\u52a8SparkR, \u6b63\u5e38\u542f\u52a8\u51fa\u73b0\u4ee5\u4e0b\u754c\u9762 \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u00b6 \u4f7f\u7528\u65b0\u5efa\u7684\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u754c\u9762\u4e2d\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u521d\u59cb\u5316SparkR Sys.setenv(\"SPARKR_SUBMIT_ARGS\"=\"--master yarn-client --num-executors 1 sparkr-shell\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10:1.2.0\") sqlContext <- sparkRSQL.init(sc) \u521d\u59cb\u5316\u6210\u529f\u540e\u5982\u4e0b\u56fe \u5728Yarn\u7684ResourceManager\u754c\u9762\u53ef\u4ee5\u770b\u5230sparkuser\u5728\u96c6\u7fa4\u542f\u52a8\u4e86\u4e00\u4e2aSparkR\u7684\u5e94\u7528 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u00b6 R DataFrame \u8f6c\u5316\u4e3aSparkR DataFrame df <- createDataFrame(sqlContext, faithful) head(df) \u901a\u8fc7JSON\u6587\u4ef6\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790\u5904\u7406 \u5c06\u6d4b\u8bd5\u6570\u636eput\u5230HDFS\u4e2d wget https://raw.githubusercontent.com/eBay/Spark/master/examples/src/main/resources/people.json hdfs dfs -put people.json /user/sparkuser/ \u6267\u884c\u6587\u4ef6\u52a0\u8f7d\u5206\u6790 people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") head(people) printSchema(people) \u4eceHive\u8868\u4e2d\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790 hiveContext <- sparkRHive.init(sc) results <- sql(hiveContext, \"SELECT * FROM employees\") head(results) DataFrame Operations Selecting rows, columns df <- createDataFrame(sqlContext, faithful) df head(select(df, df$eruptions)) head(select(df, \"eruptions\")) head(filter(df, df$waiting < 50)) Grouping, Aggregation head(summarize(groupBy(df, df$waiting), count = n(df$waiting))) waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting)) head(arrange(waiting_counts, desc(waiting_counts$count))) Operating on Columns df$waiting_secs <- df$waiting * 60 head(df) Running SQL Queries from SparkR people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") registerTempTable(people, \"people\") teenagers <- sql(sqlContext, \"SELECT name FROM people WHERE age >= 13 AND age <= 19\") head(teenagers) Machine Learning df <- createDataFrame(sqlContext, iris) model <- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = \"gaussian\") summary(model) predictions <- predict(model, newData = df) head(select(predictions, \"Sepal_Length\", \"prediction\")) \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u00b6 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u4e2d\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5b89\u88c5\u6240\u9700\u7684library install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") \u901a\u8fc7spark_connect\u8fde\u63a5spark\u96c6\u7fa4 library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") \u8fd9\u91cc\u5982\u679cSPARK_HOME\u6307\u5411/home/test/hadoopclient/Spark/spark\uff0c\u540c\u65f6\u8bbe\u7f6eversion\u4e3a1.6.1\uff0c\u5219\u4f1a\u5bf9\u63a5\u4e0a1.5.1\u7684Spark sparklyr\u5b98\u65b9\u652f\u6301\u662f1.6.1\u4ee5\u4e0a\u7684Spark\uff0c\u8fd9\u91cc\u5f3a\u5236\u6307\u5b9aversion\u4e3a1.6.1\uff0c\u4e3b\u8981\u529f\u80fd\u5747\u6b63\u5e38\uff0c\u90e8\u5206Spark1.6.1\u652f\u6301\u800c1.5.1\u4e0d\u652f\u6301\u7684\u7279\u6027\u6267\u884c\u4f1a\u5931\u8d25 \u542f\u52a8\u6210\u529f\u540e\uff0c\u5728FusionInsgiht\u7684Yarn\u7684ResourceManager\u9875\u9762\u53ef\u4ee5\u770b\u5230sparklyr\u7684\u4efb\u52a1\u5df2\u7ecf\u542f\u52a8 \u5728RStudio\u7684Spark\u9762\u677f\u5237\u65b0\u4e00\u4e0b\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u6709hive\u7684\u8868 \u9009\u62e9hive\u8868\u53f3\u8fb9\u7684\u6570\u636e\u56fe\u8868\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u00b6 Use dplyr syntax to write Apache Spark SQL queries. Use select, where, group by, joins, and window functions in Aparche Spark SQL. Setup library(sparklyr) library(dplyr) library(babynames) library(ggplot2) library(dygraphs) library(rbokeh) knitr::opts_chunk$set(message = FALSE, warning = FALSE) Connect to Spark options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(SPARK_HOME_VERSION=\"1.6.1\") sc <- spark_connect(master = \"yarn-client\", version = \"1.6.1\", spark_home = \"/home/test/hadoopclient/Spark/spark\") Total US births Plot total US births recorded from the Social Security Administration. babynames_tbl <- copy_to(sc, babynames, \"babynames\") applicants_tbl <- copy_to(sc, applicants, \"applicants\") birthsYearly <- applicants_tbl %>% mutate(male = ifelse(sex == \"M\", n_all, 0), female = ifelse(sex == \"F\", n_all, 0)) %>% group_by(year) %>% summarize(Male = sum(male) / 1000000, Female = sum(female) / 1000000) %>% arrange(year) %>% collect birthsYearly %>% dygraph(main = \"Total US Births (SSN)\", ylab = \"Millions\") %>% dySeries(\"Female\") %>% dySeries(\"Male\") %>% dyOptions(stackedGraph = TRUE) %>% dyRangeSelector(height = 20) Aggregate data by name Use Spark SQL to create a look up table. Register and cache the look up table in Spark for future queries. topNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% group_by(name, sex) %>% summarize(count = as.numeric(sum(n))) %>% filter(count > 1000) %>% select(name, sex) filteredNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% inner_join(topNames_tbl) yearlyNames_tbl <- filteredNames_tbl %>% group_by(year, name, sex) %>% summarize(count = as.numeric(sum(n))) sdf_register(yearlyNames_tbl, \"yearlyNames\") tbl_cache(sc, \"yearlyNames\") Most popular names (1986) Identify the top 5 male and female names from 1986. Visualize the popularity trend over time. topNames1986_tbl <- yearlyNames_tbl %>% filter(year == 1986) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames1986\") tbl_cache(sc, \"topNames1986\") topNames1986Yearly <- yearlyNames_tbl %>% inner_join(select(topNames1986_tbl, sex, name)) %>% collect ggplot(topNames1986Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 1986\") Most popular names (2014) Identify the top 5 male and female names from 2014. Visualize the popularity trend over time. topNames2014_tbl <- yearlyNames_tbl %>% filter(year == 2014) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames2014\") tbl_cache(sc, \"topNames2014\") topNames2014Yearly <- yearlyNames_tbl %>% inner_join(select(topNames2014_tbl, sex, name)) %>% collect ggplot(topNames2014Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 2014\") Shared names Visualize the most popular names that are shared by both males and females. sharedName <- babynames_tbl %>% mutate(male = ifelse(sex == \"M\", n, 0), female = ifelse(sex == \"F\", n, 0)) %>% group_by(name) %>% summarize(Male = as.numeric(sum(male)), Female = as.numeric(sum(female)), count = as.numeric(sum(n)), AvgYear = round(as.numeric(sum(year * n) / sum(n)),0)) %>% filter(Male > 30000 & Female > 30000) %>% collect figure(width = NULL, height = NULL, xlab = \"Log10 Number of Males\", ylab = \"Log10 Number of Females\", title = \"Top shared names (1880 - 2014)\") %>% ly_points(log10(Male), log10(Female), data = sharedName, color = AvgYear, size = scale(sqrt(count)), hover = list(name, Male, Female, AvgYear), legend = FALSE) \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x) \u00b6 Train a linear model step will failed in Spark 1.5.1, because Spark 1.5.1 does not support the coefficients method for linear model output Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier. Connect to spark2x library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") Cache the tables into memory Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl <- tbl(sc, 'flights') # Cache airlines Hive table into Spark tbl_cache(sc, 'airlines') airlines_tbl <- tbl(sc, 'airlines') # Cache airports Hive table into Spark tbl_cache(sc, 'airports') airports_tbl <- tbl(sc, 'airports') Create a model data set Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year >= 2003 & year <= 2007) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Train a linear model Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition <- model_data %>% sdf_partition(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 <- model_partition$train %>% ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) ** Assess model performance** Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles <- lapply(model_partition, function(x) { sdf_predict(ml1, x) %>% mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles <- rbind( data.frame(data = 'train', model_deciles$train), data.frame(data = 'valid', model_deciles$valid), make.row.names = FALSE ) # Plot average gains by predicted decile deciles %>% ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Visualize predictions Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier <- sdf_predict(ml1, data_2008) %>% group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') Some carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights. FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u00b6 \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167 \u914d\u7f6eEPEL\u7684\u6e90\u5b89\u88c5R \u8fdb\u884c\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u5b89\u88c5sparklyr\u62a5\u9519configuration failed for package \u2018openssl\u2019 \u00b6 \u64cd\u4f5c\u7cfb\u7edf\u9700\u8981\u6267\u884c yum install openssl-devel \u5b89\u88c5openssl-devel \u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0bshell\u811a\u672c\u83b7\u53d6\u5f85\u5206\u6790\u7684\u6570\u636e # Make download directory mkdir /tmp/flights # Download flight data by year for i in { 2006 ..2008 } do echo \" $( date ) $i Download\" fnam = $i .csv.bz2 wget -O /tmp/flights/ $fnam http://stat-computing.org/dataexpo/2009/ $fnam echo \" $( date ) $i Unzip\" bunzip2 /tmp/flights/ $fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup = L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat \u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684/tmp/flights\u76ee\u5f55\u4ee5\u53ca/tmp/airlines.csv\uff0c/tmp/airports.csv\u6587\u4ef6\u4e0a\u4f20\u5230HDFS\u7684/user/sparkuser\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u5728Hive\u4e2d\u521b\u5efa\u4e09\u5f20\u8868\uff0c\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u5bf9\u5e94\u7684\u8868\u4e2d hdfs dfs -mkdir /user/sparkuser/flights hdfs dfs -put flights/* /user/sparkuser/flights/ hdfs dfs -put airlines.csv /user/sparkuser/ hdfs dfs -put airports.csv /user/sparkuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int , month int , dayofmonth int , dayofweek int , deptime int , crsdeptime int , arrtime int , crsarrtime int , uniquecarrier string , flightnum int , tailnum string , actualelapsedtime int , crselapsedtime int , airtime string , arrdelay int , depdelay int , origin string , dest string , distance int , taxiin string , taxiout string , cancelled int , cancellationcode string , diverted int , carrierdelay string , weatherdelay string , nasdelay string , securitydelay string , lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/flights/2006.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2007.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2008.csv' INTO TABLE flights ; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string , Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/airlines.csv' INTO TABLE airlines ; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string , name string , city string , country string , faa string , icao string , lat double , lon double , alt int , tz_offset double , dst string , tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE ; LOAD DATA INPATH '/user/sparkuser/airports.csv' INTO TABLE airports ;","title":"3.4.1 <--> 8.0"},{"location":"Development/RStudio/#rsutdiofusioninsight-spark","text":"","title":"RSutdio\u5bf9\u63a5FusionInsight Spark"},{"location":"Development/RStudio/#_1","text":"RStudio 3.4.1 \u2194 FusionInsight HD V100R002C60U10 (SparkR) RStudio 3.4.1 \u2194 FusionInsight HD V100R002C70SPC100 (SparkR) RStudio 3.4.1 \u2194 FusionInsight MRS 8.0 (SparkR)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/RStudio/#_2","text":"RStudio\u4e0eSpark\u96c6\u6210\u6709\u4e24\u79cd\u65b9\u5f0f\uff1a \u901a\u8fc7RStudio\u5b98\u65b9\u53d1\u5e03\u7684sparklyr\u4e0eSpark\u8fdb\u884c\u96c6\u6210 \u901a\u8fc7Apache Spark\u793e\u533a\u53d1\u5e03\u7684SparkR\u8fdb\u884c\u96c6\u6210 \u672c\u6587\u6863\u5305\u542b\u4e86\u4e24\u79cd\u65b9\u5f0f\u5bf9\u63a5\u7684\u6b65\u9aa4, \u76f8\u5173\u5bf9\u63a5\u6b65\u9aa4\u5982\u4e0b\uff1a \u5b89\u88c5R \u5b89\u88c5RStudio Server \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790 \u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6 \u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x)","title":"\u5bf9\u63a5\u65b9\u5f0f"},{"location":"Development/RStudio/#r","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728RStudio\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u672c\u6587\u4f7f\u7528\u7684RStudio\u8282\u70b9\u4e3aRedhat7.1\uff0cFusionInsight\u96c6\u7fa4\u8282\u70b9\u4e3aRedhat6.6 \u914d\u7f6eredhat\u7684yum\u6e90\uff0c\u56fd\u5185\u53ef\u4ee5\u914d\u7f6e aliyun\u7684\u6e90 \u6216\u8005 163\u7684\u6e90 \u914d\u7f6eEPEL\u7684\u6e90 \u5b89\u88c5R-3.4.1","title":"\u5b89\u88c5R"},{"location":"Development/RStudio/#aliyun","text":"\u914d\u7f6e\u597dRedhat7.1\u7684yum\u6e90 cd ~ rpm -qa|grep yum|xargs rpm -e --nodeps rpm -qa|grep python-urlgrabber|xargs rpm -e --nodeps wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-metadata-parser-1.1.4-10.el7.x86_64.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-3.4.3-150.el7.centos.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-rhn-plugin-2.0.1-6.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.31-40.el7.noarch.rpm wget https://mirrors.aliyun.com/centos/7/os/x86_64/Packages/python-urlgrabber-3.10-8.el7.noarch.rpm rpm -ivh *.rpm cd /etc/yum.repos.d/ wget https://mirrors.aliyun.com/repo/Centos-7.repo sed -i 's/$releasever/7/g' /etc/yum.repos.d/Centos-7.repo yum clean yum makecache","title":"\u914d\u7f6ealiyun\u7684\u6e90"},{"location":"Development/RStudio/#163","text":"\u914d\u7f6e\u597dRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache","title":"\u914d\u7f6e163\u7684\u6e90"},{"location":"Development/RStudio/#epel","text":"\u5b89\u88c5EPEL\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm Redhat 7.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//7/x86_64/e/epel-release-7-10.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache","title":"\u914d\u7f6eEPEL\u7684\u6e90"},{"location":"Development/RStudio/#r-341","text":"\u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a","title":"\u5b89\u88c5R-3.4.1"},{"location":"Development/RStudio/#rstudio-server","text":"\u4e0b\u8f7d\u5e76\u5b89\u88c5RStudio Server wget https://download2.rstudio.org/rstudio-server-rhel-1.0.153-x86_64.rpm yum install --nogpgcheck rstudio-server-rhel-1.0.153-x86_64.rpm \u4f7f\u7528 vi /etc/rstudio/rserver.conf \u4fee\u6539RStudio\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u6307\u5b9aRStudio Server\u4f7f\u7528\u7684R\u7684\u8def\u5f84 rsession-which-r=/usr/bin/R \u91cd\u542frstudio-server\u540e\uff0c\u67e5\u770b\u670d\u52a1\u662f\u5426\u6b63\u5e38 sudo systemctl restart rstudio-server sudo systemctl status rstudio-server \u670d\u52a1\u6b63\u5e38\u542f\u52a8\u5982\u4e0b \u7531\u4e8eRStudio Server\u4e0d\u5141\u8bb8\u4f7f\u7528root\u7528\u6237\u767b\u9646\uff0c\u9700\u8981\u65b0\u5efa\u4e00\u4e2a\u666e\u901a\u7528\u6237\u7528\u4e8eWeb\u754c\u9762\u7684\u767b\u9646 useradd -d /home/test -m test passwd test \u7528\u6237\u65b0\u5efa\u5b8c\u6210\u540e\uff0c\u5173\u95ed\u9632\u706b\u5899\uff0c\u7136\u540e\u4f7f\u7528\u672c\u673aip:8787\u7aef\u53e3\u8bbf\u95eeRStudio Server\uff0c\u4f7f\u7528\u65b0\u5efa\u7684test\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 sudo systemctl stop firewalld","title":"\u5b89\u88c5RStudio Server"},{"location":"Development/RStudio/#fusioninsight","text":"\u767b\u5f55FusionInsight Manager\u7cfb\u7edf\uff0c\u5355\u51fb \u670d\u52a1\u7ba1\u7406 \uff0c\u5728\u83dc\u5355\u680f\u4e2d\u5355\u51fb \u4e0b\u8f7d\u5ba2\u6237\u7aef , \u5ba2\u6237\u7aef\u7c7b\u578b\u52fe\u9009 \u5b8c\u6574\u5ba2\u6237\u7aef , \u662f\u5426\u5728\u96c6\u7fa4\u7684\u8282\u70b9\u4e2d\u751f\u6210\u5ba2\u6237\u7aef\u6587\u4ef6\u9009\u62e9 \u5426 \u4f7f\u7528WinSCP\u5de5\u5177\u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684\u8f6f\u4ef6\u5305\u4e0a\u4f20\u5230Linux\u670d\u52a1\u5668\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u5207\u6362\u5230\u65b0\u5efa\u7684test\u7528\u6237 su test \u89e3\u538b\u8f6f\u4ef6\u5305\u3002\u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982 /tmp/client \u3002\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u89e3\u538b\u5b89\u88c5\u5305\u5230\u672c\u5730\u76ee\u5f55 cd /tmp/client tar -xvf FusionInsight_V100R002C60U20_Services_Client.tar tar -xvf FusionInsight_V100R002C60U20_Services_ClientConfig.tar \u8fdb\u5165\u5b89\u88c5\u5305\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\u5ba2\u6237\u7aef\u5230\u6307\u5b9a\u76ee\u5f55\uff08\u7edd\u5bf9\u8def\u5f84\uff09\uff0c\u4f8b\u5982\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55 cd /opt/tmp/FusionInsight_V100R002C60U20_Services_ClientConfig ./install.sh /home/test/hadoopclient \u5ba2\u6237\u7aef\u5c06\u88ab\u5b89\u88c5\u5230 /home/test/hadoopclient \u76ee\u5f55\u4e2d \u68c0\u67e5\u5ba2\u6237\u7aef\u8282\u70b9\u4e0eFusionInsight\u96c6\u7fa4\u65f6\u95f4\u540c\u6b65\uff08\u5dee\u8ddd\u4e0d\u80fd\u8d85\u8fc75\u5206\u949f\uff09 \u68c0\u67e5SparkR\u662f\u5426\u53ef\u7528 \u4f7f\u7528sparkuser\u8fdb\u884cKerberos\u8ba4\u8bc1(sparkuser\u4e3aFusionInsight\u4e2d\u521b\u5efa\u7684\u62e5\u6709Spark\u8bbf\u95ee\u6743\u9650\u7684\u4eba\u673a\u7528\u6237) cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u6267\u884c sparkR \u542f\u52a8SparkR, \u6b63\u5e38\u542f\u52a8\u51fa\u73b0\u4ee5\u4e0b\u754c\u9762","title":"\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef"},{"location":"Development/RStudio/#sparkrrstudio","text":"\u4f7f\u7528\u65b0\u5efa\u7684\u7528\u6237\u767b\u9646\u5373\u53ef\u8fdb\u5165RStudio\u7684Web\u5f00\u53d1\u754c\u9762 \u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u754c\u9762\u4e2d\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u521d\u59cb\u5316SparkR Sys.setenv(\"SPARKR_SUBMIT_ARGS\"=\"--master yarn-client --num-executors 1 sparkr-shell\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") .libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\",\"lib\"), .libPaths())) library(SparkR) sc <- sparkR.init(master = \"yarn-client\", sparkPackages = \"com.databricks:spark-csv_2.10:1.2.0\") sqlContext <- sparkRSQL.init(sc) \u521d\u59cb\u5316\u6210\u529f\u540e\u5982\u4e0b\u56fe \u5728Yarn\u7684ResourceManager\u754c\u9762\u53ef\u4ee5\u770b\u5230sparkuser\u5728\u96c6\u7fa4\u542f\u52a8\u4e86\u4e00\u4e2aSparkR\u7684\u5e94\u7528","title":"\u4f7f\u7528SparkR\u4e0eRStudio\u96c6\u6210\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/#rstudiosparkr","text":"R DataFrame \u8f6c\u5316\u4e3aSparkR DataFrame df <- createDataFrame(sqlContext, faithful) head(df) \u901a\u8fc7JSON\u6587\u4ef6\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790\u5904\u7406 \u5c06\u6d4b\u8bd5\u6570\u636eput\u5230HDFS\u4e2d wget https://raw.githubusercontent.com/eBay/Spark/master/examples/src/main/resources/people.json hdfs dfs -put people.json /user/sparkuser/ \u6267\u884c\u6587\u4ef6\u52a0\u8f7d\u5206\u6790 people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") head(people) printSchema(people) \u4eceHive\u8868\u4e2d\u52a0\u8f7d\u6570\u636e\u8fdb\u884c\u5206\u6790 hiveContext <- sparkRHive.init(sc) results <- sql(hiveContext, \"SELECT * FROM employees\") head(results) DataFrame Operations Selecting rows, columns df <- createDataFrame(sqlContext, faithful) df head(select(df, df$eruptions)) head(select(df, \"eruptions\")) head(filter(df, df$waiting < 50)) Grouping, Aggregation head(summarize(groupBy(df, df$waiting), count = n(df$waiting))) waiting_counts <- summarize(groupBy(df, df$waiting), count = n(df$waiting)) head(arrange(waiting_counts, desc(waiting_counts$count))) Operating on Columns df$waiting_secs <- df$waiting * 60 head(df) Running SQL Queries from SparkR people <- read.df(sqlContext, \"/user/sparkuser/people.json\", \"json\") registerTempTable(people, \"people\") teenagers <- sql(sqlContext, \"SELECT name FROM people WHERE age >= 13 AND age <= 19\") head(teenagers) Machine Learning df <- createDataFrame(sqlContext, iris) model <- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = \"gaussian\") summary(model) predictions <- predict(model, newData = df) head(select(predictions, \"Sepal_Length\", \"prediction\"))","title":"\u5728RStudio\u4e2d\u4f7f\u7528SparkR\u8fdb\u884c\u6570\u636e\u5206\u6790"},{"location":"Development/RStudio/#rstudio-sparklyrspark","text":"\u9009\u62e9 Tools \u83dc\u5355\u4e0b\u7684 Shell \u8fdb\u5165\u767b\u9646\u7528\u6237\u7684shell\u8fdb\u884ckerberos\u8ba4\u8bc1 cd /home/test/hadoopclient source bigdata_env kinit sparkuser \u5728RStudio\u4e2d\u6267\u884c\u4e0b\u9762\u7684\u547d\u4ee4\uff0c\u5b89\u88c5\u6240\u9700\u7684library install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages(\"rbokeh\") \u901a\u8fc7spark_connect\u8fde\u63a5spark\u96c6\u7fa4 library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") \u8fd9\u91cc\u5982\u679cSPARK_HOME\u6307\u5411/home/test/hadoopclient/Spark/spark\uff0c\u540c\u65f6\u8bbe\u7f6eversion\u4e3a1.6.1\uff0c\u5219\u4f1a\u5bf9\u63a5\u4e0a1.5.1\u7684Spark sparklyr\u5b98\u65b9\u652f\u6301\u662f1.6.1\u4ee5\u4e0a\u7684Spark\uff0c\u8fd9\u91cc\u5f3a\u5236\u6307\u5b9aversion\u4e3a1.6.1\uff0c\u4e3b\u8981\u529f\u80fd\u5747\u6b63\u5e38\uff0c\u90e8\u5206Spark1.6.1\u652f\u6301\u800c1.5.1\u4e0d\u652f\u6301\u7684\u7279\u6027\u6267\u884c\u4f1a\u5931\u8d25 \u542f\u52a8\u6210\u529f\u540e\uff0c\u5728FusionInsgiht\u7684Yarn\u7684ResourceManager\u9875\u9762\u53ef\u4ee5\u770b\u5230sparklyr\u7684\u4efb\u52a1\u5df2\u7ecf\u542f\u52a8 \u5728RStudio\u7684Spark\u9762\u677f\u5237\u65b0\u4e00\u4e0b\uff0c\u53ef\u4ee5\u770b\u5230\u6240\u6709hive\u7684\u8868 \u9009\u62e9hive\u8868\u53f3\u8fb9\u7684\u6570\u636e\u56fe\u8868\u53ef\u4ee5\u9884\u89c8\u8868\u4e2d\u7684\u6570\u636e","title":"\u4f7f\u7528RStudio Sparklyr\u548cSpark\u96c6\u6210\u8fdb\u884c\u5206\u6790"},{"location":"Development/RStudio/#sparklyrsparkbabynames","text":"Use dplyr syntax to write Apache Spark SQL queries. Use select, where, group by, joins, and window functions in Aparche Spark SQL. Setup library(sparklyr) library(dplyr) library(babynames) library(ggplot2) library(dygraphs) library(rbokeh) knitr::opts_chunk$set(message = FALSE, warning = FALSE) Connect to Spark options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark/spark\") Sys.setenv(SPARK_HOME_VERSION=\"1.6.1\") sc <- spark_connect(master = \"yarn-client\", version = \"1.6.1\", spark_home = \"/home/test/hadoopclient/Spark/spark\") Total US births Plot total US births recorded from the Social Security Administration. babynames_tbl <- copy_to(sc, babynames, \"babynames\") applicants_tbl <- copy_to(sc, applicants, \"applicants\") birthsYearly <- applicants_tbl %>% mutate(male = ifelse(sex == \"M\", n_all, 0), female = ifelse(sex == \"F\", n_all, 0)) %>% group_by(year) %>% summarize(Male = sum(male) / 1000000, Female = sum(female) / 1000000) %>% arrange(year) %>% collect birthsYearly %>% dygraph(main = \"Total US Births (SSN)\", ylab = \"Millions\") %>% dySeries(\"Female\") %>% dySeries(\"Male\") %>% dyOptions(stackedGraph = TRUE) %>% dyRangeSelector(height = 20) Aggregate data by name Use Spark SQL to create a look up table. Register and cache the look up table in Spark for future queries. topNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% group_by(name, sex) %>% summarize(count = as.numeric(sum(n))) %>% filter(count > 1000) %>% select(name, sex) filteredNames_tbl <- babynames_tbl %>% filter(year >= 1986) %>% inner_join(topNames_tbl) yearlyNames_tbl <- filteredNames_tbl %>% group_by(year, name, sex) %>% summarize(count = as.numeric(sum(n))) sdf_register(yearlyNames_tbl, \"yearlyNames\") tbl_cache(sc, \"yearlyNames\") Most popular names (1986) Identify the top 5 male and female names from 1986. Visualize the popularity trend over time. topNames1986_tbl <- yearlyNames_tbl %>% filter(year == 1986) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames1986\") tbl_cache(sc, \"topNames1986\") topNames1986Yearly <- yearlyNames_tbl %>% inner_join(select(topNames1986_tbl, sex, name)) %>% collect ggplot(topNames1986Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 1986\") Most popular names (2014) Identify the top 5 male and female names from 2014. Visualize the popularity trend over time. topNames2014_tbl <- yearlyNames_tbl %>% filter(year == 2014) %>% group_by(name, sex) %>% summarize(count = sum(count)) %>% group_by(sex) %>% mutate(rank = min_rank(desc(count))) %>% filter(rank < 5) %>% arrange(sex, rank) %>% select(name, sex, rank) %>% sdf_register(\"topNames2014\") tbl_cache(sc, \"topNames2014\") topNames2014Yearly <- yearlyNames_tbl %>% inner_join(select(topNames2014_tbl, sex, name)) %>% collect ggplot(topNames2014Yearly, aes(year, count, color=name)) + facet_grid(~sex) + geom_line() + ggtitle(\"Most Popular Names of 2014\") Shared names Visualize the most popular names that are shared by both males and females. sharedName <- babynames_tbl %>% mutate(male = ifelse(sex == \"M\", n, 0), female = ifelse(sex == \"F\", n, 0)) %>% group_by(name) %>% summarize(Male = as.numeric(sum(male)), Female = as.numeric(sum(female)), count = as.numeric(sum(n)), AvgYear = round(as.numeric(sum(year * n) / sum(n)),0)) %>% filter(Male > 30000 & Female > 30000) %>% collect figure(width = NULL, height = NULL, xlab = \"Log10 Number of Males\", ylab = \"Log10 Number of Females\", title = \"Top shared names (1880 - 2014)\") %>% ly_points(log10(Male), log10(Female), data = sharedName, color = AvgYear, size = scale(sqrt(count)), hover = list(name, Male, Female, AvgYear), legend = FALSE)","title":"\u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790babynames\u6570\u636e\u96c6"},{"location":"Development/RStudio/#sparklyrsparkspark2x","text":"Train a linear model step will failed in Spark 1.5.1, because Spark 1.5.1 does not support the coefficients method for linear model output Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier. Connect to spark2x library(sparklyr) library(dplyr) library(ggplot2) options(bitmapType = 'cairo') Sys.setenv(JAVA_HOME=\"/home/test/hadoopclient/JDK/jdk\") Sys.setenv(SPARK_HOME=\"/home/test/hadoopclient/Spark2x/spark\") Sys.setenv(SPARK_HOME_VERSION=\"2.1.0\") sc <- spark_connect(master = \"yarn-client\", version = \"2.1.0\", spark_home = \"/home/test/hadoopclient/Spark2x/spark\") Cache the tables into memory Use tbl_cache to load the flights table into memory. Caching tables will make analysis much faster. Create a dplyr reference to the Spark DataFrame. # Cache flights Hive table into Spark tbl_cache(sc, 'flights') flights_tbl <- tbl(sc, 'flights') # Cache airlines Hive table into Spark tbl_cache(sc, 'airlines') airlines_tbl <- tbl(sc, 'airlines') # Cache airports Hive table into Spark tbl_cache(sc, 'airports') airports_tbl <- tbl(sc, 'airports') Create a model data set Filter the data to contain only the records to be used in the fitted model. Join carrier descriptions for reference. Create a new variable called gain which represents the amount of time gained (or lost) in flight. # Filter records and create target variable 'gain' model_data <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year >= 2003 & year <= 2007) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain) # Summarize data by carrier model_data %>% group_by(uniquecarrier) %>% summarize(description = min(description), gain=mean(gain), distance=mean(distance), depdelay=mean(depdelay)) %>% select(description, gain, distance, depdelay) %>% arrange(gain) Train a linear model Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier. # Partition the data into training and validation sets model_partition <- model_data %>% sdf_partition(train = 0.8, valid = 0.2, seed = 5555) # Fit a linear model ml1 <- model_partition$train %>% ml_linear_regression(gain ~ distance + depdelay + uniquecarrier) # Summarize the linear model summary(ml1) ** Assess model performance** Compare the model performance using the validation data. # Calculate average gains by predicted decile model_deciles <- lapply(model_partition, function(x) { sdf_predict(ml1, x) %>% mutate(decile = ntile(desc(prediction), 10)) %>% group_by(decile) %>% summarize(gain = mean(gain)) %>% select(decile, gain) %>% collect() }) # Create a summary dataset for plotting deciles <- rbind( data.frame(data = 'train', model_deciles$train), data.frame(data = 'valid', model_deciles$valid), make.row.names = FALSE ) # Plot average gains by predicted decile deciles %>% ggplot(aes(factor(decile), gain, fill = data)) + geom_bar(stat = 'identity', position = 'dodge') + labs(title = 'Average gain by predicted decile', x = 'Decile', y = 'Minutes') Visualize predictions Compare actual gains to predicted gains for an out of time sample. # Select data from an out of time sample data_2008 <- flights_tbl %>% filter(!is.na(arrdelay) & !is.na(depdelay) & !is.na(distance)) %>% filter(depdelay > 15 & depdelay < 240) %>% filter(arrdelay > -60 & arrdelay < 360) %>% filter(year == 2008) %>% left_join(airlines_tbl, by = c(\"uniquecarrier\" = \"code\")) %>% mutate(gain = depdelay - arrdelay) %>% select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest) # Summarize data by carrier carrier <- sdf_predict(ml1, data_2008) %>% group_by(description) %>% summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %>% filter(freq > 10000) %>% collect # Plot actual gains and predicted gains by airline carrier ggplot(carrier, aes(gain, prediction)) + geom_point(alpha = 0.75, color = 'red', shape = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = 'blue') + geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) + labs(title='Average Gains Forecast', x = 'Actual', y = 'Predicted') Some carriers make up more time than others in flight, but the differences are relatively small. The average time gains between the best and worst airlines is only six minutes. The best predictor of time gained is not carrier but flight distance. The biggest gains were associated with the longest flights.","title":"\u4f7f\u7528sparklyr\u7ed3\u5408spark\u8fdb\u884c\u6570\u636e\u5206\u6790\u822a\u7a7a\u516c\u53f8\u98de\u884c\u6570\u636e(\u5fc5\u987b\u914d\u5957Spark2x)"},{"location":"Development/RStudio/#faq","text":"","title":"FAQ"},{"location":"Development/RStudio/#fusioninsightr","text":"\u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167 \u914d\u7f6eEPEL\u7684\u6e90\u5b89\u88c5R \u8fdb\u884c\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5","title":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R"},{"location":"Development/RStudio/#sparklyrconfiguration-failed-for-package-openssl","text":"\u64cd\u4f5c\u7cfb\u7edf\u9700\u8981\u6267\u884c yum install openssl-devel \u5b89\u88c5openssl-devel","title":"\u5b89\u88c5sparklyr\u62a5\u9519configuration failed for package \u2018openssl\u2019"},{"location":"Development/RStudio/#sparklyr","text":"\u6267\u884c\u4ee5\u4e0bshell\u811a\u672c\u83b7\u53d6\u5f85\u5206\u6790\u7684\u6570\u636e # Make download directory mkdir /tmp/flights # Download flight data by year for i in { 2006 ..2008 } do echo \" $( date ) $i Download\" fnam = $i .csv.bz2 wget -O /tmp/flights/ $fnam http://stat-computing.org/dataexpo/2009/ $fnam echo \" $( date ) $i Unzip\" bunzip2 /tmp/flights/ $fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup = L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat \u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684/tmp/flights\u76ee\u5f55\u4ee5\u53ca/tmp/airlines.csv\uff0c/tmp/airports.csv\u6587\u4ef6\u4e0a\u4f20\u5230HDFS\u7684/user/sparkuser\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u5728Hive\u4e2d\u521b\u5efa\u4e09\u5f20\u8868\uff0c\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u5bf9\u5e94\u7684\u8868\u4e2d hdfs dfs -mkdir /user/sparkuser/flights hdfs dfs -put flights/* /user/sparkuser/flights/ hdfs dfs -put airlines.csv /user/sparkuser/ hdfs dfs -put airports.csv /user/sparkuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int , month int , dayofmonth int , dayofweek int , deptime int , crsdeptime int , arrtime int , crsarrtime int , uniquecarrier string , flightnum int , tailnum string , actualelapsedtime int , crselapsedtime int , airtime string , arrdelay int , depdelay int , origin string , dest string , distance int , taxiin string , taxiout string , cancelled int , cancellationcode string , diverted int , carrierdelay string , weatherdelay string , nasdelay string , securitydelay string , lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/flights/2006.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2007.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/sparkuser/flights/2008.csv' INTO TABLE flights ; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string , Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/sparkuser/airlines.csv' INTO TABLE airlines ; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string , name string , city string , country string , faa string , icao string , lat double , lon double , alt int , tz_offset double , dst string , tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE ; LOAD DATA INPATH '/user/sparkuser/airports.csv' INTO TABLE airports ;","title":"\u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e"},{"location":"Development/Squirrel_3.8.0/","text":"Squirrel\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Squirrel 3.7.1 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) Squirrel 3.8.0 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL) \u8bf4\u660e \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86Squirrel\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4 Squirrel\u5b89\u88c5 \u00b6 \u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u3002 \u4fee\u6539 C:\\Windows\\System32\\drivers\\etc\\hosts \u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f\u3002 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dSquirrel\uff0c\u5730\u5740\uff1a http://www.squirrelsql.org/#installation \uff0c\u9009\u62e9Install jar of SQuirreL 3.7.1 for Windows/Linux/others\uff0c\u4e0b\u8f7d\u8f6f\u4ef6squirrel-sql-3.7.1-standard.jar \u53cc\u51fbsquirrel-sql-3.7.1-standard.jar\u5b89\u88c5 \u5728\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u8981\u5b89\u88c5\u54ea\u4e9b\u73af\u5883\uff0c\u4f7f\u7528\u7684\u6570\u636e\u5e93\u63d2\u4ef6\uff0c\u8bed\u8a00\u5305\u3002 Squirrel\u8fde\u63a5Fiber \u00b6 \u4f7f\u7528SQuirreL SQL Client\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9Drivers\uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\Fiber\\conf\\fiber.xml;defaultDriver=hive Extra Class Path\uff1a\u5c06Fiber/lib\u4e0b\u7684jar\u5305\u90fd\u6dfb\u52a0\u8fdb\u6765 ClassName\uff1acom.huawei.fiber.FiberDriver \u53ef\u4ee5\u770b\u5230\u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002 \u5bf9\u63a5Hive \u00b6 \u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name\uff1atest Password\uff1a\u5bc6\u7801 \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u70b9\u51fb Connect \u67e5\u770bhive\u4e2d\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a \u5bf9\u63a5SparkSQL \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aspark\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fbFiber\uff0c\u70b9\u51fb Connet \uff0c\u5c06driver\u5207\u6362\u4e3aspark \u53ef\u4ee5\u770b\u5230\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Spark\u589e\u52a0\u6570\u636e \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a \u5bf9\u63a5Phoenix \u00b6 \u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \uff0c\u5c06driver\u5207\u6362\u4e3aphoenix \u53ef\u4ee5\u770b\u5230\u6570\u636ephoenix\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5411phoenix\u8868\u4e2d\u589e\u52a0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8','company8') \u67e5\u8be2\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5220\u9664\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 delete from TB_PHOENIX where ID=109; \u67e5\u770b\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u66f4\u65b0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8_up','company8_up') \u67e5\u770b\u7ed3\u679c","title":"3.8.0 <--> C70"},{"location":"Development/Squirrel_3.8.0/#squirrelfusioninsight","text":"","title":"Squirrel\u5bf9\u63a5FusionInsight"},{"location":"Development/Squirrel_3.8.0/#_1","text":"Squirrel 3.7.1 \u2194 FusionInsight HD V100R002C60U20 (Hive/Phoenix/SparkSQL) Squirrel 3.8.0 \u2194 FusionInsight HD V100R002C70SPC200 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Squirrel_3.8.0/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001Squirrel\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u4ecb\u7ecd\u4e86Squirrel\u4e0eFusionInsight\u7684Fiber\u5bf9\u63a5\u7684\u64cd\u4f5c\u6b65\u9aa4","title":"\u8bf4\u660e"},{"location":"Development/Squirrel_3.8.0/#squirrel","text":"\u5b89\u88c5jdk1.8\uff0c\u914d\u7f6e\u73af\u5883\u53d8\u91cf\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u4f4d\u7f6e /opt/hadoopclient \u3002 \u4fee\u6539 C:\\Windows\\System32\\drivers\\etc\\hosts \u6587\u4ef6\uff0c\u52a0\u5165FusionInsight\u96c6\u7fa4\u4fe1\u606f\u3002 \u5728\u672c\u5730PC\u673a\u4e0a\u65b0\u5efa\u4e00\u4e2a\u76ee\u5f55\uff0c\u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684fiber\u5ba2\u6237\u7aef\u6587\u4ef6\u5939Fiber\u62f7\u8d1d\u81f3\u672c\u5730\uff0c\u4f8b\u5982 C:\\Fiber \u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u7684\u6743\u9650\uff0c\u4e0b\u8f7dtest\u7684keytab\u6587\u4ef6user.keytab\uff0c\u62f7\u8d1d\u5230 C:\\Fiber\\conf\\ \u6587\u4ef6\u5939\u4e0b\u3002 \u5c06FusionInsight\u5ba2\u6237\u7aef\u4e0bjaas.conf\u6587\u4ef6\u548ckrb5.conf\u62f7\u8d1d\u5230 C:\\Fiber\\conf \u76ee\u5f55\u4e0b\uff0c\u6587\u6863\u5185\u5bb9\u5982\u4e0b\uff0cprincipal\u548ckeytab\u6309\u5b9e\u9645\u586b\u5199\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539fiber.xml\u6587\u4ef6\u914d\u7f6e\uff0c\u4f4d\u7f6e C:\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Hive\\\\config;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark\u7684JDBC\u8fde\u63a5\uff1a <jdbc> <identify>spark</identify> <describe>spark jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\Spark\\\\spark\\\\conf;C:\\\\Fiber\\\\Spark\\\\spark\\\\lib</classPath> <jdbcUrl>jdbc:hive2://ha-cluster/default;saslQop=auth-conf;auth=KERBEROS;principal=spark/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=C:/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u8fde\u63a5\uff0c\u9700\u8981\u589e\u52a0\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:162.1.93.101,162.1.93.102,162.1.93.103:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>hbase.myclient.keytab</name> <value>C:\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>test</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files (x86)\\\\Java\\\\jdk1.8.0_112\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5c06Hive\u3001Spark\u3001Phoenix\u7684JDBC\u914d\u7f6e\u4e2dclassPath\u4e2d\u7684\u6587\u4ef6\u62f7\u8d1d\u81f3Fiber\u6587\u4ef6\u5939\u4e2d\u3002 \u4e0b\u8f7dSquirrel\uff0c\u5730\u5740\uff1a http://www.squirrelsql.org/#installation \uff0c\u9009\u62e9Install jar of SQuirreL 3.7.1 for Windows/Linux/others\uff0c\u4e0b\u8f7d\u8f6f\u4ef6squirrel-sql-3.7.1-standard.jar \u53cc\u51fbsquirrel-sql-3.7.1-standard.jar\u5b89\u88c5 \u5728\u8fd9\u91cc\u53ef\u4ee5\u9009\u62e9\u8981\u5b89\u88c5\u54ea\u4e9b\u73af\u5883\uff0c\u4f7f\u7528\u7684\u6570\u636e\u5e93\u63d2\u4ef6\uff0c\u8bed\u8a00\u5305\u3002","title":"Squirrel\u5b89\u88c5"},{"location":"Development/Squirrel_3.8.0/#squirrelfiber","text":"\u4f7f\u7528SQuirreL SQL Client\u901a\u8fc7Fiber\u8fde\u63a5FusionInsight\u7684Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u3002 \u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9Drivers\uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\Fiber\\conf\\fiber.xml;defaultDriver=hive Extra Class Path\uff1a\u5c06Fiber/lib\u4e0b\u7684jar\u5305\u90fd\u6dfb\u52a0\u8fdb\u6765 ClassName\uff1acom.huawei.fiber.FiberDriver \u53ef\u4ee5\u770b\u5230\u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002","title":"Squirrel\u8fde\u63a5Fiber"},{"location":"Development/Squirrel_3.8.0/#hive","text":"\u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f Name\uff1aFiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name\uff1atest Password\uff1a\u5bc6\u7801 \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u70b9\u51fb Connect \u67e5\u770bhive\u4e2d\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Hive\u589e\u52a0\u6570\u636e\uff1a \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"\u5bf9\u63a5Hive"},{"location":"Development/Squirrel_3.8.0/#sparksql","text":"\u5c06defaultDriver\u5207\u6362\u4e3aspark\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fbFiber\uff0c\u70b9\u51fb Connet \uff0c\u5c06driver\u5207\u6362\u4e3aspark \u53ef\u4ee5\u770b\u5230\u6570\u636e\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 Spark\u589e\u52a0\u6570\u636e \u7f16\u8f91\u6570\u636e\u6587\u4ef6data_input.txt\uff0c\u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684hdfs\u76ee\u5f55\u4e2d\uff0c\u4f8b\u5982 /tmp/ \u4e0b\uff0c\u6587\u672c\u5185\u5bb9\u5982\u4e0b\uff1a \u7f16\u8f91\u811a\u672c\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff1a load data inpath \u2018/tmp/data_input.txt\u2019 overwrite into table workers_info \u67e5\u770b\u7ed3\u679c\uff1a","title":"\u5bf9\u63a5SparkSQL"},{"location":"Development/Squirrel_3.8.0/#phoenix","text":"\u5c06defaultDriver\u5207\u6362\u4e3aphoenix\uff0c\u70b9\u51fb Test \u70b9\u51fb Connect \u8fde\u63a5\u6210\u529f\uff0c\u70b9\u51fb OK \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \uff0c\u5c06driver\u5207\u6362\u4e3aphoenix \u53ef\u4ee5\u770b\u5230\u6570\u636ephoenix\u8868 \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\uff0c\u5728\u4e0b\u65b9\u53ef\u4ee5\u770b\u5230\u67e5\u8be2\u7ed3\u679c\u3002 select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5411phoenix\u8868\u4e2d\u589e\u52a0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8','company8') \u67e5\u8be2\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u5220\u9664\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 delete from TB_PHOENIX where ID=109; \u67e5\u770b\u7ed3\u679c\uff1a select * from tb_phoenix \u70b9\u51fb SQL\u9762\u677f \uff0c\u7f16\u8f91SQL\u8bed\u53e5\uff0c\u66f4\u65b0\u4e00\u6761\u6570\u636e\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO TB_PHOENIX(Id, Name,Company) values (108,'phoenix_user8_up','company8_up') \u67e5\u770b\u7ed3\u679c","title":"\u5bf9\u63a5Phoenix"},{"location":"Development/Squirrel_3.9.1/","text":"SQuirreL\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Squirrel 3.9.1 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL) Squirrel 3.9.1 \u2194 FusionInsight MRS 8.0 (Hive/Phoenix/SparkSQL) \u7b80\u4ecb \u00b6 SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001SQuirreL\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cSQuirreL\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u51c6\u5907\u5de5\u4f5c \u00b6 \u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1); Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 C:\\ecotesting\\Fiber\\HBase\\hbase\\lib\\phoenix-core-4.13.1-HBase-1.3.jar \u62f7\u8d1d\u81f3 C:\\ecotesting\\Fiber\\lib \u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e \u00b6 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \u3002 <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property> SQuirreL\u5bf9\u63a5Fiber \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 SQuirreL SQL Client\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u4ece http://www.squirrelsql.org/#installation \u4e0b\u8f7d\u4e0e\u672c\u5730\u7cfb\u7edf\u76f8\u5bf9\u5e94\u7684SQuirreL\u8f6f\u4ef6\uff08\u4f8b\u5982\uff1aInstall jar of SQuirreL 3.9.1 for Windows/Linux/others\uff09\u3002\u53cc\u51fb\u4e0b\u8f7d\u7684jar\u5305\u542f\u52a8\u5b89\u88c5\uff0c\u53ef\u6309\u7167\u9ed8\u8ba4\u9009\u9879\u5b8c\u6210\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Hive \u00b6 \u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9 Drivers \uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 \u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a Name: Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml;defaultDriver=hive Extra Class Path: \u70b9\u51fbAdd\u6309\u94ae\u5c06C:\\ecotesting\\Fiber\\lib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Class Name\uff1acom.huawei.fiber.FiberDriver\uff08\u6dfb\u52a0jar\u5305\u540e\uff0c\u70b9\u51fbList Drivers\u6309\u94ae\u7136\u540e\u4ece\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\uff09 \u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002 \u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u3002\u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a Name: Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name: developuser\uff08\u53ef\u4e0d\u586b\u5199\uff09 Password\uff1a\u5bc6\u7801\uff08\u53ef\u4e0d\u586b\u5199\uff09 \u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u70b9\u51fb Connect \u3002 \u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student \uff0c\u5728 Content \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 SELECT * FROM student; \u5411Hive\u8868SQL_TEST\u63d2\u5165\u6570\u636e \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 SQL_TEST \u3002\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS SQL_TEST (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868SQL_TEST\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE SQL_TEST; \u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868SQL_TEST\u3002 SELECT * FROM SQL_TEST; SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x \u00b6 \u9009\u62e9 Aliases->Fiber \uff0c\u70b9\u51fb \u4fee\u6539Fiber\u7684defaultDriver\u5c5e\u6027\u3002 \u5c06defaultDriver\u4fee\u6539\u4e3a spark2x \uff0c\u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \u3002 \u5982\u679c\u8fd4\u56de\u201cSession startup time hint\u201d\uff0c\u5219\u70b9\u51fb Close \u3002 \u5982\u679c\u6ca1\u6709\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \uff0c\u70b9\u51fb \u6309\u94ae\u201cRefresh Object Tree and Database Meta Data Cache\u201d\uff0c\u5219\u4f1a\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u3002\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student \uff0c\u5728 Content \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 SELECT * FROM student; \u8868SQL_TEST\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868SQL_TEST\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE SQL_TEST; \u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868SQL_TEST\u3002 SELECT * FROM SQL_TEST; SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix \u00b6 \u9009\u62e9 Aliases->Fiber \uff0c\u70b9\u51fb \u4fee\u6539Fiber\u7684defaultDriver\u5c5e\u6027\u3002 \u5c06defaultDriver\u4fee\u6539\u4e3a phoenix \uff0c\u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 SQL\u64cd\u4f5c\u8868\u6570\u636e\u3002 \u5982\u679c\u4e0d\u5b58\u5728\u81ea\u5b9a\u4e49\u7684\u547d\u540d\u7a7a\u95f4\u7a7a\u95f4\u201cMY_NS\u201d\uff0c\u5219\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u521b\u5efa\u8868\u548c\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.SQL_TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.SQL_TEST VALUES(1,'John'); UPSERT INTO MY_NS.SQL_TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.SQL_TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.SQL_TEST VALUES(4,'Aurora'); SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 select * from MY_NS.SQL_TEST; SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO MY_NS.SQL_TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 DELETE FROM MY_NS.SQL_TEST WHERE ID=4; \u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 select * from MY_NS.SQL_TEST; \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb Objects->Fiber->MY_NS->TABLE->SQL_TEST->Content \u67e5\u770bSQL_TEST\u8868\u6570\u636e\u3002","title":"3.9.1 <--> 8.0"},{"location":"Development/Squirrel_3.9.1/#squirrelfusioninsight","text":"","title":"SQuirreL\u5bf9\u63a5FusionInsight"},{"location":"Development/Squirrel_3.9.1/#_1","text":"Squirrel 3.9.1 \u2194 FusionInsight HD 6.5 (Hive/Phoenix/SparkSQL) Squirrel 3.9.1 \u2194 FusionInsight MRS 8.0 (Hive/Phoenix/SparkSQL)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Squirrel_3.9.1/#_2","text":"SQL\u5f00\u53d1\u5de5\u5177\uff0c\u5982DbVisualizer\u3001DBeaver\u3001SQuirreL\u662f\u6570\u636e\u5e93\u5f00\u53d1\u7684\u5e38\u7528\u9009\u62e9\uff0c\u867d\u7136\u8fd9\u4e9b\u5de5\u5177\u5927\u591a\u4e0d\u63d0\u4f9b\u539f\u751fHive\u3001SparkSQL\u3001Phoenix\u7684\u652f\u6301\uff0c\u4f46\u662f\u901a\u8fc7\u5b83\u4eec\u652f\u6301\u7684\u81ea\u5b9a\u4e49JDBC\u7684\u80fd\u529b\uff0c\u6211\u4eec\u53ef\u4ee5\u4e0eFusionInsignt\u63d0\u4f9b\u7684Fiber\u7ec4\u4ef6\u7684JDBC\u63a5\u53e3\u8fdb\u884c\u5bf9\u63a5\uff0c\u5b9e\u73b0\u8fd9Hive\u3001SparkSQL\u3001Phoenix\u7ec4\u4ef6\u7684\u7edf\u4e00SQL\u67e5\u8be2\u3002 Fiber\u67b6\u6784\u56fe \u672c\u6587\u6863\u4e3b\u8981\u63cf\u8ff0\u5728Window\u64cd\u4f5c\u7cfb\u7edf\uff0cSQuirreL\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u7b80\u4ecb"},{"location":"Development/Squirrel_3.9.1/#_3","text":"\u767b\u5f55FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u4eba\u673a\u201d\u7528\u6237\uff0c\u4f8b\u5982\uff1adevelopuser\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u7ba1\u7406\u5458\u6307\u5357->\u7cfb\u7edf\u8bbe\u7f6e->\u6743\u9650\u8bbe\u7f6e->\u7528\u6237\u7ba1\u7406->\u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u7ed9developuser\u7528\u6237\u6388\u4e88\u6240\u6709\u8bbf\u95ee\u6743\u9650\uff0c\u5305\u542b\u4f46\u4e0d\u9650\u4e8eSpark2x\u3001Hive\u3001HBase\u3002 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u5e94\u7528\u5f00\u53d1\u6307\u5357->\u5b89\u5168\u6a21\u5f0f->\u5b89\u5168\u8ba4\u8bc1->\u914d\u7f6e\u5ba2\u6237\u7aef\u6587\u4ef6 \u7ae0\u8282\u3002 \u5df2\u5c06\u96c6\u7fa4\u7684\u8282\u70b9\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u52a0\u5165\u5230windows\u7684hosts\u6587\u4ef6\u4e2d C:\\Windows\\System32\\drivers\\etc\\hosts \u3002 \u5ba2\u6237\u7aef\u673a\u5668\u7684\u65f6\u95f4\u4e0eFusionInsight HD\u96c6\u7fa4\u7684\u65f6\u95f4\u8981\u4fdd\u6301\u4e00\u81f4\uff0c\u65f6\u95f4\u5dee\u5c0f\u4e8e5\u5206\u949f\u3002 Windows\u4e0a\u5df2\u7ecf\u5b89\u88c5\u597djdk1.8\u6216\u8005\u4ee5\u4e0a\u7248\u672c\uff0c\u5e76\u5b8c\u6210jdk\u73af\u5883\u53d8\u91cf\u914d\u7f6e\u3002 Hive\u6570\u636e\u5e93\u5df2\u5b58\u5728\u8868student\uff0c\u6570\u636e\u7c7b\u4f3c\u4e8e\uff1a \u793a\u4f8b\u5982\u4e0b\uff1a CREATE TABLE IF NOT EXISTS student(id INT, name STRING, class_id INT); INSERT INTO student VALUES (1,'Tom',1); INSERT INTO student VALUES (2,'Sandy',2); INSERT INTO student VALUES (3,'Benny',3); INSERT INTO student VALUES (4,'Tina',1);","title":"\u51c6\u5907\u5de5\u4f5c"},{"location":"Development/Squirrel_3.9.1/#fiber","text":"","title":"Fiber\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/Squirrel_3.9.1/#_4","text":"Fiber\u7684\u5b89\u5168\u8ba4\u8bc1\u6709kinit\u548ckeytab\u4e24\u79cd\u65b9\u5f0f\u3002\u5177\u4f53\u53c2\u6570\u914d\u7f6e\u8bf4\u660e\u53ef\u53c2\u8003FusionInsight HD\u4ea7\u54c1\u6587\u6863\u7684 \u4e1a\u52a1\u64cd\u4f5c\u6307\u5357->\u7edf\u4e00SQL(Fiber)->\u5ba2\u6237\u7aef\u914d\u7f6e \u7ae0\u8282\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Squirrel_3.9.1/#_5","text":"\u5df2\u5b8c\u6210\u51c6\u5907\u5de5\u4f5c\u3002 \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u7684Fiber\u3001Hive\u3001Spark2x\u3001HBase\u5ba2\u6237\u7aef\u6587\u4ef6\u5939\uff0c\u62f7\u8d1d\u81f3\u672c\u5730\u65b0\u5efa\u76ee\u5f55 C:\\ecotesting \u3002\u5047\u8bbeFusionInsight HD\u5ba2\u6237\u7aef\u5b89\u88c5\u4e8e /opt/hadoopclient \u76ee\u5f55\uff0c\u5219\uff1a \u5c06 /opt/hadoopclient/Fiber \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Hive \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/Spark2x \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 /opt/hadoopclient/HBase \u62f7\u8d1d\u81f3\u672c\u5730 C:\\ecotesting\\Fiber \u76ee\u5f55\u3002 \u5c06 C:\\ecotesting\\Fiber\\HBase\\hbase\\lib\\phoenix-core-4.13.1-HBase-1.3.jar \u62f7\u8d1d\u81f3 C:\\ecotesting\\Fiber\\lib \u3002 \u767b\u5f55FusionInsight Manager\u7684 \u7cfb\u7edf->\u7528\u6237->\u66f4\u591a\uff08developuser\uff09->\u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u8bc1 \uff0c\u4e0b\u8f7ddevelopuser\u5bf9\u5e94\u7684\u8ba4\u8bc1\u51ed\u8bc1\u3002\u5c06\u7528\u6237\u7684 krb5.conf \u548c user.keytab \u6587\u4ef6\u62f7\u8d1d\u5230 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Squirrel_3.9.1/#_6","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Squirrel_3.9.1/#kinit","text":"\u4ece http://web.mit.edu/kerberos/dist/#kfw-4.0 \u4e0b\u8f7d\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u67b6\u6784\u7684MIT Kerberos\u5e76\u5b89\u88c5\u3002 \u8bbe\u7f6eKerberos\u7684\u914d\u7f6e\u6587\u4ef6\u3002\u5c06\u7528\u6237\u7684krb5.conf\u6587\u4ef6\u91cd\u547d\u540d\u4e3a krb5.ini \u653e\u5728 C:\\ProgramData\\MIT\\Kerberos5 \u76ee\u5f55\u4e0b\u3002 \u8bbe\u7f6eKerberos\u7968\u636e\u7684\u7f13\u5b58\u6587\u4ef6 \u5728\u672c\u5730\u521b\u5efa\u5b58\u653e\u7968\u636e\u7684\u76ee\u5f55\uff0c\u4f8b\u5982 C:\\temp \u3002 \u8bbe\u7f6eWindows\u7684\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\uff0c\u53d8\u91cf\u540d\u4e3a KRB5CCNAME \uff0c\u53d8\u91cf\u503c\u4e3a C:\\temp\\krb5cache \u3002 \u5728Windows\u4e0a\u8fdb\u884c\u8ba4\u8bc1 \u6253\u5f00MIT Kerberos\uff0c\u5355\u51fb get Ticket \uff0c\u5728\u5f39\u51fa\u7684MIT Kerberos: Get Ticket\u7a97\u53e3\u4e2d\uff0c Pricipal \u8f93\u5165\u7528\u6237\u540d developuser@HADOOP.COM \uff0c Password \u8f93\u5165\u5bc6\u7801\uff0c\u5355\u51fb OK \u3002 \u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\u6240\u793a\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=false useTicketCache=true debug=true; }; - \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc>","title":"\u4f7f\u7528kinit\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/Squirrel_3.9.1/#keytab","text":"\u5728 C:\\ecotesting\\Fiber\\conf \u76ee\u5f55\u4e0b\u65b0\u5efa jaas.conf \u6587\u4ef6\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\conf\\fiber.xml \u3002 Hive\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>hive</identify> <describe>hive jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\config;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\Hive\\\\Beeline\\\\conf</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Spark2x\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>Spark2x</identify> <describe>Spark2x jdbc configuration</describe> <driverClass>org.apache.hive.jdbc.HiveDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\conf;C:\\\\ecotesting\\\\Fiber\\\\Spark2x\\\\spark\\\\jars</classPath> <jdbcUrl>jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=sparkthriftserver2x;saslQop=auth-conf;auth=KERBEROS;principal=spark2x/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=C:/ecotesting/Fiber/conf/user.keytab</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> Phoenix\u7684JDBC\u914d\u7f6e\u793a\u4f8b\uff1a <jdbc> <identify>phoenix</identify> <describe>phoenix jdbc configuration</describe> <driverClass>org.apache.phoenix.jdbc.PhoenixDriver</driverClass> <securityClass>com.huawei.fiber.DefaultAuthenticationCallback</securityClass> <classPath>C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\lib;C:\\\\ecotesting\\\\Fiber\\\\HBase\\\\hbase\\\\conf</classPath> <jdbcUrl>jdbc:phoenix:172.16.4.21,172.16.4.22,172.16.4.23:24002:/hbase</jdbcUrl> <properties> <property> <name>java.security.krb5.conf</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\krb5.conf</value> </property> <property> <name>java.security.auth.login.config</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\jaas.conf</value> </property> <property> <name>zookeeper.server.principal</name> <value>zookeeper/hadoop.hadoop.com</value> </property> <property> <name>zookeeper.kinit</name> <value>C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\\\\jre\\\\bin\\\\kinit.exe</value> </property> </properties> </jdbc> \u5bf9\u63a5Phoenix\u65f6\uff0c\u9700\u8981\u4fee\u6539\u914d\u7f6e\u6587\u4ef6 C:\\ecotesting\\Fiber\\HBase\\hbase\\conf\\hbase-site.xml \u65b0\u589e\u5c5e\u6027 hbase.myclient.keytab \u548c hbase.myclient.principal \u3002 <property> <name>hbase.myclient.keytab</name> <value>C:\\\\ecotesting\\\\Fiber\\\\conf\\\\user.keytab</value> </property> <property> <name>hbase.myclient.principal</name> <value>developuser</value> </property>","title":"\u4f7f\u7528keytab\u8ba4\u8bc1\u65b9\u5f0f\u914d\u7f6e"},{"location":"Development/Squirrel_3.9.1/#squirrelfiber","text":"","title":"SQuirreL\u5bf9\u63a5Fiber"},{"location":"Development/Squirrel_3.9.1/#_7","text":"SQuirreL SQL Client\u901a\u8fc7Fiber\u65b9\u5f0f\u5bf9\u63a5FusionInsight HD\u7684Hive\u3001Spark2x\u3001Phoenix\u7ec4\u4ef6\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Squirrel_3.9.1/#_8","text":"\u4ece http://www.squirrelsql.org/#installation \u4e0b\u8f7d\u4e0e\u672c\u5730\u7cfb\u7edf\u76f8\u5bf9\u5e94\u7684SQuirreL\u8f6f\u4ef6\uff08\u4f8b\u5982\uff1aInstall jar of SQuirreL 3.9.1 for Windows/Linux/others\uff09\u3002\u53cc\u51fb\u4e0b\u8f7d\u7684jar\u5305\u542f\u52a8\u5b89\u88c5\uff0c\u53ef\u6309\u7167\u9ed8\u8ba4\u9009\u9879\u5b8c\u6210\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Squirrel_3.9.1/#_9","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Squirrel_3.9.1/#squirrelfiberhive","text":"\u6253\u5f00SQuirreL SQL Client\uff0c\u9009\u62e9 Drivers \uff0c\u70b9\u51fb + \u3002 \u586b\u5199Driver\u4fe1\u606f\uff0c\u70b9\u51fb OK \u3002 \u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\uff1a Name: Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Example URL\uff1ajdbc:fiber://fiberconfig=C:\\\\ecotesting\\\\Fiber\\\\conf\\\\fiber.xml;defaultDriver=hive Extra Class Path: \u70b9\u51fbAdd\u6309\u94ae\u5c06C:\\ecotesting\\Fiber\\lib\u76ee\u5f55\u4e0b\u6240\u6709\u7684jar\u5305\u52a0\u5165 Class Name\uff1acom.huawei.fiber.FiberDriver\uff08\u6dfb\u52a0jar\u5305\u540e\uff0c\u70b9\u51fbList Drivers\u6309\u94ae\u7136\u540e\u4ece\u4e0b\u62c9\u6846\u4e2d\u9009\u62e9\uff09 \u6dfb\u52a0\u5b8c\u6210\u7684Driver Fiber\u3002 \u70b9\u51fb Aliases \uff0c\u70b9\u51fb + \u3002\u5728\u5f39\u51fa\u6846\u4e2d\u586b\u5199\u4fe1\u606f\u793a\u4f8b\u5982\u4e0b\u6240\u793a\uff1a Name: Fiber\uff08\u81ea\u5b9a\u4e49\uff09 Driver\uff1a\u9009\u62e9Fiber User Name: developuser\uff08\u53ef\u4e0d\u586b\u5199\uff09 Password\uff1a\u5bc6\u7801\uff08\u53ef\u4e0d\u586b\u5199\uff09 \u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u70b9\u51fb Connect \u3002 \u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Hive\u6210\u529f\u3002 \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student \uff0c\u5728 Content \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 SELECT * FROM student; \u5411Hive\u8868SQL_TEST\u63d2\u5165\u6570\u636e \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u521b\u5efa\u5b58\u50a8\u683c\u5f0f\u4e3atextfile\uff0c\u5206\u9694\u7b26\u4e3a\u201c,\u201d\u7684\u8868 SQL_TEST \u3002\u8868\u7684\u5b58\u50a8\u683c\u5f0f\u5fc5\u987b\u4e3a textfile \uff0c\u5426\u5219\u4e0d\u80fd\u4f7f\u7528LOAD DATA\u65b9\u5f0f\u5411\u8868\u63d2\u5165\u6570\u636e\u3002 CREATE TABLE IF NOT EXISTS SQL_TEST (id string, name STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS textfile; \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 1,Abbey 2,Andy \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868SQL_TEST\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE SQL_TEST; \u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868SQL_TEST\u3002 SELECT * FROM SQL_TEST;","title":"SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Hive"},{"location":"Development/Squirrel_3.9.1/#squirrelfiberspark2x","text":"\u9009\u62e9 Aliases->Fiber \uff0c\u70b9\u51fb \u4fee\u6539Fiber\u7684defaultDriver\u5c5e\u6027\u3002 \u5c06defaultDriver\u4fee\u6539\u4e3a spark2x \uff0c\u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \u3002 \u5982\u679c\u8fd4\u56de\u201cSession startup time hint\u201d\uff0c\u5219\u70b9\u51fb Close \u3002 \u5982\u679c\u6ca1\u6709\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \uff0c\u70b9\u51fb \u6309\u94ae\u201cRefresh Object Tree and Database Meta Data Cache\u201d\uff0c\u5219\u4f1a\u8fd4\u56de\u9ed8\u8ba4\u6570\u636e\u5e93 default \u3002\u901a\u8fc7Fiber\u8fde\u63a5Spark2x\u6210\u529f\u3002 \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb default->TABLE->student \uff0c\u5728 Content \u9875\u9762\u53ef\u67e5\u770bstudent\u8868\u6570\u636e\u3002 SQL\u67e5\u8be2Hive\u8868\u7684\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 SELECT * FROM student; \u8868SQL_TEST\u63d2\u5165\u6570\u636e \u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u7f16\u8f91\u6570\u636e\u6587\u4ef6 data_input.txt \uff0c\u5e76\u6267\u884c hdfs dfs -put data_input.txt /tmp \u4e0a\u4f20\u81f3\u96c6\u7fa4\u7684HDFS\u6587\u4ef6\u7cfb\u7edf\u7684/tmp/\u76ee\u5f55\u4e0b\u3002 data_input.txt\u5185\u5bb9\u5982\u4e0b\uff1a 3,Benny 4,Miki \u8f93\u5165\u4ee5\u4e0b\u811a\u672c\u5c06data_input.txt\u6570\u636e\u5bfc\u5165\u8868SQL_TEST\u3002 LOAD DATA INPATH '/tmp/data_input.txt' OVERWRITE INTO TABLE SQL_TEST; \u8f93\u5165\u811a\u672c\u67e5\u8be2\u8868SQL_TEST\u3002 SELECT * FROM SQL_TEST;","title":"SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Spark2x"},{"location":"Development/Squirrel_3.9.1/#squirrelfiberphoenix","text":"\u9009\u62e9 Aliases->Fiber \uff0c\u70b9\u51fb \u4fee\u6539Fiber\u7684defaultDriver\u5c5e\u6027\u3002 \u5c06defaultDriver\u4fee\u6539\u4e3a phoenix \uff0c\u4f9d\u6b21\u70b9\u51fb Test \uff0c Connect \uff0c\u8fd4\u56de\u201cConnection successful\u201d\u540e\uff0c\u70b9\u51fb \u786e\u5b9a \u3002 \u70b9\u51fb OK \u3002 \u53cc\u51fb Fiber \uff0c\u70b9\u51fb Connect \u3002 \u8fde\u63a5\u6210\u529f\u540e\uff0c\u8fd4\u56de\u6570\u636e\u5e93 SYSTEM \u5219\u8868\u793a\u901a\u8fc7Fiber\u8fde\u63a5Phoenix\u6210\u529f\u3002 SQL\u64cd\u4f5c\u8868\u6570\u636e\u3002 \u5982\u679c\u4e0d\u5b58\u5728\u81ea\u5b9a\u4e49\u7684\u547d\u540d\u7a7a\u95f4\u7a7a\u95f4\u201cMY_NS\u201d\uff0c\u5219\u767b\u5f55FusionInsight\u96c6\u7fa4\u5ba2\u6237\u7aef\uff0c\u6267\u884chbase shell\u7684\u201ccreate_namespace\u201d\u547d\u4ee4\u521b\u5efa\u65b0\u7684\u547d\u540d\u7a7a\u95f4 MY_NS \u3002 create_namespace 'MY_NS' \u521b\u5efa\u8868\u548c\u6570\u636e\u3002\u70b9\u51fb SQL \uff0c\u8f93\u5165\u67e5\u8be2\u8bed\u53e5\u540e\u6267\u884c\u3002 CREATE TABLE IF NOT EXISTS MY_NS.SQL_TEST (id integer not null primary key, name varchar); UPSERT INTO MY_NS.SQL_TEST VALUES(1,'John'); UPSERT INTO MY_NS.SQL_TEST VALUES(2,'Tom'); UPSERT INTO MY_NS.SQL_TEST VALUES(3,'Manson'); UPSERT INTO MY_NS.SQL_TEST VALUES(4,'Aurora'); SQL\u67e5\u8be2\u8868\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 select * from MY_NS.SQL_TEST; SQL\u4fee\u6539\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 UPSERT INTO MY_NS.SQL_TEST VALUES(1,'Jessy'); SQL\u5220\u9664\u8868\u7684\u6570\u636e\u3002\u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002 DELETE FROM MY_NS.SQL_TEST WHERE ID=4; \u7f16\u8f91SQL\u5982\u4e0b\uff0c\u70b9\u51fb \u6267\u884c \u6309\u94ae\u3002\u201cID=1\u201d\u7684\u201cNAME\u201d\u5df2\u88ab\u4fee\u6539\u4e3a\u201cJessy\u201d\uff0c\u201cID=4\u201d\u7684\u8bb0\u5f55\u5df2\u88ab\u5220\u9664\u3002 select * from MY_NS.SQL_TEST; \u76f4\u63a5\u67e5\u770b\u8868\u6570\u636e\u3002\u70b9\u51fb Objects->Fiber->MY_NS->TABLE->SQL_TEST->Content \u67e5\u770bSQL_TEST\u8868\u6570\u636e\u3002","title":"SQuirreL\u901a\u8fc7Fiber\u5bf9\u63a5Phoenix"},{"location":"Development/Zeppelin_0.7.2/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.7.2 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive/Spark/SparkR) \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.7.2 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06\u8f6f\u4ef6\u5305zeppelin-0.7.2-bin-all.tgz\u4e0a\u4f20\u81f3/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.2-bin-all\u76ee\u5f55\u3002 tar -zxvf zeppelin-0.7.2-bin-all.tgz \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.2-bin-all export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf cd /opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/jdk1.7.0_51/ \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\uff0coverwrite\u9009\u62e9n \u5728/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u65b0\u5efa\u76ee\u5f55zeppelin_hbase_jar mkdir /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/zeppelin_hbase_jar \u5c06/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u4e0eFusionInsight\u51b2\u7a81\u768438\u4e2ajar\u5305\u79fb\u52a8\u5230zeppelin_hbase_jar\u76ee\u5f55\u4e2d commons-codec-1.5.jar commons-collections-3.2.1.jar commons-configuration-1.9.jar commons-lang-2.5.jar commons-logging-1.1.1.jar guava-15.0.jar hadoop-annotations-2.6.0.jar hadoop-auth-2.5.1.jar hadoop-client-2.5.1.jar hadoop-common-2.5.1.jar hadoop-hdfs-2.5.1.jar hadoop-mapreduce-client-app-2.5.1.jar hadoop-mapreduce-client-common-2.5.1.jar hadoop-mapreduce-client-core-2.5.1.jar hadoop-mapreduce-client-jobclient-2.5.1.jar hadoop-mapreduce-client-shuffle-2.5.1.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-client-2.5.1.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-common-2.5.1.jar hbase-annotations-1.0.0.jar hbase-client-1.0.0.jar hbase-common-1.0.0.jar hbase-common-1.0.0-tests.jar hbase-hadoop2-compat-1.0.0.jar hbase-hadoop-compat-1.0.0.jar hbase-prefix-tree-1.0.0.jar hbase-protocol-1.0.0.jar hbase-server-1.0.0.jar httpclient-4.5.1.jar httpcore-4.4.1.jar jettison-1.1.jar netty-3.6.2.Final.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar xmlenc-0.52.jar zookeeper-3.4.6.jar \u6700\u7ec8/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u6709152\u4e2ajar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b\u7684\u539f\u6709\u7684\u76f8\u5173\u7684jar\u5305\u5220\u9664 hadoop-auth-2.6.0.jar hadoop-common-2.6.0.jar scala-compiler-2.11.7.jar scala-library-2.11.7.jar scala-parser-combinators_2.11-1.0.4.jar scala-reflect-2.11.7.jar scala-xml_2.11-1.0.2.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684\u4ee5\u4e0bjar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b hadoop-auth-2.7.2.jar hadoop-common-2.7.2.jar scala-compiler-2.10.4.jar scala-library-2.10.4.jar scala-reflect-2.10.4.jar \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u7684jackson\u7684\u76f8\u5173jar\u5305\u5220\u9664 jackson-annotations-2.5.0.jar jackson-core-2.5.3.jar jackson-core-asl-1.9.13.jar jackson-databind-2.5.3.jar jackson-mapper-asl-1.9.13.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684jackson\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b jackson-annotations-2.4.0.jar jackson-core-2.4.4.jar jackson-core-asl-1.9.13.jar jackson-databind-2.4.4.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-module-scala_2.10-2.4.4.jar jackson-xc-1.9.13.jar \u5c06\u6b65\u9aa41\u548c\u6b65\u9aa42\u6240\u6709\u4ecespark\u5ba2\u6237\u7aef\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.2-bin-all/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.2/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.2-bin-all/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"0.7.2 <--> C60"},{"location":"Development/Zeppelin_0.7.2/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.7.2/#_1","text":"Zeppelin 0.7.2 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive/Spark/SparkR)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#zeppelin","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.7.2/#_2","text":"\u5b89\u88c5Zeppelin0.7.2","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_4","text":"\u5c06\u8f6f\u4ef6\u5305zeppelin-0.7.2-bin-all.tgz\u4e0a\u4f20\u81f3/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.2-bin-all\u76ee\u5f55\u3002 tar -zxvf zeppelin-0.7.2-bin-all.tgz \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.2-bin-all export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf cd /opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/jdk1.7.0_51/ \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.2-bin-all/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.7.2/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.2-bin-all/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.7.2/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_10","text":"\u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\uff0coverwrite\u9009\u62e9n \u5728/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u65b0\u5efa\u76ee\u5f55zeppelin_hbase_jar mkdir /opt/zeppelin-0.7.2-bin-all/interpreter/hbase/zeppelin_hbase_jar \u5c06/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u4e0b\u4e0eFusionInsight\u51b2\u7a81\u768438\u4e2ajar\u5305\u79fb\u52a8\u5230zeppelin_hbase_jar\u76ee\u5f55\u4e2d commons-codec-1.5.jar commons-collections-3.2.1.jar commons-configuration-1.9.jar commons-lang-2.5.jar commons-logging-1.1.1.jar guava-15.0.jar hadoop-annotations-2.6.0.jar hadoop-auth-2.5.1.jar hadoop-client-2.5.1.jar hadoop-common-2.5.1.jar hadoop-hdfs-2.5.1.jar hadoop-mapreduce-client-app-2.5.1.jar hadoop-mapreduce-client-common-2.5.1.jar hadoop-mapreduce-client-core-2.5.1.jar hadoop-mapreduce-client-jobclient-2.5.1.jar hadoop-mapreduce-client-shuffle-2.5.1.jar hadoop-yarn-api-2.6.0.jar hadoop-yarn-client-2.5.1.jar hadoop-yarn-common-2.6.0.jar hadoop-yarn-server-common-2.5.1.jar hbase-annotations-1.0.0.jar hbase-client-1.0.0.jar hbase-common-1.0.0.jar hbase-common-1.0.0-tests.jar hbase-hadoop2-compat-1.0.0.jar hbase-hadoop-compat-1.0.0.jar hbase-prefix-tree-1.0.0.jar hbase-protocol-1.0.0.jar hbase-server-1.0.0.jar httpclient-4.5.1.jar httpcore-4.4.1.jar jettison-1.1.jar netty-3.6.2.Final.jar slf4j-api-1.7.10.jar slf4j-log4j12-1.7.10.jar xmlenc-0.52.jar zookeeper-3.4.6.jar \u6700\u7ec8/opt/zeppelin-0.7.2-bin-all/interpreter/hbase/\u6709152\u4e2ajar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.2-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.2-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.2-bin-all/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.7.2/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_12","text":"\u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_13","text":"\u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b\u7684\u539f\u6709\u7684\u76f8\u5173\u7684jar\u5305\u5220\u9664 hadoop-auth-2.6.0.jar hadoop-common-2.6.0.jar scala-compiler-2.11.7.jar scala-library-2.11.7.jar scala-parser-combinators_2.11-1.0.4.jar scala-reflect-2.11.7.jar scala-xml_2.11-1.0.2.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684\u4ee5\u4e0bjar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u76ee\u5f55\u4e0b hadoop-auth-2.7.2.jar hadoop-common-2.7.2.jar scala-compiler-2.10.4.jar scala-library-2.10.4.jar scala-reflect-2.10.4.jar \u5c06 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u7684jackson\u7684\u76f8\u5173jar\u5305\u5220\u9664 jackson-annotations-2.5.0.jar jackson-core-2.5.3.jar jackson-core-asl-1.9.13.jar jackson-databind-2.5.3.jar jackson-mapper-asl-1.9.13.jar \u5c06 /opt/hadoopclient/Spark/adapter/dev_lib/ \u4e0b\u7684jackson\u76f8\u5173\u7684jar\u5305\u62f7\u8d1d\u5230 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b jackson-annotations-2.4.0.jar jackson-core-2.4.4.jar jackson-core-asl-1.9.13.jar jackson-databind-2.4.4.jar jackson-jaxrs-1.9.13.jar jackson-mapper-asl-1.9.13.jar jackson-module-scala_2.10-2.4.4.jar jackson-xc-1.9.13.jar \u5c06\u6b65\u9aa41\u548c\u6b65\u9aa42\u6240\u6709\u4ecespark\u5ba2\u6237\u7aef\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.2-bin-all/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.2-bin-all/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.7.2/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.2/#_15","text":"\u5b8c\u6210Zeppelin0.7.2\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C60U20\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.2/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.2/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.2-bin-all/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.2/#faq","text":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.2-bin-all/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"FAQ"},{"location":"Development/Zeppelin_0.7.3/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C70SPC100 (HBase/Hive/Spark/SparkR) Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Spark/SparkR) \u7f16\u8bd1Zeppelin \u00b6 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u5b89\u88c5bower npm install -g bower \u914d\u7f6ebower\u5141\u8bb8root\u7528\u6237\u6267\u884c echo '{ \"allow_root\": true }' > /root/.bowerrc \u6267\u884c bower -v \u83b7\u53d6Zeppelin0.7.3\u7684\u7248\u672c git clone https://github.com/apache/zeppelin.git cd zeppelin git checkout v0.7.3 \u4fee\u6539scala\u7248\u672c\uff0c\u9002\u914dFusionInsight_HD_V100R002C70SPC100\u7684Hadoop\u7248\u672c \u5728zeppelin\u4ee3\u7801\u6839\u76ee\u5f55\u6267\u884c vi ./dev/change_scala_version.sh \uff0c\u4fee\u6539\u4e0b\u56fe\u7684SCALA_LIB_VERSION\u4e3a2.11.8 \u6267\u884c\u547d\u4ee4\u5b8c\u6210scala\u7248\u672c\u7684\u4fee\u6539 ./dev/change_scala_version.sh 2.11 \u6267\u884c vi pom.xml \u6587\u4ef6\u7684\u4fee\u6539 \u4e3a0.9.3 \u6267\u884c vi hbase/pom.xml \u4fee\u6539hbase\u7248\u672c\u548chadoop\u7248\u672c \u7f16\u8bd1Zeppelin mvn clean package -Pbuild-distr -Pspark-2.1 -Dspark.version=2.1.0 -Dhadoop.version=2.7.2 -Phadoop-2.7 -Pscala-2.11 -Psparkr -DskipTests \u7f16\u8bd1\u5b8c\u6210\u540e\u5728 zeppelin-distribution/target \u76ee\u5f55\u4e0b\u751f\u6210 zeppelin-0.7.3.tar.gz \u6587\u4ef6 \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.7.3 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06\u7f16\u8bd1\u597d\u7684zeppelin-0.7.3.tar.gz\u4e0a\u4f20\u653e\u5230/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.3\u76ee\u5f55\u3002 cp zeppelin-distribution/target/zeppelin-0.7.3.tar.gz /opt cd /opt tar -zxvf zeppelin-0.7.3.tar.gz \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.3 export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf cd /opt/zeppelin-0.7.3/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.3/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /opt/zeppelin-0.7.3/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /opt/zeppelin-0.7.3/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.3/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684sparkSQL\u8bed\u53e5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.3/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b FAQ \u00b6 FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.3/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"0.7.3 <--> C80"},{"location":"Development/Zeppelin_0.7.3/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.7.3/#_1","text":"Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C70SPC100 (HBase/Hive/Spark/SparkR) Zeppelin 0.7.3 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Spark/SparkR)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#zeppelin","text":"\u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u5b89\u88c5bower npm install -g bower \u914d\u7f6ebower\u5141\u8bb8root\u7528\u6237\u6267\u884c echo '{ \"allow_root\": true }' > /root/.bowerrc \u6267\u884c bower -v \u83b7\u53d6Zeppelin0.7.3\u7684\u7248\u672c git clone https://github.com/apache/zeppelin.git cd zeppelin git checkout v0.7.3 \u4fee\u6539scala\u7248\u672c\uff0c\u9002\u914dFusionInsight_HD_V100R002C70SPC100\u7684Hadoop\u7248\u672c \u5728zeppelin\u4ee3\u7801\u6839\u76ee\u5f55\u6267\u884c vi ./dev/change_scala_version.sh \uff0c\u4fee\u6539\u4e0b\u56fe\u7684SCALA_LIB_VERSION\u4e3a2.11.8 \u6267\u884c\u547d\u4ee4\u5b8c\u6210scala\u7248\u672c\u7684\u4fee\u6539 ./dev/change_scala_version.sh 2.11 \u6267\u884c vi pom.xml \u6587\u4ef6\u7684\u4fee\u6539 \u4e3a0.9.3 \u6267\u884c vi hbase/pom.xml \u4fee\u6539hbase\u7248\u672c\u548chadoop\u7248\u672c \u7f16\u8bd1Zeppelin mvn clean package -Pbuild-distr -Pspark-2.1 -Dspark.version=2.1.0 -Dhadoop.version=2.7.2 -Phadoop-2.7 -Pscala-2.11 -Psparkr -DskipTests \u7f16\u8bd1\u5b8c\u6210\u540e\u5728 zeppelin-distribution/target \u76ee\u5f55\u4e0b\u751f\u6210 zeppelin-0.7.3.tar.gz \u6587\u4ef6","title":"\u7f16\u8bd1Zeppelin"},{"location":"Development/Zeppelin_0.7.3/#zeppelin_1","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.7.3/#_2","text":"\u5b89\u88c5Zeppelin0.7.3","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_4","text":"\u5c06\u7f16\u8bd1\u597d\u7684zeppelin-0.7.3.tar.gz\u4e0a\u4f20\u653e\u5230/opt\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u751f\u6210zeppelin-0.7.3\u76ee\u5f55\u3002 cp zeppelin-distribution/target/zeppelin-0.7.3.tar.gz /opt cd /opt tar -zxvf zeppelin-0.7.3.tar.gz \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/opt/zeppelin-0.7.3 export PATH=$ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf cd /opt/zeppelin-0.7.3/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/ cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237test\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237test\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3/opt/\u76ee\u5f55\u4e0b\u3002 \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse\u3002 \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237test\uff0c\u5bc6\u7801Huawei@123 \u91cd\u542fzeppelin\u3002 cd /opt/zeppelin-0.7.3/ ./bin/zeppelin-daemon.sh restart \u4f7f\u7528test\u7528\u6237\u767b\u9646Zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.7.3/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c/opt/zeppelin-0.7.3/ interpreter/jdbc/\u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 501:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=test;user.keytab=/opt/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; Select * from workers_info; \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.7.3/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_10","text":"\u5c06 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /opt/zeppelin-0.7.3/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /opt/zeppelin-0.7.3/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /opt/zeppelin-0.7.3/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /opt/zeppelin-0.7.3/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/zeppelin-0.7.3/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237test\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728/etc/\u4e0b \u4f7f\u7528 vi /opt/zeppelin-0.7.3/conf/ \u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"test\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684test\u7528\u6237\uff0c\u5c06test\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7 \u6267\u884c \u6309\u94ae %hbase create 'test2', 'cf' put 'test2', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test2\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.7.3/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_12","text":"\u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_13","text":"\u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /opt/zeppelin-0.7.3/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 Master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/user.keytab test cd /opt/zeppelin-0.7.3/bin ./zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684sparkSQL\u8bed\u53e5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801zeppelin Tutorial -> Basic Features(Spark) \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.7.3/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.7.3/#_15","text":"\u5b8c\u6210Zeppelin0.7.3\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD V100R002C70SPC100\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.7.3/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u5982\u679c\u5b89\u88c5R\u7684\u8282\u70b9\u65e0\u6cd5\u8bbf\u95ee\u4e92\u8054\u7f51\uff0c\u53c2\u8003FAQ\u8fdb\u884cR\u7684\u5b89\u88c5 \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit test sparkR \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin cd /opt/zeppelin-0.7.3/bin/ ./zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.7.3/#faq","text":"FusionInsight\u96c6\u7fa4\u4e0d\u5141\u8bb8\u8bbf\u95ee\u7f51\u7edc\uff0c\u5982\u4f55\u5b89\u88c5R \u5728\u96c6\u7fa4\u5916\u540c\u7248\u672c\u7684Redhat\u7248\u672c\u4e0b\u6309\u7167\u672c\u6587\u4e2dyum\u6e90\u7684\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5R\u7684\u64cd\u4f5c\uff0c\u6700\u540e\u4e00\u6b65\u4e0d\u8981\u6267\u884c yum install R \u6267\u884c yum install yum-utils \u5b89\u88c5yumdownloader \u6267\u884c yumdownloader R --resolve --destdir=/tmp/packages \u628a\u6240\u6709\u7684rpm\u5b89\u88c5\u5305\u4e0b\u8f7d\u5230 /tmp/packages \u4e2d \u5c06 /tmp/packages \u4e2d\u7684\u6240\u6709rpm\u5305\u590d\u5236\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d \u5207\u6362\u5230\u96c6\u7fa4\u6bcf\u4e2a\u8282\u70b9\u7684 /tmp/packages \u4e2d\uff0c\u6267\u884c yum localinstall *.rpm \u5b8c\u6210\u5b89\u88c5 \u8fde\u63a5hbase\u51fa\u73b0AuthFialed for /hwbackup/hbase \u539f\u56e0\uff1azeppelin\u7684\u539f\u7406hbase\u7684jar\u5305\u4e0e\u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u62f7\u8d1d\u8fc7\u6765\u7684jar\u51b2\u7a81\u3002 \u89e3\u51b3\uff1a\u5c06zeppelin\u4e2d\u539f\u6709\u7684\u91cd\u540djar\u5305\u79fb\u8d70\u6216\u5220\u9664\uff0c\u5168\u90e8\u7528FusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684\u76f8\u5173jar\u5305\u3002 Zeppelin\u8fde\u63a5spark\u662f\u62a5\u5982\u4e0bNoSuchMethodError \u539f\u56e0\uff1ajar\u5305\u51b2\u7a81 \u89e3\u51b3\uff1a\u5220\u9664 /opt/zeppelin-0.7.3/lib/ \u4e0b\u539f\u6709jar\u5305scala-reflect-2.11.7.jar\uff0c\u66ff\u6362\u4e3aFusionInsight\u5ba2\u6237\u7aef\u4e0b\u7684jar\u5305\uff0c\u91cd\u542fzeppelin Zeppelin\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801\u65f6\u62a5GC overhead limit exceeded \u539f\u56e0\uff1a\u5185\u5b58\u4e0d\u591f \u89e3\u51b3\uff1a\u5b89\u88c5Zeppelin\u7684\u8282\u70b9\u7684\u5185\u5b58\u9700\u898116G\u4ee5\u4e0a \u6267\u884czeppelin\u7684\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial/Matplotlib (Python PySpark)\u62a5\u5982\u4e0b\u9519\u8bef \u539f\u56e0\uff1apython\u7248\u672c\u95ee\u9898 \u89e3\u51b3\uff1a\u5b89\u88c5Anaconda2-4.4","title":"FAQ"},{"location":"Development/Zeppelin_0.8.0/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.8.0 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Spark/SparkR/ELK) \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.8.0 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5Zeppelin 0.8.0,\u5728\u7f51\u5740 https://zeppelin.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf zeppelin-0.8.0-bin-all.tgz \u5b89\u88c5\u751f\u6210zeppelin-0.8.0-bin-all\u76ee\u5f55\u3002 \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME = /usr/zeppelin/zeppelin-0.8.0-bin-all export PATH = $ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cd /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 - \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all \u76ee\u5f55\u4e0b \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237developuser\uff0c\u5bc6\u7801Huawei@123\uff0c\u6743\u9650admin \u91cd\u542fzeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u4f7f\u7528\u8d26\u6237developuser\u767b\u9646zeppelin Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 502:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser/ \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/developuser/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; %jdbc select * from t2 \u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae %hbase create 'test4', 'cf' put 'test4', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test4\u548c\u6570\u636e Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b libfb303-0.9.3.jar \u548c libthrift-0.9.3.jar \u4e24\u4e2ajar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark/dep \u8def\u5f84\u4e0b \u786e\u4fdd /usr/zeppelin/zeppelin-0.8.0-bin-all/lib/interpreter \u8def\u5f84\u4e0b\u6709\u4e14\u4ec5\u6709 libthrift-0.9.3.jar \u8fd9\u4e2a\u7248\u672c\u7684jar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u5e76\u4e14\u68c0\u67e5zeppelin.spark.useHiveContext\u9879\uff0c\u4f7f\u5176\u503c\u4e3afalse\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801\uff0c\u53c2\u8003\u7f51\u5740 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH sh Anaconda2-4.4.0-Linux-x86_64.sh \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u5982\u679c\u9047\u5230\u6e90yum-plugin-fastestmirror\u65e0\u6cd5\u4e0b\u8f7d\u65f6\uff0c\u53ef\u5728\u7f51\u5740 https://rpmfind.net/linux/rpm2html/search.php?query=yum-plugin-fastestmirror \u4e0b\u9009\u62e9\u76f8\u5e94\u7684\u7248\u672c\u4ee3\u66ff\u4e0b\u8f7d\u5b89\u88c5 \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b Zeppelin\u8fde\u63a5Apache Livy \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u8fde\u63a5Livy \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8Livy\u670d\u52a1 cd /usr/livy/livy-0.5.0-incubating-bin bin/livy-server start \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9livy\uff0c\u70b9\u51fb edit \u7f16\u8f91zeppelin.livy.url\u7684\u503c\u4e3a http://172.21.3.43:8998 \uff08\u53ef\u4ee5\u4e0d\u66f4\u6539\uff09\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982livy_connection_test \u5728Zeppelin\u4e2d\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801 val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cPySpark\u6837\u4f8b\u4ee3\u7801 %livy.pyspark import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cSparkR\u6837\u4f8b\u4ee3\u7801 %livy.sparkr hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") Zeppelin\u8fde\u63a5FusionInsight Elk \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u8fde\u63a5FusionInsight Elk \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bElk\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65\uff1a \u540e\u53f0\u767b\u5f55FusionInsight Elk, \u521b\u5efa\u767b\u5f55\u7528\u6237\uff0c \u5206\u914d\u7528\u6237\u6743\u9650\uff0c \u521b\u5efa\u6570\u636e\u5e93\uff0c \u6d4b\u8bd5\u6570\u636e\u8868 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u4f7f\u7528 gsql -d postgres -p 25108 \u8fde\u63a5\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237**joe**, \u5bc6\u7801\u4e3a**Bigdata@123** CREATE USER joe WITH PASSWORD \"Bigdata@123\"; \u7528\u4e0b\u9762\u8fd9\u4e2a\u547d\u4ee4\u5c06\u7cfb\u7edf\u6743\u9650\u6388\u6743\u7ed9\u7528\u6237\u6216\u8005\u89d2\u8272 GRANT ALL PRIVILEGES TO joe; \u521b\u5efaHDFS\u8868\u7a7a\u95f4\u3002 CREATE TABLESPACE hdfs_tablespace LOCATION '/srv/BigData/hadoop/hdfs_tablespace' WITH (filesystem = 'HDFS', cfgpath = '/opt/huawei/Bigdata/mppdb/conf', storepath = '/user/elk/tablespace/ hdfs_tablespace'); \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE TABLESPACE \u521b\u5efa\u6570\u636e\u5e93\u3002 CREATE DATABASE db_tpcds; \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE DATABASE \u521b\u5efa\u5b8cdb_tpcds\u6570\u636e\u5e93\u540e\uff0c\u5c31\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u6cd5\u9000\u51fapostgres\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u65b0\u7528\u6237\u8fde\u63a5\u5230\u6b64\u6570\u636e\u5e93\u6267\u884c\u63a5\u4e0b\u6765\u7684\u521b\u5efa\u8868\u7b49\u64cd\u4f5c\u3002\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u7ee7\u7eed\u5728\u9ed8\u8ba4\u7684postgres\u6570\u636e\u5e93 \u4e0b\u505a\u540e\u7eed\u7684\u4f53\u9a8c\u3002 \\q gsql -d db_tpcds -p 25108 -U joe -W Bigdata@123 \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\u3002 CREATE TABLE hdfs_001(id int,name varchar2(20) ) WITH (orientation=orc,version=0.12,compression=no) TABLESPACE hdfs_tablespace; \u4f7f\u7528INSERT\u547d\u4ee4\u63d2\u5165\u6570\u636e\u3002 \u63d2\u5165\u4e00\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'); \u63d2\u5165\u591a\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'),(2, 'Marketing'), (2, 'Purchasing'); \u68c0\u67e5\u7ed3\u679c Select * from hdfs_001 \u7b2c\u4e8c\u6b65\uff1a \u914d\u7f6e\u96c6\u7fa4Elk\u8fdc\u7a0b\u8fde\u63a5 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u914d\u7f6e\u5ba2\u6237\u7aef\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u4ee5 joe \u7528\u6237\u8fde\u63a5\u5230\u672c\u673a\uff0c\u6b64\u5904\u8fdc\u7a0b\u8fde\u63a5\u7981\u6b62\u4f7f\u7528 omm \u7528\u6237\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u793a\u4f8b\u4e2d\u914d\u7f6e\u5141\u8bb8IP\u5730\u5740\u4e3a 172.16.52.190 \u7684\u5ba2\u6237\u7aef\u8bbf\u95ee\u96c6\u7fa4\u672c\u673a\u3002 gs_guc set -Z coordinator -N all -I all -h \"host all joe 172.16.52.190/32 sha256\" \u4f7f\u7528\u201cjoe\u201d\u7528\u6237\u524d\uff0c\u9700\u5148\u672c\u5730\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u5e76\u5728\u6570\u636e\u5e93\u4e2d\u4f7f\u7528\u5982\u4e0b\u8bed\u53e5\u5efa\u7acb\u201cjoe\u201d\u7528\u6237\u3002 -Z coordinator\u8868\u793a\u5b9e\u4f8b\u7c7b\u578b\u4e3acoordinator\u3002 -N all\u8868\u793a\u96c6\u7fa4\u7684\u6240\u6709\u4e3b\u673a\u3002 -I all\u8868\u793a\u4e3b\u673a\u7684\u6240\u6709\u5b9e\u4f8b\u3002 -h \u8868\u793a\u6307\u5b9a\u9700\u8981\u5728\u201cpg_hba.conf\u201d\u589e\u52a0\u7684\u8bed\u53e5\u3002 all\u8868\u793a\u5141\u8bb8\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u4efb\u610f\u7684\u6570\u636e\u5e93\u3002 joe \u8868\u793a\u8fde\u63a5\u6570\u636e\u5e93\u7684\u7528\u6237\u3002 172.16.52.190/32\u8868\u793a\u53ea\u5141\u8bb8IP\u5730\u5740\u4e3a10.10.0.30\u7684\u4e3b\u673a\u8fde\u63a5\u3002\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u6839\u636e\u7528\u6237\u7684\u7f51\u7edc \u8fdb\u884c\u914d\u7f6e\u4fee\u6539\u3002 sha256\u8868\u793a\u8fde\u63a5\u65f6jack\u7528\u6237\u7684\u5bc6\u7801\u4f7f\u7528sha256\u7b97\u6cd5\u52a0\u5bc6\u3002 \u914d\u7f6elisten_addresses \u4f7f\u7528\u547d\u4ee4 gs_guc set -N all -I all -Z coordinator -c \"listen_addresses = '*'\" \u6267\u884c\u5982\u4e0b\u547d\u4ee4\u91cd\u542f\u96c6\u7fa4\u3002 gs_om -t stop && gs_om -t start \u7b2c\u4e09\u6b65\uff1a \u914d\u7f6ezeppelin JDBC \u63a5\u53e3\u5bf9\u63a5 FusionInsight elk \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230Elk\u7684jdbc\u9a71\u52a8\uff1a \u9a71\u52a8\u7a0b\u5e8f\uff1aGauss200-OLAP-V100R007C10-REDHAT-64bit-Jdbc.tar.gz \u9a71\u52a8\u7c7b\uff1aorg.postgresql.Driver \u5177\u4f53\u4f4d\u7f6e\u4e3a\uff1aC:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\Elk \u9a71\u52a8jar\u5305\u7684\u540d\u5b57\u53eb gsjdbc4.jar \u5c06\u627e\u5230\u7684\u8fd9\u4e2a gsjdbc4.jar \u9a71\u52a8\u6587\u4ef6\u4f7f\u7528WinSCP\u5de5\u5177\u62f7\u8d1d\u5230 /usr/zepplein/zeppelin-0.8.0-bin-all/interpreter/jdbc \u8def\u5f84\u4e0b\uff0c \u5e76\u4e14\u4f7f\u7528 \u4e0b\u9762\u547d\u4ee4\u66f4\u6539\u9a71\u52a8\u6743\u9650\u3002 chown 502:wheel gsjdbc4.jar chmod 755 gsjdbc4.jar \u542f\u52a8Zeppelin, \u914d\u7f6e JDBC interpreter\u5982\u4e0b: 1: default.driver = org.postgresql.Driver 2: default.password = Bigdata@123 3: default.url = jdbc:postgresql://172.21.3.101:25108/db_tpcds 4: default.user = joe \u68c0\u67e5\u7ed3\u679c\uff1a","title":"0.8.0 <--> C80"},{"location":"Development/Zeppelin_0.8.0/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.8.0/#_1","text":"Zeppelin 0.8.0 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Spark/SparkR/ELK)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#zeppelin","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.8.0/#_2","text":"\u5b89\u88c5Zeppelin0.8.0","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_4","text":"\u5b89\u88c5Zeppelin 0.8.0,\u5728\u7f51\u5740 https://zeppelin.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 tar -zxvf zeppelin-0.8.0-bin-all.tgz \u5b89\u88c5\u751f\u6210zeppelin-0.8.0-bin-all\u76ee\u5f55\u3002 \u542f\u52a8\u548c\u505c\u6b62Zeppelin bin/zeppelin-daemon.sh start bin/zeppelin-daemon.sh stop \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME = /usr/zeppelin/zeppelin-0.8.0-bin-all export PATH = $ZEPPELIN_HOME/bin:$PATH \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cd /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ cp zeppelin-env.sh.template zeppelin-env.sh vi zeppelin-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 - \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf cp zeppelin-site.xml.template zeppelin-site.xml \u5c06zeppelin-site.xml\u4e2d\u7aef\u53e38080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09 sed -i 's/8080/18081/' zeppelin-site.xml \u8fd0\u884czeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh start \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\u767b\u9646\uff0czeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684\u865a\u62df\u673aIP \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all \u76ee\u5f55\u4e0b \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/shiro.ini cp shiro.ini.template shiro.ini vi shiro.ini [urls]authc\u8868\u793a\u5bf9\u4efb\u4f55url\u8bbf\u95ee\u90fd\u9700\u8981\u9a8c\u8bc1 [users]\u4e0b\u589e\u52a0\u7528\u6237developuser\uff0c\u5bc6\u7801Huawei@123\uff0c\u6743\u9650admin \u91cd\u542fzeppelin cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u4f7f\u7528\u8d26\u6237developuser\u767b\u9646zeppelin","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.8.0/#_5","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_6","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_7","text":"\u5c06 /opt/hadoopclient/Hive/Beeline/lib/ \u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u76ee\u5f55\u4e0b\u3002 \u5c06\u4ece\u65b0\u62f7\u8d1d\u8fc7\u6765\u7684jar\u5305\u7684\u5c5e\u4e3b\u548c\u6743\u9650\u4fee\u6539\u4e3a\u548c /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/jdbc/ \u4e0b\u539f\u6709\u7684jar\u5305\u76f8\u540c chown 502:wheel *.jar chmod 644 *.jar \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser/ \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539default.driver\u548cdefault.url\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 default.driver\uff1aorg.apache.hive.jdbc.HiveDriver default.url\uff1ajdbc:hive2://172.21.3.103:24002,172.21.3.101:24002,172.21.3.102:24002/;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=developuser;user.keytab=/opt/developuser/user.keytab \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; %jdbc select * from t2 \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.8.0/#_8","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_9","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.0\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_10","text":"\u5c06 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase/ \u76ee\u5f55\u4e0b cp /opt/hadoopclient/HBase/hbase/lib/hbase-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/hadoop-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/zookeeper-*.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase cp /opt/hadoopclient/HBase/hbase/lib/dynalogger-V100R002C30.jar /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/hbase \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/zeppelin-0.8.0-bin-all/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/zeppelin-0.8.0-bin-all/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u4eceFusionInsight\u5ba2\u6237\u7aef\u4e0b\u8f7d\u7528\u6237developuser\u7684user.keytab\u548ckrb5.conf\u6587\u4ef6\uff0c\u5c06krb5.conf\u6587\u4ef6\u653e\u5728 /opt/developuser \u4e0b \u5728 /usr/zeppelin/zeppelin-0.8.0-bin-all/conf/ \u8def\u5f84\u4e0b\u65b0\u5efahbase\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u4fee\u6539hbase.home\u53c2\u6570\uff0c\u70b9\u51fb save \u4fdd\u5b58 hbase.home\uff1a/opt/hadoopclient/HBase/hbase \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae %hbase create 'test4', 'cf' put 'test4', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test4\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.8.0/#_11","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_12","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_13","text":"\u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark \u5c06 /opt/client/FusionInsight_Services_ClientConfig/Spark2x/FusionInsight-Spark2x-2.1.0.tar.gz/spark/jars \u8def\u5f84\u4e0b libfb303-0.9.3.jar \u548c libthrift-0.9.3.jar \u4e24\u4e2ajar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/zeppelin-0.8.0-bin-all/interpreter/spark/dep \u8def\u5f84\u4e0b \u786e\u4fdd /usr/zeppelin/zeppelin-0.8.0-bin-all/lib/interpreter \u8def\u5f84\u4e0b\u6709\u4e14\u4ec5\u6709 libthrift-0.9.3.jar \u8fd9\u4e2a\u7248\u672c\u7684jar\u5305 \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/opt/zeppelin-0.7.3/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u5e76\u4e14\u68c0\u67e5zeppelin.spark.useHiveContext\u9879\uff0c\u4f7f\u5176\u503c\u4e3afalse\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801\uff0c\u53c2\u8003\u7f51\u5740 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51\uff0c\u68c0\u6d4b\u662f\u5426\u80fd\u6253\u5f00\u4ee5\u4e0b\u94fe\u63a5 \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5python-matplotlib yum install python-matplotlib \u5b89\u88c5Anaconda2-4.4 wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh sh Anaconda2-4.4.0-Linux-x86_64.sh \u914d\u7f6e\u73af\u5883\u53d8\u91cfPATH\uff0c\u5c06python\u6362\u6210\u5b89\u88c5Anaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python export PATH=/root/anaconda2/bin/:$PATH sh Anaconda2-4.4.0-Linux-x86_64.sh \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark)","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.8.0/#_14","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_15","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_16","text":"\u7531\u4e8eSpark\u7684Executor\u4e0a\u4e5f\u9700\u8981\u6267\u884cR\uff0c\u6240\u4ee5\u9664\u4e86\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u4ee5\u5916\uff0c\u6240\u6709FusionInsight\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e5f\u8981\u5b89\u88c5\u540c\u7248\u672c\u7684R\uff0c\u5b89\u88c5\u6b65\u9aa4\u5982\u4e0b\uff1a \u4e0d\u540cOS\u914d\u7f6eyum\u6e90\u65f6\u4e0b\u8f7d\u7684\u6587\u4ef6\u8def\u5f84\u6709\u6240\u4e0d\u540c\uff0c\u4e0b\u9762\u4ee5Redhat6.6\u5b89\u88c5R\u4e3a\u4f8b \u914d\u7f6eRedhat6.6\u7684yum\u6e90 cd ~ rpm -aq | grep yum | xargs rpm -e --nodeps wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-iniparse-0.3.1-2.1.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-metadata-parser-1.1.2-16.el6.x86_64.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-3.2.29-81.el6.centos.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm wget http://mirrors.163.com/centos/6/os/x86_64/Packages/python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh python-iniparse-0.3.1-2.1.el6.noarch.rpm rpm -ivh yum-metadata-parser-1.1.2-16.el6.x86_64.rpm rpm -U python-urlgrabber-3.9.1-11.el6.noarch.rpm rpm -ivh yum-3.2.29-81.el6.centos.noarch.rpm yum-plugin-fastestmirror-1.1.30-40.el6.noarch.rpm cd /etc/yum.repos.d/ wget http://mirrors.163.com/.help/CentOS6-Base-163.repo sed -i 's/$releasever/6/g' /etc/yum.repos.d/CentOS6-Base-163.repo yum clean all yum makecache \u5982\u679c\u9047\u5230\u6e90yum-plugin-fastestmirror\u65e0\u6cd5\u4e0b\u8f7d\u65f6\uff0c\u53ef\u5728\u7f51\u5740 https://rpmfind.net/linux/rpm2html/search.php?query=yum-plugin-fastestmirror \u4e0b\u9009\u62e9\u76f8\u5e94\u7684\u7248\u672c\u4ee3\u66ff\u4e0b\u8f7d\u5b89\u88c5 \u914d\u7f6eEPEL\u7684\u6e90 Redhat 6.x \u4f7f\u7528\u4e0b\u9762\u547d\u4ee4\u5b89\u88c5 rpm -Uvh https://mirrors.tuna.tsinghua.edu.cn/epel//6/x86_64/epel-release-6-8.noarch.rpm \u66f4\u65b0cache yum clean all yum makecache \u6267\u884c yum install R \u5b89\u88c5R\u7684\u76f8\u5173\u7684\u5305 \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528 \u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR \u53c2\u8003 http://zeppelin.apache.org/docs/0.7.3/interpreter/r.html#using-the-r-interpreter \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5sparkR\u6837\u4f8b\u9700\u8981\u7684R\u7684libraries install.packages('devtools') install.packages('knitr') install.packages('ggplot2') install.packages(c('devtools','mplot','googleVis')) install.packages('data.table') install.packages('sqldf') install.packages('glmnet') install.packages('pROC') install.packages('caret') install.packages('sqldf') install.packages('wordcloud') - \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter - \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser cd /usr/zeppelin/zeppelin-0.8.0-bin-all bin/zeppelin-daemon.sh restart \u5728Zeppelin\u4e2d\u6267\u884cZeppelin Tutorial -> R (SparkR)\u6837\u4f8b","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinapache-livy","text":"","title":"Zeppelin\u8fde\u63a5Apache Livy"},{"location":"Development/Zeppelin_0.8.0/#_17","text":"Zeppelin\u4e2d\u914d\u7f6eLivy\u89e3\u6790\u5668\uff0c\u8fde\u63a5Livy","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_18","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u5b8c\u6210Apache Livy 0.5.0\u7684\u5b89\u88c5 \u53ef\u53c2\u8003\u300aApache Livy\u5bf9\u63a5FusionInsight\u300b\u5bf9\u63a5\u6587\u6863\u5b8c\u6210Apache Livy\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_19","text":"\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8Livy\u670d\u52a1 cd /usr/livy/livy-0.5.0-incubating-bin bin/livy-server start \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9livy\uff0c\u70b9\u51fb edit \u7f16\u8f91zeppelin.livy.url\u7684\u503c\u4e3a http://172.21.3.43:8998 \uff08\u53ef\u4ee5\u4e0d\u66f4\u6539\uff09\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982livy_connection_test \u5728Zeppelin\u4e2d\u6267\u884cSpark\u6837\u4f8b\u4ee3\u7801 val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cPySpark\u6837\u4f8b\u4ee3\u7801 %livy.pyspark import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \u5728Zeppelin\u4e2d\u6267\u884cSparkR\u6837\u4f8b\u4ee3\u7801 %livy.sparkr hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\")","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.0/#zeppelinfusioninsight-elk","text":"","title":"Zeppelin\u8fde\u63a5FusionInsight Elk"},{"location":"Development/Zeppelin_0.8.0/#_20","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u8fde\u63a5FusionInsight Elk","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.0/#_21","text":"\u5b8c\u6210Zeppelin0.8.0\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bElk\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.0/#_22","text":"\u7b2c\u4e00\u6b65\uff1a \u540e\u53f0\u767b\u5f55FusionInsight Elk, \u521b\u5efa\u767b\u5f55\u7528\u6237\uff0c \u5206\u914d\u7528\u6237\u6743\u9650\uff0c \u521b\u5efa\u6570\u636e\u5e93\uff0c \u6d4b\u8bd5\u6570\u636e\u8868 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u4f7f\u7528 gsql -d postgres -p 25108 \u8fde\u63a5\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237**joe**, \u5bc6\u7801\u4e3a**Bigdata@123** CREATE USER joe WITH PASSWORD \"Bigdata@123\"; \u7528\u4e0b\u9762\u8fd9\u4e2a\u547d\u4ee4\u5c06\u7cfb\u7edf\u6743\u9650\u6388\u6743\u7ed9\u7528\u6237\u6216\u8005\u89d2\u8272 GRANT ALL PRIVILEGES TO joe; \u521b\u5efaHDFS\u8868\u7a7a\u95f4\u3002 CREATE TABLESPACE hdfs_tablespace LOCATION '/srv/BigData/hadoop/hdfs_tablespace' WITH (filesystem = 'HDFS', cfgpath = '/opt/huawei/Bigdata/mppdb/conf', storepath = '/user/elk/tablespace/ hdfs_tablespace'); \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE TABLESPACE \u521b\u5efa\u6570\u636e\u5e93\u3002 CREATE DATABASE db_tpcds; \u5f53\u7ed3\u679c\u663e\u793a\u4e3a\u5982\u4e0b\u4fe1\u606f\uff0c\u5219\u8868\u793a\u521b\u5efa\u6210\u529f\u3002 CREATE DATABASE \u521b\u5efa\u5b8cdb_tpcds\u6570\u636e\u5e93\u540e\uff0c\u5c31\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u6cd5\u9000\u51fapostgres\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u65b0\u7528\u6237\u8fde\u63a5\u5230\u6b64\u6570\u636e\u5e93\u6267\u884c\u63a5\u4e0b\u6765\u7684\u521b\u5efa\u8868\u7b49\u64cd\u4f5c\u3002\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u7ee7\u7eed\u5728\u9ed8\u8ba4\u7684postgres\u6570\u636e\u5e93 \u4e0b\u505a\u540e\u7eed\u7684\u4f53\u9a8c\u3002 \\q gsql -d db_tpcds -p 25108 -U joe -W Bigdata@123 \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\u201chdfs_001\u201d\u7684\u8868\u3002 CREATE TABLE hdfs_001(id int,name varchar2(20) ) WITH (orientation=orc,version=0.12,compression=no) TABLESPACE hdfs_tablespace; \u4f7f\u7528INSERT\u547d\u4ee4\u63d2\u5165\u6570\u636e\u3002 \u63d2\u5165\u4e00\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'); \u63d2\u5165\u591a\u884c\u6570\u636e\uff1a INSERT INTO hdfs_001 (id,name ) VALUES (1, 'Administration'),(2, 'Marketing'), (2, 'Purchasing'); \u68c0\u67e5\u7ed3\u679c Select * from hdfs_001 \u7b2c\u4e8c\u6b65\uff1a \u914d\u7f6e\u96c6\u7fa4Elk\u8fdc\u7a0b\u8fde\u63a5 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172.21.3.101 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source /opt/huawei/Bigdata/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u914d\u7f6e\u5ba2\u6237\u7aef\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u4ee5 joe \u7528\u6237\u8fde\u63a5\u5230\u672c\u673a\uff0c\u6b64\u5904\u8fdc\u7a0b\u8fde\u63a5\u7981\u6b62\u4f7f\u7528 omm \u7528\u6237\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u793a\u4f8b\u4e2d\u914d\u7f6e\u5141\u8bb8IP\u5730\u5740\u4e3a 172.16.52.190 \u7684\u5ba2\u6237\u7aef\u8bbf\u95ee\u96c6\u7fa4\u672c\u673a\u3002 gs_guc set -Z coordinator -N all -I all -h \"host all joe 172.16.52.190/32 sha256\" \u4f7f\u7528\u201cjoe\u201d\u7528\u6237\u524d\uff0c\u9700\u5148\u672c\u5730\u8fde\u63a5\u6570\u636e\u5e93\uff0c\u5e76\u5728\u6570\u636e\u5e93\u4e2d\u4f7f\u7528\u5982\u4e0b\u8bed\u53e5\u5efa\u7acb\u201cjoe\u201d\u7528\u6237\u3002 -Z coordinator\u8868\u793a\u5b9e\u4f8b\u7c7b\u578b\u4e3acoordinator\u3002 -N all\u8868\u793a\u96c6\u7fa4\u7684\u6240\u6709\u4e3b\u673a\u3002 -I all\u8868\u793a\u4e3b\u673a\u7684\u6240\u6709\u5b9e\u4f8b\u3002 -h \u8868\u793a\u6307\u5b9a\u9700\u8981\u5728\u201cpg_hba.conf\u201d\u589e\u52a0\u7684\u8bed\u53e5\u3002 all\u8868\u793a\u5141\u8bb8\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u4efb\u610f\u7684\u6570\u636e\u5e93\u3002 joe \u8868\u793a\u8fde\u63a5\u6570\u636e\u5e93\u7684\u7528\u6237\u3002 172.16.52.190/32\u8868\u793a\u53ea\u5141\u8bb8IP\u5730\u5740\u4e3a10.10.0.30\u7684\u4e3b\u673a\u8fde\u63a5\u3002\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u6839\u636e\u7528\u6237\u7684\u7f51\u7edc \u8fdb\u884c\u914d\u7f6e\u4fee\u6539\u3002 sha256\u8868\u793a\u8fde\u63a5\u65f6jack\u7528\u6237\u7684\u5bc6\u7801\u4f7f\u7528sha256\u7b97\u6cd5\u52a0\u5bc6\u3002 \u914d\u7f6elisten_addresses \u4f7f\u7528\u547d\u4ee4 gs_guc set -N all -I all -Z coordinator -c \"listen_addresses = '*'\" \u6267\u884c\u5982\u4e0b\u547d\u4ee4\u91cd\u542f\u96c6\u7fa4\u3002 gs_om -t stop && gs_om -t start \u7b2c\u4e09\u6b65\uff1a \u914d\u7f6ezeppelin JDBC \u63a5\u53e3\u5bf9\u63a5 FusionInsight elk \u5728FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230Elk\u7684jdbc\u9a71\u52a8\uff1a \u9a71\u52a8\u7a0b\u5e8f\uff1aGauss200-OLAP-V100R007C10-REDHAT-64bit-Jdbc.tar.gz \u9a71\u52a8\u7c7b\uff1aorg.postgresql.Driver \u5177\u4f53\u4f4d\u7f6e\u4e3a\uff1aC:\\FusionInsightHD\\FusionInsight_Services_ClientConfig\\Elk \u9a71\u52a8jar\u5305\u7684\u540d\u5b57\u53eb gsjdbc4.jar \u5c06\u627e\u5230\u7684\u8fd9\u4e2a gsjdbc4.jar \u9a71\u52a8\u6587\u4ef6\u4f7f\u7528WinSCP\u5de5\u5177\u62f7\u8d1d\u5230 /usr/zepplein/zeppelin-0.8.0-bin-all/interpreter/jdbc \u8def\u5f84\u4e0b\uff0c \u5e76\u4e14\u4f7f\u7528 \u4e0b\u9762\u547d\u4ee4\u66f4\u6539\u9a71\u52a8\u6743\u9650\u3002 chown 502:wheel gsjdbc4.jar chmod 755 gsjdbc4.jar \u542f\u52a8Zeppelin, \u914d\u7f6e JDBC interpreter\u5982\u4e0b: 1: default.driver = org.postgresql.Driver 2: default.password = Bigdata@123 3: default.url = jdbc:postgresql://172.21.3.101:25108/db_tpcds 4: default.user = joe \u68c0\u67e5\u7ed3\u679c\uff1a","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.1/","text":"Zeppelin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Zeppelin 0.8.1 \u2194 FusionInsight HD 6.5 (HBase/Hive/Spark2x/GaussDB) Zeppelin 0.9.0 \u2194 FusionInsight MRS 8.0 (HBase/Hive/Spark2x/GaussDB) \u6982\u8ff0 \u00b6 Appache Zeppelin \u662f\u4e00\u4e2a\u63d0\u4f9b\u4ea4\u4e92\u6570\u636e\u5206\u6790,\u6570\u636e\u53ef\u89c6\u5316\u7b49\u529f\u80fd\u7684web\u7b14\u8bb0\u672c\u3002\u652f\u6301\u591a\u79cd\u8bed\u8a00\uff0c\u5305\u62ec Scala\u3001Python\u3001SparkSQL\u3001 R\u3001Hive\u3001 Markdown\u3001Shell\u7b49\u7b49 FusionInsight HD \u662f\u4f01\u4e1a\u7ea7\u7684\u5206\u5e03\u5f0f\u5927\u6570\u636e\u5904\u7406\u7cfb\u7edf\uff0c\u5bf9\u5916\u63d0\u4f9b\u5927\u5bb9\u91cf\u7684\u6570\u636e\u5b58\u50a8\u3001\u5206\u6790\u67e5\u8be2\u548c\u5b9e\u65f6\u6d41\u5f0f\u6570\u636e\u5904\u7406\u5206\u6790\u80fd\u529b\u3002 \u672c\u6587\u6863\u4ecb\u7ecd\u4ee5FusionInsight HD\u4e3a\u540e\u7aef\uff0cZeppelin\u4e3a\u524d\u7aef\uff0c\u7528SQL,Scala,Python,R\u7b49\u8bed\u8a00\u5bf9\u63a5HD\u7ec4\u4ef6Hive,HBase,Spark2x,GaussDB\u8fdb\u884c\u4ea4\u4e92\u6570\u636e\u5206\u6790\u7684\u65b9\u6cd5\u3002 \u6d4b\u8bd5\u73af\u5883\u7269\u7406\u62d3\u6251\u7ed3\u6784 \u00b6 \u5b89\u88c5Fusioninsight HD\u5ba2\u6237\u7aef \u00b6 \u53c2\u8003 \u4ea7\u54c1\u6587\u6863 \u8f6f\u4ef6\u5b89\u88c5 > \u521d\u59cb\u914d\u7f6e > \u914d\u7f6e\u5ba2\u6237\u7aef > \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\u3002 \u6587\u6863\u5047\u5b9a\u5df2\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7ddevelopuser\u7528\u6237\u51ed\u8bc1\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser/ \u76ee\u5f55\u4e0b\uff1b\u5b8c\u6574\u5ba2\u6237\u7aef\u5047\u5b9a\u5b89\u88c5\u5728 /opt/hadoopclient/ \u3002 \u5b89\u88c5Zeppelin \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5Zeppelin0.8.1 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5Zeppelin \u5728\u7f51\u5740 https://zeppelin.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c \u4f7f\u7528WinSCP\u7b49\u5de5\u5177\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\u751f\u6210/usr/zeppelin\u76ee\u5f55\u3002 tar -zxvf zeppelin-0.8.1-bin-all.tgz mv zeppelin-0.8.1-bin-all /usr/zeppelin \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/usr/zeppelin export PATH=$ZEPPELIN_HOME/bin:$PATH \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u52a0\u5165JAVA_HOME,\u4f4d\u7f6e /usr/zeppelin/conf cd /usr/zeppelin/conf/ cp zeppelin-env.sh.template zeppelin-env.sh source /opt/hadoopclient/bigdata_env echo $JAVA_HOME echo \"export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201\">>zeppelin-env.sh ![](assets/Zeppelin_0.8.1/2019-08-29-22-22-26.png) \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/conf ,\u5c06zeppelin.server.port 8080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09\uff1b\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse cd /usr/zeppelin/conf cp zeppelin-site.xml.template zeppelin-site.xml vi zeppelin-site.xml <property> <name>zeppelin.server.port</name> <value>18081</value> <description>Server port.</description> </property> ``` <property> <name>zeppelin.anonymous.allowed</name> <value>falase</value> <description>Anonymous user allowed by default</description> </property> ``` \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/conf/shiro.ini,\u589e\u52a0\u7528\u6237developuser cp shiro.ini.template shiro.ini vi shiro.ini [users]\u4e0b\u589e\u52a0\u7528\u6237developuser\uff0c\u5bc6\u7801Huawei@123\uff0c\u6743\u9650admin developuser = Huawei@123, admin ![](assets/Zeppelin_0.8.1/2019-08-29-22-42-45.png) \u5347\u7ea7zeppelin\u7684libthrift-0.9.2\u81f3libthrift-0.9.3\uff08\u6216\u4ee5\u4e0a\uff09\u7248\u672c\u3002 \u8bf4\u660e\uff1azeppelin0.8.1\u6e90\u7801\u6253\u5305\u7684\u662flibthrift-0.9.2.jar\uff0c\u8fd0\u884c\u540e\u9762\u7684\u6837\u4f8b\u4f1a\u51fa\u73b0\u9519\u8bef java.lang.NoSuchMethodError: com.facebook.fb303.FacebookService$Client.sendBaseOneway(Ljava/lang/String;Lorg/apache/thrift/TBase;)V source /opt/hadoopclient/bigdata_env cd /opt/hadoopclient/Hive/Beeline/lib/jdbc jar xvf libthrift-0.9.3.jar org/apache/thrift/TServiceClient.class javap -p org/apache/thrift/TServiceClient.class ``` ``` source /opt/hadoopclient/bigdata_env cd /usr/zeppelin/interpreter/spark/ jar xvf spark-interpreter-0.8.1.jar org/apache/thrift/TServiceClient.class javap -p org/apache/thrift/TServiceClient.class find /usr/zeppelin -name \"libthrift*.jar\" cp -r /usr/zeppelin /usr/zeppelin_bakup find /usr/zeppelin -name \"libthrift*.jar\" | xargs rm -f {} \\; find /usr/zeppelin -name \"libthrift*.jar\" cp /opt/hadoopclient/Hive/Beeline/lib/jdbc/libthrift-0.9.3.jar /usr/zeppelin/lib/interpreter/ cp /opt/hadoopclient/Hive/Beeline/lib/jdbc/org/apache/thrift/TServiceClient.class /usr/zeppelin/interpreter/spark/org/apache/thrift/TServiceClient.class jar uvf spark-interpreter-0.8.1.jar org/apache/thrift/TServiceClient.class \u8fd0\u884czeppelin(\u5e76\u68c0\u67e5\u542f\u52a8\u53c2\u6570) zeppelin-daemon.sh start ps ef | grep /opt/hadoopclient/JDK/jdk-8u201 \u5173\u95ed\u9632\u706b\u5899,\u5141\u8bb8\u7aef\u53e318081(\u6b64\u4e3a\u6d4b\u8bd5\u73af\u5883\uff0c\u751f\u4ea7\u73af\u5883\u5efa\u8bae\u91c7\u53d6\u66f4\u5b89\u5168\u63aa\u65bd) systtemctl stop firewalld \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\uff0c\u7528developuser\u767b\u9646\u3002zeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684HD\u5ba2\u6237\u7aefIP Zeppelin\u8fde\u63a5HBase \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bf9\u63a5HBase\u524d\uff0c\u786e\u8ba4\u5ba2\u6237\u7aefhbase shell\u80fd\u8fde\u63a5HBase source /opt/hadoopclient/bigdata_env hbase shell \u5c06 /usr/zeppelin/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /usr/zeppelin/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/interpreter/hbase/ \u3002 \\cp -f /opt/hadoopclient/HBase/hbase/lib/*.jar /usr/zeppelin/interpreter/hbase/ \u65b0\u5efaJava Authentication Authorization Service\u8ba4\u8bc1\u6587\u4ef6 /usr/zeppelin/conf/jaas.conf \uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u7f16\u8f91interpreter.json\uff0c\u4f4d\u7f6e /usr/zeppelin/conf/interpreter.json ,\u4fee\u6539hbase.home\u3002 \"hbase.home\": { \"name\": \"hbase.home\", \"value\": \"/opt/hadoopclient/HBase/hbase\", \"type\": \"string\" }, - \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart \u767b\u9646Zeppelin\uff0c\u786e\u8ba4hbase\u914d\u7f6e\u3002\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae %hbase create 'test6', 'cf' put 'test6', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test4\u548c\u6570\u636e Zeppelin\u8fde\u63a5Hive \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.1\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u5b8c\u6210\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser/ \u76ee\u5f55\u4e0b \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bf9\u63a5\u524d\uff0c\u786e\u8ba4HD\u81ea\u5e26beeline\u5ba2\u6237\u7aef\u53ef\u8fde\u63a5hive source /opt/hadoopclient/bigdata_env beeline \u65b0\u5efaJava Authentication Authorization Service\u8ba4\u8bc1\u6587\u4ef6 /usr/zeppelin/conf/jaas.conf \uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/conf/zeppelin-env.sh \uff0c\u914d\u7f6eZEPPELIN_INTP_JAVA_OPTS\u7b49\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u590d\u5236 Hive JDBC \u76f8\u5173jar\u5305\u81f3zeppelin cd /opt/hadoopclient/Hive/Beeline/lib/jdbc \\cp -f hive-jdbc-3.1.0.jar hive-service-rpc-3.1.0.jar hive-service-3.1.0.jar curator-client-2.12.0.jar curator-framework-2.12.0.jar zookeeper-3.5.1.jar guava-19.0.jar hive-common-3.1.0.jar hive-shims-common-3.1.0.jar hive-standalone-metastore-3.1.0.jar hadoop-common-3.1.1.jar woodstox-core-asl-4.4.1.jar woodstox-core-5.0.3.jar stax2-api-3.1.4.jar commons-collections-3.2.2.jar commons-configuration2-2.1.1.jar hadoop-auth-3.1.1.jar hive-serde-3.1.0.jar /usr/zeppelin/interpreter/jdbc/ \u67e5\u8be2CLIENT HIVE URI source /opt/hadoopclient/bigdata_env echo $CLIENT_HIVE_URI \u7f16\u8f91interpreter.json\uff0c\u4f4d\u7f6e /usr/zeppelin/conf/interpreter.json ,\u4fee\u6539JDBC default.url,default.driver. \u8bf4\u660e:default.url\u4e3a$CLIENT_HIVE_URI\u7684\u503c\uff0c\u6ce8\u610f\u5b57\u7b26\u7b49\u53f7=\u5728json\u6587\u4ef6\u4e2d\u4e3a\\u003\uff1bdefault.driver\u4e3aorg.apache.hive.jdbc.HiveDriver ``` \"jdbc\": { \"id\": \"jdbc\", \"name\": \"jdbc\", \"group\": \"jdbc\", \"properties\": { \"default.url\": { \"name\": \"default.url\", \"value\": \"jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode\\u003dzooKeeper;zooKeeperNamespace\\u003dhiveserver2;sasl.qop\\u003dauth-conf;auth\\u003dKERBEROS;principal\\u003dhive/hadoop. hadoop.com@HADOOP.COM \", \"type\": \"string\" }, \"default.driver\": { \"name\": \"default.driver\", \"value\": \"org.apache.hive.jdbc.HiveDriver\", \"type\": \"string\" }, ``` \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart \u767b\u9646Zeppelin\uff0c\u786e\u8ba4jdbc\u7684\u914d\u7f6e\u3002\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; %jdbc select * from t2 \u5bfc\u5165\u9884\u5b9a\u4e49\u7684hive notebook, import note ,\u9009\u62e9json\u6587\u4ef6 zeppelin_notebook_hive_test.json \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5Spark \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.1\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5bf9\u63a5Spark\u524d\uff0c\u786e\u8ba4\u5ba2\u6237\u7aef\u5de5\u5177spark-beeline\u80fd\u8fde\u63a5Spark source /opt/hadoopclient/bigdata_env kinit -kt /opt/developuser/user.keytab developuser spark-beeline \u5c06 /opt/hadoopclient/Spark2x/spark/jars \u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/interpreter/spark cp /opt/hadoopclient/Spark2x/spark/jars/*.jar /usr/zeppelin/interpreter/spark/ \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u5173\u95ed\u9632\u706b\u5899(\u63d0\u4ea4spark\u4efb\u52a1\u65f6\uff0c\u80fd\u65b9\u4fbf\u8bbf\u95eeSpark Driver\u6240\u5728\u8282\u70b9\u7684\u7aef\u53e3) systemctl stop firewalld \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u5e76\u4e14\u68c0\u67e5zeppelin.spark.useHiveContext\u9879\uff0c\u4f7f\u5176\u503c\u4e3afalse\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u4f7f\u7528yarn-client\u6a21\u5f0f\uff0c\u8fd8\u9700\u8981\u5c06\u5ba2\u6237\u7aef\u7684\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u6dfb\u52a0\u5230Yarn ResourceManager\u8282\u70b9(\u672c\u6587\u5373HD\u76843\u53f0\u4e3b\u673a\u8282\u70b9\uff09\u7684hosts\u6587\u4ef6\u4e2d\u3002\uff08\u53e6\u4e00\u79cd\u65b9\u5f0f\u662f\u5728ZEPPELIN_INTP_JAVA_OPTS\u4e2d\u6dfb\u52a0-Dspark.driver.host=172.16.5.102\uff09 echo \"172.16.5.102 172-16-5-102\">>/etc/hosts - \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart - \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801\uff0c\u53c2\u8003\u7f51\u5740 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51 - \u6216\u8005\u5bfc\u5165\u9884\u5b9a\u4e49\u7684notebook, Zeppelin\u9875\u9762\u9009\u62e9 import note ,\u9009\u62e9json\u6587\u4ef6 zeppelin_notebook_spark_test.json - \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c \uff08\u53ef\u9009\uff09\u67e5\u770bzeppelin\u542f\u52a8\u53c2\u6570 ps -ef | grep org.apache.zeppelin \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5Anaconda2 wget https://repo.anaconda.com/archive/Anaconda2-2019.07-Linux-x86_64.sh sh Anaconda2-2019.07-Linux-x86_64.sh \u5b89\u88c5matplotlib conda install matplotlib \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) Zeppelin\u8fde\u63a5SparkR \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.1\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u3002 yum install R R-devel libcurl-devel openssl-devel libxml2-devel \u7531\u4e8eExecutor\u9700\u8981\u6267\u884cR\uff0c\u91c7\u7528yarn-client\u6a21\u5f0f\uff0c\u6240\u4ee5\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u3002HD\u96c6\u7fa4\u8282\u70b9\u4e0d\u9700\u8981R,\u56e0\u4e3a\u6709SparkR \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528\u3002\u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5R interpreter\u7684libraries\uff0c\u53c2\u8003 http://zeppelin.apache.org/docs/0.8.1/interpreter/r.html \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5\u540e\u9762\u6837\u4f8b\u5de5\u7a0b\u6240\u9700\u7684libraries ``` options(repos=structure(c(CRAN=\" https://mirrors.tuna.tsinghua.edu.cn/CRAN/ \"))) install.packages('devtools') install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages('knitr') install.packages(\"rbokeh\") ``` FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0cmaster \u6539\u4e3ayarn-client \uff0czeppelin.spark.useHiveContext\u6539\u4e3afalse \u70b9\u51fb save \u4fdd\u5b58 \u4f7f\u7528yarn-client\u6a21\u5f0f\uff0c\u8fd8\u9700\u8981\u5c06\u5ba2\u6237\u7aef\u7684\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u6dfb\u52a0\u5230Yarn ResourceManager\u8282\u70b9(\u672c\u6587\u5373HD\u76843\u53f0\u4e3b\u673a\u8282\u70b9\uff09\u7684hosts\u6587\u4ef6\u4e2d echo \"172.16.5.102 172-16-5-102\">>/etc/hosts \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart \u5728Zeppelin\u65b0\u5efanotebook %r if (nchar(Sys.getenv(\"SPARK_HOME\")) < 1) { Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark2x/spark\") } library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"))) results <- sql(\"FROM airports SELECT id,name\") head(results) \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c \u5bfc\u5165\u9884\u5b9a\u4e49\u7684notebook, Zeppelin\u9875\u9762\u9009\u62e9 import note ,\u9009\u62e9json\u6587\u4ef6 zeppelin_notebook_sparklyr_test.json \u8bf4\u660e\uff1a\u672c\u6837\u4f8b\u4f7f\u7528\u5e93sparklyr\u8fde\u63a5SparkR \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c Zeppelin\u8fde\u63a5GaussDB200 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u8fde\u63a5GaussDB200 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5b8c\u6210Zeppelin0.8.1\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5; \u5df2\u5b8c\u6210GaussDB200\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7b2c\u4e00\u6b65\uff1a \u540e\u53f0\u767b\u5f55GaussDB200, \u521b\u5efa\u767b\u5f55\u7528\u6237\uff0c \u5206\u914d\u7528\u6237\u6743\u9650\uff0c \u521b\u5efa\u6570\u636e\u5e93 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172-16-4-143 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source ${BIGDATA_HOME}/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u4f7f\u7528 gsql -d postgres -p 25308 \u8fde\u63a5\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237**developuser**, \u5bc6\u7801\u4e3a**Bigdata@123** CREATE USER developuser WITH PASSWORD \"Bigdata@123\"; \u7528\u4e0b\u9762\u8fd9\u4e2a\u547d\u4ee4\u5c06\u7cfb\u7edf\u6743\u9650\u6388\u6743\u7ed9\u7528\u6237\u6216\u8005\u89d2\u8272 GRANT ALL PRIVILEGES TO developuser; \u521b\u5efa\u6570\u636e\u5e93\u3002 CREATE DATABASE db_tpcds; \u521b\u5efa\u5b8cdb_tpcds\u6570\u636e\u5e93\u540e\uff0c\u5c31\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u6cd5\u9000\u51fapostgres\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u65b0\u7528\u6237\u8fde\u63a5\u5230\u6b64\u6570\u636e\u5e93\u6267\u884c\u63a5\u4e0b\u6765\u7684\u521b\u5efa\u8868\u7b49\u64cd\u4f5c\u3002\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u7ee7\u7eed\u5728\u9ed8\u8ba4\u7684postgres\u6570\u636e\u5e93 \u4e0b\u505a\u540e\u7eed\u7684\u4f53\u9a8c\u3002 \\q gsql -d db_tpcds -p 25308 -U developuser -W Bigdata@123 \u7b2c\u4e8c\u6b65\uff1a \u914d\u7f6eGaussDB200\u8fdc\u7a0b\u8fde\u63a5 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172-16-4-143 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source ${BIGDATA_HOME}/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u914d\u7f6e\u5ba2\u6237\u7aef\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u4ee5 developuser \u7528\u6237\u8fde\u63a5\u5230\u672c\u673a\uff0c\u6b64\u5904\u8fdc\u7a0b\u8fde\u63a5\u7981\u6b62\u4f7f\u7528 omm \u7528\u6237\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u793a\u4f8b\u4e2d\u914d\u7f6e\u5141\u8bb8IP\u5730\u5740\u4e3a 172.16.5.103 \u7684\u5ba2\u6237\u7aef\u8bbf\u95ee\u96c6\u7fa4\u672c\u673a\u3002 gs_guc set -Z coordinator -N all -I all -h \"host all developuser 172.16.5.103/32 sha256\" -Z coordinator\u8868\u793a\u5b9e\u4f8b\u7c7b\u578b\u4e3acoordinator\u3002 -N all\u8868\u793a\u96c6\u7fa4\u7684\u6240\u6709\u4e3b\u673a\u3002 -I all\u8868\u793a\u4e3b\u673a\u7684\u6240\u6709\u5b9e\u4f8b\u3002 -h \u8868\u793a\u6307\u5b9a\u9700\u8981\u5728\u201cpg_hba.conf\u201d\u589e\u52a0\u7684\u8bed\u53e5\u3002 all\u8868\u793a\u5141\u8bb8\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u4efb\u610f\u7684\u6570\u636e\u5e93\u3002 developuser \u8868\u793a\u8fde\u63a5\u6570\u636e\u5e93\u7684\u7528\u6237\u3002 172.16.5.103/32\u8868\u793a\u53ea\u5141\u8bb8IP\u5730\u5740\u4e3a10.10.0.30\u7684\u4e3b\u673a\u8fde\u63a5\u3002\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u6839\u636e\u7528\u6237\u7684\u7f51\u7edc \u8fdb\u884c\u914d\u7f6e\u4fee\u6539\u3002 sha256\u8868\u793a\u8fde\u63a5\u65f6jack\u7528\u6237\u7684\u5bc6\u7801\u4f7f\u7528sha256\u7b97\u6cd5\u52a0\u5bc6\u3002 \u67e5\u770b\u914d\u7f6e\u662f\u975e\u751f\u6548\uff0cvi /srv/BigData/mppdb/data1/coordinator/pg_hba.conf \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770bCN\u76ee\u524d\u7684listen_addresses\u914d\u7f6e\u3002 gs_guc check -Z coordinator -I all -c \"listen_addresses\" \u914d\u7f6elisten_addresses \u4f7f\u7528\u547d\u4ee4 gs_guc set -N all -I all -Z coordinator -c \"listen_addresses = '*'\" \u7b2c\u4e09\u6b65\uff1a \u914d\u7f6ezeppelin JDBC \u63a5\u53e3\u5bf9\u63a5 GaussDB200 \u6839\u636e\u76f8\u5173\u6587\u6863\u4e0b\u8f7dGaussDB200\u7684jdbc\u9a71\u52a8\uff1a \u9a71\u52a8\u7a0b\u5e8f\uff1agsjdbc200.jar \u9a71\u52a8\u7c7b\uff1acom.huawei.gauss200.jdbc.Driver \u5c06 gsjdbc200.jar \u9a71\u52a8\u6587\u4ef6\u4f7f\u7528WinSCP\u7b49\u5de5\u5177\u62f7\u8d1d\u5230 /usr/zepplein/interpreter/jdbc \u8def\u5f84\u4e0b \u914d\u7f6e JDBC interpreter\uff0cvi /usr/zepplein/conf/interpreter.json 1: default.driver = com.huawei.gauss200.jdbc.Driver 2: default.password = Bigdata@123 3: default.url = jdbc:postgresql://172.21.3.101:25108/db_tpcds 4: default.user = developuser \"jdbc\": { \"id\": \"jdbc\", \"name\": \"jdbc\", \"group\": \"jdbc\", \"properties\": { \"default.url\": { \"name\": \"default.url\", \"value\": \"jdbc:gaussdb://172.16.4.143:25308/db_tpcds\", \"type\": \"string\" }, \"default.driver\": { \"name\": \"default.driver\", \"value\": \"com.huawei.gauss200.jdbc.Driver\", \"type\": \"string\" }, \"zeppelin.jdbc.principal\": { \"name\": \"zeppelin.jdbc.principal\", \"value\": \"\", \"type\": \"string\" }, \"default.completer.ttlInSeconds\": { \"name\": \"default.completer.ttlInSeconds\", \"value\": \"120\", \"type\": \"number\" }, \"default.password\": { \"name\": \"default.password\", \"value\": \"Bigdata@123\", \"type\": \"password\" }, \"default.completer.schemaFilters\": { \"name\": \"default.completer.schemaFilters\", \"value\": \"\", \"type\": \"textarea\" }, \"default.splitQueries\": { \"name\": \"default.splitQueries\", \"value\": false, \"type\": \"checkbox\" }, \"default.user\": { \"name\": \"default.user\", \"value\": \"developuser\", \"type\": \"string\" }, - \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart - \u5bfc\u5165\u9884\u5b9a\u4e49\u7684notebook, Zeppelin\u9875\u9762\u9009\u62e9 import note ,\u9009\u62e9json\u6587\u4ef6 zeppelin_notebook_GaussDB200_test.json \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c FAQ \u00b6 \u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0bshell\u811a\u672c\u83b7\u53d6\u5f85\u5206\u6790\u7684\u6570\u636e(\u6216\u8005\u76f4\u63a5\u7528\u6d4f\u89c8\u5668\u8bbf\u95eehttp://stat-computing.org/dataexpo/2009 \u6765\u4e0b\u8f7d) # Make download directory mkdir /tmp/flights # Download flight data by year for i in { 2006 ..2008 } do echo \" $( date ) $i Download\" fnam = $i .csv.bz2 wget -O /tmp/flights/ $fnam http://stat-computing.org/dataexpo/2009/ $fnam echo \" $( date ) $i Unzip\" bunzip2 /tmp/flights/ $fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup = L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat \u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684/tmp/flights\u76ee\u5f55\u4ee5\u53ca/tmp/airlines.csv\uff0c/tmp/airports.csv\u6587\u4ef6\u4e0a\u4f20\u5230HDFS\u7684/user/developuser\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u5728Hive\u4e2d\u521b\u5efa\u4e09\u5f20\u8868\uff0c\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u5bf9\u5e94\u7684\u8868\u4e2d hdfs dfs -mkdir /user/developuser/flights hdfs dfs -put flights/* /user/developuser/flights/ hdfs dfs -put airlines.csv /user/developuser/ hdfs dfs -put airports.csv /user/developuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int , month int , dayofmonth int , dayofweek int , deptime int , crsdeptime int , arrtime int , crsarrtime int , uniquecarrier string , flightnum int , tailnum string , actualelapsedtime int , crselapsedtime int , airtime string , arrdelay int , depdelay int , origin string , dest string , distance int , taxiin string , taxiout string , cancelled int , cancellationcode string , diverted int , carrierdelay string , weatherdelay string , nasdelay string , securitydelay string , lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/developuser/flights/2006.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/developuser/flights/2007.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/developuser/flights/2008.csv' INTO TABLE flights ; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string , Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/developuser/airlines.csv' INTO TABLE airlines ; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string , name string , city string , country string , faa string , icao string , lat double , lon double , alt int , tz_offset double , dst string , tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE ; LOAD DATA INPATH '/user/developuser/airports.csv' INTO TABLE airports ;","title":"0.9.0 <--> 8.0"},{"location":"Development/Zeppelin_0.8.1/#zeppelinfusioninsight-hd","text":"","title":"Zeppelin\u5bf9\u63a5FusionInsight HD"},{"location":"Development/Zeppelin_0.8.1/#_1","text":"Zeppelin 0.8.1 \u2194 FusionInsight HD 6.5 (HBase/Hive/Spark2x/GaussDB) Zeppelin 0.9.0 \u2194 FusionInsight MRS 8.0 (HBase/Hive/Spark2x/GaussDB)","title":"\u9002\u7528\u573a\u666f"},{"location":"Development/Zeppelin_0.8.1/#_2","text":"Appache Zeppelin \u662f\u4e00\u4e2a\u63d0\u4f9b\u4ea4\u4e92\u6570\u636e\u5206\u6790,\u6570\u636e\u53ef\u89c6\u5316\u7b49\u529f\u80fd\u7684web\u7b14\u8bb0\u672c\u3002\u652f\u6301\u591a\u79cd\u8bed\u8a00\uff0c\u5305\u62ec Scala\u3001Python\u3001SparkSQL\u3001 R\u3001Hive\u3001 Markdown\u3001Shell\u7b49\u7b49 FusionInsight HD \u662f\u4f01\u4e1a\u7ea7\u7684\u5206\u5e03\u5f0f\u5927\u6570\u636e\u5904\u7406\u7cfb\u7edf\uff0c\u5bf9\u5916\u63d0\u4f9b\u5927\u5bb9\u91cf\u7684\u6570\u636e\u5b58\u50a8\u3001\u5206\u6790\u67e5\u8be2\u548c\u5b9e\u65f6\u6d41\u5f0f\u6570\u636e\u5904\u7406\u5206\u6790\u80fd\u529b\u3002 \u672c\u6587\u6863\u4ecb\u7ecd\u4ee5FusionInsight HD\u4e3a\u540e\u7aef\uff0cZeppelin\u4e3a\u524d\u7aef\uff0c\u7528SQL,Scala,Python,R\u7b49\u8bed\u8a00\u5bf9\u63a5HD\u7ec4\u4ef6Hive,HBase,Spark2x,GaussDB\u8fdb\u884c\u4ea4\u4e92\u6570\u636e\u5206\u6790\u7684\u65b9\u6cd5\u3002","title":"\u6982\u8ff0"},{"location":"Development/Zeppelin_0.8.1/#_3","text":"","title":"\u6d4b\u8bd5\u73af\u5883\u7269\u7406\u62d3\u6251\u7ed3\u6784"},{"location":"Development/Zeppelin_0.8.1/#fusioninsight-hd","text":"\u53c2\u8003 \u4ea7\u54c1\u6587\u6863 \u8f6f\u4ef6\u5b89\u88c5 > \u521d\u59cb\u914d\u7f6e > \u914d\u7f6e\u5ba2\u6237\u7aef > \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\u3002 \u6587\u6863\u5047\u5b9a\u5df2\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7ddevelopuser\u7528\u6237\u51ed\u8bc1\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser/ \u76ee\u5f55\u4e0b\uff1b\u5b8c\u6574\u5ba2\u6237\u7aef\u5047\u5b9a\u5b89\u88c5\u5728 /opt/hadoopclient/ \u3002","title":"\u5b89\u88c5Fusioninsight HD\u5ba2\u6237\u7aef"},{"location":"Development/Zeppelin_0.8.1/#zeppelin","text":"","title":"\u5b89\u88c5Zeppelin"},{"location":"Development/Zeppelin_0.8.1/#_4","text":"\u5b89\u88c5Zeppelin0.8.1","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.1/#_5","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.1/#_6","text":"\u5b89\u88c5Zeppelin \u5728\u7f51\u5740 https://zeppelin.apache.org/download.html \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c \u4f7f\u7528WinSCP\u7b49\u5de5\u5177\u5bfc\u5165\u4e3b\u673a\u5e76\u7528\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\u751f\u6210/usr/zeppelin\u76ee\u5f55\u3002 tar -zxvf zeppelin-0.8.1-bin-all.tgz mv zeppelin-0.8.1-bin-all /usr/zeppelin \u914d\u7f6eZeppelin\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export ZEPPELIN_HOME=/usr/zeppelin export PATH=$ZEPPELIN_HOME/bin:$PATH \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u52a0\u5165JAVA_HOME,\u4f4d\u7f6e /usr/zeppelin/conf cd /usr/zeppelin/conf/ cp zeppelin-env.sh.template zeppelin-env.sh source /opt/hadoopclient/bigdata_env echo $JAVA_HOME echo \"export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201\">>zeppelin-env.sh ![](assets/Zeppelin_0.8.1/2019-08-29-22-22-26.png) \u7f16\u8f91zeppelin-site.xml\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/conf ,\u5c06zeppelin.server.port 8080\u66ff\u6362\u621018081\uff08\u53ef\u81ea\u5b9a\u4e49\uff0c\u4e5f\u53ef\u4ee5\u4e0d\u6539\uff09\uff1b\u5c06zeppelin.anonymous.allowed\u53c2\u6570\u7684true\u4fee\u6539\u4e3afalse cd /usr/zeppelin/conf cp zeppelin-site.xml.template zeppelin-site.xml vi zeppelin-site.xml <property> <name>zeppelin.server.port</name> <value>18081</value> <description>Server port.</description> </property> ``` <property> <name>zeppelin.anonymous.allowed</name> <value>falase</value> <description>Anonymous user allowed by default</description> </property> ``` \u7f16\u8f91shiro.ini\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/conf/shiro.ini,\u589e\u52a0\u7528\u6237developuser cp shiro.ini.template shiro.ini vi shiro.ini [users]\u4e0b\u589e\u52a0\u7528\u6237developuser\uff0c\u5bc6\u7801Huawei@123\uff0c\u6743\u9650admin developuser = Huawei@123, admin ![](assets/Zeppelin_0.8.1/2019-08-29-22-42-45.png) \u5347\u7ea7zeppelin\u7684libthrift-0.9.2\u81f3libthrift-0.9.3\uff08\u6216\u4ee5\u4e0a\uff09\u7248\u672c\u3002 \u8bf4\u660e\uff1azeppelin0.8.1\u6e90\u7801\u6253\u5305\u7684\u662flibthrift-0.9.2.jar\uff0c\u8fd0\u884c\u540e\u9762\u7684\u6837\u4f8b\u4f1a\u51fa\u73b0\u9519\u8bef java.lang.NoSuchMethodError: com.facebook.fb303.FacebookService$Client.sendBaseOneway(Ljava/lang/String;Lorg/apache/thrift/TBase;)V source /opt/hadoopclient/bigdata_env cd /opt/hadoopclient/Hive/Beeline/lib/jdbc jar xvf libthrift-0.9.3.jar org/apache/thrift/TServiceClient.class javap -p org/apache/thrift/TServiceClient.class ``` ``` source /opt/hadoopclient/bigdata_env cd /usr/zeppelin/interpreter/spark/ jar xvf spark-interpreter-0.8.1.jar org/apache/thrift/TServiceClient.class javap -p org/apache/thrift/TServiceClient.class find /usr/zeppelin -name \"libthrift*.jar\" cp -r /usr/zeppelin /usr/zeppelin_bakup find /usr/zeppelin -name \"libthrift*.jar\" | xargs rm -f {} \\; find /usr/zeppelin -name \"libthrift*.jar\" cp /opt/hadoopclient/Hive/Beeline/lib/jdbc/libthrift-0.9.3.jar /usr/zeppelin/lib/interpreter/ cp /opt/hadoopclient/Hive/Beeline/lib/jdbc/org/apache/thrift/TServiceClient.class /usr/zeppelin/interpreter/spark/org/apache/thrift/TServiceClient.class jar uvf spark-interpreter-0.8.1.jar org/apache/thrift/TServiceClient.class \u8fd0\u884czeppelin(\u5e76\u68c0\u67e5\u542f\u52a8\u53c2\u6570) zeppelin-daemon.sh start ps ef | grep /opt/hadoopclient/JDK/jdk-8u201 \u5173\u95ed\u9632\u706b\u5899,\u5141\u8bb8\u7aef\u53e318081(\u6b64\u4e3a\u6d4b\u8bd5\u73af\u5883\uff0c\u751f\u4ea7\u73af\u5883\u5efa\u8bae\u91c7\u53d6\u66f4\u5b89\u5168\u63aa\u65bd) systtemctl stop firewalld \u5728\u6d4f\u89c8\u5668\u4e2d\u8f93\u5165\u5730\u5740zeppelin_ip:18081\uff0c\u7528developuser\u767b\u9646\u3002zeppelin_ip\u4e3a\u5b89\u88c5zeppelin\u7684HD\u5ba2\u6237\u7aefIP","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.1/#zeppelinhbase","text":"","title":"Zeppelin\u8fde\u63a5HBase"},{"location":"Development/Zeppelin_0.8.1/#_7","text":"Zeppelin\u4e2d\u914d\u7f6eHbase\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hbase","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.1/#_8","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.1\u7684\u5b89\u88c5 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bHBase\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.1/#_9","text":"\u5bf9\u63a5HBase\u524d\uff0c\u786e\u8ba4\u5ba2\u6237\u7aefhbase shell\u80fd\u8fde\u63a5HBase source /opt/hadoopclient/bigdata_env hbase shell \u5c06 /usr/zeppelin/interpreter/hbase/ \u76ee\u5f55\u4e0b\u65e7\u7684jar\u5305\u79fb\u8d70 cd /usr/zeppelin/interpreter/hbase mkdir hbase_jar mv hbase*.jar hbase_jar mv hadoop*.jar hbase_jar mv zookeeper-3.4.6.jar hbase_jar \u5c06 /opt/hadoopclient/HBase/hbase/lib/ \u4ee5\u4e0b\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/interpreter/hbase/ \u3002 \\cp -f /opt/hadoopclient/HBase/hbase/lib/*.jar /usr/zeppelin/interpreter/hbase/ \u65b0\u5efaJava Authentication Authorization Service\u8ba4\u8bc1\u6587\u4ef6 /usr/zeppelin/conf/jaas.conf \uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e/usr/zeppelin/conf\uff0c\u52a0\u5165\u4ee5\u4e0b\u4e09\u4e2a\u914d\u7f6e\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" export HBASE_HOME=/opt/hadoopclient/HBase/hbase \u7f16\u8f91interpreter.json\uff0c\u4f4d\u7f6e /usr/zeppelin/conf/interpreter.json ,\u4fee\u6539hbase.home\u3002 \"hbase.home\": { \"name\": \"hbase.home\", \"value\": \"/opt/hadoopclient/HBase/hbase\", \"type\": \"string\" }, - \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart \u767b\u9646Zeppelin\uff0c\u786e\u8ba4hbase\u914d\u7f6e\u3002\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9hbase \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hbase_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae %hbase create 'test6', 'cf' put 'test6', 'row1', 'cf:a', 'value1' \u5728FusionInsight\u7684\u5ba2\u6237\u7aef\u4e0b\u53ef\u4ee5\u770b\u5230\u521b\u5efa\u7684hbase\u8868test4\u548c\u6570\u636e","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.1/#zeppelinhive","text":"","title":"Zeppelin\u8fde\u63a5Hive"},{"location":"Development/Zeppelin_0.8.1/#_10","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u5bf9\u63a5Hive\u7684JDBC\u63a5\u53e3\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.1/#_11","text":"\u5df2\u7ecf\u5b8c\u6210Zeppelin 0.8.1\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u6839\u636e\u4ea7\u54c1\u6587\u6863\u5b8c\u6210\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser/ \u76ee\u5f55\u4e0b","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.1/#_12","text":"\u5bf9\u63a5\u524d\uff0c\u786e\u8ba4HD\u81ea\u5e26beeline\u5ba2\u6237\u7aef\u53ef\u8fde\u63a5hive source /opt/hadoopclient/bigdata_env beeline \u65b0\u5efaJava Authentication Authorization Service\u8ba4\u8bc1\u6587\u4ef6 /usr/zeppelin/conf/jaas.conf \uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/developuser/user.keytab\" principal=\"developuser\" useTicketCache=false storeKey=true debug=true; }; \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/conf/zeppelin-env.sh \uff0c\u914d\u7f6eZEPPELIN_INTP_JAVA_OPTS\u7b49\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201 export ZEPPELIN_INTP_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/developuser/krb5.conf -Djava.security.auth.login.config=/usr/zeppelin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u590d\u5236 Hive JDBC \u76f8\u5173jar\u5305\u81f3zeppelin cd /opt/hadoopclient/Hive/Beeline/lib/jdbc \\cp -f hive-jdbc-3.1.0.jar hive-service-rpc-3.1.0.jar hive-service-3.1.0.jar curator-client-2.12.0.jar curator-framework-2.12.0.jar zookeeper-3.5.1.jar guava-19.0.jar hive-common-3.1.0.jar hive-shims-common-3.1.0.jar hive-standalone-metastore-3.1.0.jar hadoop-common-3.1.1.jar woodstox-core-asl-4.4.1.jar woodstox-core-5.0.3.jar stax2-api-3.1.4.jar commons-collections-3.2.2.jar commons-configuration2-2.1.1.jar hadoop-auth-3.1.1.jar hive-serde-3.1.0.jar /usr/zeppelin/interpreter/jdbc/ \u67e5\u8be2CLIENT HIVE URI source /opt/hadoopclient/bigdata_env echo $CLIENT_HIVE_URI \u7f16\u8f91interpreter.json\uff0c\u4f4d\u7f6e /usr/zeppelin/conf/interpreter.json ,\u4fee\u6539JDBC default.url,default.driver. \u8bf4\u660e:default.url\u4e3a$CLIENT_HIVE_URI\u7684\u503c\uff0c\u6ce8\u610f\u5b57\u7b26\u7b49\u53f7=\u5728json\u6587\u4ef6\u4e2d\u4e3a\\u003\uff1bdefault.driver\u4e3aorg.apache.hive.jdbc.HiveDriver ``` \"jdbc\": { \"id\": \"jdbc\", \"name\": \"jdbc\", \"group\": \"jdbc\", \"properties\": { \"default.url\": { \"name\": \"default.url\", \"value\": \"jdbc:hive2://172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/;serviceDiscoveryMode\\u003dzooKeeper;zooKeeperNamespace\\u003dhiveserver2;sasl.qop\\u003dauth-conf;auth\\u003dKERBEROS;principal\\u003dhive/hadoop. hadoop.com@HADOOP.COM \", \"type\": \"string\" }, \"default.driver\": { \"name\": \"default.driver\", \"value\": \"org.apache.hive.jdbc.HiveDriver\", \"type\": \"string\" }, ``` \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart \u767b\u9646Zeppelin\uff0c\u786e\u8ba4jdbc\u7684\u914d\u7f6e\u3002\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9JDBC \u9875\u9762\u9009\u62e9Notebook -> Create new note \u81ea\u5b9a\u4e49note\u540d\u79f0\uff0c\u4f8b\u5982hive_test \u7f16\u8f91note\uff0c\u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d\u6309\u94ae\u3002 %jdbc Show tables; %jdbc select * from t2 \u5bfc\u5165\u9884\u5b9a\u4e49\u7684hive notebook, import note ,\u9009\u62e9json\u6587\u4ef6 zeppelin_notebook_hive_test.json \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.1/#zeppelinspark","text":"","title":"Zeppelin\u8fde\u63a5Spark"},{"location":"Development/Zeppelin_0.8.1/#_13","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.1/#_14","text":"\u5b8c\u6210Zeppelin0.8.1\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.1/#_15","text":"\u5bf9\u63a5Spark\u524d\uff0c\u786e\u8ba4\u5ba2\u6237\u7aef\u5de5\u5177spark-beeline\u80fd\u8fde\u63a5Spark source /opt/hadoopclient/bigdata_env kinit -kt /opt/developuser/user.keytab developuser spark-beeline \u5c06 /opt/hadoopclient/Spark2x/spark/jars \u8def\u5f84\u4e0b\u6240\u6709\u7684jar\u5305\u62f7\u8d1d\u81f3 /usr/zeppelin/interpreter/spark cp /opt/hadoopclient/Spark2x/spark/jars/*.jar /usr/zeppelin/interpreter/spark/ \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u5173\u95ed\u9632\u706b\u5899(\u63d0\u4ea4spark\u4efb\u52a1\u65f6\uff0c\u80fd\u65b9\u4fbf\u8bbf\u95eeSpark Driver\u6240\u5728\u8282\u70b9\u7684\u7aef\u53e3) systemctl stop firewalld \u767b\u9646Zeppelin\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u83dc\u5355\u4e2d\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 master \u53c2\u6570\u6539\u4e3a yarn-client\uff0c\u5e76\u4e14\u68c0\u67e5zeppelin.spark.useHiveContext\u9879\uff0c\u4f7f\u5176\u503c\u4e3afalse\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u4f7f\u7528yarn-client\u6a21\u5f0f\uff0c\u8fd8\u9700\u8981\u5c06\u5ba2\u6237\u7aef\u7684\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u6dfb\u52a0\u5230Yarn ResourceManager\u8282\u70b9(\u672c\u6587\u5373HD\u76843\u53f0\u4e3b\u673a\u8282\u70b9\uff09\u7684hosts\u6587\u4ef6\u4e2d\u3002\uff08\u53e6\u4e00\u79cd\u65b9\u5f0f\u662f\u5728ZEPPELIN_INTP_JAVA_OPTS\u4e2d\u6dfb\u52a0-Dspark.driver.host=172.16.5.102\uff09 echo \"172.16.5.102 172-16-5-102\">>/etc/hosts - \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart - \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801\uff0c\u53c2\u8003\u7f51\u5740 https://www.zepl.com/viewer/notebooks/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2hvcnRvbndvcmtzLWdhbGxlcnkvemVwcGVsaW4tbm90ZWJvb2tzL21hc3Rlci8yQTk0TTVKMVovbm90ZS5qc29u/ \u6837\u4f8b\u4ee3\u7801\u9700\u8981\u8bbf\u95eeInternet\u4e0a\u7684\u8d44\u6e90\uff0c\u6240\u4ee5\u4fdd\u8bc1zeppelin\u6240\u5728\u7684\u8282\u70b9\u53ef\u4ee5\u8054\u7f51 - \u6216\u8005\u5bfc\u5165\u9884\u5b9a\u4e49\u7684notebook, Zeppelin\u9875\u9762\u9009\u62e9 import note ,\u9009\u62e9json\u6587\u4ef6 zeppelin_notebook_spark_test.json - \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c \uff08\u53ef\u9009\uff09\u67e5\u770bzeppelin\u542f\u52a8\u53c2\u6570 ps -ef | grep org.apache.zeppelin \u6267\u884czeppelin\u7684spark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark) \u5b89\u88c5Anaconda2 wget https://repo.anaconda.com/archive/Anaconda2-2019.07-Linux-x86_64.sh sh Anaconda2-2019.07-Linux-x86_64.sh \u5b89\u88c5matplotlib conda install matplotlib \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.pyspark.python \u53c2\u6570\u6539\u4e3aAnaconda\u5b89\u88c5\u76ee\u5f55\u4e2d\u7684python\uff0c\u70b9\u51fb save \u4fdd\u5b58 \u6267\u884czeppelin\u7684pyspark\u6837\u4f8b\u4ee3\u7801Zeppelin Tutorial -> Matplotlib (Python \u2022 PySpark)","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.1/#zeppelinsparkr","text":"","title":"Zeppelin\u8fde\u63a5SparkR"},{"location":"Development/Zeppelin_0.8.1/#_16","text":"Zeppelin\u4e2d\u914d\u7f6eSpark\u89e3\u6790\u5668\uff0c\u8fde\u63a5SparkR","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.1/#_17","text":"\u5b8c\u6210Zeppelin0.8.1\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\uff0c\u5305\u542bSpark2x\u7ec4\u4ef6\u3002 \u53c2\u8003 http://zeppelin.apache.org/docs/latest/interpreter/spark.html","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.1/#_18","text":"\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u3002 yum install R R-devel libcurl-devel openssl-devel libxml2-devel \u7531\u4e8eExecutor\u9700\u8981\u6267\u884cR\uff0c\u91c7\u7528yarn-client\u6a21\u5f0f\uff0c\u6240\u4ee5\u5728Zeppelin\u7684\u8282\u70b9\u4e0a\u5b89\u88c5R\u3002HD\u96c6\u7fa4\u8282\u70b9\u4e0d\u9700\u8981R,\u56e0\u4e3a\u6709SparkR \u6267\u884c R \uff0c\u68c0\u67e5R\u662f\u5426\u53ef\u7528\u3002\u6b63\u5e38\u542f\u52a8\u5982\u4e0b\u56fe\u6240\u793a \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5R interpreter\u7684libraries\uff0c\u53c2\u8003 http://zeppelin.apache.org/docs/0.8.1/interpreter/r.html \u5728R\u7684\u547d\u4ee4\u884c\u4e2d\u5b89\u88c5\u540e\u9762\u6837\u4f8b\u5de5\u7a0b\u6240\u9700\u7684libraries ``` options(repos=structure(c(CRAN=\" https://mirrors.tuna.tsinghua.edu.cn/CRAN/ \"))) install.packages('devtools') install.packages(\"sparklyr\") install.packages(\"dplyr\") install.packages(\"ggplot2\") install.packages(\"babynames\") install.packages(\"dygraphs\") install.packages('knitr') install.packages(\"rbokeh\") ``` FusionInsight\u5ba2\u6237\u7aef\u4e0b\u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u4f7f\u7528sparkR source /opt/hadoopclient/bigdata_env kinit developuser sparkR \u7f16\u8f91zeppelin-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/zeppelin/conf \uff0c\u52a0\u5165\u4ee5\u4e0b\u5185\u5bb9 export JAVA_HOME=/opt/hadoopclient/JDK/jdk-8u201 export MASTER=yarn-client export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop \u5728zeppelin\u7684\u754c\u9762\u4e2d\uff0c\u9009\u62e9\u53f3\u4e0a\u89d2\u7684 Interpreter \u9009\u62e9Spark\uff0c\u70b9\u51fb edit \u7f16\u8f91\uff0c\u5c06 zeppelin.R.cmd \u53c2\u6570\u6539\u4e3aR\u7684\u542f\u52a8\u6587\u4ef6\uff0cmaster \u6539\u4e3ayarn-client \uff0czeppelin.spark.useHiveContext\u6539\u4e3afalse \u70b9\u51fb save \u4fdd\u5b58 \u4f7f\u7528yarn-client\u6a21\u5f0f\uff0c\u8fd8\u9700\u8981\u5c06\u5ba2\u6237\u7aef\u7684\u4e3b\u673a\u540d\u4e0eIP\u7684\u6620\u5c04\u5173\u7cfb\u6dfb\u52a0\u5230Yarn ResourceManager\u8282\u70b9(\u672c\u6587\u5373HD\u76843\u53f0\u4e3b\u673a\u8282\u70b9\uff09\u7684hosts\u6587\u4ef6\u4e2d echo \"172.16.5.102 172-16-5-102\">>/etc/hosts \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart \u5728Zeppelin\u65b0\u5efanotebook %r if (nchar(Sys.getenv(\"SPARK_HOME\")) < 1) { Sys.setenv(SPARK_HOME=\"/opt/hadoopclient/Spark2x/spark\") } library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"))) results <- sql(\"FROM airports SELECT id,name\") head(results) \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c \u5bfc\u5165\u9884\u5b9a\u4e49\u7684notebook, Zeppelin\u9875\u9762\u9009\u62e9 import note ,\u9009\u62e9json\u6587\u4ef6 zeppelin_notebook_sparklyr_test.json \u8bf4\u660e\uff1a\u672c\u6837\u4f8b\u4f7f\u7528\u5e93sparklyr\u8fde\u63a5SparkR \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.1/#zeppelingaussdb200","text":"","title":"Zeppelin\u8fde\u63a5GaussDB200"},{"location":"Development/Zeppelin_0.8.1/#_19","text":"Zeppelin\u4e2d\u914d\u7f6eJDBC\u89e3\u6790\u5668\uff0c\u8fde\u63a5GaussDB200","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Development/Zeppelin_0.8.1/#_20","text":"\u5b8c\u6210Zeppelin0.8.1\u7684\u5b89\u88c5\uff1b \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5; \u5df2\u5b8c\u6210GaussDB200\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Development/Zeppelin_0.8.1/#_21","text":"\u7b2c\u4e00\u6b65\uff1a \u540e\u53f0\u767b\u5f55GaussDB200, \u521b\u5efa\u767b\u5f55\u7528\u6237\uff0c \u5206\u914d\u7528\u6237\u6743\u9650\uff0c \u521b\u5efa\u6570\u636e\u5e93 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172-16-4-143 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source ${BIGDATA_HOME}/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u4f7f\u7528 gsql -d postgres -p 25308 \u8fde\u63a5\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u7528\u6237**developuser**, \u5bc6\u7801\u4e3a**Bigdata@123** CREATE USER developuser WITH PASSWORD \"Bigdata@123\"; \u7528\u4e0b\u9762\u8fd9\u4e2a\u547d\u4ee4\u5c06\u7cfb\u7edf\u6743\u9650\u6388\u6743\u7ed9\u7528\u6237\u6216\u8005\u89d2\u8272 GRANT ALL PRIVILEGES TO developuser; \u521b\u5efa\u6570\u636e\u5e93\u3002 CREATE DATABASE db_tpcds; \u521b\u5efa\u5b8cdb_tpcds\u6570\u636e\u5e93\u540e\uff0c\u5c31\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u6cd5\u9000\u51fapostgres\u6570\u636e\u5e93\uff0c\u4f7f\u7528\u65b0\u7528\u6237\u8fde\u63a5\u5230\u6b64\u6570\u636e\u5e93\u6267\u884c\u63a5\u4e0b\u6765\u7684\u521b\u5efa\u8868\u7b49\u64cd\u4f5c\u3002\u5f53\u7136\uff0c\u4e5f\u53ef\u4ee5\u9009\u62e9\u7ee7\u7eed\u5728\u9ed8\u8ba4\u7684postgres\u6570\u636e\u5e93 \u4e0b\u505a\u540e\u7eed\u7684\u4f53\u9a8c\u3002 \\q gsql -d db_tpcds -p 25308 -U developuser -W Bigdata@123 \u7b2c\u4e8c\u6b65\uff1a \u914d\u7f6eGaussDB200\u8fdc\u7a0b\u8fde\u63a5 \u4ee5omm\u7528\u6237\u8eab\u4efd\u767b\u5f55CN\u6240\u5728\u670d\u52a1\u5668\uff08172-16-4-143 \u96c6\u7fa4\u4e3b\u8282\u70b9\uff09\uff0c\u6267\u884c source ${BIGDATA_HOME}/mppdb/.mppdbgs_profile \u547d\u4ee4\u542f\u52a8\u73af\u5883\u53d8\u91cf \u914d\u7f6e\u5ba2\u6237\u7aef\u8ba4\u8bc1\u65b9\u5f0f\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u4ee5 developuser \u7528\u6237\u8fde\u63a5\u5230\u672c\u673a\uff0c\u6b64\u5904\u8fdc\u7a0b\u8fde\u63a5\u7981\u6b62\u4f7f\u7528 omm \u7528\u6237\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u793a\u4f8b\u4e2d\u914d\u7f6e\u5141\u8bb8IP\u5730\u5740\u4e3a 172.16.5.103 \u7684\u5ba2\u6237\u7aef\u8bbf\u95ee\u96c6\u7fa4\u672c\u673a\u3002 gs_guc set -Z coordinator -N all -I all -h \"host all developuser 172.16.5.103/32 sha256\" -Z coordinator\u8868\u793a\u5b9e\u4f8b\u7c7b\u578b\u4e3acoordinator\u3002 -N all\u8868\u793a\u96c6\u7fa4\u7684\u6240\u6709\u4e3b\u673a\u3002 -I all\u8868\u793a\u4e3b\u673a\u7684\u6240\u6709\u5b9e\u4f8b\u3002 -h \u8868\u793a\u6307\u5b9a\u9700\u8981\u5728\u201cpg_hba.conf\u201d\u589e\u52a0\u7684\u8bed\u53e5\u3002 all\u8868\u793a\u5141\u8bb8\u5ba2\u6237\u7aef\u8fde\u63a5\u5230\u4efb\u610f\u7684\u6570\u636e\u5e93\u3002 developuser \u8868\u793a\u8fde\u63a5\u6570\u636e\u5e93\u7684\u7528\u6237\u3002 172.16.5.103/32\u8868\u793a\u53ea\u5141\u8bb8IP\u5730\u5740\u4e3a10.10.0.30\u7684\u4e3b\u673a\u8fde\u63a5\u3002\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8bf7\u6839\u636e\u7528\u6237\u7684\u7f51\u7edc \u8fdb\u884c\u914d\u7f6e\u4fee\u6539\u3002 sha256\u8868\u793a\u8fde\u63a5\u65f6jack\u7528\u6237\u7684\u5bc6\u7801\u4f7f\u7528sha256\u7b97\u6cd5\u52a0\u5bc6\u3002 \u67e5\u770b\u914d\u7f6e\u662f\u975e\u751f\u6548\uff0cvi /srv/BigData/mppdb/data1/coordinator/pg_hba.conf \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770bCN\u76ee\u524d\u7684listen_addresses\u914d\u7f6e\u3002 gs_guc check -Z coordinator -I all -c \"listen_addresses\" \u914d\u7f6elisten_addresses \u4f7f\u7528\u547d\u4ee4 gs_guc set -N all -I all -Z coordinator -c \"listen_addresses = '*'\" \u7b2c\u4e09\u6b65\uff1a \u914d\u7f6ezeppelin JDBC \u63a5\u53e3\u5bf9\u63a5 GaussDB200 \u6839\u636e\u76f8\u5173\u6587\u6863\u4e0b\u8f7dGaussDB200\u7684jdbc\u9a71\u52a8\uff1a \u9a71\u52a8\u7a0b\u5e8f\uff1agsjdbc200.jar \u9a71\u52a8\u7c7b\uff1acom.huawei.gauss200.jdbc.Driver \u5c06 gsjdbc200.jar \u9a71\u52a8\u6587\u4ef6\u4f7f\u7528WinSCP\u7b49\u5de5\u5177\u62f7\u8d1d\u5230 /usr/zepplein/interpreter/jdbc \u8def\u5f84\u4e0b \u914d\u7f6e JDBC interpreter\uff0cvi /usr/zepplein/conf/interpreter.json 1: default.driver = com.huawei.gauss200.jdbc.Driver 2: default.password = Bigdata@123 3: default.url = jdbc:postgresql://172.21.3.101:25108/db_tpcds 4: default.user = developuser \"jdbc\": { \"id\": \"jdbc\", \"name\": \"jdbc\", \"group\": \"jdbc\", \"properties\": { \"default.url\": { \"name\": \"default.url\", \"value\": \"jdbc:gaussdb://172.16.4.143:25308/db_tpcds\", \"type\": \"string\" }, \"default.driver\": { \"name\": \"default.driver\", \"value\": \"com.huawei.gauss200.jdbc.Driver\", \"type\": \"string\" }, \"zeppelin.jdbc.principal\": { \"name\": \"zeppelin.jdbc.principal\", \"value\": \"\", \"type\": \"string\" }, \"default.completer.ttlInSeconds\": { \"name\": \"default.completer.ttlInSeconds\", \"value\": \"120\", \"type\": \"number\" }, \"default.password\": { \"name\": \"default.password\", \"value\": \"Bigdata@123\", \"type\": \"password\" }, \"default.completer.schemaFilters\": { \"name\": \"default.completer.schemaFilters\", \"value\": \"\", \"type\": \"textarea\" }, \"default.splitQueries\": { \"name\": \"default.splitQueries\", \"value\": false, \"type\": \"checkbox\" }, \"default.user\": { \"name\": \"default.user\", \"value\": \"developuser\", \"type\": \"string\" }, - \u91cd\u542fzeppelin\u3002 source /opt/hadoopclient/bigdata_env kinit \u2013kt /opt/developuser/user.keytab developuser zeppelin-daemon.sh restart - \u5bfc\u5165\u9884\u5b9a\u4e49\u7684notebook, Zeppelin\u9875\u9762\u9009\u62e9 import note ,\u9009\u62e9json\u6587\u4ef6 zeppelin_notebook_GaussDB200_test.json \u70b9\u51fb\u53f3\u4fa7\u201c\u6267\u884c\u201d(Shift+Enter)\u6309\u94ae\uff0c\u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Development/Zeppelin_0.8.1/#faq","text":"","title":"FAQ"},{"location":"Development/Zeppelin_0.8.1/#sparklyr","text":"\u6267\u884c\u4ee5\u4e0bshell\u811a\u672c\u83b7\u53d6\u5f85\u5206\u6790\u7684\u6570\u636e(\u6216\u8005\u76f4\u63a5\u7528\u6d4f\u89c8\u5668\u8bbf\u95eehttp://stat-computing.org/dataexpo/2009 \u6765\u4e0b\u8f7d) # Make download directory mkdir /tmp/flights # Download flight data by year for i in { 2006 ..2008 } do echo \" $( date ) $i Download\" fnam = $i .csv.bz2 wget -O /tmp/flights/ $fnam http://stat-computing.org/dataexpo/2009/ $fnam echo \" $( date ) $i Unzip\" bunzip2 /tmp/flights/ $fnam done # Download airline carrier data wget --no-check-certificate -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup = L_UNIQUE_CARRIERS # Download airports data wget --no-check-certificate -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat \u5c06\u4e0b\u8f7d\u4e0b\u6765\u7684/tmp/flights\u76ee\u5f55\u4ee5\u53ca/tmp/airlines.csv\uff0c/tmp/airports.csv\u6587\u4ef6\u4e0a\u4f20\u5230HDFS\u7684/user/developuser\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u5728Hive\u4e2d\u521b\u5efa\u4e09\u5f20\u8868\uff0c\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u5bf9\u5e94\u7684\u8868\u4e2d hdfs dfs -mkdir /user/developuser/flights hdfs dfs -put flights/* /user/developuser/flights/ hdfs dfs -put airlines.csv /user/developuser/ hdfs dfs -put airports.csv /user/developuser/ CREATE EXTERNAL TABLE IF NOT EXISTS flights ( year int , month int , dayofmonth int , dayofweek int , deptime int , crsdeptime int , arrtime int , crsarrtime int , uniquecarrier string , flightnum int , tailnum string , actualelapsedtime int , crselapsedtime int , airtime string , arrdelay int , depdelay int , origin string , dest string , distance int , taxiin string , taxiout string , cancelled int , cancellationcode string , diverted int , carrierdelay string , weatherdelay string , nasdelay string , securitydelay string , lateaircraftdelay string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n' STORED AS TEXTFILE TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/developuser/flights/2006.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/developuser/flights/2007.csv' INTO TABLE flights ; LOAD DATA INPATH '/user/developuser/flights/2008.csv' INTO TABLE flights ; CREATE EXTERNAL TABLE IF NOT EXISTS airlines ( Code string , Description string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE tblproperties ( \"skip.header.line.count\" = \"1\" ); LOAD DATA INPATH '/user/developuser/airlines.csv' INTO TABLE airlines ; CREATE EXTERNAL TABLE IF NOT EXISTS airports ( id string , name string , city string , country string , faa string , icao string , lat double , lon double , alt int , tz_offset double , dst string , tz_name string ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \"separatorChar\" = '\\,' , \"quoteChar\" = '\\\"' ) STORED AS TEXTFILE ; LOAD DATA INPATH '/user/developuser/airports.csv' INTO TABLE airports ;","title":"\u5982\u4f55\u83b7\u53d6\u672c\u6587\u4e2d\u4f7f\u7528sparklyr\u5206\u6790\u7684\u6e90\u6570\u636e"},{"location":"Other/","text":"\u5176\u4ed6 \u00b6 Apache Airflow 1.10.6 \u2194 6.5 1.10.6 \u2194 8.0 Apache Livy 0.5.0 \u2194 C80 0.6.0 \u2194 6.5 0.7.0 \u2194 8.0 GIS Tools for Hadoop 1.0 \u2194 C60 IBM WAS 8.5.5.9 \u2194 C50 Kibana 6.1.3 \u2194 C80 6.1.3 \u2194 8.0 Logstash 6.4.2 \u2194 C80 6.7.1 \u2194 6.5 6.7.1 \u2194 8.0 NeoKylin 6.9 \u2194 C70 7.2 \u2194 C70 Tensorflow 1.15.0 \u2194 6.5 2.1.0 \u2194 6.5 1.15.0 \u2194 8.0 2.1.0 \u2194 8.0 elasticsearch-head 1.0 \u2194 C80 1.0 \u2194 8.0 filebeat 6.5.1 \u2194 C80 6.5.1 \u2194 8.0 librdkafka 1.0 \u2194 6.5 \u751f\u6001\u5bf9\u63a5\u5e38\u89c1\u95ee\u9898\u603b\u7ed3 1.0 \u2194 6.5","title":"Index"},{"location":"Other/#_1","text":"Apache Airflow 1.10.6 \u2194 6.5 1.10.6 \u2194 8.0 Apache Livy 0.5.0 \u2194 C80 0.6.0 \u2194 6.5 0.7.0 \u2194 8.0 GIS Tools for Hadoop 1.0 \u2194 C60 IBM WAS 8.5.5.9 \u2194 C50 Kibana 6.1.3 \u2194 C80 6.1.3 \u2194 8.0 Logstash 6.4.2 \u2194 C80 6.7.1 \u2194 6.5 6.7.1 \u2194 8.0 NeoKylin 6.9 \u2194 C70 7.2 \u2194 C70 Tensorflow 1.15.0 \u2194 6.5 2.1.0 \u2194 6.5 1.15.0 \u2194 8.0 2.1.0 \u2194 8.0 elasticsearch-head 1.0 \u2194 C80 1.0 \u2194 8.0 filebeat 6.5.1 \u2194 C80 6.5.1 \u2194 8.0 librdkafka 1.0 \u2194 6.5 \u751f\u6001\u5bf9\u63a5\u5e38\u89c1\u95ee\u9898\u603b\u7ed3 1.0 \u2194 6.5","title":"\u5176\u4ed6"},{"location":"Other/Airflow/","text":"Apache Airflow\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Airflow 1.10.6 \u2194 FusionInsight HD 6.5 (HDFS/Hive/HeTu) Apache Airflow 1.10.6 \u2194 FusionInsight HD 8.0 (HDFS/Hive) \u8bf4\u660e\uff1a\u5bf9\u63a5hive\u63a8\u8350\u662f\u7528jdbc\u8fde\u63a5\u65b9\u5f0f \u6d4b\u8bd5\u73af\u5883\u63cf\u8ff0 \u00b6 Apache Airflow\u5b89\u88c5\u4e3b\u673a\uff1a 172.16.2.121 \u5bf9\u63a5FI HD\u96c6\u7fa4\uff1a 172.16.5.161-163 \u5b89\u88c5anaconda \u00b6 \u53c2\u8003jupyternotebook\u6216\u8005jupyterhub\u6587\u6863\uff0c\u5b8c\u6210anaconda\u73af\u5883\u7684\u5b89\u88c5\u3002\u8fd9\u91cc\u9009\u7528\u7684\u662fPython2\u7248\u672c\u7684\u5b89\u88c5\u3002\u56e0\u4e3aPython3\u7248\u672c\u5728\u540e\u7eed\u7684\u5b89\u88c5airflow kerberos\u7684\u65f6\u5019\u4f1a\u62a5\u9519\u3002 python3 \u5b89\u88c5\u95ee\u9898\u5355\uff1a https://issues.apache.org/jira/browse/AIRFLOW-5033 \u5b89\u88c5airflow \u00b6 \u5728\u5b89\u88c5\u597d\u7684anaconda\u7684bin\u8def\u5f84\u4e0b /opt/anaconda2/bin \uff0c\u8f93\u5165\u5982\u4e0b\u547d\u4ee4 pip install apache-airflow \u5f00\u59cb\u5b89\u88c5airflow \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\u76f8\u5173\u7684\u4f9d\u8d56 yum install gcc-c++ python-devel.x86_64 cyrus-sasl-devel.x86_64 yum install cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-md5 cyrus-sasl-plain \u5b89\u88c5\u5b8c\u6210\u540e\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\u7ee7\u7eed\u5b89\u88c5airflow\u5bf9kerberos, hive\uff0cgcp\u7684\u989d\u5916\u7279\u6027 pip install apache-airflow[kerberos,hive,gcp] \u5177\u4f53\u4ecb\u7ecd\u8bf7\u53c2\u89c1airflow\u6587\u6863\uff1a https://airflow.apache.org/docs/stable/installation.html \u542f\u52a8airflow: \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521d\u59cb\u5316airflow\u5173\u8054\u7684\u6570\u636e\u5e93 airflow initdb \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8airflow\u7684\u7f51\u7edc\u670d\u52a1 airflow webserver -p 8080 \u53ef\u4ee5\u770b\u5230\u542f\u52a8\u540eairflow\u9ed8\u8ba4\u7684home\u8def\u5f84\u4e3a/root/airflow \u6253\u5f00\u53e6\u5916\u4e00\u4e2a\u7ec8\u7aef\uff0c source ~/.bashrc.anaconda2 \u521d\u59cb\u5b8c\u73af\u5883\u53d8\u91cf\u540e\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8airflow\u7684scheduler\u670d\u52a1 airflow scheduler \u6ce8\u610f\uff1a\u9047\u5230\u5982\u56fe\u62a5\u9519\u4e0d\u5f71\u54cd\u4f7f\u7528 \u767b\u9646airflow\u7684webUI\u68c0\u67e5: kerberos\u76f8\u5173\u914d\u7f6e \u00b6 \u505c\u6b62\u5df2\u7ecf\u542f\u52a8\u7684airflow \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4FI HD manager\u521b\u5efa\u6d4b\u8bd5\u7528\u6237airflow\uff0c\u5e76\u4e14\u4e0b\u8f7d\u5bf9\u5e94\u7684user.keytab\u6587\u4ef6\u4ee5\u53cakrb5.conf\u6587\u4ef6\u3002\u5c06user.keytab\u6587\u4ef6\u91cd\u547d\u540d\u4e3aairflow.keytab\u6587\u4ef6 \u5c06\u4e0a\u4e00\u6b65\u83b7\u5f97\u7684airflow.ketab\u6587\u4ef6\u548ckrb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230airflow\u4e3b\u673a\u7684/opt\u8def\u5f84 \u767b\u9646\u8def\u5f84/root/airflow\u4fee\u6539\u914d\u7f6e\u6587\u4ef6airflow.cfg\uff1a \u4fee\u6539kerberos\u76f8\u5173\u914d\u7f6e [kerberos] ccache = /tmp/krb5cc_0 # gets augmented with fqdn principal = airflow reinit_frequency = 3600 kinit_path = /opt/165_651hdclient/hadoopclient/KrbClient/kerberos/bin/kinit keytab = /opt/airflow.keytab \u5176\u4e2d /opt/165_651hdclient/hadoopclient/KrbClient/kerberos/bin/kinit \u4e3a\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef\u5bf9\u5e94\u7684kinit\u6587\u4ef6 \u4fee\u6539security\u76f8\u5173\u4f60\u914d\u7f6e\uff1a airflow\u7684\u8c03\u5ea6\u4f1a\u4f7f\u7528dag\u6587\u4ef6\uff0c\u9700\u8981\u5728 /root/airflow \u8def\u5f84\u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 dags \u6587\u4ef6\u5939\uff1a airflow\u4e2dhive\u76f8\u5173connection\u914d\u7f6e \u00b6 \u8bf4\u660e\uff08\u91cd\u8981\uff09\uff1a\u672c\u8282\u6d4b\u8bd53\u79cd\u53ef\u4ee5\u548chive\u8fde\u63a5\u7684\u65b9\u5f0f\uff0c\u7b2c1\u79cd\u548c\u7b2c2\u4e2d\u9700\u8981\u66f4\u6539airflow\u4e3b\u673a/etc/hosts\u6587\u4ef6\uff0c\u6240\u4ee5\u4e0d\u80fd\u4e0e\u5176\u4ed6\u7684\u8fde\u63a5\u65b9\u5f0f\u517c\u5bb9\uff08\u6bd4\u5982hdfs\uff09\uff0c\u5728\u5b9e\u9645\u8fd0\u7528\u4e2d\u63a8\u8350\u4f7f\u7528jdbc\u7684\u8fde\u63a5\u65b9\u5f0f\u6765\u8fdb\u884c\u8fde\u63a5\u3002 airflow\u4e2dhive metastore connection\u914d\u7f6e \u00b6 \u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ahive metastore\u7684connection\uff1a airflow connections --add \\ --conn_id metastore_cluster1 \\ --conn_type 'hive_metastore' \\ --conn_host '172.16.4.162' \\ --conn_port 21088 \\ --conn_extra '{\"authMechanism\":\"GSSAPI\", \"kerberos_service_name\":\"hive\"}' \u6ce8\u610f\uff1ametastore_cluster1\u4e3aairflow\u7684conn_id\uff0c\u540e\u7eed\u914d\u7f6edag\u65f6\u9700\u7528\u5230\uff0c\u4e0d\u80fd\u4e0e\u5df2\u6709\u7684conn_id\u91cd\u540d \u53ef\u767b\u9646airflow\u7684webUI\u68c0\u67e5\u589e\u52a0\u7684hive metastore connection\u914d\u7f6e\uff1a \u4fee\u6539airflow\u4e3b\u673a\u7684/etc/hosts\u6587\u4ef6: \u6ce8\u610f\uff1a\u9700\u8981\u5c06\u5bf9\u5e94\u8fde\u63a5\u7684\u4e3b\u673a\u540d\u4e4b\u524d\u7684\u6ce8\u91ca\u6389\uff0c\u6539\u6210hadoop.hadoop.com \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import hive_hooks hm = hive_hooks.HiveMetastoreHook(metastore_conn_id='metastore_cluster1') hm.get_databases() t=hm.get_table(db='default', table_name='iris') t.tableName [col.name for col in t.sd.cols] airflow\u4e2dhiveserver2 connection\u914d\u7f6e \u00b6 \u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ahiveserver2\u7684connection\uff1a airflow connections --add \\ --conn_id hiveserver2_test \\ --conn_type 'hiveserver2' \\ --conn_host '172.16.4.162' \\ --conn_port 21066 \\ --conn_extra '{\"authMechanism\":\"KERBEROS\", \"kerberos_service_name\":\"hive\"}' \u6ce8\u610f\uff1ahiveserver2_test\u4e3aairflow\u7684conn_id\uff0c\u540e\u7eed\u914d\u7f6edag\u65f6\u9700\u7528\u5230\uff0c\u4e0d\u80fd\u4e0e\u5df2\u6709\u7684conn_id\u91cd\u540d \u767b\u9646airflow\u7684webUI\u68c0\u67e5\u589e\u52a0\u7684hiveserver2 connection\u914d\u7f6e\uff1a \u505a\u5982\u4e0b\u589e\u52a0\u4fee\u6539\u5e76\u4fdd\u5b58\uff1a \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import hive_hooks hh = hive_hooks.HiveServer2Hook(hiveserver2_conn_id='hiveserver2_test') sql = \"SELECT * FROM default.iris\" len(hh.get_records(sql)) hh.get_records(sql) airflow\u4e2djdbc connection\u5bf9\u63a5hive\u914d\u7f6e \u00b6 \u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/165_651hdclient/hadoopclient/bigdata_env kinit airflow \u914d\u7f6e /opt/jaas.conf \u6587\u4ef6 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5bfc\u5165jvm\u53c2\u6570\uff1a export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ajdbc\u7684connection\u540d\u5b57\u53eb\u505ahive_jdbc\uff1a airflow connections --add \\ --conn_id hive_jdbc \\ --conn_type 'jdbc' \u767b\u9646airflow\u7684webUI\u914d\u7f6ehive_jdbc\uff0c\u53c2\u8003\u56fe\u914d\u7f6e\u597d\u53c2\u6570\u70b9save\u4fdd\u5b58 1. hive_jdbc 2. Jdbc Connection 3. jdbc:hive2://172.16.4.161:24002,172.16.4.162:24002,172.16.4.163:24002/default;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=airflow;user.keytab=/opt/airflow.keytab 4. /opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-jdbc-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/ant-1.10.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cglib-3.2.10.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/common-0.0.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-collections-3.2.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-collections4-4.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-configuration-1.6.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-configuration2-2.1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-io-2.4.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-lang-2.6.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-lang3-3.3.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-logging-1.1.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-net-3.6.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/crypter-0.0.6.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/curator-client-2.12.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/curator-framework-2.12.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-core-3.1.16.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-rt-frontend-jaxrs-3.1.16.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-rt-transports-http-3.1.16.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/FMS-v1r2c60-20160429.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/guava-19.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-auth-3.1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-common-3.1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-mapreduce-client-core-3.1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/HA-v1r2c60-20160429.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-common-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-metastore-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-serde-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-service-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-service-rpc-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-shims-0.23-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-shims-common-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-standalone-metastore-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/httpclient-4.5.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/httpcore-4.4.4.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-annotations-2.9.8.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-core-2.9.8.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-core-asl-1.9.13.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-databind-2.9.8.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-jaxrs-1.9.13.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-mapper-asl-1.9.13.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/javax.annotation-api-1.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/javax.ws.rs-api-2.0.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jdbc_pom.xml,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jettison-1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jsch-0.1.54.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/libthrift-0.9.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/log4j-1.2.17.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/mockito-all-1.10.19.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/netty-all-4.1.17.Final.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/om-controller-api-0.0.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/om-monitor-plugin-0.0.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/pms-v1r2c60-20160429.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/protobuf-java-2.5.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/slf4j-api-1.7.10.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/slf4j-log4j12-1.7.5.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-aop-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-beans-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-context-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-core-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-expression-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/stax2-api-3.1.4.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/stax-api-1.0-2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/woodstox-core-5.0.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/woodstox-core-asl-4.4.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xercesImpl-2.9.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xmlpull-1.1.3.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xmlschema-core-2.2.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xpp3_min-1.1.4c.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xstream-1.4.10.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/zookeeper-3.5.1.jar 5. org.apache.hive.jdbc.HiveDriver \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import jdbc_hook jh = jdbc_hook.JdbcHook(jdbc_conn_id='hive_jdbc') jh.get_conn() jh.get_records(\"select * from test\") airflow\u4e2dhetu\u76f8\u5173connection\u914d\u7f6e \u00b6 airflow\u4e2djdbc connection\u5bf9\u63a5hetu\u914d\u7f6e \u00b6 \u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/165_651hdclient/hadoopclient/bigdata_env kinit airflow \u914d\u7f6e /opt/jaas.conf \u6587\u4ef6 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5bfc\u5165jvm\u53c2\u6570\uff1a export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ajdbc\u7684connection\u540d\u5b57\u53eb\u505ahetu\uff1a airflow connections --add \\ --conn_id hetu \\ --conn_type 'jdbc' \u767b\u9646airflow\u7684webUI\u914d\u7f6ehetu\uff0c\u53c2\u8003\u56fe\u914d\u7f6e\u597d\u53c2\u6570\u70b9save\u4fdd\u5b58 1. hetu 2. Jdbc Connection 3. jdbc:presto://172.16.4.161:24002,172.16.4.162:24002,172.16.4.163:24002/hive/default?serviceDiscoveryMode=zooKeeper&zooKeeperNamespace=hsbroker&deploymentMode=on_yarn&SSL=true&SSLTrustStorePath=/opt/hetuserver.jks&KerberosConfigPath=/opt/krb5.conf&KerberosPrincipal=airflow&KerberosKeytabPath=/opt/airflow.keytab&KerberosRemoteServiceName=HTTP&KerberosServicePrincipalPattern=%24%7BSERVICE%7D%40%24%7BHOST%7D 4. /opt/presto-jdbc-316.jar 5. io.prestosql.jdbc.PrestoDriver \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import jdbc_hook jh = jdbc_hook.JdbcHook(jdbc_conn_id='hetu') jh.get_conn() jh.get_records(\"select * from test\") airflow\u4e2dhdfs\u76f8\u5173connection\u914d\u7f6e \u00b6 \u914d\u7f6eairflow\u4e2dwebhdfs connection\u914d\u7f6e \u00b6 \u56e0\u4e3a\u6b64\u6b21\u4f7f\u7528webhdfs\u7684http\u8fde\u63a5\u65b9\u5f0f\u5bf9\u63a5\u96c6\u7fa4\uff0c\u9996\u5148\u5148\u68c0\u67e5\u96c6\u7fa4\u914d\u7f6e\u9879\u662f\u5426\u7b26\u5408\u8981\u6c42\uff1a \u767b\u5f55FusionInsight Manager\u9875\u9762\uff0c\u5355\u51fb\u201c\u96c6\u7fa4 > \u5f85\u64cd\u4f5c\u96c6\u7fa4\u7684\u540d\u79f0 > \u670d\u52a1 > HDFS > \u914d\u7f6e >\u5168\u90e8\u914d\u7f6e\u201d\uff0c\u5728\u201c\u641c\u7d22\u201d\u6846\u91cc\u641c\u7d22\u201cdfs.http.policy\u201d\uff0c\u7136\u540e\u52fe\u9009\u201cHTTP_AND_HTTPS\u201d\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u5355\u51fb\u201c\u66f4\u591a > \u91cd\u542f\u201d\u91cd\u542fHDFS\u670d\u52a1 \u5982\u679c\u5df2\u7ecf\u914d\u7f6e\u7565\u8fc7\u6b64\u6b65\u9aa4 \u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ahdfs\u7684connection\u540d\u5b57\u53eb\u505awebhdfs_fusioninsight1\uff1a airflow connections --add \\ --conn_id webhdfs_fusioninsight1 \\ --conn_type 'hdfs' \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import webhdfs_hook whdfs = webhdfs_hook.WebHDFSHook(webhdfs_conn_id='webhdfs_fusioninsight1') whdfs.get_conn() \u767b\u9646airflow\u7684webUI\u914d\u7f6ewebhdfs_fusioninsight1\uff0c\u53c2\u8003\u56fe\u914d\u7f6e\u597d\u53c2\u6570\u70b9save\u4fdd\u5b58 \u4e0b\u9762\u5728airflow\u4e3b\u673a\u521b\u5efa\u8def\u5f84 /opt/loadfile ,\u521b\u5efa\u4e00\u4e2a iris.txt \u6587\u4ef6\u5185\u5bb9\uff1a 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import webhdfs_hook whdfs = webhdfs_hook.WebHDFSHook(webhdfs_conn_id='webhdfs_fusioninsight1') whdfs.get_conn() \u4e0b\u9762\u6d4b\u8bd5\u4e00\u4e0b\u6570\u636e\u5bfc\u5165\u7684\u6837\u4f8b\uff0c\u63a5\u7740\u4e0a\u9762\u7684\u547d\u4ee4\u7ee7\u7eed\u8f93\u5165 whdfs.load_file('/opt/loadfile/','/tmp/') \u767b\u9646\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u68c0\u67e5\uff1a \u914d\u7f6eairflow\u4e2d\u7684DAG \u00b6 \u8bf4\u660e\uff1aairflow\u4f7f\u7528dag\u6765\u63a7\u5236\u8c03\u5ea6\uff0c\u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a .py \u6587\u4ef6\uff0c\u4f7f\u7528\u65f6\u9700\u8981\u7f16\u5199\u4efb\u52a1\u6d41\u7684py\u6587\u4ef6\u540e\u518d\u6267\u884c \u767b\u9646airflow\u4e3b\u673a /opt/airflow/dags \u8def\u5f84\uff0c\u65b0\u5efa\u4e00\u4e2ahivetest.py\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a #!/usr/bin/env python # -*- coding: utf-8 -*- from datetime import datetime, timedelta from airflow.operators.python_operator import PythonOperator, BranchPythonOperator from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.hive_operator import HiveOperator from airflow.hooks import hive_hooks def test_select_conn(): from airflow.hooks.hive_hooks import HiveServer2Hook hook = hive_hooks.HiveServer2Hook(hiveserver2_conn_id='hiveserver2_test') sql = \"\"\"SELECT * FROM iris\"\"\" records = hook.get_records(sql) return records default_args = { 'owner': 'airflow', 'start_date': datetime(2019, 1, 21), } dag = DAG( 'hive_server_run_test', default_args=default_args, schedule_interval='@once') t1 = BashOperator( task_id='print_date', bash_command='date', dag=dag) t2 = BranchPythonOperator( task_id='hive_server_run', python_callable=test_select_conn, dag=dag) t2.set_upstream(t1) \u6ce8\u610f:\u8be5\u4efb\u52a1\u6d41\u6709\u4e24\u4e2a\u52a8\u4f5c\uff0c\u7b2c\u4e00\u4e2a\u662ft1\uff0c\u5728\u547d\u4ee4\u884c\u4e2d\u8f93\u5165data\u547d\u4ee4\u6253\u5370\u51fa\u5f53\u524d\u65f6\u95f4\uff0c\u7b2c\u4e8c\u4e2a\u52a8\u4f5c\u662ft2\uff0c\u6253\u5370\u51fa select * from iris \u67e5\u8be2\u8bed\u53e5\uff0c\u4e24\u4e2a\u52a8\u4f5c\u6709\u5bf9\u5e94\u7684task_id \u53c2\u8003airflow\u5b98\u65b9\u6587\u6863\uff1a https://airflow.apache.org/docs/stable/tutorial.html \u4f7f\u7528\u547d\u4ee4 airflow test hive_server_run_test hive_server_run 2019-12-17 \u68c0\u67e5hive\u8fde\u63a5: \u5176\u4e2dhive_server_run_test\u662fdag id, hive_server_run\u662f task id \u4e0a\u8ff0test\u6b65\u9aa4\u6d4b\u8bd5\u6210\u529f\u4e4b\u540e\uff0c\u4f7f\u7528\u547d\u4ee4 airflow backfill hive_server_run_test -s 2019-12-17 -e 2019-12-17 \u63d0\u4ea4\u4efb\u52a1\uff1a \u53bbwebUI\u68c0\u67e5\uff1a \u70b9\u51fbprint_data\u4ee5\u53cahive_server_run\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a print_data: hive_server_run: FAQ \u00b6 \u5728\u300a\u5b89\u88c5airflow\u300b\u7684\u65f6\u5019\u8f93\u5165 pip install apache-airflow \u4f1a\u9047\u5230\u9519\u8bef\uff1a \u9700\u8981\u628amarshmallow\u91cd\u65b0\u88c5\u62102.18.0\u7684\u7248\u672c\uff0c\u4f7f\u7528\u547d\u4ee4 ./pip install marshmallow==2.18.0 --force-reinstall \u5728anaconda2\u7684bin\u76ee\u5f55\u4e0b\u91cd\u65b0\u5b89\u88c5\u6b63\u786e\u7248\u672c\u95ee\u9898\u89e3\u51b3 \u5728\u300a\u5b89\u88c5airflow\u300b\u7684\u65f6\u5019\u8f93\u5165 pip install apache-airflow[kerberos,hive,gcp] \u4f1a\u9047\u5230\u9519\u8bef\uff1a \u95ee\u9898\u5728\u4e8e \"#include \"\u6ca1\u6709\u5b89\u88c5\u6210\u529f\uff0c\u53c2\u8003 https://blog.csdn.net/coder_gray/article/details/77189002 \uff0c \u770binclude\u7f3a\u7684\u662f\u4ec0\u4e48\u5c31\u5b89\u88c5\u4ec0\u4e48\uff0c\u8f93\u5165 yum install gcc-c++ python-devel.x86_64 cyrus-sasl-devel.x86_64 \u5148\u5b89\u88c5yum\u4f9d\u8d56\uff0c\u5b8c\u4e86\u4e4b\u540e\u518d\u91cd\u65b0\u6267\u884c pip install apache-airflow[kerberos,hive,gcp] \u95ee\u9898\u89e3\u51b3 \u5728\u300aairflow\u4e2dhive\u76f8\u5173connection\u914d\u7f6e\u300b\u4e2d\u8c03\u8bd5\u7684\u65f6\u5019\uff0c\u8f93\u5165 hm.get_databases() \u9047\u5230\u62a5\u9519\uff1a thrift.transport.TTransport.TTransportException: Could not start SASL: Error in sasl_client_start (-4) SASL(-4): no mechanism available: No worthy mechs found \u8fd9\u4e2a\u95ee\u9898\u662fpython\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u53c2\u8003\uff1a https://stackoverflow.com/questions/30705576/python-cannot-connect-hiveserver2/30707252 \u5b89\u88c5\u547d\u4ee4\uff1a sudo yum install cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-md5 cyrus-sasl-plain \u5b89\u88c5\u5916\u4f9d\u8d56\u540e\u95ee\u9898\u89e3\u51b3 \u5728\u505awebhdfs\u7684\u65f6\u5019\u4f7f\u7528python\u547d\u4ee4\u5728\u540e\u53f0\u8c03\u6d4b\u9047\u5230\u95ee\u9898\uff1a ImportError: No module named hdfs \u89e3\u51b3\u529e\u6cd5: \u53bb/opt/anaconda2/bin\u4e0b\u9762\u4f7f\u7528\u547d\u4ee4 pip install hdfs \u540e\u91cd\u65b0\u6267\u884c\u7a0b\u5e8f\u95ee\u9898\u89e3\u51b3 \u5728\u9a8c\u8bc1jdbc\u94fe\u63a5\u7684\u65f6\u5019\u9047\u5230\u62a5\u9519\uff1a \u627e\u4e0d\u5230jdbc\u9a71\u52a8\u4e3b\u7c7b\uff0c\u5e76\u4e14\u5728\u914d\u7f6e\u4e0a\u5df2\u7ecf\u5236\u5b9a\u4e86\u6b63\u786e\u7684jar\u5305\u8def\u5f84 \u89e3\u51b3\u529e\u6cd5\uff1a \u53c2\u8003 https://www.reddit.com/r/dataengineering/comments/aqfvug/airflow_issues_setting_up_a_jdbc_connection_to/ \u5728\u4f7f\u7528python\u547d\u4ee4\u540e\u53f0\u6d4b\u8bd5\u524d kinit /opt/hadoopclient/bigdata_env \u518d\u6d4b\u8bd5\u95ee\u9898\u89e3\u51b3","title":"1.10.6 <--> 8.0"},{"location":"Other/Airflow/#apache-airflowfusioninsight","text":"","title":"Apache Airflow\u5bf9\u63a5FusionInsight"},{"location":"Other/Airflow/#_1","text":"Apache Airflow 1.10.6 \u2194 FusionInsight HD 6.5 (HDFS/Hive/HeTu) Apache Airflow 1.10.6 \u2194 FusionInsight HD 8.0 (HDFS/Hive) \u8bf4\u660e\uff1a\u5bf9\u63a5hive\u63a8\u8350\u662f\u7528jdbc\u8fde\u63a5\u65b9\u5f0f","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Airflow/#_2","text":"Apache Airflow\u5b89\u88c5\u4e3b\u673a\uff1a 172.16.2.121 \u5bf9\u63a5FI HD\u96c6\u7fa4\uff1a 172.16.5.161-163","title":"\u6d4b\u8bd5\u73af\u5883\u63cf\u8ff0"},{"location":"Other/Airflow/#anaconda","text":"\u53c2\u8003jupyternotebook\u6216\u8005jupyterhub\u6587\u6863\uff0c\u5b8c\u6210anaconda\u73af\u5883\u7684\u5b89\u88c5\u3002\u8fd9\u91cc\u9009\u7528\u7684\u662fPython2\u7248\u672c\u7684\u5b89\u88c5\u3002\u56e0\u4e3aPython3\u7248\u672c\u5728\u540e\u7eed\u7684\u5b89\u88c5airflow kerberos\u7684\u65f6\u5019\u4f1a\u62a5\u9519\u3002 python3 \u5b89\u88c5\u95ee\u9898\u5355\uff1a https://issues.apache.org/jira/browse/AIRFLOW-5033","title":"\u5b89\u88c5anaconda"},{"location":"Other/Airflow/#airflow","text":"\u5728\u5b89\u88c5\u597d\u7684anaconda\u7684bin\u8def\u5f84\u4e0b /opt/anaconda2/bin \uff0c\u8f93\u5165\u5982\u4e0b\u547d\u4ee4 pip install apache-airflow \u5f00\u59cb\u5b89\u88c5airflow \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\u76f8\u5173\u7684\u4f9d\u8d56 yum install gcc-c++ python-devel.x86_64 cyrus-sasl-devel.x86_64 yum install cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-md5 cyrus-sasl-plain \u5b89\u88c5\u5b8c\u6210\u540e\u8f93\u5165\u5982\u4e0b\u547d\u4ee4\u7ee7\u7eed\u5b89\u88c5airflow\u5bf9kerberos, hive\uff0cgcp\u7684\u989d\u5916\u7279\u6027 pip install apache-airflow[kerberos,hive,gcp] \u5177\u4f53\u4ecb\u7ecd\u8bf7\u53c2\u89c1airflow\u6587\u6863\uff1a https://airflow.apache.org/docs/stable/installation.html \u542f\u52a8airflow: \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521d\u59cb\u5316airflow\u5173\u8054\u7684\u6570\u636e\u5e93 airflow initdb \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8airflow\u7684\u7f51\u7edc\u670d\u52a1 airflow webserver -p 8080 \u53ef\u4ee5\u770b\u5230\u542f\u52a8\u540eairflow\u9ed8\u8ba4\u7684home\u8def\u5f84\u4e3a/root/airflow \u6253\u5f00\u53e6\u5916\u4e00\u4e2a\u7ec8\u7aef\uff0c source ~/.bashrc.anaconda2 \u521d\u59cb\u5b8c\u73af\u5883\u53d8\u91cf\u540e\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8airflow\u7684scheduler\u670d\u52a1 airflow scheduler \u6ce8\u610f\uff1a\u9047\u5230\u5982\u56fe\u62a5\u9519\u4e0d\u5f71\u54cd\u4f7f\u7528 \u767b\u9646airflow\u7684webUI\u68c0\u67e5:","title":"\u5b89\u88c5airflow"},{"location":"Other/Airflow/#kerberos","text":"\u505c\u6b62\u5df2\u7ecf\u542f\u52a8\u7684airflow \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4FI HD manager\u521b\u5efa\u6d4b\u8bd5\u7528\u6237airflow\uff0c\u5e76\u4e14\u4e0b\u8f7d\u5bf9\u5e94\u7684user.keytab\u6587\u4ef6\u4ee5\u53cakrb5.conf\u6587\u4ef6\u3002\u5c06user.keytab\u6587\u4ef6\u91cd\u547d\u540d\u4e3aairflow.keytab\u6587\u4ef6 \u5c06\u4e0a\u4e00\u6b65\u83b7\u5f97\u7684airflow.ketab\u6587\u4ef6\u548ckrb5.conf\u6587\u4ef6\u62f7\u8d1d\u5230airflow\u4e3b\u673a\u7684/opt\u8def\u5f84 \u767b\u9646\u8def\u5f84/root/airflow\u4fee\u6539\u914d\u7f6e\u6587\u4ef6airflow.cfg\uff1a \u4fee\u6539kerberos\u76f8\u5173\u914d\u7f6e [kerberos] ccache = /tmp/krb5cc_0 # gets augmented with fqdn principal = airflow reinit_frequency = 3600 kinit_path = /opt/165_651hdclient/hadoopclient/KrbClient/kerberos/bin/kinit keytab = /opt/airflow.keytab \u5176\u4e2d /opt/165_651hdclient/hadoopclient/KrbClient/kerberos/bin/kinit \u4e3a\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef\u5bf9\u5e94\u7684kinit\u6587\u4ef6 \u4fee\u6539security\u76f8\u5173\u4f60\u914d\u7f6e\uff1a airflow\u7684\u8c03\u5ea6\u4f1a\u4f7f\u7528dag\u6587\u4ef6\uff0c\u9700\u8981\u5728 /root/airflow \u8def\u5f84\u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 dags \u6587\u4ef6\u5939\uff1a","title":"kerberos\u76f8\u5173\u914d\u7f6e"},{"location":"Other/Airflow/#airflowhiveconnection","text":"\u8bf4\u660e\uff08\u91cd\u8981\uff09\uff1a\u672c\u8282\u6d4b\u8bd53\u79cd\u53ef\u4ee5\u548chive\u8fde\u63a5\u7684\u65b9\u5f0f\uff0c\u7b2c1\u79cd\u548c\u7b2c2\u4e2d\u9700\u8981\u66f4\u6539airflow\u4e3b\u673a/etc/hosts\u6587\u4ef6\uff0c\u6240\u4ee5\u4e0d\u80fd\u4e0e\u5176\u4ed6\u7684\u8fde\u63a5\u65b9\u5f0f\u517c\u5bb9\uff08\u6bd4\u5982hdfs\uff09\uff0c\u5728\u5b9e\u9645\u8fd0\u7528\u4e2d\u63a8\u8350\u4f7f\u7528jdbc\u7684\u8fde\u63a5\u65b9\u5f0f\u6765\u8fdb\u884c\u8fde\u63a5\u3002","title":"airflow\u4e2dhive\u76f8\u5173connection\u914d\u7f6e"},{"location":"Other/Airflow/#airflowhive-metastore-connection","text":"\u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ahive metastore\u7684connection\uff1a airflow connections --add \\ --conn_id metastore_cluster1 \\ --conn_type 'hive_metastore' \\ --conn_host '172.16.4.162' \\ --conn_port 21088 \\ --conn_extra '{\"authMechanism\":\"GSSAPI\", \"kerberos_service_name\":\"hive\"}' \u6ce8\u610f\uff1ametastore_cluster1\u4e3aairflow\u7684conn_id\uff0c\u540e\u7eed\u914d\u7f6edag\u65f6\u9700\u7528\u5230\uff0c\u4e0d\u80fd\u4e0e\u5df2\u6709\u7684conn_id\u91cd\u540d \u53ef\u767b\u9646airflow\u7684webUI\u68c0\u67e5\u589e\u52a0\u7684hive metastore connection\u914d\u7f6e\uff1a \u4fee\u6539airflow\u4e3b\u673a\u7684/etc/hosts\u6587\u4ef6: \u6ce8\u610f\uff1a\u9700\u8981\u5c06\u5bf9\u5e94\u8fde\u63a5\u7684\u4e3b\u673a\u540d\u4e4b\u524d\u7684\u6ce8\u91ca\u6389\uff0c\u6539\u6210hadoop.hadoop.com \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import hive_hooks hm = hive_hooks.HiveMetastoreHook(metastore_conn_id='metastore_cluster1') hm.get_databases() t=hm.get_table(db='default', table_name='iris') t.tableName [col.name for col in t.sd.cols]","title":"airflow\u4e2dhive metastore connection\u914d\u7f6e"},{"location":"Other/Airflow/#airflowhiveserver2-connection","text":"\u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ahiveserver2\u7684connection\uff1a airflow connections --add \\ --conn_id hiveserver2_test \\ --conn_type 'hiveserver2' \\ --conn_host '172.16.4.162' \\ --conn_port 21066 \\ --conn_extra '{\"authMechanism\":\"KERBEROS\", \"kerberos_service_name\":\"hive\"}' \u6ce8\u610f\uff1ahiveserver2_test\u4e3aairflow\u7684conn_id\uff0c\u540e\u7eed\u914d\u7f6edag\u65f6\u9700\u7528\u5230\uff0c\u4e0d\u80fd\u4e0e\u5df2\u6709\u7684conn_id\u91cd\u540d \u767b\u9646airflow\u7684webUI\u68c0\u67e5\u589e\u52a0\u7684hiveserver2 connection\u914d\u7f6e\uff1a \u505a\u5982\u4e0b\u589e\u52a0\u4fee\u6539\u5e76\u4fdd\u5b58\uff1a \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import hive_hooks hh = hive_hooks.HiveServer2Hook(hiveserver2_conn_id='hiveserver2_test') sql = \"SELECT * FROM default.iris\" len(hh.get_records(sql)) hh.get_records(sql)","title":"airflow\u4e2dhiveserver2 connection\u914d\u7f6e"},{"location":"Other/Airflow/#airflowjdbc-connectionhive","text":"\u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/165_651hdclient/hadoopclient/bigdata_env kinit airflow \u914d\u7f6e /opt/jaas.conf \u6587\u4ef6 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5bfc\u5165jvm\u53c2\u6570\uff1a export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ajdbc\u7684connection\u540d\u5b57\u53eb\u505ahive_jdbc\uff1a airflow connections --add \\ --conn_id hive_jdbc \\ --conn_type 'jdbc' \u767b\u9646airflow\u7684webUI\u914d\u7f6ehive_jdbc\uff0c\u53c2\u8003\u56fe\u914d\u7f6e\u597d\u53c2\u6570\u70b9save\u4fdd\u5b58 1. hive_jdbc 2. Jdbc Connection 3. jdbc:hive2://172.16.4.161:24002,172.16.4.162:24002,172.16.4.163:24002/default;serviceDiscoveryMode=zooKeeper;principal=hive/hadoop.hadoop.com@HADOOP.COM;user.principal=airflow;user.keytab=/opt/airflow.keytab 4. /opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-jdbc-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/ant-1.10.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cglib-3.2.10.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/common-0.0.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-collections-3.2.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-collections4-4.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-configuration-1.6.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-configuration2-2.1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-io-2.4.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-lang-2.6.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-lang3-3.3.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-logging-1.1.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/commons-net-3.6.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/crypter-0.0.6.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/curator-client-2.12.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/curator-framework-2.12.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-core-3.1.16.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-rt-frontend-jaxrs-3.1.16.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/cxf-rt-transports-http-3.1.16.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/FMS-v1r2c60-20160429.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/guava-19.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-auth-3.1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-common-3.1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hadoop-mapreduce-client-core-3.1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/HA-v1r2c60-20160429.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-common-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-metastore-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-serde-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-service-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-service-rpc-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-shims-0.23-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-shims-common-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/hive-standalone-metastore-3.1.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/httpclient-4.5.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/httpcore-4.4.4.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-annotations-2.9.8.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-core-2.9.8.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-core-asl-1.9.13.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-databind-2.9.8.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-jaxrs-1.9.13.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jackson-mapper-asl-1.9.13.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/javax.annotation-api-1.2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/javax.ws.rs-api-2.0.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jdbc_pom.xml,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jettison-1.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/jsch-0.1.54.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/libthrift-0.9.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/log4j-1.2.17.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/mockito-all-1.10.19.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/netty-all-4.1.17.Final.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/om-controller-api-0.0.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/om-monitor-plugin-0.0.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/pms-v1r2c60-20160429.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/protobuf-java-2.5.0.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/slf4j-api-1.7.10.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/slf4j-log4j12-1.7.5.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-aop-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-beans-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-context-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-core-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/spring-expression-4.3.20.RELEASE.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/stax2-api-3.1.4.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/stax-api-1.0-2.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/woodstox-core-5.0.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/woodstox-core-asl-4.4.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xercesImpl-2.9.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xmlpull-1.1.3.1.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xmlschema-core-2.2.3.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xpp3_min-1.1.4c.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/xstream-1.4.10.jar,/opt/165_651hdclient/hadoopclient/Hive/Beeline/lib/jdbc/zookeeper-3.5.1.jar 5. org.apache.hive.jdbc.HiveDriver \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import jdbc_hook jh = jdbc_hook.JdbcHook(jdbc_conn_id='hive_jdbc') jh.get_conn() jh.get_records(\"select * from test\")","title":"airflow\u4e2djdbc connection\u5bf9\u63a5hive\u914d\u7f6e"},{"location":"Other/Airflow/#airflowhetuconnection","text":"","title":"airflow\u4e2dhetu\u76f8\u5173connection\u914d\u7f6e"},{"location":"Other/Airflow/#airflowjdbc-connectionhetu","text":"\u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/165_651hdclient/hadoopclient/bigdata_env kinit airflow \u914d\u7f6e /opt/jaas.conf \u6587\u4ef6 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5bfc\u5165jvm\u53c2\u6570\uff1a export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=120000\" \u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ajdbc\u7684connection\u540d\u5b57\u53eb\u505ahetu\uff1a airflow connections --add \\ --conn_id hetu \\ --conn_type 'jdbc' \u767b\u9646airflow\u7684webUI\u914d\u7f6ehetu\uff0c\u53c2\u8003\u56fe\u914d\u7f6e\u597d\u53c2\u6570\u70b9save\u4fdd\u5b58 1. hetu 2. Jdbc Connection 3. jdbc:presto://172.16.4.161:24002,172.16.4.162:24002,172.16.4.163:24002/hive/default?serviceDiscoveryMode=zooKeeper&zooKeeperNamespace=hsbroker&deploymentMode=on_yarn&SSL=true&SSLTrustStorePath=/opt/hetuserver.jks&KerberosConfigPath=/opt/krb5.conf&KerberosPrincipal=airflow&KerberosKeytabPath=/opt/airflow.keytab&KerberosRemoteServiceName=HTTP&KerberosServicePrincipalPattern=%24%7BSERVICE%7D%40%24%7BHOST%7D 4. /opt/presto-jdbc-316.jar 5. io.prestosql.jdbc.PrestoDriver \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import jdbc_hook jh = jdbc_hook.JdbcHook(jdbc_conn_id='hetu') jh.get_conn() jh.get_records(\"select * from test\")","title":"airflow\u4e2djdbc connection\u5bf9\u63a5hetu\u914d\u7f6e"},{"location":"Other/Airflow/#airflowhdfsconnection","text":"","title":"airflow\u4e2dhdfs\u76f8\u5173connection\u914d\u7f6e"},{"location":"Other/Airflow/#airflowwebhdfs-connection","text":"\u56e0\u4e3a\u6b64\u6b21\u4f7f\u7528webhdfs\u7684http\u8fde\u63a5\u65b9\u5f0f\u5bf9\u63a5\u96c6\u7fa4\uff0c\u9996\u5148\u5148\u68c0\u67e5\u96c6\u7fa4\u914d\u7f6e\u9879\u662f\u5426\u7b26\u5408\u8981\u6c42\uff1a \u767b\u5f55FusionInsight Manager\u9875\u9762\uff0c\u5355\u51fb\u201c\u96c6\u7fa4 > \u5f85\u64cd\u4f5c\u96c6\u7fa4\u7684\u540d\u79f0 > \u670d\u52a1 > HDFS > \u914d\u7f6e >\u5168\u90e8\u914d\u7f6e\u201d\uff0c\u5728\u201c\u641c\u7d22\u201d\u6846\u91cc\u641c\u7d22\u201cdfs.http.policy\u201d\uff0c\u7136\u540e\u52fe\u9009\u201cHTTP_AND_HTTPS\u201d\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u5355\u51fb\u201c\u66f4\u591a > \u91cd\u542f\u201d\u91cd\u542fHDFS\u670d\u52a1 \u5982\u679c\u5df2\u7ecf\u914d\u7f6e\u7565\u8fc7\u6b64\u6b65\u9aa4 \u91cd\u542fairflow \u91cd\u65b0\u6253\u5f00\u4e00\u4e2a\u7ec8\u7aef\uff0c\u521d\u59cb\u5316\u73af\u5883\u53d8\u91cf\uff08source ~/.bashrc.anaconda2\uff09\uff0c\u8f93\u5165\u4ee5\u4e0b\u547d\u4ee4\u589e\u52a0\u4e00\u4e2ahdfs\u7684connection\u540d\u5b57\u53eb\u505awebhdfs_fusioninsight1\uff1a airflow connections --add \\ --conn_id webhdfs_fusioninsight1 \\ --conn_type 'hdfs' \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import webhdfs_hook whdfs = webhdfs_hook.WebHDFSHook(webhdfs_conn_id='webhdfs_fusioninsight1') whdfs.get_conn() \u767b\u9646airflow\u7684webUI\u914d\u7f6ewebhdfs_fusioninsight1\uff0c\u53c2\u8003\u56fe\u914d\u7f6e\u597d\u53c2\u6570\u70b9save\u4fdd\u5b58 \u4e0b\u9762\u5728airflow\u4e3b\u673a\u521b\u5efa\u8def\u5f84 /opt/loadfile ,\u521b\u5efa\u4e00\u4e2a iris.txt \u6587\u4ef6\u5185\u5bb9\uff1a 1,5.1,3.5,1.4,0.2,setosa 2,4.9,3,1.4,0.2,setosa 3,4.7,3.2,1.3,0.2,setosa 4,4.6,3.1,1.5,0.2,setosa 5,5,3.6,1.4,0.2,setosa 6,5.4,3.9,1.7,0.4,setosa 7,4.6,3.4,1.4,0.3,setosa 8,5,3.4,1.5,0.2,setosa 9,4.4,2.9,1.4,0.2,setosa 10,4.9,3.1,1.5,0.1,setosa \u5728\u7ec8\u7aef\u8f93\u5165 python \u8fdb\u884c\u9a8c\u8bc1 \u8f93\u5165\u4ee3\u7801\uff1a from airflow.hooks import webhdfs_hook whdfs = webhdfs_hook.WebHDFSHook(webhdfs_conn_id='webhdfs_fusioninsight1') whdfs.get_conn() \u4e0b\u9762\u6d4b\u8bd5\u4e00\u4e0b\u6570\u636e\u5bfc\u5165\u7684\u6837\u4f8b\uff0c\u63a5\u7740\u4e0a\u9762\u7684\u547d\u4ee4\u7ee7\u7eed\u8f93\u5165 whdfs.load_file('/opt/loadfile/','/tmp/') \u767b\u9646\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u68c0\u67e5\uff1a","title":"\u914d\u7f6eairflow\u4e2dwebhdfs connection\u914d\u7f6e"},{"location":"Other/Airflow/#airflowdag","text":"\u8bf4\u660e\uff1aairflow\u4f7f\u7528dag\u6765\u63a7\u5236\u8c03\u5ea6\uff0c\u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a .py \u6587\u4ef6\uff0c\u4f7f\u7528\u65f6\u9700\u8981\u7f16\u5199\u4efb\u52a1\u6d41\u7684py\u6587\u4ef6\u540e\u518d\u6267\u884c \u767b\u9646airflow\u4e3b\u673a /opt/airflow/dags \u8def\u5f84\uff0c\u65b0\u5efa\u4e00\u4e2ahivetest.py\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a #!/usr/bin/env python # -*- coding: utf-8 -*- from datetime import datetime, timedelta from airflow.operators.python_operator import PythonOperator, BranchPythonOperator from airflow import DAG from airflow.operators.bash_operator import BashOperator from airflow.operators.hive_operator import HiveOperator from airflow.hooks import hive_hooks def test_select_conn(): from airflow.hooks.hive_hooks import HiveServer2Hook hook = hive_hooks.HiveServer2Hook(hiveserver2_conn_id='hiveserver2_test') sql = \"\"\"SELECT * FROM iris\"\"\" records = hook.get_records(sql) return records default_args = { 'owner': 'airflow', 'start_date': datetime(2019, 1, 21), } dag = DAG( 'hive_server_run_test', default_args=default_args, schedule_interval='@once') t1 = BashOperator( task_id='print_date', bash_command='date', dag=dag) t2 = BranchPythonOperator( task_id='hive_server_run', python_callable=test_select_conn, dag=dag) t2.set_upstream(t1) \u6ce8\u610f:\u8be5\u4efb\u52a1\u6d41\u6709\u4e24\u4e2a\u52a8\u4f5c\uff0c\u7b2c\u4e00\u4e2a\u662ft1\uff0c\u5728\u547d\u4ee4\u884c\u4e2d\u8f93\u5165data\u547d\u4ee4\u6253\u5370\u51fa\u5f53\u524d\u65f6\u95f4\uff0c\u7b2c\u4e8c\u4e2a\u52a8\u4f5c\u662ft2\uff0c\u6253\u5370\u51fa select * from iris \u67e5\u8be2\u8bed\u53e5\uff0c\u4e24\u4e2a\u52a8\u4f5c\u6709\u5bf9\u5e94\u7684task_id \u53c2\u8003airflow\u5b98\u65b9\u6587\u6863\uff1a https://airflow.apache.org/docs/stable/tutorial.html \u4f7f\u7528\u547d\u4ee4 airflow test hive_server_run_test hive_server_run 2019-12-17 \u68c0\u67e5hive\u8fde\u63a5: \u5176\u4e2dhive_server_run_test\u662fdag id, hive_server_run\u662f task id \u4e0a\u8ff0test\u6b65\u9aa4\u6d4b\u8bd5\u6210\u529f\u4e4b\u540e\uff0c\u4f7f\u7528\u547d\u4ee4 airflow backfill hive_server_run_test -s 2019-12-17 -e 2019-12-17 \u63d0\u4ea4\u4efb\u52a1\uff1a \u53bbwebUI\u68c0\u67e5\uff1a \u70b9\u51fbprint_data\u4ee5\u53cahive_server_run\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a print_data: hive_server_run:","title":"\u914d\u7f6eairflow\u4e2d\u7684DAG"},{"location":"Other/Airflow/#faq","text":"\u5728\u300a\u5b89\u88c5airflow\u300b\u7684\u65f6\u5019\u8f93\u5165 pip install apache-airflow \u4f1a\u9047\u5230\u9519\u8bef\uff1a \u9700\u8981\u628amarshmallow\u91cd\u65b0\u88c5\u62102.18.0\u7684\u7248\u672c\uff0c\u4f7f\u7528\u547d\u4ee4 ./pip install marshmallow==2.18.0 --force-reinstall \u5728anaconda2\u7684bin\u76ee\u5f55\u4e0b\u91cd\u65b0\u5b89\u88c5\u6b63\u786e\u7248\u672c\u95ee\u9898\u89e3\u51b3 \u5728\u300a\u5b89\u88c5airflow\u300b\u7684\u65f6\u5019\u8f93\u5165 pip install apache-airflow[kerberos,hive,gcp] \u4f1a\u9047\u5230\u9519\u8bef\uff1a \u95ee\u9898\u5728\u4e8e \"#include \"\u6ca1\u6709\u5b89\u88c5\u6210\u529f\uff0c\u53c2\u8003 https://blog.csdn.net/coder_gray/article/details/77189002 \uff0c \u770binclude\u7f3a\u7684\u662f\u4ec0\u4e48\u5c31\u5b89\u88c5\u4ec0\u4e48\uff0c\u8f93\u5165 yum install gcc-c++ python-devel.x86_64 cyrus-sasl-devel.x86_64 \u5148\u5b89\u88c5yum\u4f9d\u8d56\uff0c\u5b8c\u4e86\u4e4b\u540e\u518d\u91cd\u65b0\u6267\u884c pip install apache-airflow[kerberos,hive,gcp] \u95ee\u9898\u89e3\u51b3 \u5728\u300aairflow\u4e2dhive\u76f8\u5173connection\u914d\u7f6e\u300b\u4e2d\u8c03\u8bd5\u7684\u65f6\u5019\uff0c\u8f93\u5165 hm.get_databases() \u9047\u5230\u62a5\u9519\uff1a thrift.transport.TTransport.TTransportException: Could not start SASL: Error in sasl_client_start (-4) SASL(-4): no mechanism available: No worthy mechs found \u8fd9\u4e2a\u95ee\u9898\u662fpython\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u53c2\u8003\uff1a https://stackoverflow.com/questions/30705576/python-cannot-connect-hiveserver2/30707252 \u5b89\u88c5\u547d\u4ee4\uff1a sudo yum install cyrus-sasl-devel cyrus-sasl-gssapi cyrus-sasl-md5 cyrus-sasl-plain \u5b89\u88c5\u5916\u4f9d\u8d56\u540e\u95ee\u9898\u89e3\u51b3 \u5728\u505awebhdfs\u7684\u65f6\u5019\u4f7f\u7528python\u547d\u4ee4\u5728\u540e\u53f0\u8c03\u6d4b\u9047\u5230\u95ee\u9898\uff1a ImportError: No module named hdfs \u89e3\u51b3\u529e\u6cd5: \u53bb/opt/anaconda2/bin\u4e0b\u9762\u4f7f\u7528\u547d\u4ee4 pip install hdfs \u540e\u91cd\u65b0\u6267\u884c\u7a0b\u5e8f\u95ee\u9898\u89e3\u51b3 \u5728\u9a8c\u8bc1jdbc\u94fe\u63a5\u7684\u65f6\u5019\u9047\u5230\u62a5\u9519\uff1a \u627e\u4e0d\u5230jdbc\u9a71\u52a8\u4e3b\u7c7b\uff0c\u5e76\u4e14\u5728\u914d\u7f6e\u4e0a\u5df2\u7ecf\u5236\u5b9a\u4e86\u6b63\u786e\u7684jar\u5305\u8def\u5f84 \u89e3\u51b3\u529e\u6cd5\uff1a \u53c2\u8003 https://www.reddit.com/r/dataengineering/comments/aqfvug/airflow_issues_setting_up_a_jdbc_connection_to/ \u5728\u4f7f\u7528python\u547d\u4ee4\u540e\u53f0\u6d4b\u8bd5\u524d kinit /opt/hadoopclient/bigdata_env \u518d\u6d4b\u8bd5\u95ee\u9898\u89e3\u51b3","title":"FAQ"},{"location":"Other/Apache_Livy_0_5_0/","text":"Apache Livy\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Livy 0.5.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x) \u5b89\u88c5Livy \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u5b89\u88c5 Apache Livy 0.5.0-incubating\u3002 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u5b89\u88c5Apache Livy 0.5.0-incubating\uff0c\u5728\u7f51\u5740 https://livy.incubator.apache.org/download/ \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 unzip livy-0.5.0-incubating-bin.zip \u89e3\u538b\u751f\u6210livy-0.5.0-incubating-bin\u76ee\u5f55 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser \u76ee\u5f55\u4e0b \u5728 /usr/livy/livy-0.5.0-incubating-bin/conf \u8def\u5f84\u4e0b\u65b0\u5efalivy\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"livy\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u914d\u7f6eLivy\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export LIVY_HOME=/usr/livy/livy-0.5.0-incubating-bin export PATH=$LIVY_HOME/bin:$PATH \u7f16\u8f91livy.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy.conf.template livy.conf vi livy.conf \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a livy.spark.master = yarn livy.spark.deploy-mode = client livy.server.session.timeout = 1h livy.impersonation.enabled = true livy.repl.enable-hive-context = true livy.server.launch.kerberos.keytab=/opt/user.keytab livy.server.launch.kerberos.principal=livy@HADOOP.COM \u7f16\u8f91livy-client.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-client.conf.template livy-client.conf vi livy-client.conf \u52a0\u5165\u5982\u672c\u673aip\u5730\u5740\uff1a livy.rsc.rpc.server.address =172.16.52.190 - \u7f16\u8f91livy-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-env.sh.template livy-env.sh vi livy-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export SPARK_CONF_DIR=/opt/hadoopclient/Spark2x/spark/conf export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop export LIVY_SERVER_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/usr/livy/livy-0.5.0-incubating-bin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=12000\" export SPARK_LOCAL_IP=172.16.52.190 \u7f16\u8f91spark-blacklist.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp spark-blacklist.conf.template spark-blacklist.conf vi spark-blacklist.conf \u6ce8\u9500\u6389\u5982\u4e0b\u5185\u5bb9\uff1a spark.master spark.submit.deployMode \u542f\u52a8\u548c\u505c\u6b62Livy\uff0c\u5728\u8def\u5f84 /usr/livy/livy-0.5.0-incubating-bin \u4e0b bin/livy-server start \u542f\u52a8\u6210\u529f\u540e\u53ef\u4ee5\u5728http://172.16.52.190:8998\u8bbf\u95ee\u5230Livy\u670d\u52a1\u5668\uff1a \u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801 \u00b6 \u64cd\u4f5c\u573a\u666f \u00b6 \u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801\uff0c\u5305\u62ecSpark Shell\uff0cPySpark\uff0cSparkR \u6837\u4f8b\u4ee3\u7801\u53c2\u8003\u7f51\u5740 https://livy.incubator.apache.org/examples/ \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u5df2\u5b8c\u6210Anaconda\u548cR\u5728\u5ba2\u6237\u7aef\u4e3b\u673a\u4e0a\u7684\u5b89\u88c5\u3002 \u82e5\u6ca1\u6709\u5b89\u88c5Anaconda\u548cR\uff0c\u8bf7\u53c2\u8003Zeppelin0.8.0\u5bf9\u63a5FusionInsight HD V100R002C80SPC200 (Spark2.x)\u6307\u5bfc\u6587\u6863\u4e2d\u8fde\u63a5Spark\u548cSparkR\u90e8\u5206\u76f8\u5173\u5185\u5bb9 \u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u8f93\u5165\u547d\u4ee4 python \u542f\u52a8Anaconda \u8f93\u5165\u5982\u4e0bpython\u4ee3\u7801\u542f\u52a8\u4e00\u4e2aLivy session import json, pprint, requests, textwrap host = 'http://172.16.52.190:8998' data = {'kind': 'spark'} headers = {'Content-Type': 'application/json'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) \u5f53\u4e00\u4e2asession\u5b8c\u6210\u542f\u52a8\u540e\uff0c \u5b83\u5c06\u4f1a\u53d8\u4e3a\u95f2\u7f6e\u72b6\u6001 session_url = host + r.headers['location'] r = requests.get(session_url, headers=headers) r.json() \u4e0b\u9762\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u7b80\u5355JSON\u547d\u4ee4\u884c\u7684\u65b9\u5f0f\u6765\u6267\u884cScala statements_url = session_url + '/statements' data = {'code': '1 + 1'} r = requests.post(statements_url, data=json.dumps(data), headers=headers) r.json() statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u4e5f\u53ef\u4ee5\u5728\u7ec8\u7aef\u770b\u5230\u4ee5JSON\u683c\u5f0f\u8fd4\u56de\u7684\u7ed3\u679c \u66f4\u65b0Scala\u518d\u6b21\u8fd0\u884c data = { 'code': textwrap.dedent(\"\"\" val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\\\"Pi is roughly \\\" + 4.0 * count / NUM_SAMPLES) \"\"\") } r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession0 session_url = 'http://172.16.52.190:8998/sessions/0' requests.delete(session_url, headers=headers) \u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3apyspark data = {'kind': 'pyspark'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session1 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cPython\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/1/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session1\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession1 session_url = 'http://172.16.52.190:8998/sessions/1' requests.delete(session_url, headers=headers) \u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3asparkr data = {'kind': 'sparkr'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session2 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cR\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/2/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session2\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession2 session_url = 'http://172.16.52.190:8998/sessions/2' requests.delete(session_url, headers=headers)","title":"0.5.0 <--> C80"},{"location":"Other/Apache_Livy_0_5_0/#apache-livyfusioninsight","text":"","title":"Apache Livy\u5bf9\u63a5FusionInsight"},{"location":"Other/Apache_Livy_0_5_0/#_1","text":"Apache Livy 0.5.0 \u2194 FusionInsight HD V100R002C80SPC200 (Spark2x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Apache_Livy_0_5_0/#livy","text":"","title":"\u5b89\u88c5Livy"},{"location":"Other/Apache_Livy_0_5_0/#_2","text":"\u5b89\u88c5 Apache Livy 0.5.0-incubating\u3002","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Other/Apache_Livy_0_5_0/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Apache_Livy_0_5_0/#_4","text":"\u5b89\u88c5Apache Livy 0.5.0-incubating\uff0c\u5728\u7f51\u5740 https://livy.incubator.apache.org/download/ \u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u5e76\u7528 unzip livy-0.5.0-incubating-bin.zip \u89e3\u538b\u751f\u6210livy-0.5.0-incubating-bin\u76ee\u5f55 \u6267\u884csource\u547d\u4ee4\u5230\u5ba2\u6237\u7aef\uff0c\u83b7\u53d6java\u914d\u7f6e\u4fe1\u606f source /opt/hadoopclient/bigdata_env echo $JAVA_HOME \u6839\u636e\u4ea7\u54c1\u6587\u6863\u521b\u5efa\u7528\u6237developuser\uff0c\u5e76\u8d4b\u4e88\u8db3\u591f\u6743\u9650\uff0c\u4e0b\u8f7d\u7528\u6237developuser\u7684keytab\u6587\u4ef6user.keytab\uff0c\u4e0a\u4f20\u81f3 /opt/developuser \u76ee\u5f55\u4e0b \u5728 /usr/livy/livy-0.5.0-incubating-bin/conf \u8def\u5f84\u4e0b\u65b0\u5efalivy\u7684\u8ba4\u8bc1\u6587\u4ef6jaas.conf\uff0c\u5185\u5bb9\u5982\u4e0b: Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=\"/opt/user.keytab\" principal=\"livy\" useTicketCache=false storeKey=true debug=true; }; \u5176\u4e2d\u7528\u6237\u4e3a\u5728FusionInsight Manager\u4e2d\u521b\u5efa\u7684developuser\u7528\u6237\uff0c\u5c06developuser\u7684keytab\u6587\u4ef6user.key\u653e\u5728/opt/developuser/\u76ee\u5f55\u4e0b \u914d\u7f6eLivy\u73af\u5883\u53d8\u91cf\uff0c\u5728profile\u6587\u4ef6\u4e2d\u52a0\u5165\u5982\u4e0b\u53d8\u91cf vi /etc/profile export LIVY_HOME=/usr/livy/livy-0.5.0-incubating-bin export PATH=$LIVY_HOME/bin:$PATH \u7f16\u8f91livy.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy.conf.template livy.conf vi livy.conf \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a livy.spark.master = yarn livy.spark.deploy-mode = client livy.server.session.timeout = 1h livy.impersonation.enabled = true livy.repl.enable-hive-context = true livy.server.launch.kerberos.keytab=/opt/user.keytab livy.server.launch.kerberos.principal=livy@HADOOP.COM \u7f16\u8f91livy-client.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-client.conf.template livy-client.conf vi livy-client.conf \u52a0\u5165\u5982\u672c\u673aip\u5730\u5740\uff1a livy.rsc.rpc.server.address =172.16.52.190 - \u7f16\u8f91livy-env.sh\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp livy-env.sh.template livy-env.sh vi livy-env.sh \u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff1a export JAVA_HOME=/opt/hadoopclient/JDK/jdk1.8.0_162 export SPARK_HOME=/opt/hadoopclient/Spark2x/spark export SPARK_CONF_DIR=/opt/hadoopclient/Spark2x/spark/conf export HADOOP_CONF_DIR=/opt/hadoopclient/HDFS/hadoop/etc/hadoop export LIVY_SERVER_JAVA_OPTS=\"-Djava.security.krb5.conf=/opt/krb5.conf -Djava.security.auth.login.config=/usr/livy/livy-0.5.0-incubating-bin/conf/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dzookeeper.request.timeout=12000\" export SPARK_LOCAL_IP=172.16.52.190 \u7f16\u8f91spark-blacklist.conf\u6587\u4ef6\uff0c\u4f4d\u7f6e /usr/livy/livy-0.5.0-incubating-bin/conf cd /usr/livy/livy-0.5.0-incubating-bin/conf cp spark-blacklist.conf.template spark-blacklist.conf vi spark-blacklist.conf \u6ce8\u9500\u6389\u5982\u4e0b\u5185\u5bb9\uff1a spark.master spark.submit.deployMode \u542f\u52a8\u548c\u505c\u6b62Livy\uff0c\u5728\u8def\u5f84 /usr/livy/livy-0.5.0-incubating-bin \u4e0b bin/livy-server start \u542f\u52a8\u6210\u529f\u540e\u53ef\u4ee5\u5728http://172.16.52.190:8998\u8bbf\u95ee\u5230Livy\u670d\u52a1\u5668\uff1a","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy_0_5_0/#livy_1","text":"","title":"\u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801"},{"location":"Other/Apache_Livy_0_5_0/#_5","text":"\u6d4b\u8bd5\u8fd0\u884cLivy\u6837\u4f8b\u4ee3\u7801\uff0c\u5305\u62ecSpark Shell\uff0cPySpark\uff0cSparkR \u6837\u4f8b\u4ee3\u7801\u53c2\u8003\u7f51\u5740 https://livy.incubator.apache.org/examples/","title":"\u64cd\u4f5c\u573a\u666f"},{"location":"Other/Apache_Livy_0_5_0/#_6","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u5df2\u5b8c\u6210Anaconda\u548cR\u5728\u5ba2\u6237\u7aef\u4e3b\u673a\u4e0a\u7684\u5b89\u88c5\u3002 \u82e5\u6ca1\u6709\u5b89\u88c5Anaconda\u548cR\uff0c\u8bf7\u53c2\u8003Zeppelin0.8.0\u5bf9\u63a5FusionInsight HD V100R002C80SPC200 (Spark2.x)\u6307\u5bfc\u6587\u6863\u4e2d\u8fde\u63a5Spark\u548cSparkR\u90e8\u5206\u76f8\u5173\u5185\u5bb9","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Apache_Livy_0_5_0/#spark","text":"\u8f93\u5165\u547d\u4ee4 python \u542f\u52a8Anaconda \u8f93\u5165\u5982\u4e0bpython\u4ee3\u7801\u542f\u52a8\u4e00\u4e2aLivy session import json, pprint, requests, textwrap host = 'http://172.16.52.190:8998' data = {'kind': 'spark'} headers = {'Content-Type': 'application/json'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) \u5f53\u4e00\u4e2asession\u5b8c\u6210\u542f\u52a8\u540e\uff0c \u5b83\u5c06\u4f1a\u53d8\u4e3a\u95f2\u7f6e\u72b6\u6001 session_url = host + r.headers['location'] r = requests.get(session_url, headers=headers) r.json() \u4e0b\u9762\u901a\u8fc7\u4f20\u9012\u4e00\u4e2a\u7b80\u5355JSON\u547d\u4ee4\u884c\u7684\u65b9\u5f0f\u6765\u6267\u884cScala statements_url = session_url + '/statements' data = {'code': '1 + 1'} r = requests.post(statements_url, data=json.dumps(data), headers=headers) r.json() statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u4e5f\u53ef\u4ee5\u5728\u7ec8\u7aef\u770b\u5230\u4ee5JSON\u683c\u5f0f\u8fd4\u56de\u7684\u7ed3\u679c \u66f4\u65b0Scala\u518d\u6b21\u8fd0\u884c data = { 'code': textwrap.dedent(\"\"\" val NUM_SAMPLES = 100000; val count = sc.parallelize(1 to NUM_SAMPLES).map { i => val x = Math.random(); val y = Math.random(); if (x*x + y*y < 1) 1 else 0 }.reduce(_ + _); println(\\\"Pi is roughly \\\" + 4.0 * count / NUM_SAMPLES) \"\"\") } r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) statement_url = host + r.headers['location'] r = requests.get(statement_url, headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session0\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession0 session_url = 'http://172.16.52.190:8998/sessions/0' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cSpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy_0_5_0/#pyspark","text":"\u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3apyspark data = {'kind': 'pyspark'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session1 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cPython\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" import random NUM_SAMPLES = 100000 def sample(p): x, y = random.random(), random.random() return 1 if x*x + y*y < 1 else 0 count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b) print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES) \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/1/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session1\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession1 session_url = 'http://172.16.52.190:8998/sessions/1' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cPySpark\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy_0_5_0/#sparkr","text":"\u7ee7\u7eed\u63a5\u7740\u4e0a\u9762\u7684\u6b65\u9aa4\uff0c\u66f4\u6539\u7c7b\u578b\u4e3asparkr data = {'kind': 'sparkr'} r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers) r.json() \u53ef\u4ee5\u5728Session\u72b6\u6001\u680f\u770b\u5230\u65b0\u542f\u52a8\u7684Session2 \u901a\u8fc7\u4f20\u9012JSON\u547d\u4ee4\u7684\u65b9\u5f0f\u6267\u884cR\u6837\u4f8b\u4ee3\u7801\uff0c\u6ce8\u610f\u8981\u66f4\u6539statements_url data = { 'code': textwrap.dedent(\"\"\" hello <- function( name ) { sprintf( \"Hello, %s\", name ); } hello(\"livy\") \"\"\") } statements_url = 'http://172.16.52.190:8998/sessions/2/statements' r = requests.post(statements_url, data=json.dumps(data), headers=headers) pprint.pprint(r.json()) \u53ef\u4ee5\u5728Session2\u72b6\u6001\u680f\u770b\u5230\u4e4b\u524d\u8fd0\u884c\u7684\u6837\u4f8b\u4ee3\u7801\u4ee5\u53ca\u7ed3\u679c \u5173\u95edsession2 session_url = 'http://172.16.52.190:8998/sessions/2' requests.delete(session_url, headers=headers)","title":"\u8fd0\u884cSparkR\u6837\u4f8b\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Apache_Livy_0_6_0/","text":"Apache Livy\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Livy 0.6.0 \u2194 FusionInsight HD 6.5 (Spark2x) Apache Livy 0.7.0 \u2194 FusionInsight MRS 8.0 (Spark2x) \u90e8\u7f72\u5bf9\u5916\u9a8c\u8bc1\u7684livy\u670d\u52a1\u5e76\u4f7f\u7528session\uff0cbatch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1 \u00b6 \u573a\u666f\u8bf4\u660e \u00b6 \u590d\u6742\u573a\u666f\u4e0b\u9700\u8981\u5bf9\u63d0\u4ea4\u4efb\u52a1\u7684\u7528\u6237\u8fdb\u884c\u8bbf\u95ee\u63a7\u5236\uff0clivy\u670d\u52a1\u652f\u6301\u5bf9\u5916\u90e8\u8bbf\u95ee\u63d0\u4f9bkerberos SPNEGO\u8ba4\u8bc1\uff0c\u4e0b\u9762\u4e3a\u5177\u4f53\u7684\u6d4b\u8bd5\u573a\u666f \u5bf9\u63a5FI HD\u96c6\u7fa4\uff1a172.16.6.10 - 12\uff0c\u4e09\u8282\u70b9\u90e8\u7f72 Apache Livy\u670d\u52a1\u7aef\uff1a 172.16.2.118\uff0c\u5728\u8be5\u8282\u70b9\u4e0a\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\u5e76\u4e14\u53c2\u8003\u4e0a\u4e00\u7ae0\u5b8c\u6210Livy\u7684\u4e0b\u8f7d\uff0c\u5b89\u88c5 \u5ba2\u6237\u7aef\uff1a172.16.2.119\uff0c\u5728\u8be5\u8282\u70b9\u4e0a\u4f7f\u7528curl\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u8bf7\u6c42\uff0c\u9700\u8981\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\u5e76\u4e14\u68c0\u67e5curl\u662f\u5426\u652f\u6301SPNEGO\u8ba4\u8bc1 \u5728\u6b64\u573a\u666f\u4e2d\u4f7f\u7528\u4e24\u4e2a\u8ba4\u8bc1\u7528\u6237\uff0c\u7528\u6237developuser\u548c\u7528\u6237livy \u7528\u6237livy\u4e3alivy\u670d\u52a1\u5b9e\u9645\u5411FI HD\u96c6\u7fa4\u63d0\u4ea4spark\u4efb\u52a1\u8bf7\u6c42\u6240\u9700\u8981\u7528\u5230\u7684\u7528\u6237 \u7528\u6237developuser\u662f\u5ba2\u6237\u7aef\u5411Livy\u670d\u52a1\u7aef\u63d0\u4ea4\u4efb\u52a1\u6240\u7528\u7684\u7528\u6237 \u6574\u4e2a\u4e1a\u52a1\u6d41\u7a0b\u5b9e\u9645\u4e0a\u662f\u4ee3\u7406\u7528\u6237developuser\u4ee5\u7528\u6237livy\u7684\u540d\u4e49\u5411FI HD\u96c6\u7fa4\u63d0\u4ea4spark\u4efb\u52a1\uff0c\u4f46\u662f\u6267\u884c\u4efb\u52a1\u4e4b\u524d\u7528\u6237developuser\u9700\u8981\u901a\u8fc7FI HD\u96c6\u7fa4\u7684kerberos\u8ba4\u8bc1\uff0c\u901a\u8fc7\u8fd9\u6837\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0Apache Livy\u670d\u52a1\u7aef\u8bbf\u95ee\u63a7\u5236 Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u00b6 \u767b\u5f55FI HD manager\u521b\u5efa\u6d4b\u8bd5\u4e2d\u9700\u8981\u7528\u5230\u7684\u7528\u6237developuser, livy\u3002 \u5e76\u4e14\u5c06\u7528\u6237livy\u7684\u8ba4\u8bc1\u4fe1\u606f\u4e0b\u8f7d\u4e0b\u6765\uff08user.keytab, krb5.conf\uff09 \uff08livy \u7aef\u4e0d\u914d\u7f6e\u8ba4\u8bc1\u53ef\u4e0d\u505a\uff09\u4f7f\u7528FI HD\u5ba2\u6237\u7aef\u767b\u5f55kadmin\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\u7528\u4e8eFI HD\u5bf9Livy HTTP\u670d\u52a1\u7684Kerberos\u8ba4\u8bc1,\u5176\u540d\u79f0\u4e3a\u201cHTTP/host-172-16-2-118\u201d,\u5176\u4e2dhost-172-16-2-118\u4e3aApache Livy\u90e8\u7f72\u7684\u8282\u70b9\u7684\u4e3b\u673a\u540d\u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u5c06\u751f\u6210\u7684http2.keytab(keytab\u6587\u4ef6\u540d\u53ef\u81ea\u5b9a\u4e49)\u8ba4\u8bc1\u6587\u4ef6\u4f20\u5230livy\u670d\u52a1\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528kinit -kt\u547d\u4ee4\u68c0\u67e5\u8ba4\u8bc1\u662f\u5426\u6210\u529f kinit -kt /opt/http2.keytab HTTP/host-172-16-2-118@HADOOP.COM \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4kdestroy\u6e05\u9664\u7f13\u5b58\u7684\u7968\u636e \u767b\u5f55\u9700\u8981\u5bf9\u63a5\u96c6\u7fa4\uff0c\u70b9\u51fb\u670d\u52a1\u7ba1\u7406 -> Yarn -> \u670d\u52a1\u914d\u7f6e -> \u9009\u62e9\u5168\u90e8\u914d\u7f6e -> \u81ea\u5b9a\u4e49\uff0c \u5728\u5bf9\u5e94\u53c2\u6570\u6587\u4ef6\u4e3acore-site.xml\u4e0b\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a hadoop.proxyuser.livy.hosts = * hadoop.proxyuser.livy.groups = * \u53c2\u7167\u4e0a\u9762\u7684\u540c\u6837\u65b9\u6cd5\u5bf9hdfs\u670d\u52a1\uff0c hive\u670d\u52a1\u7684 core-site.xml \u6587\u4ef6\u589e\u52a0\u76f8\u540c\u7684\u914d\u7f6e\uff1a \u5b8c\u6210\u540e\u91cd\u542f\u76f8\u5173\u670d\u52a1 \u5ba2\u6237\u7aef\u76f8\u5173\u68c0\u67e5 \u00b6 \u4f7f\u7528curl -V\u547d\u4ee4\u68c0\u67e5\u5ba2\u6237\u7aefcurl\u547d\u4ee4\u662f\u5426\u652f\u6301Kerberos Spnego \u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u5ba2\u6237\u7aef \u68c0\u67e5\u5ba2\u6237\u7aef\u65f6\u95f4\u4e0e\u5bf9\u63a5FI HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f Livy\u670d\u52a1\u7aef\u914d\u7f6e \u00b6 \u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u5ba2\u6237\u7aef \u68c0\u67e5Livy\u670d\u52a1\u7aef\u65f6\u95f4\u4e0e\u5bf9\u63a5FI HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u68c0\u67e5livy.conf\u6587\u4ef6\u914d\u7f6e \u9700\u8981\u7279\u522b\u6ce8\u610f\u7684\u662f\uff1a livy.file.local-dir-whitelist=/opt/ \u914d\u7f6e\u53c2\u6570\u662f\u4f7f\u7528livy batch\u65b9\u5f0f\u672c\u5730\u63d0\u4ea4\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u5c06\u672c\u5730\u8def\u5f84\u6253\u5f00\u767d\u540d\u5355 launch.kerberos\u76f8\u5173\u53c2\u6570\u4e3alivy\u5b9e\u9645\u540cFI HD\u96c6\u7fa4\u4ea4\u4e92\u6240\u9700\u8981\u7684\u914d\u7f6e auth.kerberos\u76f8\u5173\u53c2\u6570\u4e3alivy\u5bf9\u5916\u4f7f\u7528kerberos\u8fdb\u884c\u8bbf\u95ee\u7ba1\u63a7\u6240\u9700\u8981\u7684\u914d\u7f6e \u68c0\u67e5livy-client.conf\u6587\u4ef6\u914d\u7f6e \u68c0\u67e5livy-env.sh\u6587\u4ef6\u914d\u7f6e \u68c0\u67e5spark-blacklist.conf\u6587\u4ef6\u914d\u7f6e \u5728log4j.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0\u5982\u4e0b\u4e00\u6761\u6765\u8c03\u6574\u65e5\u5fd7\u7ea7\u522b\uff08\u53ef\u9009\uff09 log4j.logger.org.eclipse.jetty=DEBUG \u4f7f\u7528Livy session\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1 \u00b6 livy session\u65b9\u5f0f\u5bf9\u5e94spark console\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u63d0\u4ea4\u5177\u4f53\u7684\u4ee3\u7801\u7684\u65b9\u5f0f\u6765\u63d0\u4ea4\uff0c\u8fd0\u884cspark\u4efb\u52a1 \u767b\u5f55livy\u670d\u52a1\u7aef\u4f7f\u7528 bin/livy-server start \u542f\u52a8livy\u670d\u52a1 \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770blivy\u662f\u5426\u542f\u52a8\u6210\u529f \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728livy\u4e2d\u8d77\u4e00\u4e2apyspark\u7684session curl --negotiate -k -v -u developuser : -X POST --data '{\"kind\": \"pyspark\"}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/sessions \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e3asession/0\u63d0\u4ea4\u4e00\u6bb5\u4ee3\u7801 curl --negotiate -k -v -u developuser : -X POST -H 'Content-Type: application/json' -d '{\"code\":\"1 + 1\"}' http://host-172-16-2-118:8998/sessions/0/statements \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770b\u7ed3\u679c\uff1a curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0/statements | python -m json.tool \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5173\u95edsession curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0 -X DELETE \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u53e6\u5916\u5728\u5ba2\u6237\u7aef(172.16.2.119)\u5b8c\u6210curl\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u4e4b\u540e\u53ef\u4ee5\u4f7f\u7528klist\u67e5\u770b\u7968\u636e\u4fe1\u606f\uff1a \u53ef\u4ee5\u770b\u5230\u4f1a\u589e\u52a0\u8ba4\u8bc1\u7968\u636eHTTP/host-172-16-2-118@HADOOP.COM \u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b1 \u00b6 Livy batch\u65b9\u5f0f\u5bf9\u5e94spark-submit\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u63d0\u4ea4\u4e00\u4e2a\u7f16\u8bd1\u597d\u7684jar\u5305\uff0c\u6216\u8005\u662f\u5199\u597d\u7684py\u6587\u4ef6\u7b49\u6765\u63d0\u4ea4\uff0c\u8fd0\u884cspark\u4efb\u52a1 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn client\u6a21\u5f0f\u672c\u5730\u63d0\u4ea4\u4e00\u4e2ajar\u5305\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u6d4b\u8bd5jar\u5305spark-examples_2.11-2.1.0.jar\u5e76\u4f20\u5230livy\u670d\u52a1\u7aef/opt/\u8def\u5f84\u4e0b \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b2 \u00b6 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn client\u6a21\u5f0f\u672c\u5730\u63d0\u4ea4\u4e00\u4e2apy\u6587\u4ef6\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u521b\u5efapy2.py\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u81f3Livy\u670d\u52a1\u7aef/opt/\u8def\u5f84\u4e0b\uff0c\u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a import sys from random import random from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession\\ .builder\\ .appName(\"PythonPi\")\\ .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add) print(\"Pi is roughly %f\" % (4.0 * count / n)) spark.stop() \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/pi2.py\" }' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b3 \u00b6 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn cluster\u6a21\u5f0f\u5728\u96c6\u7fa4hdfs\u8def\u5f84\u4e0b\u63d0\u4ea4jar\u5305\u5e76\u8fd0\u884c\uff0c\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u4fee\u6539livy.conf\u6587\u4ef6\u914d\u7f6e\u4e3a\uff1a \u5728\u5bf9\u63a5FI HD\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\u4e0a\u4f20jar\u5305 \u91cd\u542fLivy \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"/tmp/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b4 \u00b6 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn cluster\u6a21\u5f0f\u5728\u96c6\u7fa4\u672c\u5730\u8def\u5f84\u4e0b\u63d0\u4ea4jar\u5305\u5e76\u8fd0\u884c\uff0c\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u56e0\u4e3a\u4f7f\u7528yarn cluster\u672c\u5730\u63d0\u4ea4jar\u5305\u6a21\u5f0f\uff0c\u4e8b\u5148\u5e76\u4e0d\u77e5\u9053worker\u5728\u54ea\u4e2a\u96c6\u7fa4\u8282\u70b9\uff0c\u6240\u4ee5\u5c06jar\u5305spark-examples_2.11-2.1.0.jar\u5206\u522b\u653e\u5230\u5404\u96c6\u7fa4\u8282\u70b9\u7684/home\u8def\u5f84\u4e0b\uff1a \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"local:/home/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a Windows\u8df3\u677f\u673a\u914d\u7f6eKerberos Spnego\u8bbf\u95eelivy web ui \u00b6 Windows\u8df3\u677f\u673a\uff08172.16.2.111\uff09\u8bbf\u95eeLivy web ui\u7684\u8ba4\u8bc1\u539f\u7406\u540c\u4e0a\u6587\u5ba2\u6237\u7aef\uff08172.16.2.119\uff09\u4f7f\u7528curl\u547d\u4ee4\u8bbf\u95eeLivy\u670d\u52a1\u7aef\uff0c\u63d0\u4ea4spark\u4efb\u52a1\u4e00\u6837 \u53c2\u8003\u4ea7\u54c1\u6587\u6863 -> \u5e94\u7528\u5f00\u53d1\u6307\u5357 -> \u5b89\u5168\u6a21\u5f0f -> Spark2x\u5f00\u53d1\u6307\u5357 -> \u73af\u5883\u51c6\u5907 -> \u51c6\u5907HiveODBC\u5f00\u53d1\u73af\u5883 -> Windows\u73af\u5883 -> \u64cd\u4f5c\u6b65\u9aa4\u7b2c1\u5230\u7b2c4\u6b65 \u5b8c\u6210MIT Kerberos\u7684\u5b89\u88c5\u914d\u7f6e \u914d\u7f6eJCE \u5230java\u5b98\u7f51\u4e0a\u4e0b\u8f7dJava Cryptography Extension (JCE)\uff0c\u7136\u540e\u89e3\u538b\u5230%JAVA_HOME%/jre/lib/security\u4e2d\u66ff\u6362\u76f8\u5e94\u7684\u6587\u4ef6\u3002 \u68c0\u67e5livy\u670d\u52a1\u7aef\u4e3b\u673a\u540d\u662f\u5426\u52a0\u5165hosts\u6587\u4ef6\uff1a \u914d\u7f6eFirefox windows\u4e0bFirefox\u9700\u8981\u901a\u8fc7\u8bbf\u95eeabout:config \u9875\u9762\u8c03\u6574\u4ee5\u4e0b\u53c2\u6570\uff1a network.negotiate-auth.trusted-uris \u5141\u8bb8\u4f7f\u7528gssapi\u94fe\u63a5\u9a8c\u8bc1\u7684\u5730\u5740 network.auth.use-sspi \u5173\u95edsspi\u9a8c\u8bc1\u534f\u8bae \u4f7f\u7528MIT Kerberos\u5b8c\u6210\u8ba4\u8bc1 \u767b\u5f55Livy\u7684web ui\u5730\u5740\u4e3ahttp://host-172-16-2-118:8998/ui \u4f7f\u7528\u4e4b\u524d\u4e00\u4e2a\u6837\u4f8b\u63d0\u4ea4\u4efb\u52a1\u5e76\u5728Livy web ui\u68c0\u67e5 \u68c0\u67e5MIT Kerberos\u751f\u6210\u7684\u670d\u52a1\u7968\u636e \u666e\u901a\u63d0\u4ea4\u547d\u4ee4 \u00b6 session: curl -X POST --data '{\"kind\": \"pyspark\"}' -H \"Content-Type: application/json\" http://172-16-9-107:8998/sessions curl -X POST -H 'Content-Type: application/json' -d '{\"code\":\"1 + 1\"}' http://172-16-9-107:8998/sessions/0/statements curl --negotiate -k -v -u : http://172-16-9-107:8998/sessions/0/statements | python -m json.tool batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b1: curl -X POST --data '{\"file\": \"file:/opt/spark-examples_2.11-2.3.2.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://172-16-9-107:8998/batches batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b2: curl -X POST --data '{\"file\": \"file:/opt/py2.py\" }' -H \"Content-Type: application/json\" http://172-16-9-107:8998/batches batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b3: curl -X POST --data '{\"file\": \"/tmp/spark-examples_2.11-2.3.2.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://172-16-9-107:8998/batches batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b4: curl -X POST --data '{\"file\": \"local:/home/spark-examples_2.11-2.3.2.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://172-16-9-107:8998/batches","title":"0.7.0 <--> 8.0"},{"location":"Other/Apache_Livy_0_6_0/#apache-livyfusioninsight","text":"","title":"Apache Livy\u5bf9\u63a5FusionInsight"},{"location":"Other/Apache_Livy_0_6_0/#_1","text":"Apache Livy 0.6.0 \u2194 FusionInsight HD 6.5 (Spark2x) Apache Livy 0.7.0 \u2194 FusionInsight MRS 8.0 (Spark2x)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Apache_Livy_0_6_0/#livysessionbatch","text":"","title":"\u90e8\u7f72\u5bf9\u5916\u9a8c\u8bc1\u7684livy\u670d\u52a1\u5e76\u4f7f\u7528session\uff0cbatch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1"},{"location":"Other/Apache_Livy_0_6_0/#_2","text":"\u590d\u6742\u573a\u666f\u4e0b\u9700\u8981\u5bf9\u63d0\u4ea4\u4efb\u52a1\u7684\u7528\u6237\u8fdb\u884c\u8bbf\u95ee\u63a7\u5236\uff0clivy\u670d\u52a1\u652f\u6301\u5bf9\u5916\u90e8\u8bbf\u95ee\u63d0\u4f9bkerberos SPNEGO\u8ba4\u8bc1\uff0c\u4e0b\u9762\u4e3a\u5177\u4f53\u7684\u6d4b\u8bd5\u573a\u666f \u5bf9\u63a5FI HD\u96c6\u7fa4\uff1a172.16.6.10 - 12\uff0c\u4e09\u8282\u70b9\u90e8\u7f72 Apache Livy\u670d\u52a1\u7aef\uff1a 172.16.2.118\uff0c\u5728\u8be5\u8282\u70b9\u4e0a\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\u5e76\u4e14\u53c2\u8003\u4e0a\u4e00\u7ae0\u5b8c\u6210Livy\u7684\u4e0b\u8f7d\uff0c\u5b89\u88c5 \u5ba2\u6237\u7aef\uff1a172.16.2.119\uff0c\u5728\u8be5\u8282\u70b9\u4e0a\u4f7f\u7528curl\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u8bf7\u6c42\uff0c\u9700\u8981\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u7684\u5ba2\u6237\u7aef\u5e76\u4e14\u68c0\u67e5curl\u662f\u5426\u652f\u6301SPNEGO\u8ba4\u8bc1 \u5728\u6b64\u573a\u666f\u4e2d\u4f7f\u7528\u4e24\u4e2a\u8ba4\u8bc1\u7528\u6237\uff0c\u7528\u6237developuser\u548c\u7528\u6237livy \u7528\u6237livy\u4e3alivy\u670d\u52a1\u5b9e\u9645\u5411FI HD\u96c6\u7fa4\u63d0\u4ea4spark\u4efb\u52a1\u8bf7\u6c42\u6240\u9700\u8981\u7528\u5230\u7684\u7528\u6237 \u7528\u6237developuser\u662f\u5ba2\u6237\u7aef\u5411Livy\u670d\u52a1\u7aef\u63d0\u4ea4\u4efb\u52a1\u6240\u7528\u7684\u7528\u6237 \u6574\u4e2a\u4e1a\u52a1\u6d41\u7a0b\u5b9e\u9645\u4e0a\u662f\u4ee3\u7406\u7528\u6237developuser\u4ee5\u7528\u6237livy\u7684\u540d\u4e49\u5411FI HD\u96c6\u7fa4\u63d0\u4ea4spark\u4efb\u52a1\uff0c\u4f46\u662f\u6267\u884c\u4efb\u52a1\u4e4b\u524d\u7528\u6237developuser\u9700\u8981\u901a\u8fc7FI HD\u96c6\u7fa4\u7684kerberos\u8ba4\u8bc1\uff0c\u901a\u8fc7\u8fd9\u6837\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0Apache Livy\u670d\u52a1\u7aef\u8bbf\u95ee\u63a7\u5236","title":"\u573a\u666f\u8bf4\u660e"},{"location":"Other/Apache_Livy_0_6_0/#kerberos","text":"\u767b\u5f55FI HD manager\u521b\u5efa\u6d4b\u8bd5\u4e2d\u9700\u8981\u7528\u5230\u7684\u7528\u6237developuser, livy\u3002 \u5e76\u4e14\u5c06\u7528\u6237livy\u7684\u8ba4\u8bc1\u4fe1\u606f\u4e0b\u8f7d\u4e0b\u6765\uff08user.keytab, krb5.conf\uff09 \uff08livy \u7aef\u4e0d\u914d\u7f6e\u8ba4\u8bc1\u53ef\u4e0d\u505a\uff09\u4f7f\u7528FI HD\u5ba2\u6237\u7aef\u767b\u5f55kadmin\uff0c\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\u7528\u4e8eFI HD\u5bf9Livy HTTP\u670d\u52a1\u7684Kerberos\u8ba4\u8bc1,\u5176\u540d\u79f0\u4e3a\u201cHTTP/host-172-16-2-118\u201d,\u5176\u4e2dhost-172-16-2-118\u4e3aApache Livy\u90e8\u7f72\u7684\u8282\u70b9\u7684\u4e3b\u673a\u540d\u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u5c06\u751f\u6210\u7684http2.keytab(keytab\u6587\u4ef6\u540d\u53ef\u81ea\u5b9a\u4e49)\u8ba4\u8bc1\u6587\u4ef6\u4f20\u5230livy\u670d\u52a1\u7aef\u7684/opt\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528kinit -kt\u547d\u4ee4\u68c0\u67e5\u8ba4\u8bc1\u662f\u5426\u6210\u529f kinit -kt /opt/http2.keytab HTTP/host-172-16-2-118@HADOOP.COM \u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4kdestroy\u6e05\u9664\u7f13\u5b58\u7684\u7968\u636e \u767b\u5f55\u9700\u8981\u5bf9\u63a5\u96c6\u7fa4\uff0c\u70b9\u51fb\u670d\u52a1\u7ba1\u7406 -> Yarn -> \u670d\u52a1\u914d\u7f6e -> \u9009\u62e9\u5168\u90e8\u914d\u7f6e -> \u81ea\u5b9a\u4e49\uff0c \u5728\u5bf9\u5e94\u53c2\u6570\u6587\u4ef6\u4e3acore-site.xml\u4e0b\u589e\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a hadoop.proxyuser.livy.hosts = * hadoop.proxyuser.livy.groups = * \u53c2\u7167\u4e0a\u9762\u7684\u540c\u6837\u65b9\u6cd5\u5bf9hdfs\u670d\u52a1\uff0c hive\u670d\u52a1\u7684 core-site.xml \u6587\u4ef6\u589e\u52a0\u76f8\u540c\u7684\u914d\u7f6e\uff1a \u5b8c\u6210\u540e\u91cd\u542f\u76f8\u5173\u670d\u52a1","title":"Kerberos\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e"},{"location":"Other/Apache_Livy_0_6_0/#_3","text":"\u4f7f\u7528curl -V\u547d\u4ee4\u68c0\u67e5\u5ba2\u6237\u7aefcurl\u547d\u4ee4\u662f\u5426\u652f\u6301Kerberos Spnego \u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u5ba2\u6237\u7aef \u68c0\u67e5\u5ba2\u6237\u7aef\u65f6\u95f4\u4e0e\u5bf9\u63a5FI HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f","title":"\u5ba2\u6237\u7aef\u76f8\u5173\u68c0\u67e5"},{"location":"Other/Apache_Livy_0_6_0/#livy","text":"\u5b89\u88c5\u5bf9\u63a5FI HD\u96c6\u7fa4\u5ba2\u6237\u7aef \u68c0\u67e5Livy\u670d\u52a1\u7aef\u65f6\u95f4\u4e0e\u5bf9\u63a5FI HD\u96c6\u7fa4\u65f6\u95f4\u5c0f\u4e8e5\u5206\u949f \u68c0\u67e5livy.conf\u6587\u4ef6\u914d\u7f6e \u9700\u8981\u7279\u522b\u6ce8\u610f\u7684\u662f\uff1a livy.file.local-dir-whitelist=/opt/ \u914d\u7f6e\u53c2\u6570\u662f\u4f7f\u7528livy batch\u65b9\u5f0f\u672c\u5730\u63d0\u4ea4\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u5c06\u672c\u5730\u8def\u5f84\u6253\u5f00\u767d\u540d\u5355 launch.kerberos\u76f8\u5173\u53c2\u6570\u4e3alivy\u5b9e\u9645\u540cFI HD\u96c6\u7fa4\u4ea4\u4e92\u6240\u9700\u8981\u7684\u914d\u7f6e auth.kerberos\u76f8\u5173\u53c2\u6570\u4e3alivy\u5bf9\u5916\u4f7f\u7528kerberos\u8fdb\u884c\u8bbf\u95ee\u7ba1\u63a7\u6240\u9700\u8981\u7684\u914d\u7f6e \u68c0\u67e5livy-client.conf\u6587\u4ef6\u914d\u7f6e \u68c0\u67e5livy-env.sh\u6587\u4ef6\u914d\u7f6e \u68c0\u67e5spark-blacklist.conf\u6587\u4ef6\u914d\u7f6e \u5728log4j.properties\u914d\u7f6e\u6587\u4ef6\u4e2d\u589e\u52a0\u5982\u4e0b\u4e00\u6761\u6765\u8c03\u6574\u65e5\u5fd7\u7ea7\u522b\uff08\u53ef\u9009\uff09 log4j.logger.org.eclipse.jetty=DEBUG","title":"Livy\u670d\u52a1\u7aef\u914d\u7f6e"},{"location":"Other/Apache_Livy_0_6_0/#livy-session","text":"livy session\u65b9\u5f0f\u5bf9\u5e94spark console\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u63d0\u4ea4\u5177\u4f53\u7684\u4ee3\u7801\u7684\u65b9\u5f0f\u6765\u63d0\u4ea4\uff0c\u8fd0\u884cspark\u4efb\u52a1 \u767b\u5f55livy\u670d\u52a1\u7aef\u4f7f\u7528 bin/livy-server start \u542f\u52a8livy\u670d\u52a1 \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770blivy\u662f\u5426\u542f\u52a8\u6210\u529f \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728livy\u4e2d\u8d77\u4e00\u4e2apyspark\u7684session curl --negotiate -k -v -u developuser : -X POST --data '{\"kind\": \"pyspark\"}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/sessions \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e3asession/0\u63d0\u4ea4\u4e00\u6bb5\u4ee3\u7801 curl --negotiate -k -v -u developuser : -X POST -H 'Content-Type: application/json' -d '{\"code\":\"1 + 1\"}' http://host-172-16-2-118:8998/sessions/0/statements \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u770b\u7ed3\u679c\uff1a curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0/statements | python -m json.tool \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5173\u95edsession curl --negotiate -k -v -u : http://host-172-16-2-118:8998/sessions/0 -X DELETE \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a \u53e6\u5916\u5728\u5ba2\u6237\u7aef(172.16.2.119)\u5b8c\u6210curl\u547d\u4ee4\u63d0\u4ea4\u4efb\u52a1\u4e4b\u540e\u53ef\u4ee5\u4f7f\u7528klist\u67e5\u770b\u7968\u636e\u4fe1\u606f\uff1a \u53ef\u4ee5\u770b\u5230\u4f1a\u589e\u52a0\u8ba4\u8bc1\u7968\u636eHTTP/host-172-16-2-118@HADOOP.COM","title":"\u4f7f\u7528Livy session\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1"},{"location":"Other/Apache_Livy_0_6_0/#livy-batch1","text":"Livy batch\u65b9\u5f0f\u5bf9\u5e94spark-submit\u4ea4\u4e92\u65b9\u5f0f\uff0c\u901a\u8fc7\u63d0\u4ea4\u4e00\u4e2a\u7f16\u8bd1\u597d\u7684jar\u5305\uff0c\u6216\u8005\u662f\u5199\u597d\u7684py\u6587\u4ef6\u7b49\u6765\u63d0\u4ea4\uff0c\u8fd0\u884cspark\u4efb\u52a1 \u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn client\u6a21\u5f0f\u672c\u5730\u63d0\u4ea4\u4e00\u4e2ajar\u5305\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u5728FI HD\u5ba2\u6237\u7aef\u4e2d\u627e\u5230\u6d4b\u8bd5jar\u5305spark-examples_2.11-2.1.0.jar\u5e76\u4f20\u5230livy\u670d\u52a1\u7aef/opt/\u8def\u5f84\u4e0b \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a","title":"\u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b1"},{"location":"Other/Apache_Livy_0_6_0/#livy-batch2","text":"\u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn client\u6a21\u5f0f\u672c\u5730\u63d0\u4ea4\u4e00\u4e2apy\u6587\u4ef6\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u521b\u5efapy2.py\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u81f3Livy\u670d\u52a1\u7aef/opt/\u8def\u5f84\u4e0b\uff0c\u5177\u4f53\u5185\u5bb9\u5982\u4e0b\uff1a import sys from random import random from operator import add from pyspark.sql import SparkSession if __name__ == \"__main__\": \"\"\" Usage: pi [partitions] \"\"\" spark = SparkSession\\ .builder\\ .appName(\"PythonPi\")\\ .getOrCreate() partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2 n = 100000 * partitions def f(_): x = random() * 2 - 1 y = random() * 2 - 1 return 1 if x ** 2 + y ** 2 <= 1 else 0 count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add) print(\"Pi is roughly %f\" % (4.0 * count / n)) spark.stop() \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"file:/opt/pi2.py\" }' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a","title":"\u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b2"},{"location":"Other/Apache_Livy_0_6_0/#livy-batch3","text":"\u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn cluster\u6a21\u5f0f\u5728\u96c6\u7fa4hdfs\u8def\u5f84\u4e0b\u63d0\u4ea4jar\u5305\u5e76\u8fd0\u884c\uff0c\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u4fee\u6539livy.conf\u6587\u4ef6\u914d\u7f6e\u4e3a\uff1a \u5728\u5bf9\u63a5FI HD\u96c6\u7fa4hdfs\u7684/tmp\u8def\u5f84\u4e0b\u4e0a\u4f20jar\u5305 \u91cd\u542fLivy \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"/tmp/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a","title":"\u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b3"},{"location":"Other/Apache_Livy_0_6_0/#livy-batch4","text":"\u672c\u6837\u4f8b\u4e3a\u4f7f\u7528yarn cluster\u6a21\u5f0f\u5728\u96c6\u7fa4\u672c\u5730\u8def\u5f84\u4e0b\u63d0\u4ea4jar\u5305\u5e76\u8fd0\u884c\uff0c\u8ba1\u7b97pi\u503c \u767b\u5f55\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528 kinit developuser \u8f93\u5165\u5bc6\u7801\u83b7\u53d6\u7968\u636e\uff0c\u505a\u8fc7\u53ef\u4e0d\u7528\u518d\u505a \u56e0\u4e3a\u4f7f\u7528yarn cluster\u672c\u5730\u63d0\u4ea4jar\u5305\u6a21\u5f0f\uff0c\u4e8b\u5148\u5e76\u4e0d\u77e5\u9053worker\u5728\u54ea\u4e2a\u96c6\u7fa4\u8282\u70b9\uff0c\u6240\u4ee5\u5c06jar\u5305spark-examples_2.11-2.1.0.jar\u5206\u522b\u653e\u5230\u5404\u96c6\u7fa4\u8282\u70b9\u7684/home\u8def\u5f84\u4e0b\uff1a \u5728\u5ba2\u6237\u7aef(172.16.2.119)\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u63d0\u4ea4spark\u4efb\u52a1 curl --negotiate -k -v -u developuser : -X POST --data '{\"file\": \"local:/home/spark-examples_2.11-2.1.0.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://host-172-16-2-118:8998/batches \u6253\u5f00Livy\u7aeflivy-root-server.out\u65e5\u5fd7\u67e5\u770b\u7ed3\u679c\uff1a \u767b\u5f55\u5bf9\u63a5\u96c6\u7fa4\u7684yarn\u67e5\u770b\u7ed3\u679c\uff1a","title":"\u4f7f\u7528Livy batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b4"},{"location":"Other/Apache_Livy_0_6_0/#windowskerberos-spnegolivy-web-ui","text":"Windows\u8df3\u677f\u673a\uff08172.16.2.111\uff09\u8bbf\u95eeLivy web ui\u7684\u8ba4\u8bc1\u539f\u7406\u540c\u4e0a\u6587\u5ba2\u6237\u7aef\uff08172.16.2.119\uff09\u4f7f\u7528curl\u547d\u4ee4\u8bbf\u95eeLivy\u670d\u52a1\u7aef\uff0c\u63d0\u4ea4spark\u4efb\u52a1\u4e00\u6837 \u53c2\u8003\u4ea7\u54c1\u6587\u6863 -> \u5e94\u7528\u5f00\u53d1\u6307\u5357 -> \u5b89\u5168\u6a21\u5f0f -> Spark2x\u5f00\u53d1\u6307\u5357 -> \u73af\u5883\u51c6\u5907 -> \u51c6\u5907HiveODBC\u5f00\u53d1\u73af\u5883 -> Windows\u73af\u5883 -> \u64cd\u4f5c\u6b65\u9aa4\u7b2c1\u5230\u7b2c4\u6b65 \u5b8c\u6210MIT Kerberos\u7684\u5b89\u88c5\u914d\u7f6e \u914d\u7f6eJCE \u5230java\u5b98\u7f51\u4e0a\u4e0b\u8f7dJava Cryptography Extension (JCE)\uff0c\u7136\u540e\u89e3\u538b\u5230%JAVA_HOME%/jre/lib/security\u4e2d\u66ff\u6362\u76f8\u5e94\u7684\u6587\u4ef6\u3002 \u68c0\u67e5livy\u670d\u52a1\u7aef\u4e3b\u673a\u540d\u662f\u5426\u52a0\u5165hosts\u6587\u4ef6\uff1a \u914d\u7f6eFirefox windows\u4e0bFirefox\u9700\u8981\u901a\u8fc7\u8bbf\u95eeabout:config \u9875\u9762\u8c03\u6574\u4ee5\u4e0b\u53c2\u6570\uff1a network.negotiate-auth.trusted-uris \u5141\u8bb8\u4f7f\u7528gssapi\u94fe\u63a5\u9a8c\u8bc1\u7684\u5730\u5740 network.auth.use-sspi \u5173\u95edsspi\u9a8c\u8bc1\u534f\u8bae \u4f7f\u7528MIT Kerberos\u5b8c\u6210\u8ba4\u8bc1 \u767b\u5f55Livy\u7684web ui\u5730\u5740\u4e3ahttp://host-172-16-2-118:8998/ui \u4f7f\u7528\u4e4b\u524d\u4e00\u4e2a\u6837\u4f8b\u63d0\u4ea4\u4efb\u52a1\u5e76\u5728Livy web ui\u68c0\u67e5 \u68c0\u67e5MIT Kerberos\u751f\u6210\u7684\u670d\u52a1\u7968\u636e","title":"Windows\u8df3\u677f\u673a\u914d\u7f6eKerberos Spnego\u8bbf\u95eelivy web ui"},{"location":"Other/Apache_Livy_0_6_0/#_4","text":"session: curl -X POST --data '{\"kind\": \"pyspark\"}' -H \"Content-Type: application/json\" http://172-16-9-107:8998/sessions curl -X POST -H 'Content-Type: application/json' -d '{\"code\":\"1 + 1\"}' http://172-16-9-107:8998/sessions/0/statements curl --negotiate -k -v -u : http://172-16-9-107:8998/sessions/0/statements | python -m json.tool batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b1: curl -X POST --data '{\"file\": \"file:/opt/spark-examples_2.11-2.3.2.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://172-16-9-107:8998/batches batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b2: curl -X POST --data '{\"file\": \"file:/opt/py2.py\" }' -H \"Content-Type: application/json\" http://172-16-9-107:8998/batches batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b3: curl -X POST --data '{\"file\": \"/tmp/spark-examples_2.11-2.3.2.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://172-16-9-107:8998/batches batch\u65b9\u5f0f\u63d0\u4ea4\u4efb\u52a1\u6837\u4f8b4: curl -X POST --data '{\"file\": \"local:/home/spark-examples_2.11-2.3.2.jar\", \"className\": \"org.apache.spark.examples.SparkPi\", \"args\": [\"100\"]}' -H \"Content-Type: application/json\" http://172-16-9-107:8998/batches","title":"\u666e\u901a\u63d0\u4ea4\u547d\u4ee4"},{"location":"Other/Elasticsearch_Related/","text":"FusionInsight HD ES\u7ec4\u4ef6\u4e0e\u5468\u8fb9\u751f\u6001\u5bf9\u63a5 \u00b6 Kibana\uff0cLogstash,beats\u5bf9\u63a5\u8bf4\u660e\uff1a \u00b6 MRS 8.0\u7248\u672c\u4e0a\u8ff0\u4e09\u4e2a\u8f6f\u4ef6\u5bf9\u63a5ES\u7ec4\u4ef6\u76f8\u5173\u6587\u6863\u8bf7\u53c2\u8003\u4ea7\u54c1\u6587\u6863\u8fdb\u884c\u914d\u7f6e \u751f\u6001\u7b80\u4ecb \u00b6 Kibana: \u53ef\u6269\u5c55\u7684\u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u7ba1\u7406\u6574\u4e2a\u751f\u6001\u7ec4\u4ef6\uff08elasticsearch, logstash, beats\uff09\u4ee5\u53ca\u6570\u636e Elasticsearch: \u517c\u6709\u641c\u7d22\u5f15\u64ce\u548cNoSQL\u6570\u636e\u5e93\u529f\u80fd\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u57fa\u4e8eJAVA/Lucene\u6784\u5efa\uff0c\u5f00\u6e90\u3001\u5206\u5e03\u5f0f\u3001\u652f\u6301RESTful\u8bf7\u6c42 Logstash: \u5f00\u6e90\u7684\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u80fd\u591f\u540c\u65f6\u4ece\u591a\u4e2a\u6e90\u5934\u6536\u96c6\u6570\u636e\uff0c\u4f20\u5230Elasticsearch\uff0c\u80fd\u591f\u548cElasticsearch\u4ea7\u751f\u534f\u540c\u6548\u5e94 beats: \u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u642c\u8fd0\u5de5\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u670d\u52a1\u5668\u4e0a\u5c06\u6570\u636e\u4f20\u8f93\u5230Logstash\u6216\u8005Elasticsearch elasticsearch-head: \u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u67e5\u8be2Elasticsearch\u4e2d\u7684\u6570\u636e \u6ce8\uff1a FusionInsight HD\u7684Elasticsearch\u7ec4\u4ef6\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u4f46\u662f\u76f8\u5173\u7684\u5468\u8fb9\u751f\u6001Kibana\uff0cLogstash\uff0cbeats\uff0c elasticseach-head\u4e3a\u5f00\u6e90\uff0c\u6682\u65f6\u65e0\u6cd5\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u6545\u91c7\u7528\u5b89\u5168FI HD\u96c6\u7fa4\u7684\u975e\u5b89\u5168ES\u7ec4\u4ef6\u8fdb\u884c\u5bf9\u63a5 Logstash\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Logstash 6.4.2 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dlogstash 6.4.2, \u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**logstash-6.4.2.zip**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt/logstash \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 unzip logstash-6.4.2.zip \u89e3\u538b\u5b89\u88c5\u5305 \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2/config \u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u914d\u7f6e\u6587\u4ef6 logstash-Simple.conf ,\u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ stdin{ } } output { stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"hellow_world\" #user => \"elastic\" #password => \"changeme\" } } \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2 \u4e0b\uff0c\u6267\u884c\u547d\u4ee4 bin/logstash -f config/logstash-Simple.conf \u6839\u636e\u4e4b\u524d\u7684 logstash-Simple.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash, \u7136\u540e\u5728\u7ec8\u7aef\u624b\u52a8\u8f93\u5165\u6570\u5b571\u52306\uff1a \u767b\u5f55elasticsearch-head\u670d\u52a1\u5668\u67e5\u770b\u7ed3\u679c\uff08\u5bf9\u63a5\u6b65\u9aa4\u53c2\u89c1\u540e\u6587\uff09 Kibana\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kibana 6.1.3 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dKibana 6.1.3, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Kibana 6.1.3**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf kibana-6.1.3-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 vi /opt/kibana-6.1.3-linux-x86_64/config/kibana.yml \u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\u9009\u9879: server.port: 5601 server.host: \"172.16.52.190\" server.name: \"LinuxTest\" elasticsearch.url: \"http://172.21.3.101:24100\" \u4f7f\u7528 bin/kibana \u542f\u52a8kibana \u8bbf\u95eekibana\u767b\u5f55\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u683c\u5f0f\u4e3ahttp://Kibana\u670d\u52a1IP\u5730\u5740:5601 \u9009\u62e9**Dev Tools** \u4f7f\u7528 GET _cat/indices \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4ES\u4e2d\u7684indices \u4f7f\u7528 GET hellow_world/_search \u547d\u4ee4\u67e5\u770b\u7ed3\u679c elasticsearch-head\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6 \u00b6 \u9002\u7528\u573a\u666f \u00b6 elasticsearch-head 1.0 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u5728FusionInsight Manager\uff0c\u9009\u62e9\u670d\u52a1\u7ba1\u7406->Elasticsearch\u670d\u52a1\u914d\u7f6e->\u670d\u52a1\u914d\u7f6e(\u9009\u62e9\u5168\u90e8\u914d\u7f6e)->\u81ea\u5b9a\u4e49\uff0c\u5728elasticsearch.yml\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4e0b\u9762\u4e24\u4e2a\u914d\u7f6e\u9879\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1 http.cors.enabled = true http.cors.allow-origin = \"*\" \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e0b\u8f7d\u5e76\u4e14\u5b89\u88c5**elasticsearch-head**\u5230\u4e3b\u673a\u4e0a,\u5e76\u4e14\u542f\u52a8**elasticsearch-head**\u670d\u52a1 git clone git://github.com/mobz/elasticsearch-head.git cd elasticsearch-head npm install npm run start \u8bbf\u95eeelasticsearch-head\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u4e3ahttp://elasticsearch-head\u670d\u52a1IP\u5730\u5740:9100, \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u4e3a http://172.21.3.101:24100/ ,\u70b9\u51fb\u8fde\u63a5: \u67e5\u770b\u7ed3\u679c \u5728FI HD\u96c6\u7fa4\u4e0a\u90e8\u7f72beats \u00b6 \u9002\u7528\u573a\u666f \u00b6 filebeat 6.5.1 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch) \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u4e0b\u8f7dFilebeat 6.5.1, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Filebeat 6.5.1**\u4f7f\u7528WinSCP\u5bfc\u5165FI HD\u96c6\u7fa4\u8282\u70b9\uff08172.21.3.103\uff09\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf filebeat-6.5.1-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 cd /opt/filebeat-6.5.1-linux-x86_64 \u8fdb\u5165filebeat\u5b89\u88c5\u8def\u5f84\uff0c\u65b0\u5efa\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6 filebeat_new.yml ,\u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-host3.log path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5046\"] \u6ce8\uff1a\u7aef\u53e35046\u53ef\u4ee5\u81ea\u5df1\u6307\u5b9a\uff0c\u4e0d\u51b2\u7a81\u5373\u53ef \u4f7f\u7528\u547d\u4ee4 ./filebeat -e -c filebeat_new.yml \u542f\u52a8filebeat \u5e94\u7528\u573a\u666f\u4e3e\u4f8b\u8bf4\u660e \u00b6 \u573a\u666f\u7b80\u4ecb \u00b6 \u5206\u522b\u5728FI HD\u4e24\u4e2a\u8282\u70b9\u4e0a\u90e8\u7f72filebeat\u5b9e\u65f6\u83b7\u53d6\u4e24\u53f0\u670d\u52a1\u5668\u7684\u7684\u65e5\u5fd7\u6587\u4ef6\uff08/var/log/ipmitool.fi.log\uff09\uff0c\u901a\u8fc7logstash\u7ba1\u9053\u83b7\u53d6\u5e76\u8fc7\u6ee4\u5143\u65e5\u5fd7\u6587\u4ef6\u4e3a\u591a\u4e2a\u5b57\u6bb5\uff0c\u5e76\u4f20\u5230FI HD Elasticsearch\u7ec4\u4ef6\u4e0a\uff0c\u6700\u540e\u901a\u8fc7Kibana\u6765\u67e5\u770b\u83b7\u53d6\u7684\u65e5\u5fd7\u6587\u4ef6 \u524d\u63d0\u6761\u4ef6 \u00b6 \u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210Kibana\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210logstash\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210filebeat\u7684\u5b89\u88c5 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u9996\u5148\u767b\u9646FusionInsight HD\u96c6\u7fa4\u8282\u70b9172.21.3.102\u548c172.21.3.103\u4e0a \u53c2\u8003\u4e4b\u524d\u7684\u6b65\u9aa4\u5206\u522b\u90e8\u7f72filebeat\u5230172.21.3.102\u548c172.21.3.103\u4e0a\uff0c\u5e76\u4e14\u5bf9\u5e94\u7684\u521b\u5efafilebeat\u914d\u7f6e\u6587\u4ef6 filebeat_new_host2.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5045\"] \u914d\u7f6e\u6587\u4ef6 filebeat_new_host3.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5047\"] \u767b\u9646\u5b89\u88c5logstash\u7684\u4e3b\u673a\uff0c\u914d\u7f6e\u4e00\u4e2a\u65b0\u7684\u542f\u52a8\u6587\u4ef6 logstash-beats-ipmitool.conf \u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ beats{ port => \"5045\" } beats{ port => \"5047\" } } filter{ grok{ match =>{\"message\" => \"(?<object>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<object2>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<status>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<number>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|%{GREEDYDATA:additional_info}\"} } } output{ stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"ipmitool_log\" } } \u542f\u52a8kibana \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-beats-ipmitool.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash \u5206\u522b\u542f\u52a8172.21.3.102\u548c172.21.3.103\u4e0a\u7684filebeat\u6765\u83b7\u53d6\u65e5\u5fd7\u6587\u4ef6 \u767b\u9646kibana\u7f51\u9875\u754c\u9762\uff0c\u9009\u62e9**Management**\u4e0b\u9762\u7684**Index Patterns** \u5728**Create index pattern**\u4e0b\u9009\u62e9\u521a\u521a\u7531logstash\u4f20\u8f93\u521b\u5efa\u7684index ipmitool_log \uff0c \u70b9\u51fb Next step \u5728step 2\u4e2d\u9009\u62e9**@timestamp**, \u70b9\u51fb**Create index pattern** \u5728 Discover \u90e8\u5206\u9009\u62e9\u521a\u521a\u751f\u6210\u7684index pattern ipmitool_log \u53ef\u4ee5\u770b\u5230\u6574\u4e2a\u65e5\u5fd7\u7684\u60c5\u51b5 \u53ef\u6839\u636e\u4e0d\u540c\u7684\u5b57\u6bb5\u60c5\u51b5\u6765\u6574\u4f53\u4e86\u89e3\u65e5\u5fd7\u60c5\u51b5 \u5b8c\u6210","title":"6.5.1 <--> 8.0"},{"location":"Other/Elasticsearch_Related/#fusioninsight-hd-es","text":"","title":"FusionInsight HD ES\u7ec4\u4ef6\u4e0e\u5468\u8fb9\u751f\u6001\u5bf9\u63a5"},{"location":"Other/Elasticsearch_Related/#kibanalogstashbeats","text":"MRS 8.0\u7248\u672c\u4e0a\u8ff0\u4e09\u4e2a\u8f6f\u4ef6\u5bf9\u63a5ES\u7ec4\u4ef6\u76f8\u5173\u6587\u6863\u8bf7\u53c2\u8003\u4ea7\u54c1\u6587\u6863\u8fdb\u884c\u914d\u7f6e","title":"Kibana\uff0cLogstash,beats\u5bf9\u63a5\u8bf4\u660e\uff1a"},{"location":"Other/Elasticsearch_Related/#_1","text":"Kibana: \u53ef\u6269\u5c55\u7684\u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u7ba1\u7406\u6574\u4e2a\u751f\u6001\u7ec4\u4ef6\uff08elasticsearch, logstash, beats\uff09\u4ee5\u53ca\u6570\u636e Elasticsearch: \u517c\u6709\u641c\u7d22\u5f15\u64ce\u548cNoSQL\u6570\u636e\u5e93\u529f\u80fd\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u57fa\u4e8eJAVA/Lucene\u6784\u5efa\uff0c\u5f00\u6e90\u3001\u5206\u5e03\u5f0f\u3001\u652f\u6301RESTful\u8bf7\u6c42 Logstash: \u5f00\u6e90\u7684\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u80fd\u591f\u540c\u65f6\u4ece\u591a\u4e2a\u6e90\u5934\u6536\u96c6\u6570\u636e\uff0c\u4f20\u5230Elasticsearch\uff0c\u80fd\u591f\u548cElasticsearch\u4ea7\u751f\u534f\u540c\u6548\u5e94 beats: \u8f7b\u91cf\u7ea7\u7684\u6570\u636e\u642c\u8fd0\u5de5\uff0c\u80fd\u591f\u90e8\u7f72\u5728\u670d\u52a1\u5668\u4e0a\u5c06\u6570\u636e\u4f20\u8f93\u5230Logstash\u6216\u8005Elasticsearch elasticsearch-head: \u7528\u6237\u754c\u9762\uff0c\u80fd\u591f\u67e5\u8be2Elasticsearch\u4e2d\u7684\u6570\u636e \u6ce8\uff1a FusionInsight HD\u7684Elasticsearch\u7ec4\u4ef6\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u4f46\u662f\u76f8\u5173\u7684\u5468\u8fb9\u751f\u6001Kibana\uff0cLogstash\uff0cbeats\uff0c elasticseach-head\u4e3a\u5f00\u6e90\uff0c\u6682\u65f6\u65e0\u6cd5\u652f\u6301\u5b89\u5168\u6a21\u5f0f\uff0c\u6545\u91c7\u7528\u5b89\u5168FI HD\u96c6\u7fa4\u7684\u975e\u5b89\u5168ES\u7ec4\u4ef6\u8fdb\u884c\u5bf9\u63a5","title":"\u751f\u6001\u7b80\u4ecb"},{"location":"Other/Elasticsearch_Related/#logstashfusioninsight-hd-es","text":"","title":"Logstash\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_2","text":"Logstash 6.4.2 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_3","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_4","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dlogstash 6.4.2, \u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**logstash-6.4.2.zip**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt/logstash \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 unzip logstash-6.4.2.zip \u89e3\u538b\u5b89\u88c5\u5305 \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2/config \u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u914d\u7f6e\u6587\u4ef6 logstash-Simple.conf ,\u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ stdin{ } } output { stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"hellow_world\" #user => \"elastic\" #password => \"changeme\" } } \u5230\u8def\u5f84 /opt/logstash/logstash-6.4.2 \u4e0b\uff0c\u6267\u884c\u547d\u4ee4 bin/logstash -f config/logstash-Simple.conf \u6839\u636e\u4e4b\u524d\u7684 logstash-Simple.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash, \u7136\u540e\u5728\u7ec8\u7aef\u624b\u52a8\u8f93\u5165\u6570\u5b571\u52306\uff1a \u767b\u5f55elasticsearch-head\u670d\u52a1\u5668\u67e5\u770b\u7ed3\u679c\uff08\u5bf9\u63a5\u6b65\u9aa4\u53c2\u89c1\u540e\u6587\uff09","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#kibanafusioninsight-hd-es","text":"","title":"Kibana\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_5","text":"Kibana 6.1.3 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_6","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_7","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u4e0b\u8f7dKibana 6.1.3, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Kibana 6.1.3**\u4f7f\u7528WinSCP\u5bfc\u5165\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf kibana-6.1.3-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 vi /opt/kibana-6.1.3-linux-x86_64/config/kibana.yml \u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\u9009\u9879: server.port: 5601 server.host: \"172.16.52.190\" server.name: \"LinuxTest\" elasticsearch.url: \"http://172.21.3.101:24100\" \u4f7f\u7528 bin/kibana \u542f\u52a8kibana \u8bbf\u95eekibana\u767b\u5f55\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u683c\u5f0f\u4e3ahttp://Kibana\u670d\u52a1IP\u5730\u5740:5601 \u9009\u62e9**Dev Tools** \u4f7f\u7528 GET _cat/indices \u547d\u4ee4\u67e5\u770b\u96c6\u7fa4ES\u4e2d\u7684indices \u4f7f\u7528 GET hellow_world/_search \u547d\u4ee4\u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#elasticsearch-headfusioninsight-hd-es","text":"","title":"elasticsearch-head\u5bf9\u63a5FusionInsight HD ES\u7ec4\u4ef6"},{"location":"Other/Elasticsearch_Related/#_8","text":"elasticsearch-head 1.0 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_9","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_10","text":"\u767b\u5f55FusionInsight Manager\u7f51\u9875\uff0c\u68c0\u67e5ES\u7ec4\u4ef6\u662f\u5426\u4e3a\u5b89\u5168\u6a21\u5f0f\uff0c\u5982\u679c\u662f\uff0c\u4fee\u6539\u914d\u7f6e\u4f7f\u5176\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1\uff1a \u5728FusionInsight Manager\uff0c\u9009\u62e9\u670d\u52a1\u7ba1\u7406->Elasticsearch\u670d\u52a1\u914d\u7f6e->\u670d\u52a1\u914d\u7f6e(\u9009\u62e9\u5168\u90e8\u914d\u7f6e)->\u81ea\u5b9a\u4e49\uff0c\u5728elasticsearch.yml\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4e0b\u9762\u4e24\u4e2a\u914d\u7f6e\u9879\uff0c\u5b8c\u6210\u540e\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\u91cd\u542felasticsearch\u670d\u52a1 http.cors.enabled = true http.cors.allow-origin = \"*\" \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u4e0b\u8f7d\u5e76\u4e14\u5b89\u88c5**elasticsearch-head**\u5230\u4e3b\u673a\u4e0a,\u5e76\u4e14\u542f\u52a8**elasticsearch-head**\u670d\u52a1 git clone git://github.com/mobz/elasticsearch-head.git cd elasticsearch-head npm install npm run start \u8bbf\u95eeelasticsearch-head\u754c\u9762\uff0c\u8bbf\u95ee\u5730\u5740\u4e3ahttp://elasticsearch-head\u670d\u52a1IP\u5730\u5740:9100, \u8f93\u5165\u8fde\u63a5\u4fe1\u606f\u4e3a http://172.21.3.101:24100/ ,\u70b9\u51fb\u8fde\u63a5: \u67e5\u770b\u7ed3\u679c","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#fi-hdbeats","text":"","title":"\u5728FI HD\u96c6\u7fa4\u4e0a\u90e8\u7f72beats"},{"location":"Other/Elasticsearch_Related/#_11","text":"filebeat 6.5.1 \u2194 FusionInsight HD V100R002C80SPC200 (ElasticSearch)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Elasticsearch_Related/#_12","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5\u3002","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_13","text":"\u4e0b\u8f7dFilebeat 6.5.1, \u4e0b\u8f7d\u7f51\u5740\u4e3a\uff1a https://www.elastic.co/downloads/past-releases \u5c06\u4e0b\u8f7d\u540e\u7684**Filebeat 6.5.1**\u4f7f\u7528WinSCP\u5bfc\u5165FI HD\u96c6\u7fa4\u8282\u70b9\uff08172.21.3.103\uff09\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u4f7f\u7528 tar -xzf filebeat-6.5.1-linux-x86_64.tar.gz \u89e3\u538b\u5b89\u88c5\u5305 \u4f7f\u7528 cd /opt/filebeat-6.5.1-linux-x86_64 \u8fdb\u5165filebeat\u5b89\u88c5\u8def\u5f84\uff0c\u65b0\u5efa\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6 filebeat_new.yml ,\u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/Bigdata/zookeeper/quorumpeer/zookeeper-omm-server-host3.log path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5046\"] \u6ce8\uff1a\u7aef\u53e35046\u53ef\u4ee5\u81ea\u5df1\u6307\u5b9a\uff0c\u4e0d\u51b2\u7a81\u5373\u53ef \u4f7f\u7528\u547d\u4ee4 ./filebeat -e -c filebeat_new.yml \u542f\u52a8filebeat","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/Elasticsearch_Related/#_14","text":"","title":"\u5e94\u7528\u573a\u666f\u4e3e\u4f8b\u8bf4\u660e"},{"location":"Other/Elasticsearch_Related/#_15","text":"\u5206\u522b\u5728FI HD\u4e24\u4e2a\u8282\u70b9\u4e0a\u90e8\u7f72filebeat\u5b9e\u65f6\u83b7\u53d6\u4e24\u53f0\u670d\u52a1\u5668\u7684\u7684\u65e5\u5fd7\u6587\u4ef6\uff08/var/log/ipmitool.fi.log\uff09\uff0c\u901a\u8fc7logstash\u7ba1\u9053\u83b7\u53d6\u5e76\u8fc7\u6ee4\u5143\u65e5\u5fd7\u6587\u4ef6\u4e3a\u591a\u4e2a\u5b57\u6bb5\uff0c\u5e76\u4f20\u5230FI HD Elasticsearch\u7ec4\u4ef6\u4e0a\uff0c\u6700\u540e\u901a\u8fc7Kibana\u6765\u67e5\u770b\u83b7\u53d6\u7684\u65e5\u5fd7\u6587\u4ef6","title":"\u573a\u666f\u7b80\u4ecb"},{"location":"Other/Elasticsearch_Related/#_16","text":"\u5df2\u5b8c\u6210FusionInsight HD\u548c\u5ba2\u6237\u7aef\u7684\u5b89\u88c5 FusionInsight HD\u5305\u542bElasticSearch\u7ec4\u4ef6 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210Kibana\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210logstash\u7684\u5b89\u88c5 \u5df2\u4e86\u89e3\u548c\u5b8c\u6210filebeat\u7684\u5b89\u88c5","title":"\u524d\u63d0\u6761\u4ef6"},{"location":"Other/Elasticsearch_Related/#_17","text":"\u9996\u5148\u767b\u9646FusionInsight HD\u96c6\u7fa4\u8282\u70b9172.21.3.102\u548c172.21.3.103\u4e0a \u53c2\u8003\u4e4b\u524d\u7684\u6b65\u9aa4\u5206\u522b\u90e8\u7f72filebeat\u5230172.21.3.102\u548c172.21.3.103\u4e0a\uff0c\u5e76\u4e14\u5bf9\u5e94\u7684\u521b\u5efafilebeat\u914d\u7f6e\u6587\u4ef6 filebeat_new_host2.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5045\"] \u914d\u7f6e\u6587\u4ef6 filebeat_new_host3.yml \u5185\u5bb9\u5982\u4e0b\uff1a filebeat.prospectors: - type: log enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/ipmitool* path.home: /opt/filebeat-6.5.1-linux-x86_64 path.config: ${path.home} setup.kibana: host: \"172.16.52.190:5601\" setup.template.settings: index.number_of_shards: 3 output.logstash: hosts: [\"172.16.52.190:5047\"] \u767b\u9646\u5b89\u88c5logstash\u7684\u4e3b\u673a\uff0c\u914d\u7f6e\u4e00\u4e2a\u65b0\u7684\u542f\u52a8\u6587\u4ef6 logstash-beats-ipmitool.conf \u5185\u5bb9\u5982\u4e0b\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input{ beats{ port => \"5045\" } beats{ port => \"5047\" } } filter{ grok{ match =>{\"message\" => \"(?<object>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<object2>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<status>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|(?<number>[A-Za-z0-9$.+!*'(){},~@#%&/=:;_?\\-\\[\\]<> ]+)\\|%{GREEDYDATA:additional_info}\"} } } output{ stdout{ codec => dots {} } elasticsearch { hosts => [\"http://172.21.3.101:24100\"] index => \"ipmitool_log\" } } \u542f\u52a8kibana \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-beats-ipmitool.conf \u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u6765\u542f\u52a8logstash \u5206\u522b\u542f\u52a8172.21.3.102\u548c172.21.3.103\u4e0a\u7684filebeat\u6765\u83b7\u53d6\u65e5\u5fd7\u6587\u4ef6 \u767b\u9646kibana\u7f51\u9875\u754c\u9762\uff0c\u9009\u62e9**Management**\u4e0b\u9762\u7684**Index Patterns** \u5728**Create index pattern**\u4e0b\u9009\u62e9\u521a\u521a\u7531logstash\u4f20\u8f93\u521b\u5efa\u7684index ipmitool_log \uff0c \u70b9\u51fb Next step \u5728step 2\u4e2d\u9009\u62e9**@timestamp**, \u70b9\u51fb**Create index pattern** \u5728 Discover \u90e8\u5206\u9009\u62e9\u521a\u521a\u751f\u6210\u7684index pattern ipmitool_log \u53ef\u4ee5\u770b\u5230\u6574\u4e2a\u65e5\u5fd7\u7684\u60c5\u51b5 \u53ef\u6839\u636e\u4e0d\u540c\u7684\u5b57\u6bb5\u60c5\u51b5\u6765\u6574\u4f53\u4e86\u89e3\u65e5\u5fd7\u60c5\u51b5 \u5b8c\u6210","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/FUSE/","text":"FUSE\u5bf9\u63a5FusionInsight HDFS \u00b6 \u9002\u7528\u573a\u666f \u00b6 FUSE 2.8.3 \u2194 FusionInsight HD V100R002C60U20\uff08HDFS\uff09 \u8bf4\u660e \u00b6 \u901a\u8fc7\u4f7f\u7528FUSE\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c06\u8fdc\u7aef\u7684HDFS\u6587\u4ef6\u7cfb\u7edfmount\u5230\u672c\u7aef\u7684Linux\u7cfb\u7edf\u4e2d\u4f7f\u7528\u3002FusionInsight HD\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\u3002 \u914d\u7f6e\u5bf9\u63a5 \u00b6 \u5b89\u88c5jdk1.8 tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf /etc/profile \uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf export JAVA_HOME = /opt/jdk1.8.0_112 export CLASSPATH = .: $JAVA_HOME /lib/dt.jar: $JAVA_HOME /lib/tools.jar export PATH = $JAVA_HOME /bin: $PATH source /etc/profile \u5b89\u88c5rpm\u5305 yum install fuse fuse-devel fuse-libs \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u4ea7\u54c1\u6587\u6863\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u4f8b\u5982\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \u4e0b\u8f7dHadoop-2.7.2\u6e90\u7801hadoop-2.7.2-src.tar.gz\uff0c\u7f16\u8bd1fuse_dfs\uff0c\u5c06\u7f16\u8bd1\u597d\u7684fuse_dfs\u62f7\u8d1d\u5230/opt\u76ee\u5f55\u4e0b \u5c06\u6e90\u7801\u4e0b\u7684fuse_dfs_wrapper.sh\u811a\u672c\u62f7\u8d1d\u81f3 /opt \u76ee\u5f55\u4e0b\uff0c\u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u4f5c\u5982\u4e0b\u4fee\u6539(\u4fee\u6539HADOOP_PREFIX\u548cJAVA_HOME\u7684\u914d\u7f6e)\uff1a export HADOOP_PREFIX = /opt/hadoopclient/HDFS/hadoop if [ \" $OS_ARCH \" = \"\" ] ; then export OS_ARCH = amd64 fi if [ \" $JAVA_HOME \" = \"\" ] ; then export JAVA_HOME = /opt/jdk1.8.0_112 fi if [ \" $LD_LIBRARY_PATH \" = \"\" ] ; then export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib fi # If dev build set paths accordingly if [ -d $HADOOP_PREFIX /share ] ; then export HADOOP_PREFIX = $HADOOP_PREFIX for f in ${ HADOOP_PREFIX } /share/hadoop/hdfs/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in $HADOOP_PREFIX /share/hadoop/hdfs/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/*.jar ; do export CLASSPATH = $CLASSPATH : $f done export PATH = /opt: $PATH export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib: $LD_LIBRARY_PATH fi fuse_dfs $@ \u66f4\u6539\u6587\u4ef6\u6743\u9650 chmod 755 fuse_dfs chmod 755 fuse_dfs_wrapper.sh Source\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u672c\u5730\u521b\u5efamount\u76ee\u5f55 mkdir \u2013p /mnt/hdfs \u6267\u884c\u6302\u8f7d\u811a\u672c\uff0c\u5176\u4e2d162-1-95-196\u662fHDFS\u7684NameNode(hacluster,\u4e3b)\u7684\u4e3b\u673a\u540d ./fuse_dfs_wrapper.sh dfs://162-1-95-196:25000 /mnt/hdfs/ \u67e5\u770b/mnt/hdfs\u76ee\u5f55 \u5982\u679c\u9700\u8981\u5378\u8f7d\u6302\u8f7d\u70b9\uff0c\u6267\u884cumount /mnt/hdfs\u5373\u53ef \u9a8c\u8bc1\u5bf9\u63a5 \u00b6 hdfsclient\u5199 dd if=/dev/zero bs=4096 count=1024 | hadoop fs -put - /tmp/fuse/hdfsclient-01.dat hdfsclient\u8bfb hadoop fs -get /tmp/fuse/hdfsclient-01.dat - > /dev/null fuse\u5199 dd if=/dev/zero bs=4096 count=1024 of=/mnt/hdfs/tmp/fuse/fuse-01.dat fuse\u8bfb dd if=/mnt/hdfs/tmp/fuse/fuse-01.dat bs=4096 of=/dev/null","title":"FUSE\u5bf9\u63a5FusionInsight HDFS"},{"location":"Other/FUSE/#fusefusioninsight-hdfs","text":"","title":"FUSE\u5bf9\u63a5FusionInsight HDFS"},{"location":"Other/FUSE/#_1","text":"FUSE 2.8.3 \u2194 FusionInsight HD V100R002C60U20\uff08HDFS\uff09","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/FUSE/#_2","text":"\u901a\u8fc7\u4f7f\u7528FUSE\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u5c06\u8fdc\u7aef\u7684HDFS\u6587\u4ef6\u7cfb\u7edfmount\u5230\u672c\u7aef\u7684Linux\u7cfb\u7edf\u4e2d\u4f7f\u7528\u3002FusionInsight HD\u4e3a\u975e\u5b89\u5168\u6a21\u5f0f\u3002","title":"\u8bf4\u660e"},{"location":"Other/FUSE/#_3","text":"\u5b89\u88c5jdk1.8 tar -xvf jdk-8u112-linux-x64.tar.gz \u914d\u7f6e\u73af\u5883\u53d8\u91cf /etc/profile \uff0c\u52a0\u5165\u5982\u4e0b\u5185\u5bb9\uff0csource\u73af\u5883\u53d8\u91cf export JAVA_HOME = /opt/jdk1.8.0_112 export CLASSPATH = .: $JAVA_HOME /lib/dt.jar: $JAVA_HOME /lib/tools.jar export PATH = $JAVA_HOME /bin: $PATH source /etc/profile \u5b89\u88c5rpm\u5305 yum install fuse fuse-devel fuse-libs \u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u4ea7\u54c1\u6587\u6863\u7684 \u5b89\u88c5\u5ba2\u6237\u7aef \u7ae0\u8282\uff0c\u4f8b\u5982\u5ba2\u6237\u7aef\u5b89\u88c5\u76ee\u5f55\u4e3a /opt/hadoopclient/ \u4e0b\u8f7dHadoop-2.7.2\u6e90\u7801hadoop-2.7.2-src.tar.gz\uff0c\u7f16\u8bd1fuse_dfs\uff0c\u5c06\u7f16\u8bd1\u597d\u7684fuse_dfs\u62f7\u8d1d\u5230/opt\u76ee\u5f55\u4e0b \u5c06\u6e90\u7801\u4e0b\u7684fuse_dfs_wrapper.sh\u811a\u672c\u62f7\u8d1d\u81f3 /opt \u76ee\u5f55\u4e0b\uff0c\u5e76\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u4f5c\u5982\u4e0b\u4fee\u6539(\u4fee\u6539HADOOP_PREFIX\u548cJAVA_HOME\u7684\u914d\u7f6e)\uff1a export HADOOP_PREFIX = /opt/hadoopclient/HDFS/hadoop if [ \" $OS_ARCH \" = \"\" ] ; then export OS_ARCH = amd64 fi if [ \" $JAVA_HOME \" = \"\" ] ; then export JAVA_HOME = /opt/jdk1.8.0_112 fi if [ \" $LD_LIBRARY_PATH \" = \"\" ] ; then export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib fi # If dev build set paths accordingly if [ -d $HADOOP_PREFIX /share ] ; then export HADOOP_PREFIX = $HADOOP_PREFIX for f in ${ HADOOP_PREFIX } /share/hadoop/hdfs/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in $HADOOP_PREFIX /share/hadoop/hdfs/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/lib/*.jar ; do export CLASSPATH = $CLASSPATH : $f done for f in ${ HADOOP_PREFIX } /share/hadoop/common/*.jar ; do export CLASSPATH = $CLASSPATH : $f done export PATH = /opt: $PATH export LD_LIBRARY_PATH = $JAVA_HOME /jre/lib/ $OS_ARCH /server: $HADOOP_PREFIX /lib/native:/usr/local/lib: $LD_LIBRARY_PATH fi fuse_dfs $@ \u66f4\u6539\u6587\u4ef6\u6743\u9650 chmod 755 fuse_dfs chmod 755 fuse_dfs_wrapper.sh Source\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u672c\u5730\u521b\u5efamount\u76ee\u5f55 mkdir \u2013p /mnt/hdfs \u6267\u884c\u6302\u8f7d\u811a\u672c\uff0c\u5176\u4e2d162-1-95-196\u662fHDFS\u7684NameNode(hacluster,\u4e3b)\u7684\u4e3b\u673a\u540d ./fuse_dfs_wrapper.sh dfs://162-1-95-196:25000 /mnt/hdfs/ \u67e5\u770b/mnt/hdfs\u76ee\u5f55 \u5982\u679c\u9700\u8981\u5378\u8f7d\u6302\u8f7d\u70b9\uff0c\u6267\u884cumount /mnt/hdfs\u5373\u53ef","title":"\u914d\u7f6e\u5bf9\u63a5"},{"location":"Other/FUSE/#_4","text":"hdfsclient\u5199 dd if=/dev/zero bs=4096 count=1024 | hadoop fs -put - /tmp/fuse/hdfsclient-01.dat hdfsclient\u8bfb hadoop fs -get /tmp/fuse/hdfsclient-01.dat - > /dev/null fuse\u5199 dd if=/dev/zero bs=4096 count=1024 of=/mnt/hdfs/tmp/fuse/fuse-01.dat fuse\u8bfb dd if=/mnt/hdfs/tmp/fuse/fuse-01.dat bs=4096 of=/dev/null","title":"\u9a8c\u8bc1\u5bf9\u63a5"},{"location":"Other/GIS_Tools/","text":"GIS Tools for Hadoop\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 GIS Tools for Hadoop 1.0 \u2194 FusionInsight HD V100R002C60U20 (Hive/MapReduce) aggregation-hive \u00b6 \u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-hive \u4e2d\u5173\u4e8e\u96c6\u6210Hive\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2aHiveAdmin\u89d2\u8272\uff0c\u5177\u4f53\u8bf7\u53c2\u52a0\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efaHive\u89d2\u8272 \u7ae0\u8282\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237 testuser \u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7FusionInsight HD\u7684\u5ba2\u6237\u7aef\u4e0a\u4f20\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u5c06\u76ee\u5f55gis-tools-for-hadoop-master\u76f4\u63a5\u653e\u5230HDFS\u7684\u6839\u76ee\u5f55\u4e0b\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser hadoop fs -put -f /opt/gis-tools-for-hadoop-master /gis-tools-for-hadoop-master \u4fee\u6539\u6267\u884chive\u793a\u4f8b\u7684sql\u6587\u4ef6\uff0c\u4fee\u6539\u540e\u7684\u6587\u4ef6\u5982\u4e0b set role admin ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / esri - geometry - api . jar ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / spatial - sdk - hadoop . jar ; reload function ; DROP TABLE earthquakes ; DROP TABLE counties ; create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point' ; create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains' ; CREATE EXTERNAL TABLE IF NOT EXISTS earthquakes ( earthquake_date STRING , latitude DOUBLE , longitude DOUBLE , depth DOUBLE , magnitude DOUBLE , magtype string , mbstations string , gap string , distance string , rms string , source string , eventid string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/earthquake-data' ; CREATE EXTERNAL TABLE IF NOT EXISTS counties ( Area string , Perimeter string , State string , County string , Name string , BoundaryShape binary ) ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde' STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedJsonInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/counties-data' ; SELECT counties . name , count ( * ) cnt FROM counties JOIN earthquakes WHERE ST_Contains ( counties . boundaryshape , ST_Point ( earthquakes . longitude , earthquakes . latitude )) GROUP BY counties . name ORDER BY cnt desc ; \u4f7f\u7528FusionInsight HD\u5ba2\u6237\u7aef\u6267\u884c\u4fee\u6539\u540e\u7684sql\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt beeline -f gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-hive/run-sample.sql \u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff0c\u4e0eGIS\u5f00\u6e90\u7f51\u7ad9\u63cf\u8ff0\u4e00\u81f4 aggregation-mr \u00b6 \u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-mr \u4e2d\u5173\u4e8e\u96c6\u6210MR\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctestuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/sample-config.sh \u5982\u4e0b\uff0c\u5176\u4e2d26004\u4e3ayarn\u914d\u7f6e\u7684yarn.resourcemanager.port\u7aef\u53e3 #!/bin/bash NAME_NODE_URL = hdfs://hacluster JOB_TRACKER_URL = 162 .1.93.103:26004 SAMPLE_DIR = /tmp/gistest JOB_DIR = $SAMPLE_DIR /job LIB_DIR = $SAMPLE_DIR /lib DATA_DIR = $SAMPLE_DIR /data OUTPUT_DIR = $SAMPLE_DIR /output \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/run-sample.sh \u7684\u6267\u884c\u6743\u9650\uff0c\u5e76\u6267\u884c source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/ chmod u+x run-sample.sh sh run-sample.sh \u6267\u884c\u5b8c\u6bd5\u5f97\u5230\u5982\u4e0b\u7ed3\u679c\u6587\u4ef6result.txt","title":"1.0 <--> C60"},{"location":"Other/GIS_Tools/#gis-tools-for-hadoopfusioninsight","text":"","title":"GIS Tools for Hadoop\u5bf9\u63a5FusionInsight"},{"location":"Other/GIS_Tools/#_1","text":"GIS Tools for Hadoop 1.0 \u2194 FusionInsight HD V100R002C60U20 (Hive/MapReduce)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/GIS_Tools/#aggregation-hive","text":"\u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-hive \u4e2d\u5173\u4e8e\u96c6\u6210Hive\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2aHiveAdmin\u89d2\u8272\uff0c\u5177\u4f53\u8bf7\u53c2\u52a0\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efaHive\u89d2\u8272 \u7ae0\u8282\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237 testuser \u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7FusionInsight HD\u7684\u5ba2\u6237\u7aef\u4e0a\u4f20\u5230HDFS\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u5c06\u76ee\u5f55gis-tools-for-hadoop-master\u76f4\u63a5\u653e\u5230HDFS\u7684\u6839\u76ee\u5f55\u4e0b\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser hadoop fs -put -f /opt/gis-tools-for-hadoop-master /gis-tools-for-hadoop-master \u4fee\u6539\u6267\u884chive\u793a\u4f8b\u7684sql\u6587\u4ef6\uff0c\u4fee\u6539\u540e\u7684\u6587\u4ef6\u5982\u4e0b set role admin ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / esri - geometry - api . jar ; add jar hdfs : /// gis - tools - for - hadoop - master / samples / lib / spatial - sdk - hadoop . jar ; reload function ; DROP TABLE earthquakes ; DROP TABLE counties ; create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point' ; create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains' ; CREATE EXTERNAL TABLE IF NOT EXISTS earthquakes ( earthquake_date STRING , latitude DOUBLE , longitude DOUBLE , depth DOUBLE , magnitude DOUBLE , magtype string , mbstations string , gap string , distance string , rms string , source string , eventid string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/earthquake-data' ; CREATE EXTERNAL TABLE IF NOT EXISTS counties ( Area string , Perimeter string , State string , County string , Name string , BoundaryShape binary ) ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde' STORED AS INPUTFORMAT 'com.esri.json.hadoop.EnclosedJsonInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs:///gis-tools-for-hadoop-master/samples/data/counties-data' ; SELECT counties . name , count ( * ) cnt FROM counties JOIN earthquakes WHERE ST_Contains ( counties . boundaryshape , ST_Point ( earthquakes . longitude , earthquakes . latitude )) GROUP BY counties . name ORDER BY cnt desc ; \u4f7f\u7528FusionInsight HD\u5ba2\u6237\u7aef\u6267\u884c\u4fee\u6539\u540e\u7684sql\u6587\u4ef6\uff0c\u547d\u4ee4\u53c2\u8003 source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt beeline -f gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-hive/run-sample.sql \u6267\u884c\u7ed3\u679c\u5982\u4e0b\uff0c\u4e0eGIS\u5f00\u6e90\u7f51\u7ad9\u63cf\u8ff0\u4e00\u81f4","title":"aggregation-hive"},{"location":"Other/GIS_Tools/#aggregation-mr","text":"\u53c2\u8003GIS\u8bf4\u660e https://github.com/Esri/gis-tools-for-hadoop/tree/master/samples/point-in-polygon-aggregation-mr \u4e2d\u5173\u4e8e\u96c6\u6210MR\u7684\u793a\u4f8b\uff0c\u5728\u534e\u4e3aFusionInsight HD\u4e2d\u6267\u884c\u8be5\u793a\u4f8b\u3002 \u83b7\u53d6gis\u6e90\u4ee3\u7801https://github.com/Esri/gis-tools-for-hadoop/ \u5b8c\u6210FusionInsight HD V100R002C60U20\u7684\u5b89\u88c5\uff0c\u5305\u542bHive\u7ec4\u4ef6\u3002 \u5728FusionInsight Manager\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u521b\u5efa\u7528\u6237\u7ae0\u8282\u3002\u5c06\u7528\u6237\u52a0\u5165\u4e0a\u9762\u521b\u5efa\u7684\u89d2\u8272HiveAdmin\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237\u201ctestuser\u201d\u5e76\u4e0b\u8f7d\u5bf9\u5e94\u7684keytab\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6 \u5b89\u88c5FusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD \u7ba1\u7406\u5458\u6307\u5357\u300b\u7684\u5b89\u88c5\u548c\u4f7f\u7528\u5ba2\u6237\u7aef\u7ae0\u8282\u3002 \u5c06\u4e0b\u8f7d\u7684gis tools\u6e90\u7801\u901a\u8fc7WinSCP\u5de5\u5177\u4e0a\u4f20\u5230\u5b89\u88c5\u6709FusionInsight HD\u5ba2\u6237\u7aef\u6240\u5728\u8282\u70b9\u7684 /opt \u76ee\u5f55\u4e0b\uff0c\u4e0a\u4f20\u6e90\u7801\u76ee\u5f55\u4e3agis-tools-for-hadoop-master \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/sample-config.sh \u5982\u4e0b\uff0c\u5176\u4e2d26004\u4e3ayarn\u914d\u7f6e\u7684yarn.resourcemanager.port\u7aef\u53e3 #!/bin/bash NAME_NODE_URL = hdfs://hacluster JOB_TRACKER_URL = 162 .1.93.103:26004 SAMPLE_DIR = /tmp/gistest JOB_DIR = $SAMPLE_DIR /job LIB_DIR = $SAMPLE_DIR /lib DATA_DIR = $SAMPLE_DIR /data OUTPUT_DIR = $SAMPLE_DIR /output \u4fee\u6539 /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/run-sample.sh \u7684\u6267\u884c\u6743\u9650\uff0c\u5e76\u6267\u884c source /opt/hadoopclient/bigdata_env kinit -k -t /opt/user.keytab testuser cd /opt/gis-tools-for-hadoop-master/samples/point-in-polygon-aggregation-mr/cmd/ chmod u+x run-sample.sh sh run-sample.sh \u6267\u884c\u5b8c\u6bd5\u5f97\u5230\u5982\u4e0b\u7ed3\u679c\u6587\u4ef6result.txt","title":"aggregation-mr"},{"location":"Other/IBM_WAS/","text":"IBM WAS\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 IBM WAS 8.5.5.9 \u2194 FusionInsight HD V100R002C50 (IBM_JDK)","title":"8.5.5.9 <--> C50"},{"location":"Other/IBM_WAS/#ibm-wasfusioninsight","text":"","title":"IBM WAS\u5bf9\u63a5FusionInsight"},{"location":"Other/IBM_WAS/#_1","text":"IBM WAS 8.5.5.9 \u2194 FusionInsight HD V100R002C50 (IBM_JDK)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Logstash/","text":"Logstash\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Logstash 6.7.1 \u2194 FusionInsight HD 6.5 (HDFS/Kafka) Logstash 6.7.1 \u2194 FusionInsight MRS 8.0 (HDFS/Kafka) HDFS\u5bf9\u63a5 \u00b6 HDFS\u5bf9\u63a5\u73af\u5883\u8bf4\u660e\uff1a logstash\u4e3b\u673a\uff1a172.16.9.107 FI HD\u96c6\u7fa4\uff1a172.16.4.121-123 HDFS\u5bf9\u63a5\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u00b6 logstash \u4f7f\u7528\u5305\u542b webhdfs output\u63d2\u4ef6\u540c\u96c6\u7fa4\u4ea4\u4e92\uff0c\u9996\u5148\u9700\u8981\u914d\u7f6ewebhdfs\u76f8\u5173\u914d\u7f6e\u9879 HDFS\u662f\u901a\u8fc7WebHDFS\u8fde\u63a5\uff0c\u524d\u63d0\u6761\u4ef6\u662f\u83b7\u53d6kerberos\u7684\u7f13\u5b58\u7968\u636e\uff0c\u6240\u4ee5\u9700\u8981\u5728Logstash\u90e8\u7f72\u7684\u4e3b\u673a\u4e0a\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u53ef\u53c2\u8003\u4ea7\u54c1\u6587\u6863\u76f8\u5173\u7ae0\u8282\u300a\u5b89\u88c5\u90e8\u7f72\u300b->\u300a\u8f6f\u4ef6\u5b89\u88c5\u300b->\u300a\u521d\u59cb\u914d\u7f6e\u300b->\u300a\u914d\u7f6e\u5ba2\u6237\u7aef\u300b->\u300a\u5b89\u88c5\u5ba2\u6237\u7aef\u300b \u5728FusionInsight Manager\u4e2d\u4fee\u6539HDFS\u7684\u914d\u7f6e\uff1a dfs.http.policy \u4fee\u6539\u4e3aHTTP_AND_HTTPS\uff0c\u91cd\u542fHDFS \u9700\u8981\u5728logstash\u4e0a\u5b89\u88c5gssapi\u63d2\u4ef6\u505akerberos\u8ba4\u8bc1\uff0c\u6b64\u8fc7\u7a0b\u9700\u8981logstash\u90e8\u7f72\u4e3b\u673a\u80fd\u8fde\u5916\u7f51\uff0c\u5177\u4f53\u6b65\u9aa4\u4e3a\uff1a \u4f7f\u7528\u547d\u4ee4 cd /opt/logstash/logstash-6.7.1 \u767b\u9646logstash\u5b89\u88c5\u76ee\u5f55\uff0c\u7136\u540e\u4f7f\u7528\u547d\u4ee4 vendor/jruby/bin/jruby vendor/jruby/bin/gem install gssapi \u5728jruby\u4e0b\u5b89\u88c5gssapi \u4f7f\u7528\u547d\u4ee4 cd /opt/logstash/logstash-6.7.1 \u767b\u9646logstash\u5b89\u88c5\u76ee\u5f55\uff0c\u7136\u540e\u4f7f\u7528\u547d\u4ee4 bin/logstash-plugin install --no-verify gssapi \u5b89\u88c5gssapi plugin \u5c06\u8ba4\u8bc1\u6240\u9700\u8981\u7684\u7684user.keytab\u6587\u4ef6\u653e\u7f6e\u5728/opt\u8def\u5f84\u4e0b logstash webhdfs output\u7528\u4f8b \u00b6 \u5728logstash\u7684config\u76ee\u5f55\u4e0b\u521b\u5efa\u914d\u7f6e\u6587\u4ef6 logstash-webhdfs.conf ,\u5185\u5bb9\u4e3a\uff1a input { stdin{} } output { webhdfs { host => \"172.16.4.123\" port => 25002 path => \"/tmp/logstash/dt=%{+YYYY-MM-dd}/logstash-%{+HH}.log\" user => \"developuser\" kerberos_keytab => \"/opt/user.keytab\" use_kerberos_auth => true } } \u5176\u4e2d172.16.4.123\u4e3a\u5bf9\u63a5\u96c6\u7fa4HDFS\u7684NameNode\u4e3b\u8282\u70b9 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728logstash\u90e8\u7f72\u4e3b\u673a\u901a\u8fc7FusionInsight\u5ba2\u6237\u7aef\u505a\u8ba4\u8bc1 source /opt/125_hadoopclient/hadoopclient/bigdata_env kinit developuser \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-webhdfs.conf \u542f\u52a8logstash webhdfs output \u8f93\u5165\u51e0\u6761\u6570\u636e\uff1a \u767b\u9646hdfs\u5bf9\u5e94\u8def\u5f84\u68c0\u67e5\u7ed3\u679c\uff1a HDFS\u5bf9\u63a5 FAQ \u00b6 \u95ee\u98981\uff1a \u5bf9\u63a5webhdfs \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-webhdfs.conf \u5f97\u5230\u62a5\u9519\uff1a LoadError: no such file to load -- gssapi \u6ca1\u6709\u88c5gssapi\u8fd9\u4e2a\u63d2\u4ef6 \u89e3\u51b3\u529e\u6cd5\uff1a\u53c2\u8003\u5bf9\u63a5\u6587\u6863\u76f8\u5173\u7ae0\u8282\u5b8c\u6210gssapi\u63d2\u4ef6\u5b89\u88c5 \u95ee\u98982\uff1a\u5bf9\u63a5webhdfs \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-webhdfs.conf \u5f97\u5230\u62a5\u9519\uff1a <title>Error 401 Authentication required</title></head><body><h2>HTTP ERROR 401</h2><p>Problem accessing /webhdfs/v1/. Reason:<pre> \u95ee\u9898\u539f\u56e0\uff1a use_kerberos_auth => true \u8fd9\u4e2a\u914d\u7f6e\u9879\u6ca1\u6709\u52a0\u5165\u5230 logstash-webhdfs.conf \u914d\u7f6e\u6587\u4ef6\u4e2d \u89e3\u51b3\u529e\u6cd5\uff1a\u589e\u52a0\u4e0a\u8ff0\u914d\u7f6e\u9879\u91cd\u542f\u95ee\u9898\u89e3\u51b3 \u95ee\u98983\uff1a\u5bf9\u63a5webhdfs \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-webhdfs.conf \u5f97\u5230\u62a5\u9519\uff1a [ERROR][logstash.outputs.webhdfs ] Webhdfs check request failed. (namenode: 172.16.4.123:25002, Exception: undefined method `read_uint32' for #<FFI::MemoryPointer address=0x7f8eb0059fa0 size=4>) \u95ee\u9898\u539f\u56e0\uff1a\u542f\u52a8\u65f6\u672a\u4f7f\u7528 kinit developuser \u83b7\u5f97\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e \u89e3\u51b3\u529e\u6cd5\uff1a\u53c2\u8003\u5bf9\u63a5\u6587\u6863\u76f8\u5173\u7ae0\u8282\u52a0\u8f7d\u5bf9\u63a5\u73af\u5883\u53d8\u91cf\u4ee5\u53cakinit\u505a\u8ba4\u8bc1\u83b7\u5f97\u7f13\u5b58\u7684\u7968\u636e\u5728\u542f\u52a8logstash kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5 \u00b6 kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5\u73af\u5883\u8bf4\u660e\uff1a logstash\u4e3b\u673a\uff1a172.16.2.124 FI HD\u96c6\u7fa4\uff1a172.16.10.131-133 kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e \u00b6 \u767b\u9646\u96c6\u7fa4manager\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6\uff0cuser.keytab\u548ckrb5.conf\u5e76\u5c06\u8fd9\u4e24\u4e2a\u6587\u4ef6\u653e\u5230logstash\u5b89\u88c5\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b \u51c6\u5907\u8ba4\u8bc1\u7684jaas.conf\u6587\u4ef6\uff0c\u653e\u7f6e\u5728logstash\u5b89\u88c5\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u5185\u5bb9\u4e3a\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u767b\u9646logstash\u5b89\u88c5\u4e3b\u673a /opt/logstash/logstash-6.7.1/config \u8def\u5f84\uff0c\u7f16\u8f91\u914d\u7f6e\u6587\u4ef6 jvm.options \u5982\u56fe\u589e\u52a0\u914d\u7f6e\u9879\uff1a -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/opt/krb5.conf \u627e\u5230logstash\u5b89\u88c5\u8def\u5f84\u4e0bkafka\u76f8\u5173jar\u5305\u8def\u5f84 \u6309\u7167\u4e0a\u56fe\uff1a kafka input\u7684jar\u5305\u8def\u5f84: /opt/logstash/logstash-6.7.1/vendor/bundle/jruby/2.5.0/gems/logstash-input-kafka-8.3.1/vendor/jar-dependencies/org/apache/kafka/kafka-clients/2.1.0/ kafka output\u7684jar\u5305\u8def\u5f84\uff1a /opt/logstash/logstash-6.7.1/vendor/bundle/jruby/2.5.0/gems/logstash-output-kafka-7.3.2/vendor/jar-dependencies/org/apache/kafka/kafka-clients/2.1.0/ \u5206\u522b\u767b\u9646\u5230\u8fd9\u4e24\u4e2a\u8def\u5f84\u4e0b\uff0c\u5c06\u5bf9\u5e94FI HD kafa\u5ba2\u6237\u7aef\u7684 kafka-client jar\u5305 kafka-clients-1.1.0.jar \u62f7\u8d1d\u5230input , output\u7684\u8def\u5f84\u4e0b cp /opt/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar /opt/logstash/logstash-6.7.1/vendor/bundle/jruby/2.5.0/gems/logstash-input-kafka-8.3.1/vendor/jar-dependencies/org/apache/kafka/kafka-clients/2.1.0/ cp /opt/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar /opt/logstash/logstash-6.7.1/vendor/bundle/jruby/2.5.0/gems/logstash-output-kafka-7.3.2/vendor/jar-dependencies/org/apache/kafka/kafka-clients/2.1.0/ \u53c2\u8003\u4e0b\u56fe\u547d\u4ee4\uff0c\u5c06input,output\u8def\u5f84\u4e0b\u539f\u6765\u7684jar\u5305\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06\u62f7\u8d1d\u8fc7\u6765\u7684 kafka-clients-1.1.0.jar \u540d\u5b57\u4fee\u6539\u4e3a kafka-clients-2.1.0.jar ,\u5426\u5219\u5728\u542f\u52a8logstash\u7684\u65f6\u5019\u4f1a\u62a5\u9519 logstash kafka input\u7528\u4f8b \u00b6 \u5728logsatash\u5b89\u88c5\u8def\u5f84config\u4e0b\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6 logstash-21007input.conf \u5185\u5bb9\u4e3a\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input { kafka { bootstrap_servers => \"172.16.10.131:21007\" codec => plain topics => \"logstashtest21007input\" security_protocol => \"SASL_PLAINTEXT\" sasl_mechanism => \"GSSAPI\" sasl_kerberos_service_name => \"kafka\" jaas_path => \"/opt/jaas.conf\" } } output { stdout{ codec => plain } } \u767b\u9646\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u521b\u5efatopic\uff0c\u540d\u5b57\u4e3a\uff1a logstashtest21007input \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-21007input.conf \u542f\u52a8logstash kafka input\u4efb\u52a1 \u767b\u9646kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u547d\u4ee4 ./bin/kafka-console-producer.sh --broker-list 172.16.10.131:21007,172.16.10.132:21007,172.16.10.133:21007 --topic logstashtest21007input --producer.config config/producer.properties \u8d77\u4e00\u4e2akafka\u751f\u4ea7\u8005\uff0c\u5e76\u8f93\u5165\u4e00\u4e9b\u6570\u636e\uff1a \u5728logstash\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a logstash kafka output\u7528\u4f8b \u00b6 \u5728logsatash\u5b89\u88c5\u8def\u5f84config\u4e0b\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6 logstash-21007output.conf \u5185\u5bb9\u4e3a\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input { stdin{} } output { kafka { bootstrap_servers => \"172.16.10.131:21007\" codec => json topic_id => \"logstashtest21007\" security_protocol => \"SASL_PLAINTEXT\" sasl_mechanism => \"GSSAPI\" sasl_kerberos_service_name => \"kafka\" jaas_path => \"/opt/jaas.conf\" } } \u767b\u9646\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u521b\u5efatopic\uff0c\u540d\u5b57\u4e3a\uff1a logstashtest21007 \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-21007output.conf \u542f\u52a8logstash kafka output\u4efb\u52a1 \u767b\u9646kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u8d77\u4e00\u4e2a\u6d88\u8d39\u8005\uff1a bin/kafka-console-consumer.sh --topic logstashtest21007 --bootstrap-server 172.16.10.131:21007,172.16.10.132:21007,172.16.10.133:21007 --consumer.config config/consumer.properties \u5728logstash\u7aef\u624b\u52a8\u8f93\u5165\u6570\u636e\uff1a \u53bbkafka\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a \u5173\u4e8e\u4f7f\u7528ARM\u670d\u52a1\u5668\u4f7f\u7528logstash\u9047\u5230\u7684\u95ee\u9898 \u00b6 \u5728\u4f7f\u7528arm\u670d\u52a1\u5668\u4f7f\u7528logstash\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-sample.conf \u5c1d\u8bd5\u542f\u52a8logstash\u7684\u65f6\u5019\u9047\u5230\u95ee\u9898\uff1a [ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (LoadError) load error: ffi/ffi -- java.lang.NullPointerException: null \u53c2\u8003\u793e\u533a\uff1a 1. https://github.com/elastic/logstash/issues/10888 2. https://github.com/elastic/logstash/issues/10755 3. https://gist.github.com/alexalouit/a857a6de10dfdaf7485f7c0cccadb98c 4. https://github.com/mew2057/CAST/blob/6c7f7d514b7af3c512635ec145aa829c535467dc/csm_big_data/config-scripts/logstashFixupScript.sh \u53ef\u4ee5\u5f97\u77e5\u95ee\u9898\u539f\u56e0\u4e3a\uff1alogstash\u542f\u52a8\u7684\u65f6\u5019\u8bfb\u53d6\u7684\u76f8\u5173jar\u5305jruby-complete-9.2.6.0.jar\uff08\u4ee5logstash 6.7.1\u4e3a\u4f8b\uff09\u6709\u914d\u7f6e\u95ee\u9898\u3002\u5177\u4f53\u4e3ajar\u5305\u7f16\u8bd1\u7684\u65f6\u5019\u5bf9\u5e94aarch64-linux\u8def\u5f84\u4e0b\u7f3a\u5c11\u914d\u7f6e\u6587\u4ef6platform.conf \u89e3\u51b3\u529e\u6cd5\uff1a \u8bf4\u660e\uff1a\u89e3\u51b3\u529e\u6cd5\u53c2\u8003\u793e\u533a\uff0c\u5c06logstash\u5b89\u88c5\u8def\u5f84\u91cc\u7684jar\u5305jruby-complete-9.2.6.0.jar\u91cc\u9762\u7684\u8def\u5f84 META-INF/jruby.home/lib/ruby/stdlib/ffi/platform/aarch64-linux \u4e0b\u7684\u914d\u7f6e\u6587\u4ef6 types.conf \u5185\u5bb9\u590d\u5236\u5e76\u4fdd\u5b58\u4e3a\u65b0\u7684\u6587\u4ef6\u540d platform.conf \u3002 \u5177\u4f53\u64cd\u4f5c\u6b65\u9aa4\u5982\u4e0b\uff1a \u9996\u5148\u5148\u5728logstash\u5b89\u88c5\u8def\u5f84\u4e2d\u627e\u5230\u76f8\u5173jar\u5305\u7684\u4f4d\u7f6e\uff0c\u6bd4\u5982\uff1a /opt/logstash/logstash-6.7.1/logstash-core/lib/jars/jruby-complete-9.2.6.0.jar \u4f7f\u7528winSCP\u5de5\u5177\u5c06\u8be5jar\u5305\u5bfc\u51fa\u5230windows\u672c\u5730 \u53f3\u952e\u70b9\u51fb\u8be5jar\u5305\uff0c\u9009\u62e9\u4f7f\u7528winRAR\u5de5\u5177\u6253\u5f00 \u9996\u5148\u767b\u9646\u5230\u8def\u5f84 jruby-complete-9.2.6.0.jar\\META-INF\\jruby.home\\lib\\ruby\\stdlib\\ffi\\platform \u4e0b \u767b\u9646\u8def\u5f84 x86-linux ,\u9009\u4e2dplatform.conf\u6587\u4ef6\u4f7f\u7528 Ctrl + C \u590d\u5236\u8be5\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5230\u8def\u5f84 aarch64-linux \u4f7f\u7528 Ctrl + V \u5c06\u4e0a\u4e00\u6b65\u590d\u5236\u7684\u914d\u7f6e\u6587\u4ef6\u7c98\u8d34\u5230\u8be5\u8def\u5f84\u4e0b \u53cc\u51fb type.conf \u6587\u4ef6\uff0c\u5c06\u8be5\u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u590d\u5236\uff0c\u5e76\u66ff\u6362\u5230 platform.conf \u6587\u4ef6\u4e2d\u5e76\u4fdd\u5b58\uff0c\u4f7f\u5f97\u4e24\u4e2a\u6587\u4ef6\u7684\u5185\u5bb9\u4e00\u81f4 \u5c06\u4fee\u6539\u597d\u7684jar\u5305\u4f7f\u7528winSCP\u5de5\u5177\u91cd\u65b0\u5bfc\u5165\u5230logstash\u7684 /opt/logstash/logstash-6.7.1/logstash-core/lib/jars/ \u4e2d\uff0c\u5e76\u66ff\u6362\u4e4b\u524d\u7684jar\u5305 \u91cd\u65b0\u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-sample.conf \u542f\u52a8\u6210\u529f \u6ce8\u610f\uff1a\u53ef\u5ffd\u7565\u8be5\u62a5\u9519\uff0c\u56e0\u4e3a\u6ca1\u6709\u586b\u5165\u6709\u6548es\u4fe1\u606f","title":"6.7.1 <--> 8.0"},{"location":"Other/Logstash/#logstashfusioninsight","text":"","title":"Logstash\u5bf9\u63a5FusionInsight"},{"location":"Other/Logstash/#_1","text":"Logstash 6.7.1 \u2194 FusionInsight HD 6.5 (HDFS/Kafka) Logstash 6.7.1 \u2194 FusionInsight MRS 8.0 (HDFS/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Logstash/#hdfs","text":"HDFS\u5bf9\u63a5\u73af\u5883\u8bf4\u660e\uff1a logstash\u4e3b\u673a\uff1a172.16.9.107 FI HD\u96c6\u7fa4\uff1a172.16.4.121-123","title":"HDFS\u5bf9\u63a5"},{"location":"Other/Logstash/#hdfs_1","text":"logstash \u4f7f\u7528\u5305\u542b webhdfs output\u63d2\u4ef6\u540c\u96c6\u7fa4\u4ea4\u4e92\uff0c\u9996\u5148\u9700\u8981\u914d\u7f6ewebhdfs\u76f8\u5173\u914d\u7f6e\u9879 HDFS\u662f\u901a\u8fc7WebHDFS\u8fde\u63a5\uff0c\u524d\u63d0\u6761\u4ef6\u662f\u83b7\u53d6kerberos\u7684\u7f13\u5b58\u7968\u636e\uff0c\u6240\u4ee5\u9700\u8981\u5728Logstash\u90e8\u7f72\u7684\u4e3b\u673a\u4e0a\u5b89\u88c5FusionInsight HD\u5ba2\u6237\u7aef\uff0c\u5177\u4f53\u53ef\u53c2\u8003\u4ea7\u54c1\u6587\u6863\u76f8\u5173\u7ae0\u8282\u300a\u5b89\u88c5\u90e8\u7f72\u300b->\u300a\u8f6f\u4ef6\u5b89\u88c5\u300b->\u300a\u521d\u59cb\u914d\u7f6e\u300b->\u300a\u914d\u7f6e\u5ba2\u6237\u7aef\u300b->\u300a\u5b89\u88c5\u5ba2\u6237\u7aef\u300b \u5728FusionInsight Manager\u4e2d\u4fee\u6539HDFS\u7684\u914d\u7f6e\uff1a dfs.http.policy \u4fee\u6539\u4e3aHTTP_AND_HTTPS\uff0c\u91cd\u542fHDFS \u9700\u8981\u5728logstash\u4e0a\u5b89\u88c5gssapi\u63d2\u4ef6\u505akerberos\u8ba4\u8bc1\uff0c\u6b64\u8fc7\u7a0b\u9700\u8981logstash\u90e8\u7f72\u4e3b\u673a\u80fd\u8fde\u5916\u7f51\uff0c\u5177\u4f53\u6b65\u9aa4\u4e3a\uff1a \u4f7f\u7528\u547d\u4ee4 cd /opt/logstash/logstash-6.7.1 \u767b\u9646logstash\u5b89\u88c5\u76ee\u5f55\uff0c\u7136\u540e\u4f7f\u7528\u547d\u4ee4 vendor/jruby/bin/jruby vendor/jruby/bin/gem install gssapi \u5728jruby\u4e0b\u5b89\u88c5gssapi \u4f7f\u7528\u547d\u4ee4 cd /opt/logstash/logstash-6.7.1 \u767b\u9646logstash\u5b89\u88c5\u76ee\u5f55\uff0c\u7136\u540e\u4f7f\u7528\u547d\u4ee4 bin/logstash-plugin install --no-verify gssapi \u5b89\u88c5gssapi plugin \u5c06\u8ba4\u8bc1\u6240\u9700\u8981\u7684\u7684user.keytab\u6587\u4ef6\u653e\u7f6e\u5728/opt\u8def\u5f84\u4e0b","title":"HDFS\u5bf9\u63a5\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e"},{"location":"Other/Logstash/#logstash-webhdfs-output","text":"\u5728logstash\u7684config\u76ee\u5f55\u4e0b\u521b\u5efa\u914d\u7f6e\u6587\u4ef6 logstash-webhdfs.conf ,\u5185\u5bb9\u4e3a\uff1a input { stdin{} } output { webhdfs { host => \"172.16.4.123\" port => 25002 path => \"/tmp/logstash/dt=%{+YYYY-MM-dd}/logstash-%{+HH}.log\" user => \"developuser\" kerberos_keytab => \"/opt/user.keytab\" use_kerberos_auth => true } } \u5176\u4e2d172.16.4.123\u4e3a\u5bf9\u63a5\u96c6\u7fa4HDFS\u7684NameNode\u4e3b\u8282\u70b9 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5728logstash\u90e8\u7f72\u4e3b\u673a\u901a\u8fc7FusionInsight\u5ba2\u6237\u7aef\u505a\u8ba4\u8bc1 source /opt/125_hadoopclient/hadoopclient/bigdata_env kinit developuser \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-webhdfs.conf \u542f\u52a8logstash webhdfs output \u8f93\u5165\u51e0\u6761\u6570\u636e\uff1a \u767b\u9646hdfs\u5bf9\u5e94\u8def\u5f84\u68c0\u67e5\u7ed3\u679c\uff1a","title":"logstash webhdfs output\u7528\u4f8b"},{"location":"Other/Logstash/#hdfs-faq","text":"\u95ee\u98981\uff1a \u5bf9\u63a5webhdfs \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-webhdfs.conf \u5f97\u5230\u62a5\u9519\uff1a LoadError: no such file to load -- gssapi \u6ca1\u6709\u88c5gssapi\u8fd9\u4e2a\u63d2\u4ef6 \u89e3\u51b3\u529e\u6cd5\uff1a\u53c2\u8003\u5bf9\u63a5\u6587\u6863\u76f8\u5173\u7ae0\u8282\u5b8c\u6210gssapi\u63d2\u4ef6\u5b89\u88c5 \u95ee\u98982\uff1a\u5bf9\u63a5webhdfs \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-webhdfs.conf \u5f97\u5230\u62a5\u9519\uff1a <title>Error 401 Authentication required</title></head><body><h2>HTTP ERROR 401</h2><p>Problem accessing /webhdfs/v1/. Reason:<pre> \u95ee\u9898\u539f\u56e0\uff1a use_kerberos_auth => true \u8fd9\u4e2a\u914d\u7f6e\u9879\u6ca1\u6709\u52a0\u5165\u5230 logstash-webhdfs.conf \u914d\u7f6e\u6587\u4ef6\u4e2d \u89e3\u51b3\u529e\u6cd5\uff1a\u589e\u52a0\u4e0a\u8ff0\u914d\u7f6e\u9879\u91cd\u542f\u95ee\u9898\u89e3\u51b3 \u95ee\u98983\uff1a\u5bf9\u63a5webhdfs \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-webhdfs.conf \u5f97\u5230\u62a5\u9519\uff1a [ERROR][logstash.outputs.webhdfs ] Webhdfs check request failed. (namenode: 172.16.4.123:25002, Exception: undefined method `read_uint32' for #<FFI::MemoryPointer address=0x7f8eb0059fa0 size=4>) \u95ee\u9898\u539f\u56e0\uff1a\u542f\u52a8\u65f6\u672a\u4f7f\u7528 kinit developuser \u83b7\u5f97\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e \u89e3\u51b3\u529e\u6cd5\uff1a\u53c2\u8003\u5bf9\u63a5\u6587\u6863\u76f8\u5173\u7ae0\u8282\u52a0\u8f7d\u5bf9\u63a5\u73af\u5883\u53d8\u91cf\u4ee5\u53cakinit\u505a\u8ba4\u8bc1\u83b7\u5f97\u7f13\u5b58\u7684\u7968\u636e\u5728\u542f\u52a8logstash","title":"HDFS\u5bf9\u63a5 FAQ"},{"location":"Other/Logstash/#kafka","text":"kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5\u73af\u5883\u8bf4\u660e\uff1a logstash\u4e3b\u673a\uff1a172.16.2.124 FI HD\u96c6\u7fa4\uff1a172.16.10.131-133","title":"kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5"},{"location":"Other/Logstash/#kafka_1","text":"\u767b\u9646\u96c6\u7fa4manager\u4e0b\u8f7d\u8ba4\u8bc1\u7528\u6237\u7684\u914d\u7f6e\u6587\u4ef6\uff0cuser.keytab\u548ckrb5.conf\u5e76\u5c06\u8fd9\u4e24\u4e2a\u6587\u4ef6\u653e\u5230logstash\u5b89\u88c5\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b \u51c6\u5907\u8ba4\u8bc1\u7684jaas.conf\u6587\u4ef6\uff0c\u653e\u7f6e\u5728logstash\u5b89\u88c5\u4e3b\u673a\u7684 /opt \u8def\u5f84\u4e0b\uff0c\u5185\u5bb9\u4e3a\uff1a KafkaClient { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false serviceName=\"kafka\" storeKey=true debug=true; }; Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u767b\u9646logstash\u5b89\u88c5\u4e3b\u673a /opt/logstash/logstash-6.7.1/config \u8def\u5f84\uff0c\u7f16\u8f91\u914d\u7f6e\u6587\u4ef6 jvm.options \u5982\u56fe\u589e\u52a0\u914d\u7f6e\u9879\uff1a -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/opt/krb5.conf \u627e\u5230logstash\u5b89\u88c5\u8def\u5f84\u4e0bkafka\u76f8\u5173jar\u5305\u8def\u5f84 \u6309\u7167\u4e0a\u56fe\uff1a kafka input\u7684jar\u5305\u8def\u5f84: /opt/logstash/logstash-6.7.1/vendor/bundle/jruby/2.5.0/gems/logstash-input-kafka-8.3.1/vendor/jar-dependencies/org/apache/kafka/kafka-clients/2.1.0/ kafka output\u7684jar\u5305\u8def\u5f84\uff1a /opt/logstash/logstash-6.7.1/vendor/bundle/jruby/2.5.0/gems/logstash-output-kafka-7.3.2/vendor/jar-dependencies/org/apache/kafka/kafka-clients/2.1.0/ \u5206\u522b\u767b\u9646\u5230\u8fd9\u4e24\u4e2a\u8def\u5f84\u4e0b\uff0c\u5c06\u5bf9\u5e94FI HD kafa\u5ba2\u6237\u7aef\u7684 kafka-client jar\u5305 kafka-clients-1.1.0.jar \u62f7\u8d1d\u5230input , output\u7684\u8def\u5f84\u4e0b cp /opt/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar /opt/logstash/logstash-6.7.1/vendor/bundle/jruby/2.5.0/gems/logstash-input-kafka-8.3.1/vendor/jar-dependencies/org/apache/kafka/kafka-clients/2.1.0/ cp /opt/hadoopclient/Kafka/kafka/libs/kafka-clients-1.1.0.jar /opt/logstash/logstash-6.7.1/vendor/bundle/jruby/2.5.0/gems/logstash-output-kafka-7.3.2/vendor/jar-dependencies/org/apache/kafka/kafka-clients/2.1.0/ \u53c2\u8003\u4e0b\u56fe\u547d\u4ee4\uff0c\u5c06input,output\u8def\u5f84\u4e0b\u539f\u6765\u7684jar\u5305\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06\u62f7\u8d1d\u8fc7\u6765\u7684 kafka-clients-1.1.0.jar \u540d\u5b57\u4fee\u6539\u4e3a kafka-clients-2.1.0.jar ,\u5426\u5219\u5728\u542f\u52a8logstash\u7684\u65f6\u5019\u4f1a\u62a5\u9519","title":"kafka\u5b89\u5168\u6a21\u5f0f\u5bf9\u63a5\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e"},{"location":"Other/Logstash/#logstash-kafka-input","text":"\u5728logsatash\u5b89\u88c5\u8def\u5f84config\u4e0b\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6 logstash-21007input.conf \u5185\u5bb9\u4e3a\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input { kafka { bootstrap_servers => \"172.16.10.131:21007\" codec => plain topics => \"logstashtest21007input\" security_protocol => \"SASL_PLAINTEXT\" sasl_mechanism => \"GSSAPI\" sasl_kerberos_service_name => \"kafka\" jaas_path => \"/opt/jaas.conf\" } } output { stdout{ codec => plain } } \u767b\u9646\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u521b\u5efatopic\uff0c\u540d\u5b57\u4e3a\uff1a logstashtest21007input \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-21007input.conf \u542f\u52a8logstash kafka input\u4efb\u52a1 \u767b\u9646kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u547d\u4ee4 ./bin/kafka-console-producer.sh --broker-list 172.16.10.131:21007,172.16.10.132:21007,172.16.10.133:21007 --topic logstashtest21007input --producer.config config/producer.properties \u8d77\u4e00\u4e2akafka\u751f\u4ea7\u8005\uff0c\u5e76\u8f93\u5165\u4e00\u4e9b\u6570\u636e\uff1a \u5728logstash\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a","title":"logstash kafka input\u7528\u4f8b"},{"location":"Other/Logstash/#logstash-kafka-output","text":"\u5728logsatash\u5b89\u88c5\u8def\u5f84config\u4e0b\u65b0\u5efa\u914d\u7f6e\u6587\u4ef6 logstash-21007output.conf \u5185\u5bb9\u4e3a\uff1a # Sample Logstash configuration for creating a simple # Beats -> Logstash -> Elasticsearch pipeline. input { stdin{} } output { kafka { bootstrap_servers => \"172.16.10.131:21007\" codec => json topic_id => \"logstashtest21007\" security_protocol => \"SASL_PLAINTEXT\" sasl_mechanism => \"GSSAPI\" sasl_kerberos_service_name => \"kafka\" jaas_path => \"/opt/jaas.conf\" } } \u767b\u9646\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u521b\u5efatopic\uff0c\u540d\u5b57\u4e3a\uff1a logstashtest21007 \u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-21007output.conf \u542f\u52a8logstash kafka output\u4efb\u52a1 \u767b\u9646kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u8d77\u4e00\u4e2a\u6d88\u8d39\u8005\uff1a bin/kafka-console-consumer.sh --topic logstashtest21007 --bootstrap-server 172.16.10.131:21007,172.16.10.132:21007,172.16.10.133:21007 --consumer.config config/consumer.properties \u5728logstash\u7aef\u624b\u52a8\u8f93\u5165\u6570\u636e\uff1a \u53bbkafka\u5ba2\u6237\u7aef\u68c0\u67e5\u7ed3\u679c\uff1a","title":"logstash kafka output\u7528\u4f8b"},{"location":"Other/Logstash/#armlogstash","text":"\u5728\u4f7f\u7528arm\u670d\u52a1\u5668\u4f7f\u7528logstash\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-sample.conf \u5c1d\u8bd5\u542f\u52a8logstash\u7684\u65f6\u5019\u9047\u5230\u95ee\u9898\uff1a [ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (LoadError) load error: ffi/ffi -- java.lang.NullPointerException: null \u53c2\u8003\u793e\u533a\uff1a 1. https://github.com/elastic/logstash/issues/10888 2. https://github.com/elastic/logstash/issues/10755 3. https://gist.github.com/alexalouit/a857a6de10dfdaf7485f7c0cccadb98c 4. https://github.com/mew2057/CAST/blob/6c7f7d514b7af3c512635ec145aa829c535467dc/csm_big_data/config-scripts/logstashFixupScript.sh \u53ef\u4ee5\u5f97\u77e5\u95ee\u9898\u539f\u56e0\u4e3a\uff1alogstash\u542f\u52a8\u7684\u65f6\u5019\u8bfb\u53d6\u7684\u76f8\u5173jar\u5305jruby-complete-9.2.6.0.jar\uff08\u4ee5logstash 6.7.1\u4e3a\u4f8b\uff09\u6709\u914d\u7f6e\u95ee\u9898\u3002\u5177\u4f53\u4e3ajar\u5305\u7f16\u8bd1\u7684\u65f6\u5019\u5bf9\u5e94aarch64-linux\u8def\u5f84\u4e0b\u7f3a\u5c11\u914d\u7f6e\u6587\u4ef6platform.conf \u89e3\u51b3\u529e\u6cd5\uff1a \u8bf4\u660e\uff1a\u89e3\u51b3\u529e\u6cd5\u53c2\u8003\u793e\u533a\uff0c\u5c06logstash\u5b89\u88c5\u8def\u5f84\u91cc\u7684jar\u5305jruby-complete-9.2.6.0.jar\u91cc\u9762\u7684\u8def\u5f84 META-INF/jruby.home/lib/ruby/stdlib/ffi/platform/aarch64-linux \u4e0b\u7684\u914d\u7f6e\u6587\u4ef6 types.conf \u5185\u5bb9\u590d\u5236\u5e76\u4fdd\u5b58\u4e3a\u65b0\u7684\u6587\u4ef6\u540d platform.conf \u3002 \u5177\u4f53\u64cd\u4f5c\u6b65\u9aa4\u5982\u4e0b\uff1a \u9996\u5148\u5148\u5728logstash\u5b89\u88c5\u8def\u5f84\u4e2d\u627e\u5230\u76f8\u5173jar\u5305\u7684\u4f4d\u7f6e\uff0c\u6bd4\u5982\uff1a /opt/logstash/logstash-6.7.1/logstash-core/lib/jars/jruby-complete-9.2.6.0.jar \u4f7f\u7528winSCP\u5de5\u5177\u5c06\u8be5jar\u5305\u5bfc\u51fa\u5230windows\u672c\u5730 \u53f3\u952e\u70b9\u51fb\u8be5jar\u5305\uff0c\u9009\u62e9\u4f7f\u7528winRAR\u5de5\u5177\u6253\u5f00 \u9996\u5148\u767b\u9646\u5230\u8def\u5f84 jruby-complete-9.2.6.0.jar\\META-INF\\jruby.home\\lib\\ruby\\stdlib\\ffi\\platform \u4e0b \u767b\u9646\u8def\u5f84 x86-linux ,\u9009\u4e2dplatform.conf\u6587\u4ef6\u4f7f\u7528 Ctrl + C \u590d\u5236\u8be5\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5230\u8def\u5f84 aarch64-linux \u4f7f\u7528 Ctrl + V \u5c06\u4e0a\u4e00\u6b65\u590d\u5236\u7684\u914d\u7f6e\u6587\u4ef6\u7c98\u8d34\u5230\u8be5\u8def\u5f84\u4e0b \u53cc\u51fb type.conf \u6587\u4ef6\uff0c\u5c06\u8be5\u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9\u590d\u5236\uff0c\u5e76\u66ff\u6362\u5230 platform.conf \u6587\u4ef6\u4e2d\u5e76\u4fdd\u5b58\uff0c\u4f7f\u5f97\u4e24\u4e2a\u6587\u4ef6\u7684\u5185\u5bb9\u4e00\u81f4 \u5c06\u4fee\u6539\u597d\u7684jar\u5305\u4f7f\u7528winSCP\u5de5\u5177\u91cd\u65b0\u5bfc\u5165\u5230logstash\u7684 /opt/logstash/logstash-6.7.1/logstash-core/lib/jars/ \u4e2d\uff0c\u5e76\u66ff\u6362\u4e4b\u524d\u7684jar\u5305 \u91cd\u65b0\u4f7f\u7528\u547d\u4ee4 bin/logstash -f config/logstash-sample.conf \u542f\u52a8\u6210\u529f \u6ce8\u610f\uff1a\u53ef\u5ffd\u7565\u8be5\u62a5\u9519\uff0c\u56e0\u4e3a\u6ca1\u6709\u586b\u5165\u6709\u6548es\u4fe1\u606f","title":"\u5173\u4e8e\u4f7f\u7528ARM\u670d\u52a1\u5668\u4f7f\u7528logstash\u9047\u5230\u7684\u95ee\u9898"},{"location":"Other/NeoKylin/","text":"NeoKylin\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 NeoKylin 6.9 \u2194 FusionInsight HD V100R002C70SPC200 (OS) NeoKylin 7.2 \u2194 FusionInsight HD V100R002C70SPC200 (OS)","title":"7.2 <--> C70"},{"location":"Other/NeoKylin/#neokylinfusioninsight","text":"","title":"NeoKylin\u5bf9\u63a5FusionInsight"},{"location":"Other/NeoKylin/#_1","text":"NeoKylin 6.9 \u2194 FusionInsight HD V100R002C70SPC200 (OS) NeoKylin 7.2 \u2194 FusionInsight HD V100R002C70SPC200 (OS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Tensorflow/","text":"Tensorflow\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Tensorflow 1.15.0 \u2194 FusionInsight HD 6.5 (HDFS) Tensorflow 2.1.0 \u2194 FusionInsight HD 6.5 (HDFS) Tensorflow 1.15.0 \u2194 FusionInsight MRS 8.0 (HDFS) Tensorflow 2.1.0 \u2194 FusionInsight MRS 8.0 (HDFS) \u6d4b\u8bd5\u73af\u5883\u63cf\u8ff0 \u00b6 Apache Airflow\u5b89\u88c5\u4e3b\u673a\uff1a 172.16.2.124 \u5bf9\u63a5FI HD\u96c6\u7fa4\uff1a 172.16.4.121-123 \u5b89\u88c5anaconda \u00b6 \u53c2\u8003jupyternotebook\u6216\u8005jupyterhub\u6587\u6863\uff0c\u5b8c\u6210anaconda\u73af\u5883\u7684\u5b89\u88c5\u3002\u8fd9\u91cc\u9009\u7528\u7684\u662fPython3\u7248\u672c\u7684\u5b89\u88c5\u3002 \u5b89\u88c5Tensorflow \u00b6 \u767b\u9646anaconda3\u7684\u5b89\u88c5\u76ee\u5f55\u4e0b\u627e\u5230bin\u76ee\u5f55\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5tensorflow\u6307\u5b9a\u7248\u672c: ./pip install tensorflow \u5982\u679c\u9700\u8981\u5b89\u88c5Tensorflow 1.15.1\u7248\u672c\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a ./pip install tensorflow==1.15.0 --force-reinstall \u6ce8\u610f\uff1a\u5728\u5b89\u88c5tensorflow 1.15.0 \u8fc7\u7a0b\u4e2d\u5982\u679c\u62a5\u76f8\u5173\u7248\u672c\u5339\u914d\u7684\u95ee\u9898\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u91cd\u65b0\u5b89\u88c5\u6307\u5b9a\u7248\u672c ./pip install typed-ast==1.3.0 --force-reinstall ./pip install markdown==2.6.8 --force-reinstall \u914d\u7f6e\u73af\u5883\u4fe1\u606f \u00b6 \u4f7f\u7528\u547d\u4ee4 vi ~/.bash_profile \u914d\u7f6e\u73af\u5883\u53d8\u91cf export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/125_651hdclient/hadoopclient/JDK/jdk-8u201/jre/lib/amd64/server export KRB5CCNAME=/tmp/krb5cc_0 \u6ce8\u610f\uff1a\u914d\u7f6eLD_LIBRARY_PATH\u65f6\u9700\u6839\u636e\u5b89\u88c5\u5ba2\u6237\u7aef\u5177\u4f53jdk\u8def\u5f84\u4f4d\u7f6e\u6765\u914d\u7f6e\uff0c\u914d\u7f6eKRB5CCNAME\u7f13\u5b58\u7968\u636e\u8981\u6839\u636ekinit\u8ba4\u8bc1\u540e\u5177\u4f53\u7968\u636e\u540d\u5b57\u914d\u7f6e\u3002 \u5c06\u5bf9\u63a5\u96c6\u7fa4\u83b7\u53d6\u7684\u8ba4\u8bc1\u6587\u4ef6krb5.conf\u6587\u4ef6\u653e\u7f6e/etc/\u8def\u5f84\u4e0b\u9762\uff0c\u7cfb\u7edf\u9ed8\u8ba4\u4ece\u8be5\u8def\u5f84\u4e0b\u83b7\u53d6\u914d\u7f6e\u6587\u4ef6 \u8fd0\u884ctensorflow 2.1.0\u6837\u4f8b\u4ee3\u7801 \u00b6 \u51c6\u5907tensorflow2-hdfs.py\u653e\u7f6e\u5728/opt\u8def\u5f84\u4e0b import tensorflow as tf out1 = tf.io.gfile.GFile(\"hdfs://172.16.4.123:25000/tmp/iris.csv\") for a in out1.readlines(): print(a) \u4f7f\u7528\u547d\u4ee4 su - root \u91cd\u65b0\u52a0\u8f7d\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u73af\u5883\u53d8\u91cf\u5e76\u4e14\u5b8c\u6210\u8ba4\u8bc1 source /opt/125_651hdclient/hadoopclient/bigdata_env kinit developuser \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7danaconda3\u73af\u5883\u53d8\u91cf source ~/.bashrc.anaconda3 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u8fd0\u884c\u6837\u4f8b\u4ee3\u7801\uff1a CLASSPATH=$(/opt/125_651hdclient/hadoopclient/HDFS/hadoop/bin/hadoop classpath --glob) python /opt/tensorflow2-hdfs.py \u6ce8\u610f\uff1a\u5176\u4e2d\u6d89\u53ca\u7684hadoop\u8def\u5f84\u4e3a\u5b89\u88c5\u7684\u96c6\u7fa4\u5ba2\u6237\u7aefHDFS\u8def\u5f84 FAQ \u00b6 \u95ee\u98981\uff1a\u5728\u914d\u7f6e\u597d\u4e4b\u540e\uff0c\u6267\u884c CLASSPATH=$(/opt/125_651hdclient/hadoopclient/HDFS/hadoop/bin/hadoop classpath --glob) python /opt/tensorflow-hdfs.py \u7684\u65f6\u5019\u9047\u5230\u8fde\u63a5\u62a5\u9519 Caused by: java.io.IOException: Couldn't setup connection for developuser@HADOOP.COM to host-172-16-4-123/172.16.4.123:25000 at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:782) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:753) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:846) at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:436) at org.apache.hadoop.ipc.Client.getConnection(Client.java:1613) at org.apache.hadoop.ipc.Client.call(Client.java:1444) ... 24 more Caused by: org.apache.hadoop.ipc.RemoteException(javax.security.sasl.SaslException): GSS initiate failed at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:374) at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:640) at org.apache.hadoop.ipc.Client$Connection.access$2400(Client.java:436) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:833) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:829) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:829) ... 27 more \u89e3\u51b3\u529e\u6cd5\uff1a\u53ef\u4f7f\u7528 export JAVA_TOOL_OPTIONS=\"-Dsun.security.krb5.debug=true\" \u6253\u5f00\u8ba4\u8bc1\u65e5\u5fd7\u6392\u67e5\u95ee\u9898\uff0c\u68c0\u67e5 /etc/krb5.conf\u6587\u4ef6\u662f\u5426\u4e3a\u5bf9\u63a5\u96c6\u7fa4\u5339\u914d\u7684\u8ba4\u8bc1\u6587\u4ef6\u3002\u66f4\u6362\u6b63\u786e\u8ba4\u8bc1\u6587\u4ef6\u540e\u95ee\u9898\u89e3\u51b3\u3002","title":"2.1.0 <--> 8.0"},{"location":"Other/Tensorflow/#tensorflowfusioninsight","text":"","title":"Tensorflow\u5bf9\u63a5FusionInsight"},{"location":"Other/Tensorflow/#_1","text":"Tensorflow 1.15.0 \u2194 FusionInsight HD 6.5 (HDFS) Tensorflow 2.1.0 \u2194 FusionInsight HD 6.5 (HDFS) Tensorflow 1.15.0 \u2194 FusionInsight MRS 8.0 (HDFS) Tensorflow 2.1.0 \u2194 FusionInsight MRS 8.0 (HDFS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/Tensorflow/#_2","text":"Apache Airflow\u5b89\u88c5\u4e3b\u673a\uff1a 172.16.2.124 \u5bf9\u63a5FI HD\u96c6\u7fa4\uff1a 172.16.4.121-123","title":"\u6d4b\u8bd5\u73af\u5883\u63cf\u8ff0"},{"location":"Other/Tensorflow/#anaconda","text":"\u53c2\u8003jupyternotebook\u6216\u8005jupyterhub\u6587\u6863\uff0c\u5b8c\u6210anaconda\u73af\u5883\u7684\u5b89\u88c5\u3002\u8fd9\u91cc\u9009\u7528\u7684\u662fPython3\u7248\u672c\u7684\u5b89\u88c5\u3002","title":"\u5b89\u88c5anaconda"},{"location":"Other/Tensorflow/#tensorflow","text":"\u767b\u9646anaconda3\u7684\u5b89\u88c5\u76ee\u5f55\u4e0b\u627e\u5230bin\u76ee\u5f55\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5tensorflow\u6307\u5b9a\u7248\u672c: ./pip install tensorflow \u5982\u679c\u9700\u8981\u5b89\u88c5Tensorflow 1.15.1\u7248\u672c\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a ./pip install tensorflow==1.15.0 --force-reinstall \u6ce8\u610f\uff1a\u5728\u5b89\u88c5tensorflow 1.15.0 \u8fc7\u7a0b\u4e2d\u5982\u679c\u62a5\u76f8\u5173\u7248\u672c\u5339\u914d\u7684\u95ee\u9898\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u91cd\u65b0\u5b89\u88c5\u6307\u5b9a\u7248\u672c ./pip install typed-ast==1.3.0 --force-reinstall ./pip install markdown==2.6.8 --force-reinstall","title":"\u5b89\u88c5Tensorflow"},{"location":"Other/Tensorflow/#_3","text":"\u4f7f\u7528\u547d\u4ee4 vi ~/.bash_profile \u914d\u7f6e\u73af\u5883\u53d8\u91cf export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/125_651hdclient/hadoopclient/JDK/jdk-8u201/jre/lib/amd64/server export KRB5CCNAME=/tmp/krb5cc_0 \u6ce8\u610f\uff1a\u914d\u7f6eLD_LIBRARY_PATH\u65f6\u9700\u6839\u636e\u5b89\u88c5\u5ba2\u6237\u7aef\u5177\u4f53jdk\u8def\u5f84\u4f4d\u7f6e\u6765\u914d\u7f6e\uff0c\u914d\u7f6eKRB5CCNAME\u7f13\u5b58\u7968\u636e\u8981\u6839\u636ekinit\u8ba4\u8bc1\u540e\u5177\u4f53\u7968\u636e\u540d\u5b57\u914d\u7f6e\u3002 \u5c06\u5bf9\u63a5\u96c6\u7fa4\u83b7\u53d6\u7684\u8ba4\u8bc1\u6587\u4ef6krb5.conf\u6587\u4ef6\u653e\u7f6e/etc/\u8def\u5f84\u4e0b\u9762\uff0c\u7cfb\u7edf\u9ed8\u8ba4\u4ece\u8be5\u8def\u5f84\u4e0b\u83b7\u53d6\u914d\u7f6e\u6587\u4ef6","title":"\u914d\u7f6e\u73af\u5883\u4fe1\u606f"},{"location":"Other/Tensorflow/#tensorflow-210","text":"\u51c6\u5907tensorflow2-hdfs.py\u653e\u7f6e\u5728/opt\u8def\u5f84\u4e0b import tensorflow as tf out1 = tf.io.gfile.GFile(\"hdfs://172.16.4.123:25000/tmp/iris.csv\") for a in out1.readlines(): print(a) \u4f7f\u7528\u547d\u4ee4 su - root \u91cd\u65b0\u52a0\u8f7d\u73af\u5883\u53d8\u91cf \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u73af\u5883\u53d8\u91cf\u5e76\u4e14\u5b8c\u6210\u8ba4\u8bc1 source /opt/125_651hdclient/hadoopclient/bigdata_env kinit developuser \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7danaconda3\u73af\u5883\u53d8\u91cf source ~/.bashrc.anaconda3 \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u8fd0\u884c\u6837\u4f8b\u4ee3\u7801\uff1a CLASSPATH=$(/opt/125_651hdclient/hadoopclient/HDFS/hadoop/bin/hadoop classpath --glob) python /opt/tensorflow2-hdfs.py \u6ce8\u610f\uff1a\u5176\u4e2d\u6d89\u53ca\u7684hadoop\u8def\u5f84\u4e3a\u5b89\u88c5\u7684\u96c6\u7fa4\u5ba2\u6237\u7aefHDFS\u8def\u5f84","title":"\u8fd0\u884ctensorflow 2.1.0\u6837\u4f8b\u4ee3\u7801"},{"location":"Other/Tensorflow/#faq","text":"\u95ee\u98981\uff1a\u5728\u914d\u7f6e\u597d\u4e4b\u540e\uff0c\u6267\u884c CLASSPATH=$(/opt/125_651hdclient/hadoopclient/HDFS/hadoop/bin/hadoop classpath --glob) python /opt/tensorflow-hdfs.py \u7684\u65f6\u5019\u9047\u5230\u8fde\u63a5\u62a5\u9519 Caused by: java.io.IOException: Couldn't setup connection for developuser@HADOOP.COM to host-172-16-4-123/172.16.4.123:25000 at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:782) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:753) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:846) at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:436) at org.apache.hadoop.ipc.Client.getConnection(Client.java:1613) at org.apache.hadoop.ipc.Client.call(Client.java:1444) ... 24 more Caused by: org.apache.hadoop.ipc.RemoteException(javax.security.sasl.SaslException): GSS initiate failed at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:374) at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:640) at org.apache.hadoop.ipc.Client$Connection.access$2400(Client.java:436) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:833) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:829) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:829) ... 27 more \u89e3\u51b3\u529e\u6cd5\uff1a\u53ef\u4f7f\u7528 export JAVA_TOOL_OPTIONS=\"-Dsun.security.krb5.debug=true\" \u6253\u5f00\u8ba4\u8bc1\u65e5\u5fd7\u6392\u67e5\u95ee\u9898\uff0c\u68c0\u67e5 /etc/krb5.conf\u6587\u4ef6\u662f\u5426\u4e3a\u5bf9\u63a5\u96c6\u7fa4\u5339\u914d\u7684\u8ba4\u8bc1\u6587\u4ef6\u3002\u66f4\u6362\u6b63\u786e\u8ba4\u8bc1\u6587\u4ef6\u540e\u95ee\u9898\u89e3\u51b3\u3002","title":"FAQ"},{"location":"Other/librdkafka/","text":"\u5f00\u6e90\u7b2c\u4e09\u65b9librdkafka kerberos\u8ba4\u8bc1\u8fde\u63a5FI Kafka C\u53caC++\u7248\u6307\u5bfc\u4e66 \u00b6 \u9002\u7528\u573a\u666f \u00b6 librdkafka 1.0 \u2194 FusionInsight HD 6.5 (Kafka) 1.1\u63d0\u524d\u51c6\u5907\uff1a \u00b6 librdkafka\u5f00\u6e90\u4ee3\u7801\uff1a C/C++\u7248\u5f00\u6e90\u7f51\u5740\uff1a https://github.com/edenhill/librdkafka \u5177\u6709gcc\u7f16\u8bd1\u73af\u5883\u7684Linux\u64cd\u4f5c\u7cfb\u7edf\uff0c\u672c\u6587\u4ee5Suse 11.3\u64cd\u4f5c\u7cfb\u7edf\u4e3a\u4f8b\uff08\u4e0b\u6587\u4e2d\u4e0d\u505a\u7279\u6b8a\u8bf4\u660e\u5747\u4e3aSuse 11.3\u64cd\u4f5c\u7cfb\u7edf\uff09\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\u64cd\u4f5c\u7cfb\u7edf\u5747\u4f1a\u9ed8\u8ba4\u643a\u5e26\uff0c\u53ef\u901a\u8fc7gcc -v\u547d\u4ee4\u8fdb\u884c\u9a8c\u8bc1\u3002 \u9700\u8981libsasl-dev\u4f9d\u8d56\u5e93\uff0c\u4e00\u822c\u64cd\u4f5c\u7cfb\u7edf\u4e0d\u4f1a\u643a\u5e26\uff0c\u9700\u8981\u4ece\u7f51\u4e0a\u4e0b\u8f7d\uff0c\u53ef\u901a\u8fc7rpm -qa | grep cyrus\u8fdb\u884c\u67e5\u8be2\u3002 \u82e5\u4e0d\u5b58\u5728sasl-dev\u5e93\uff0c\u9700\u8981\u5728\u7f51\u7edc\u4e0a\u4e0b\u8f7d\uff0cSuse 11.3\u64cd\u4f5c\u7cfb\u7edf\u7684\u6b21rpm\u5305\u4e0b\u8f7d\u5730\u5740\u4e3a http://ftp.riken.jp/Linux/cern/slc52/updates/x86_64/RPMS/cyrus-sasl-devel-2.1.22-7.el5_8.1.x86_64.rpm \u4e0b\u8f7d\u540e\uff0c\u6267\u884crpm -ihv cyrus-sasl-devel-2.1.22-7.el5_8.1.x86_64.rpm \u2013nodep \u2013force\u8fdb\u884c\u5b89\u88c5\u3002 \u6ce8\u610f\uff1a\u82e5\u65e0sasl-plain\u3001sasl-devel\u3001sasl-gssapi\u5305\u9700\u8981\u8865\u9f50\u3002 \u82e5\u9700\u8981SSL\u80fd\u529b\uff0c\u9700\u8981\u4e0b\u8f7dlibssl-dev\u4f9d\u8d56\u5e93\uff0c\u65b9\u6cd5\u540c\u4e0a\u3002 1.2\u4fee\u6539\u6e90\u7801\u90e8\u5206\uff1a \u00b6 \u8bf4\u660e\uff1a\u7531\u4e8e\u5f00\u6e90\u793e\u533a\u7684\u57df\u540d\u4e0d\u652f\u6301\u914d\u7f6e\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u6e90\u7801\u8fdb\u884c\u4fee\u6539\uff0c\u6b64\u4fee\u6539\u4ec5\u9700\u4fee\u6539\u4e00\u6b21\uff0c\u7f16\u8bd1\u540e\u670d\u52a1\u7aef\u7684\u57df\u540d\u53d8\u66f4\uff0c\u4ec5\u9700\u8981\u901a\u8fc7\u4fee\u6539\u914d\u7f6e\u53c2\u6570\u5373\u53ef\uff0c\u65e0\u9700\u518d\u6b21\u7f16\u8bd1\u3002\uff08\u4e00\u5171\u9700\u8981\u4fee\u6539\u4e09\u5904\uff0c\u5f88\u7b80\u5355\uff09 \u7b2c\u4e00\u5904\uff1a \u4fee\u6539\u6587\u4ef6\uff1asrc/rdkafka_conf.h \u4fee\u6539\u884c\u53f7\uff1a223 \u4fee\u6539\u5185\u5bb9\uff1a\u5728rd_kafka_conf_s\u7684\u7ed3\u6784\u4f53\u5185\u7684sasl\u7ed3\u6784\u4f53\u4e2d\u589e\u52a0realm\u7684\u5b57\u7b26\u4e32\u6307\u9488\u3002 char *realm; \u4fee\u6539\u76ee\u7684\uff1a\u589e\u52a0realm\u7684\u914d\u7f6e\u3002 \u4fee\u6539\u793a\u4f8b\uff1a\u5982\u56fe \u7b2c\u4e8c\u5904\uff1a \u4fee\u6539\u6587\u4ef6\uff1a/src/rdkafka_conf.c \u4fee\u6539\u884c\u53f7\uff1a686-688\uff08\u589e\u52a0\u4e09\u884c\uff09 \u4fee\u6539\u5185\u5bb9\uff1a\u5728\u7ed3\u6784\u4f53rd_kafka_properties\u4e2d\u589e\u52a0realm\u7684\u5b9a\u4e49\u3001\u63cf\u8ff0\u3001\u9ed8\u8ba4\u503c\u7b49\u3002 {_RK_GLOBAL, \"sasl.kerberos.realm\", _RK_C_STR, _RK(sasl.realm), \"The realm of kafka cluster.\", .sdef = \"hadoop.com\"}, \u4fee\u6539\u76ee\u7684\uff1a\u589e\u52a0realm\u7684\u9ed8\u8ba4\u503c\u3002 \u4fee\u6539\u793a\u4f8b\uff1a\u5982\u56fe \u7b2c\u4e09\u5904\uff1a \u4fee\u6539\u6587\u4ef6\uff1a/src/rdkafka_sasl.c \u4fee\u6539\u884c\u53f7\uff1a198-200\uff08\u589e\u52a0\u4e09\u884c\uff0c\u5220\u9664\u4e00\u884c\uff09 \u4fee\u6539\u5185\u5bb9\uff1a\u5728\u51fd\u6570rd_kafka_sasl_client_new\u4e2d\u53d8\u66f4hostname\u7684\u8d4b\u503c\u3002 char tmp[2048]; rd_snprintf(tmp, sizeof(tmp), \"hadoop.%s\", rk->rk_conf.sasl.realm); rd_strdupa(&hostname, tmp); \u4fee\u6539\u76ee\u7684\uff1a\u4fee\u6539hostname\u7684\u8d4b\u503c\u4e3a\u57df\u540d\u3002 \u4fee\u6539\u793a\u4f8b\uff1a\u5982\u56fe 1.3\u7f16\u8bd1\uff1a \u00b6 \u5c06\u4fee\u6539\u540e\u7684\u4ee3\u7801\u6253\u5305\u6210zip\uff08\u4f8b\u5982librdkafka-master.zip\uff09\u683c\u5f0f\u4e0a\u4f20\u81f3\u7f16\u8bd1\u673a\u5668\u76ee\u5f55\uff0c\u4f8b\u5982/opt/build \u4f7f\u7528unzip librdkafka-master.zip\u89e3\u538b\u5230/opt/build\u5f53\u524d\u76ee\u5f55\u3002 cd /opt/build/librdkafka-master ./configure make make install \u7f16\u8bd1\u5b8c\u6210\u540e\uff0cexamples\u6587\u4ef6\u5939\u5185\u4f1a\u751f\u6210\u5f88\u591a\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u5176\u4e2drdkafka_example\u548crdkafka_example_cpp\u662f\u6211\u4eec\u6240\u9700\u8981\u7684\uff0crdkafka_example\u5bf9\u5e94\u7684\u662fC\u8bed\u8a00\u7248\u672c\u7684\u4f8b\u5b50\uff0crdkafka_example_cpp\u5bf9\u5e94\u7684\u662fC++\u7248\u672c\u7684\u4f8b\u5b50\uff0c\u4e8c\u8005\u4f7f\u7528\u65f6\u53c2\u6570\u76f8\u540c\uff0c\u8c03\u901aC\u8bed\u8a00\u7248\u672c\u7684\uff0cC++\u7248\u672c\u540c\u6837\u53ef\u4ee5\u8fd0\u884c\u3002 1.4\u4f7f\u7528\uff1a \u00b6 \u4e0b\u8f7dFusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5e76\u5b89\u88c5\uff0c\u5047\u8bbe\u5b89\u88c5\u8def\u5f84\u4e3a/opt/client\u76ee\u5f55 \u4e0b\u8f7d\u5177\u6709Kafka\u6743\u9650\u7684\u7528\u6237userA\u7684keytab\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u81f3\u73af\u5883\uff0c\u5047\u8bbe\u4e0a\u4f20\u76ee\u5f55\u4e3a/tmp/user.keytab \u5148\u6d4b\u8bd5\u4e00\u4e0b\u7528\u6237\u7684keytab\u662f\u5426\u6b63\u786e source /opt/client/bigdata_env kinit -k -t \"/tmp/user.keytab\" userA 4. \u5728\u521a\u624d\u7684\u7f16\u8bd1\u73af\u5883\u4e0a\u6267\u884c ./opt/build/librdkafka-master/examples/rdkafka_example -P -t zdtest -b 187.4.64.103:21007 -X sasl.kerberos.service.name=kafka -X sasl.kerberos.principal=userA -X sasl.kerberos.keytab=/tmp/user.keytab -X security.protocol=SASL_PLAINTEXT -X sasl.kerberos.kinit.cmd='/opt/client/KrbClient/kerberos/bin/kinit -k -t \"/tmp/user.keytab\" userA' \u2013X sasl.kerberos.realm=hadoop.com \u5bf9\u5e94\u7684\u53c2\u6570\u8bb2\u89e3\u5982\u4e0b\u56fe\u6240\u793a\uff1a -P \u5bf9\u5e94\u7684\u662fProducer -C\u5bf9\u5e94\u7684\u662fConsumer 187.4.64.103:21007\u4e3a\u670d\u52a1\u5668\u7684brokers\u5730\u5740 -t zdtest\u4e3a\u8981\u5199\u5165\u7684topic\u540d\u79f0 /tmp/user.keytab \u4e3auserA\u7684keytab\u6587\u4ef6\u7684\u8def\u5f84 sasl_plaintext \u4e3a\u5b89\u5168\u8ba4\u8bc1\u7684\u534f\u8bae /opt/client/KrbClient/kerberos/bin/kinit -k -t \"/tmp/user.keytab\" userA\u4e3a\u7528\u6237userA\u7684\u767b\u5f55\u547d\u4ee4 sasl.kerberos.realm=hadoop.com\u4e3a\u96c6\u7fa4\u7684\u57df\u540d\uff0c\u9ed8\u8ba4\u4e3ahadoop.com\u53ef\u4ee5\u4e0d\u7528\u6307\u5b9a\uff0c\u82e5Kafka\u96c6\u7fa4\u57df\u540d\u53d1\u751f\u53d8\u66f4\uff0c\u4ec5\u9700\u53d8\u66f4\u6b21\u540d\u79f0\u5373\u53ef\u3002 \u4ee5\u4e0a\u8fd0\u884c\u7684\u662fproducer \uff0c\u5982\u60f3\u8fd0\u884ccustomer\uff0c\u53ea\u9700\u5c06-P\u4fee\u6539\u4e3a-C\u5373\u53ef 1.5FAQ \u00b6 \u9047\u5230\u4ec0\u4e48\u95ee\u9898\uff0c\u53ef\u4ee5\u5728\u6b64\u8865\u5145\u5b8c\u5584\u3002 1.6\u9644\u5f55 \u00b6 1.6.1\u914d\u7f6e\u8bf4\u660e \u00b6 C/P \u542b\u4e49\uff1aC = \u751f\u4ea7\u8005, P = \u6d88\u8d39\u8005, * = \u4e8c\u8005\u90fd\u6709 \u5168\u5c40\u914d\u7f6e\u5c5e\u6027 \u00b6 \u5c5e\u6027 C/P \u8303\u56f4 \u9ed8\u8ba4\u503c \u63cf\u8ff0 1 builtin.features * nan gzip, snappy, ssl, safsl, regex, lz4 \u6807\u793a\u8be5librdkafka\u7684\u652f\u6301\u7684\u5185\u5efa\u7279\u6027\u3002\u5e94\u7528\u7a0b\u5e8f\u53ef\u4ee5\u67e5\u770b\u6216\u8bbe\u7f6e\u8fd9\u4e9b\u503c\u6765\u68c0\u67e5\u662f\u5426\u652f\u6301\u8fd9\u4e9b\u7279\u6027\u3002Type: CSV flags 2 client.id * nan rdkafka \u5ba2\u6237\u7aef\u6807\u793a\u3002Type: string 3 metadata.broker.list * nan nan \u521d\u59cb\u5316\u7684broker\u5217\u8868\u3002\u5e94\u7528\u7a0b\u5e8f\u4e5f\u53ef\u4ee5\u4f7f\u7528 rd_kafka_brokers_add() \u5728\u8fd0\u884c\u65f6\u6dfb\u52a0 broker\u3002Type: string 4 bootstrap.servers * nan nan \u53c2\u8003 metadata.broker.list 5 message.max.bytes * 1000 .. 1000000000 1000000 \u6700\u5927\u53d1\u9001\u6d88\u606f\u5927\u5c0f\u3002Type: integer 6 message.copy.max.bytes * 0 .. 1000000000 65535 \u6d88\u606f\u62f7\u8d1d\u5230\u7f13\u5b58\u7684\u6700\u5927\u5927\u5c0f\u3002\u5982\u679c\u6d88\u606f\u5927\u4e8e\u8fd9\u4e2a\u503c\uff0c\u5c06\u4f1a\u6d88\u8017\u66f4\u591a\u7684iovec\u800c\u91c7\u7528\u5f15\u7528\uff08\u96f6\u62f7\u8d1d\uff09\u65b9\u5f0f\u3002Type: integer 7 receive.message.max.bytes * 1000 .. 1000000000 100000000 \u6700\u5927\u63a5\u6536\u6d88\u606f\u5927\u5c0f\u3002\u8fd9\u662f\u4e00\u4e2a\u5b89\u5168\u9884\u9632\u63aa\u65bd\uff0c\u9632\u6b62\u534f\u8bae\u9971\u548c\u65f6\u5185\u5b58\u8017\u5c3d\u3002\u8fd9\u4e2a\u503c\u81f3\u5c11\u4e3a fetch.message.max.bytes * \u6d88\u8d39\u8005\u5206\u533a\u6570 + \u6d88\u606f\u5934\u5927\u5c0f (e.g. 200000 bytes). Type: integer 8 max.in.flight.requests.per.connection * 1 .. 1000000 1000000 \u5ba2\u6237\u7aef\u4fdd\u6301\u7684\u6700\u5927\u53d1\u9001\u8bf7\u6c42\u6570\u3002 \u8be5\u914d\u7f6e\u5e94\u7528\u4e8e\u6bcf\u4e00\u4e2a broker \u8fde\u63a5. Type: integer 9 metadata.request.timeout.ms * 10 .. 900000 60000 \u65e0\u6570\u636e\u8bf7\u6c42\u8d85\u65f6\u65f6\u95f4\uff0c\u6beb\u79d2\u3002 \u9002\u7528\u4e8e metadata \u8bf7\u6c42\u7b49\u3002Type: integer 10 topic.metadata.refresh.interval.ms * -1 .. 3600000 300000 Topic metadata \u5237\u65b0\u95f4\u9694\uff0c\u6beb\u79d2\u3002metadata \u81ea\u52a8\u5237\u65b0\u9519\u8bef\u548c\u8fde\u63a5\u3002\u8bbe\u7f6e\u4e3a -1 \u5173\u95ed\u5237\u65b0\u95f4\u9694\u3002 Type: integer 11 metadata.max.age.ms * nan nan \u53c2\u8003 topic.metadata.refresh.interval.ms 12 topic.metadata.refresh.fast.cnt * 0 .. 1000 10 \u5f53 topic \u4e22\u5931 leader\uff0c metadata \u8bf7\u6c42\u7684\u53d1\u9001\u6b21\u6570\uff0c\u53d1\u9001\u95f4\u9694\u662f topic.metadata.refresh.fast.interval.ms \u800c\u4e0d\u662f topic.metadata.refresh.interval.ms\u3002 \u8be5\u914d\u7f6e\u7528\u4e8e\u5feb\u901f\u4fee\u590dbroker leader\u3002Type: integer 13 topic.metadata.refresh.fast.interval.ms * 1 .. 60000 250 \u53c2\u8003 topic.metadata.refresh.fast.cnt\u3002Type: integer 14 topic.metadata.refresh.sparse * true, false TRUE \u6781\u5c11\u7684 metadata \u8bf7\u6c42 (\u6d88\u8d39\u8005\u7684\u7f51\u7edc\u5e26\u5bbd\u5f88\u5c0f) Type: boolean 15 topic.blacklist * nan nan Topic \u9ed1\u540d\u5355\uff0c\u9017\u53f7\u5206\u9694\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u5217\u8868\uff0c\u5339\u914dtopic\u540d\u5b57\uff0c\u5339\u914d\u5230\u7684 topic \u5982\u679c\u4e0d\u5b58\u5728\uff0c\u5c31\u5728 broker metadata \u4fe1\u606f\u4e2d\u5ffd\u7565\u3002Type: pattern list 16 debug * generic, broker, topic, metadata, queue, msg, protocol, cgrp, security, fetch, feature, all nan \u9017\u53f7\u5206\u9694\u7684\u5217\u8868\uff0c\u63a7\u5236 debug \u4e0a\u4e0b\u6587\u3002\u8c03\u8bd5\u751f\u4ea7\u8005\uff1abroker,topic,msg\u3002\u8c03\u8bd5\u6d88\u8d39\u8005\uff1acgrp,topic,fetch Type: CSV flags 17 socket.timeout.ms * 10 .. 300000 60000 \u7f51\u7edc\u8bf7\u6c42\u8d85\u65f6\u65f6\u95f4\u3002Type: integer 18 socket.blocking.max.ms * 1 .. 60000 100 broker \u5728 socket \u64cd\u4f5c\u65f6\u6700\u5927\u963b\u585e\u65f6\u95f4\u3002\u503c\u8d8a\u4f4e\uff0c\u54cd\u5e94\u8d8a\u5feb\uff0c\u4f46\u4f1a\u7565\u5fae\u63d0\u9ad8CPU\u4f7f\u7528\u7387\u3002Type: integer 19 socket.send.buffer.bytes * 0 .. 100000000 0 Broker socket \u53d1\u9001\u7f13\u51b2\u5927\u5c0f\u3002\u7cfb\u7edf\u9ed8\u8ba4\u4e3a 0\u3002Type: integer 20 socket.receive.buffer.bytes * 0 .. 100000000 0 Broker socket \u63a5\u6536\u7f13\u51b2\u5927\u5c0f\u3002\u7cfb\u7edf\u9ed8\u8ba4\u4e3a 0\u3002Type: integer 21 socket.keepalive.enable * true, false FALSE Broker sockets \u5141\u8bb8 TCP \u4fdd\u6301\u6d3b\u529b (SO_KEEPALIVE)\u3002Type: boolean 22 socket.max.fails * 0 .. 1000000 3 Broker \u5173\u95ed\u8fde\u63a5\u7684\u6700\u5927\u9519\u8bef\u6b21\u6570(e.g., timed out requests)\u30020\u4e0d\u5173\u95ed\u3002\u63d0\u793a\uff1a\u8fde\u63a5\u81ea\u52a8\u91cd\u65b0\u5efa\u7acb\u3002Type: integer 23 broker.address.ttl * 0 .. 86400000 1000 \u4fdd\u5b58 broker \u5730\u5740\u54cd\u5e94\u7ed3\u679c\u7684\u65f6\u95f4 (\u6beb\u79d2)\u3002Type: integer 24 broker.address.family * any, v4, v6 any \u5141\u8bb8\u7684 broker IP \u5730\u5740\u65cf\uff1aany, v4, v6\u3002Type: enum value 25 reconnect.backoff.jitter.ms * 0 .. 3600000 500 \u901a\u8fc7\u8fd9\u4e2a\u503c\u8c03\u8282 broker \u91cd\u8fde\u5c1d\u8bd5 +-50%\u3002Type: integer 26 statistics.interval.ms * 0 .. 86400000 0 librdkafka \u7edf\u8ba1\u95f4\u9694\u3002\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u901a\u8fc7 rd_kafka_conf_set_stats_cb()\u8bbe\u7f6e\u7edf\u8ba1\u7684\u56de\u8c03\u51fd\u6570\u3002\u7c92\u5ea6\u662f 1000ms. 0 \u5173\u95ed\u7edf\u8ba1\u3002Type: integer 27 enabled_events * 0 .. 2147483647 0 \u53c2\u8003 rd_kafka_conf_set_events()\u3002Type: integer 28 error_cb * nan nan \u9519\u8bef\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_error_cb()) Type: pointer 29 throttle_cb * nan nan \u8c03\u8282\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_throttle_cb()) Type: pointer 30 stats_cb * nan nan \u7edf\u8ba1\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_stats_cb()) Type: pointer 31 log_cb * nan nan \u65e5\u5fd7\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_log_cb()) Type: pointer 32 log_level * 0 .. 7 6 \u65e5\u5fd7\u754c\u522b (syslog(3) levels) Type: integer 33 log.thread.name * true, false FALSE \u5728\u65e5\u5fd7\u6d88\u606f\u4e2d\u6253\u5370\u5185\u90e8\u7ebf\u7a0b\u540d\u3002(useful for debugging librdkafka internals) Type: boolean 34 log.connection.close * true, false TRUE \u8bb0\u5f55 broker \u65ad\u5f00\u8fde\u63a5\u3002\u7531\u4e8e\u53d7 0.9 \u7248\u672c broker \u7684 connection.max.idle.ms \u7684\u5f71\u54cd\uff0c\u6700\u597d\u5173\u95ed\u3002Type: boolean 35 socket_cb * nan nan \u4e3aSocket\u521b\u5efa\u56de\u8c03\u51fd\u6570\u63d0\u4f9b\u65e0\u7f1d CLOEXEC Type: pointer 36 open_cb * nan nan \u4e3a\u6587\u4ef6\u6253\u5f00\u56de\u8c03\u51fd\u6570\u63d0\u4f9b\u65e0\u7f1d CLOEXEC Type: pointer 37 opaque * nan nan \u5bf9\u5e94\u7528\u7a0b\u5e8f\u4e0d\u5f00\u653e (set with rd_kafka_conf_set_opaque()) Type: pointer 38 default_topic_conf * nan nan \u9ed8\u8ba4 topic \u914d\u7f6e\uff0c\u7528\u4e8e\u81ea\u52a8\u8ba2\u9605 topics Type: pointer 39 internal.termination.signal * 0 .. 128 0 \u7528\u4e8e librdkafka \u8c03\u7528 rd_kafka_destroy() \u5feb\u901f\u7ec8\u6b62\u7684\u4fe1\u53f7\u3002\u5982\u679c\u6ca1\u6709\u8bbe\u7f6e\u4fe1\u53f7\uff0c \u7ec8\u6b62\u8fc7\u7a0b\u4f1a\u5ef6\u8fdf\u76f4\u5230\u6240\u6709\u5185\u90e8\u7ebf\u7a0b\u7684\u7cfb\u7edf\u8c03\u7528\u8d85\u65f6\u8fd4\u56de\uff0c\u4e14 rd_kafka_wait_destroyed() \u8fd4\u56de true\u3002\u5982\u679c\u8bbe\u7f6e\u4e86\u4fe1\u53f7\uff0c\u5ef6\u8fdf\u4f1a\u6700\u5c0f\u5316\u3002\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u5c4f\u853d\u8be5\u4fe1\u53f7\uff0c\u800c\u4f5c\u4e3a\u5185\u90e8\u4fe1\u53f7\u53e5\u67c4\u3002Type: integer 40 api.version.request * true, false FALSE \u8bf7\u6c42 broker \u652f\u6301\u7684API\u7248\u672c\uff0c\u8c03\u6574\u53ef\u7528\u534f\u8bae\u7279\u6027\u7684\u529f\u80fd\u3002\u5982\u679c\u8bbe\u7f6e\u4e3afalse\uff0c\u5c06\u4f7f\u7528 broker.version.fallback\u8bbe\u7f6e\u7684\u56de\u9000\u7248\u672c\u3002 \u63d0\u793a: \u4f9d\u8d56\u7684 broker \u7248\u672c >=0.10.0\u3002\u5982\u679c broker\uff08\u8001\u7248\u672c\uff09\u4e0d\u652f\u6301\u8be5\u8bf7\u6c42\uff0c\u4f7f\u7528 broker.version.fallback\u8bbe\u7f6e\u7684\u56de\u9000\u7248\u672c\u3002Type: boolean 41 api.version.fallback.ms * 0 .. 604800000 1200000 \u914d\u7f6e ApiVersionRequest \u5931\u8d25\u591a\u957f\u65f6\u95f4\u540e\uff0c\u4f7f\u7528 broker.version.fallback \u56de\u9000\u7248\u672c\u3002\u63d0\u793a: ApiVersionRequest \u53ea\u7528\u65b0\u7684 broker \u80fd\u4f7f\u7528\u3002Type: integer 42 broker.version.fallback * nan 0.9.0 \u8001\u7248\u672c\u7684 broker\uff08<0.10.0\uff09\u4e0d\u652f\u6301\u5ba2\u6237\u7aef\u67e5\u8be2\u652f\u6301\u534f\u8bae\u7279\u6027(ApiVersionRequest, see api.version.request)\uff0c\u6240\u4ee5\u8981\u5ba2\u6237\u7aef\u4e0d\u77e5\u9053\u4ec0\u4e48\u7279\u6027\u53ef\u4ee5\u4f7f\u7528\u3002 \u7528\u6237\u4f7f\u7528\u672c\u5c5e\u6027\u6307\u793a broker \u7248\u672c\uff0c\u5982\u679c ApiVersionRequest \u5931\u8d25\uff08\u6216\u4e0d\u53ef\u7528\uff09\uff0c\u5ba2\u6237\u7aef\u636e\u6b64\u5c5e\u6027\u81ea\u52a8\u8c03\u6574\u7279\u6027\u3002\u4e0e api.version.fallback.ms \u914d\u5408\u4f7f\u7528\u3002\u6709\u6548\u503c\uff1a0.9.0, 0.8.2, 0.8.1, 0.8.0. Type: string 43 security.protocol * plaintext, ssl, sasl_plaintext, sasl_ssl plaintext \u4e0e broker \u901a\u8baf\u7684\u534f\u8bae\u3002Type: enum value 44 ssl.cipher.suites * nan nan \u5bc6\u7801\u5957\u4ef6\u662f\u4e2a\u7ec4\u5408\u4f53\uff0c\u5305\u62ec\u9274\u6743\uff0c\u52a0\u5bc6\uff0c\u8ba4\u8bc1\u548c\u79d8\u94a5\u4ea4\u6362\u7a0b\u5e8f\uff0c\u7528\u4e8e\u7f51\u7edc\u8fde\u63a5\u7684\u5b89\u5168\u8bbe\u7f6e\u4ea4\u6362\uff0c\u4f7f\u7528 TLS \u6216 SSL \u7f51\u7edc\u534f\u8bae\u3002\u67e5\u770b\u624b\u518c ciphers(1) \u548c `SSL_CTX_set_cipher_list(3)\u3002 Type: string 45 ssl.key.location * nan nan \u5ba2\u6237\u7aef\u7684\u79c1\u94a5(PEM)\u8def\u5f84\uff0c\u7528\u4e8e\u9274\u6743\u3002Type: string 46 ssl.key.password * nan nan \u79c1\u94a5\u5bc6\u7801\u3002Type: string 47 ssl.certificate.location * nan nan \u5ba2\u6237\u7aef\u7684\u516c\u94a5(PEM)\u8def\u5f84\uff0c\u7528\u4e8e\u9274\u6743\u3002Type: string 48 ssl.ca.location * nan nan CA \u8bc1\u4e66\u6587\u4ef6\u6216\u8def\u5f84\uff0c\u7528\u4e8e\u6821\u9a8c broker key\u3002Type: string 49 ssl.crl.location * nan nan CRL \u8def\u5f84\uff0c\u7528\u4e8e broker \u7684\u8bc1\u4e66\u6821\u9a8c\u3002Type: string 50 sasl.mechanisms * GSSAPI, PLAIN GSSAPI \u4f7f\u7528 SASL \u673a\u5236\u9274\u6743\u3002 \u652f\u6301\uff1aGSSAPI, PLAIN. \u63d0\u793a: \u53ea\u80fd\u914d\u7f6e\u4e00\u79cd\u673a\u5236\u540d\u3002Type: string 51 sasl.kerberos.service.name * nan kafka Kafka \u8fd0\u884c\u7684 Kerberos \u9996\u8981\u540d\u3002Type: string 52 sasl.kerberos.principal * nan kafkaclient \u5ba2\u6237\u7aef\u7684 Kerberos \u9996\u8981\u540d\u3002Type: string 53 sasl.kerberos.kinit.cmd * nan kinit -S \u201c%{sasl.kerberos.service.name}/%{broker.name}\u201d -k -t \u201c%{sasl.kerberos.keytab}\u201d %{sasl.kerberos.principal} \u5b8c\u6574\u7684 kerberos kinit \u547d\u4ee4\u4e32\uff0c%{config.prop.name} \u66ff\u6362\u4e3a\u4e0e\u914d\u7f6e\u5bf9\u8c61\u4e00\u76f4\u7684\u503c\uff0c%{broker.name} broker \u7684\u4e3b\u673a\u540d\u3002Type: string 54 sasl.kerberos.keytab * nan nan Kerberos keytab \u6587\u4ef6\u7684\u8def\u5f84\u3002\u5982\u679c\u4e0d\u8bbe\u7f6e\uff0c\u5219\u4f7f\u7528\u7cfb\u7edf\u9ed8\u8ba4\u7684\u3002\u63d0\u793a\uff1a\u4e0d\u4f1a\u81ea\u52a8\u4f7f\u7528\uff0c\u5fc5\u987b\u5728 sasl.kerberos.kinit.cmd \u4e2d\u6dfb\u52a0\u5230\u6a21\u677f\uff0c\u5982 ... -t %{sasl.kerberos.keytab}\u3002 Type: string 55 sasl.kerberos.realm * nan hadoop.com \u534e\u4e3a\u5927\u6570\u636e\u4ea7\u54c1\uff08FusionInsight HD\uff09\u4e2d\u7684\u96c6\u7fa4\u57df\u540d 56 sasl.kerberos.min.time.before.relogin * 1 .. 86400000 60000 Key \u6062\u590d\u5c1d\u8bd5\u7684\u6700\u5c0f\u65f6\u95f4\uff0c\u6beb\u79d2\u3002Type: integer 57 sasl.username * nan nan \u4f7f\u7528 PLAIN \u673a\u5236\u65f6\uff0cSASL \u7528\u6237\u540d\u3002Type: string 58 sasl.password * nan nan \u4f7f\u7528 PLAIN \u673a\u5236\u65f6\uff0cSASL \u5bc6\u7801\u3002Type: string 59 group.id * nan nan \u5ba2\u6237\u7aef\u5206\u7ec4\u5b57\u7b26\u4e32\u3002\u540c\u7ec4\u7684\u5ba2\u6237\u7aef\u4f7f\u7528\u76f8\u540c\u7684 group.id\u3002Type: string 60 partition.assignment.strategy * nan range,roundrobin partition \u5206\u914d\u7b56\u7565\uff0c\u5f53\u9009\u4e3e\u7ec4 leader \u65f6\uff0c\u5206\u914d partition \u7ed9\u7ec4\u6210\u5458\u7684\u7b56\u7565\u3002Type: string 61 session.timeout.ms * 1 .. 3600000 30000 \u5ba2\u6237\u7aef\u7ec4\u4f1a\u8bdd\u63a2\u6d4b\u5931\u8d25\u8d85\u5e02\u65f6\u95f4\u3002Type: integer 62 heartbeat.interval.ms * 1 .. 3600000 1000 \u7ec4\u4f1a\u8bdd\u4fdd\u6d3b\u5fc3\u8df3\u95f4\u9694\u3002Type: integer 63 group.protocol.type * nan consumer \u7ec4\u534f\u8bae\u7c7b\u578b\u3002Type: string 64 coordinator.query.interval.ms * 1 .. 3600000 600000 \u591a\u4e45\u67e5\u8be2\u4e00\u6b21\u5f53\u524d\u7684\u5ba2\u6237\u7aef\u7ec4\u534f\u8c03\u4eba\u3002\u5982\u679c\u5f53\u524d\u7684\u5206\u914d\u534f\u8c03\u4eba\u6302\u4e86\uff0c\u4e3a\u4e86\u66f4\u5feb\u7684\u6062\u590d\u534f\u8c03\u4eba\uff0c\u63a2\u6d4b\u65f6\u95f4\u95f4\u9694\u4f1a\u9664\u4ee5 10\u3002Type: integer 65 enable.auto.commit C true, false TRUE \u5728\u540e\u53f0\u5468\u671f\u6027\u7684\u81ea\u52a8\u63d0\u4ea4\u504f\u79fb\u91cf\u3002Type: boolean 66 auto.commit.interval.ms C 0 .. 86400000 5000 \u6d88\u8d39\u8005\u504f\u79fb\u91cf\u63d0\u4ea4\uff08\u5199\u5165\uff09\u5230\u5b58\u50a8\u7684\u9891\u7387\uff0c\u6beb\u79d2\u3002(0 = \u4e0d\u53ef\u7528) Type: integer 67 enable.auto.offset.store C true, false TRUE \u4e3a\u5e94\u7528\u7a0b\u5e8f\u81ea\u52a8\u4fdd\u5b58\u6700\u540e\u6d88\u606f\u7684\u504f\u79fb\u91cf\u3002Type: boolean 68 queued.min.messages C 1 .. 10000000 100000 \u6bcf\u4e00\u4e2a topic+partition\uff0c\u672c\u5730\u6d88\u8d39\u8005\u961f\u5217\u7684\u6700\u5c0f\u6d88\u606f\u6570\u3002Type: integer 69 queued.max.messages.kbytes C 1 .. 1000000000 1000000 \u6bcf\u4e00\u4e2a topic+partition\uff0c\u672c\u5730\u6d88\u8d39\u8005\u961f\u5217\u7684\u6700\u5927\u5927\u5c0f\uff0c\u5355\u4f4dkilobytes\u3002\u8be5\u503c\u5e94\u8be5\u5927\u4e8e fetch.message.max.bytes\u3002Type: integer 70 fetch.wait.max.ms C 0 .. 300000 100 \u4e3a\u5199\u6ee1fetch.min.bytes\uff0cbroker \u7684\u6700\u5927\u7b49\u5f85\u65f6\u95f4\u3002Type: integer 71 fetch.message.max.bytes C 1 .. 1000000000 1048576 \u6bcf\u4e00\u4e2a topic+partition \u521d\u59cb\u5316\u7684\u6700\u5927\u5927\u5c0f\uff08bytes\uff09\u7528\u4e8e\u4ece broker \u8bfb\u6d88\u606f\u3002\u5982\u679c\u5ba2\u6237\u7aef\u9047\u5230\u6d88\u606f\u5927\u4e8e\u8fd9\u4e2a\u503c\uff0c\u4f1a\u9010\u6b65\u6269\u5927\u76f4\u5230\u585e\u4e0b\u8fd9\u4e2a\u6d88\u606f\u3002Type: integer 72 max.partition.fetch.bytes C nan nan \u53c2\u8003 fetch.message.max.bytes 73 fetch.min.bytes C 1 .. 100000000 1 broker \u8bf7\u6c42\u7684\u6700\u5c0f\u6570\u636e\u5927\u5c0f\uff0c\u5355\u4f4dbytes\u3002\u5982\u679c\u8fbe\u5230 fetch.wait.max.ms \u65f6\u95f4\uff0c\u5219\u4e0d\u7ba1\u8fd9\u4e2a\u914d\u7f6e\uff0c\u5c06\u5df2\u6536\u5230\u7684\u6570\u636e\u53d1\u9001\u7ed9\u5ba2\u6237\u7aef\u3002Type: integer 74 fetch.error.backoff.ms C 0 .. 300000 500 \u5bf9\u4e8e topic+partition\uff0c\u5982\u679c\u63a5\u53d7\u9519\u8bef\uff0c\u4e0b\u4e00\u4e2a\u63a5\u53d7\u8bf7\u6c42\u95f4\u9694\u591a\u957f\u65f6\u95f4\u3002Type: integer 75 offset.store.method C none, file, broker broker \u504f\u79fb\u91cf\u5b58\u50a8\u65b9\u5f0f\uff1a\u2019file\u2019 - \u672c\u5730\u6587\u4ef6\u5b58\u50a8 (offset.store.path, et.al), \u2018broker\u2019 - \u5728 broker \u4e0a\u63d0\u4ea4\u5b58\u50a8 (\u8981\u6c42 Apache Kafka 0.8.2 \u6216\u4ee5\u540e\u7248\u672c)\u3002Type: enum value 76 consume_cb C nan nan \u6d88\u606f\u6d88\u8d39\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_consume_cb()) Type: pointer 77 rebalance_cb C nan nan \u6d88\u8d39\u8005\u7ec4\u91cd\u65b0\u5206\u914d\u540e\u8c03\u7528 (\u53c2\u8003 rd_kafka_conf_set_rebalance_cb()) Type: pointer 78 offset_commit_cb C nan nan \u504f\u79fb\u91cf\u63d0\u4ea4\u7ed3\u679c\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_offset_commit_cb()) Type: pointer 79 enable.partition.eof C true, false TRUE \u5f53\u6d88\u8d39\u8005\u5230\u8fbe\u5206\u533a\u7ed3\u5c3e\uff0c\u53d1\u9001 RD_KAFKA_RESP_ERR__PARTITION_EOF \u4e8b\u4ef6\u3002Type: boolean 80 queue.buffering.max.messages P 1 .. 10000000 100000 \u751f\u4ea7\u8005\u961f\u5217\u5141\u8bb8\u7684\u6700\u5927\u6d88\u606f\u6570\u3002Type: integer 81 queue.buffering.max.kbytes P 1 .. 2147483647 4000000 \u751f\u4ea7\u8005\u961f\u5217\u5141\u8bb8\u7684\u6700\u5927\u5927\u5c0f\uff0c\u5355\u4f4dkb\u3002Type: integer 82 queue.buffering.max.ms P 1 .. 900000 1000 \u751f\u4ea7\u8005\u961f\u5217\u7f13\u5b58\u6570\u636e\u7684\u6700\u5927\u65f6\u95f4\uff0c\u6beb\u79d2\u3002Type: integer 83 message.send.max.retries P 0 .. 10000000 2 \u6d88\u606f\u96c6\u53d1\u9001\u5931\u8d25\u91cd\u8bd5\u6b21\u6570\u3002\u63d0\u793a \u91cd\u8bd5\u4f1a\u5bfc\u81f4\u91cd\u6392\u3002Type: integer 84 retries P nan nan \u53c2\u8003 message.send.max.retries 85 retry.backoff.ms P 1 .. 300000 100 \u91cd\u8bd5\u6d88\u606f\u53d1\u9001\u524d\u7684\u8865\u507f\u65f6\u95f4\u3002Type: integer 86 compression.codec P none, gzip, snappy, lz4 none \u538b\u7f29\u6d88\u606f\u96c6\u4f7f\u7528\u7684\u538b\u7f29\u7f16\u89e3\u7801\u5668\u3002\u8fd9\u91cc\u914d\u7f6e\u7684\u662f\u6240\u6709 topic \u7684\u9ed8\u8ba4\u503c\uff0c\u53ef\u80fd\u4f1a\u88ab topic \u4e0a\u7684 compression.codec \u5c5e\u6027\u8986\u76d6\u3002Type: enum value 87 batch.num.messages P 1 .. 1000000 10000 \u4e00\u4e2a\u6d88\u606f\u96c6\u6700\u5927\u6253\u5305\u6d88\u606f\u6570\u91cf\u3002\u6574\u4e2a\u6d88\u606f\u96c6\u7684\u5927\u5c0f\u4ecd\u53d7\u9650\u4e8e message.max.bytes\u3002 Type: integer 88 delivery.report.only.error P true, false FALSE \u53ea\u5bf9\u5931\u8d25\u7684\u6d88\u606f\u63d0\u4f9b\u5206\u53d1\u62a5\u544a\u3002Type: boolean 89 dr_cb P nan nan \u5206\u53d1\u62a5\u544a\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_dr_cb()) Type: pointer 90 dr_msg_cb P nan nan \u5206\u53d1\u62a5\u544a\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_dr_msg_cb()) Type: pointer Topic\u914d\u7f6e\u5c5e\u6027 \u00b6 \u5c5e\u6027 C/P \u8303\u56f4 \u9ed8\u8ba4\u503c \u63cf\u8ff0 1 request.required.acks P -1 .. 1000 1 \u8fd9\u4e2a\u5b57\u6bb5\u6807\u793a leader broker \u8981\u4ece ISR broker \u63a5\u6536\u591a\u5c11\u4e2a ack\uff0c\u7136\u540e\u624d\u786e\u8ba4\u53d1\u9001\u8bf7\u6c42\uff1a0=\u4e0d\u53d1\u9001\u4efb\u4f55 response/ack \u7ed9\u5ba2\u6237\u7aef, 1=\u53ea\u6709 leader broker \u9700\u8981 ack \u6d88\u606f, -1 or all=broker \u963b\u585e\u76f4\u5230\u6240\u6709\u7684\u540c\u6b65\u5907\u4efd(ISRs)\u6216in.sync.replicas\u8bbe\u7f6e\u7684\u5907\u4efd\u8fd4\u56de\u6d88\u606f\u63d0\u4ea4\u7684\u786e\u8ba4\u5e94\u7b54\u3002Type: integer 2 acks P nan nan \u53c2\u8003 request.required.acks 3 request.timeout.ms P 1 .. 900000 5000 \u751f\u4ea7\u8005\u8bf7\u6c42\u7b49\u5f85\u5e94\u7b54\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u6beb\u79d2\u3002\u8fd9\u4e2a\u503c\u4ec5\u5728 broker \u4e0a\u5f3a\u5236\u6267\u884c\u3002\u53c2\u8003 request.required.acks\uff0c\u4e0d\u80fd\u7b49\u4e8e 0. Type: integer 4 message.timeout.ms P 0 .. 900000 300000 \u672c\u5730\u6d88\u606f\u8d85\u65f6\u65f6\u95f4\u3002\u8fd9\u4e2a\u503c\u4ec5\u5728\u672c\u5730\u5f3a\u5236\u6267\u884c\uff0c\u9650\u5236\u751f\u4ea7\u7684\u6d88\u606f\u7b49\u5f85\u88ab\u6210\u529f\u53d1\u9001\u7684\u7b49\u5f85\u65f6\u95f4\uff0c0 \u662f\u4e0d\u9650\u5236\u3002Type: integer 5 produce.offset.report P true, false FALSE \u62a5\u544a\u751f\u4ea7\u6d88\u606f\u7684\u504f\u79fb\u91cf\u7ed9\u5e94\u7528\u7a0b\u5e8f\u3002\u5e94\u7528\u7a0b\u5e8f\u5fc5\u987b\u4f7f\u7528 dr_msg_cb\u4ece rd_kafka_message_t.offset \u4e2d\u83b7\u53d6\u504f\u79fb\u91cf\u3002Type: boolean 6 partitioner_cb P nan nan \u5206\u533a\u65b9\u6cd5\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_topic_conf_set_partitioner_cb()) Type: pointer 7 opaque * nan nan \u5e94\u7528\u7a0b\u5e8f\u4e0d\u53ef\u89c1 (\u53c2\u8003 rd_kafka_topic_conf_set_opaque()) Type: pointer 8 compression.codec P none, gzip, snappy, lz4, inherit inherit \u538b\u7f29\u6d88\u606f\u96c6\u7684\u538b\u7f29\u7f16\u89e3\u7801\u5668\u3002Type: enum value 9 auto.commit.enable C true, false TRUE \u5982\u679c\u662f true\uff0c\u5468\u671f\u6027\u7684\u63d0\u4ea4\u6700\u540e\u4e00\u4e2a\u6d88\u606f\u7684\u504f\u79fb\u91cf\u3002\u7528\u4e8e\u5f53\u7a0b\u5e8f\u91cd\u542f\u65f6\u629b\u5f03\u4e0d\u7528\u7684\u6d88\u606f\u3002\u5982\u679c\u662f false\uff0c\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u8c03\u7528 rd_kafka_offset_store() \u4fdd\u5b58\u504f\u79fb\u91cf (\u53ef\u9009). \u63d0\u793a \u8fd9\u4e2a\u5c5e\u6027\u65f6\u80fd\u7528\u4e8e\u7b80\u5355\u6d88\u8d39\u8005\uff0chigh-level KafkaConsumer \u4f1a\u88ab\u5168\u5c40\u7684 enable.auto.commit\u5c5e\u6027\u66ff\u4ee3\u3002\u63d0\u793a \u76ee\u524d\u6ca1\u6709\u6574\u5408 zookeeper\uff0c\u6839\u636e offset.store.method \u7684\u914d\u7f6e \u504f\u79fb\u91cf\u5c06\u5199\u5165 broker \u6216\u672c\u5730\u6587\u4ef6 file according to offset.store.method. Type: boolean 10 enable.auto.commit C nan nan \u53c2\u8003 auto.commit.enable 11 auto.commit.interval.ms C 10 .. 86400000 60000 \u6d88\u8d39\u8005\u504f\u79fb\u91cf\u63d0\u4ea4\uff08\u5199\u5165\uff09\u5230\u5b58\u50a8\u7684\u9891\u7387\uff0c\u6beb\u79d2\u3002Type: integer 12 auto.offset.reset C smallest, earliest, beginning, largest, latest, end, error largest \u5982\u679c\u504f\u79fb\u91cf\u5b58\u50a8\u8fd8\u6ca1\u6709\u521d\u59cb\u5316\u6216\u504f\u79fb\u91cf\u8d85\u8fc7\u8303\u56f4\u65f6\u7684\u5904\u7406\u65b9\u5f0f\uff1aAction to take when there is no initial offset in offset store or the desired offset is out of range: \u2018smallest\u2019,\u2019earliest\u2019 - \u81ea\u52a8\u91cd\u8bbe\u504f\u79fb\u91cf\u4e3a\u6700\u5c0f\u504f\u79fb\u91cf\uff0c\u2019largest\u2019,\u2019latest\u2019 - \u81ea\u52a8\u91cd\u8bbe\u504f\u79fb\u91cf\u4e3a\u6700\u5927\u504f\u79fb\u91cf\uff0c\u2019error\u2019 - \u901a\u8fc7\u6d88\u8d39\u6d88\u606f\u89e6\u53d1\u4e00\u4e2a\u9519\u8bef\uff0c\u8bf7\u68c0\u67e5 message->err\u3002 Type: enum value 13 offset.store.path C nan . \u5b58\u50a8\u504f\u79fb\u91cf\u7684\u672c\u5730\u6587\u4ef6\u8def\u5f84\u3002\u5982\u679c\u8def\u5f84\u662f\u4e2a\u76ee\u5f55\uff0c\u5728\u76ee\u5f55\u4e0b\u81ea\u52a8\u521b\u5efa\u57fa\u4e8e topic \u548c partition \u7684\u6587\u4ef6\u540d\u3002Type: string 14 offset.store.sync.interval.ms C -1 .. 86400000 -1 \u504f\u79fb\u91cf\u6587\u4ef6 fsync() \u7684\u95f4\u9694\uff0c\u6beb\u79d2\u3002-1 \u4e0d\u540c\u6b65\uff0c0 \u6bcf\u6b21\u5199\u5165\u540e\u7acb\u5373\u540c\u6b65\u3002Type: integer 15 offset.store.method C file, broker broker \u504f\u79fb\u91cf\u5b58\u50a8\u65b9\u5f0f\uff1a\u2019file\u2019 - \u672c\u5730\u6587\u4ef6\u5b58\u50a8 (offset.store.path, et.al), \u2018broker\u2019 - \u5728 broker \u4e0a\u63d0\u4ea4\u5b58\u50a8 (\u8981\u6c42 Apache Kafka 0.8.2 \u6216\u4ee5\u540e\u7248\u672c)\u3002Type: enum value 16 consume.callback.max.messages C 0 .. 1000000 0 \u4e00\u6b21 rd_kafka_consume_callback*() \u8c03\u914d\u7684\u6700\u5927\u6d88\u606f\u6570 (0 = \u65e0\u9650\u5236) Type: integer","title":"1.0 <--> 6.5"},{"location":"Other/librdkafka/#librdkafka-kerberosfi-kafka-cc","text":"","title":"\u5f00\u6e90\u7b2c\u4e09\u65b9librdkafka kerberos\u8ba4\u8bc1\u8fde\u63a5FI Kafka C\u53caC++\u7248\u6307\u5bfc\u4e66"},{"location":"Other/librdkafka/#_1","text":"librdkafka 1.0 \u2194 FusionInsight HD 6.5 (Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/librdkafka/#11","text":"librdkafka\u5f00\u6e90\u4ee3\u7801\uff1a C/C++\u7248\u5f00\u6e90\u7f51\u5740\uff1a https://github.com/edenhill/librdkafka \u5177\u6709gcc\u7f16\u8bd1\u73af\u5883\u7684Linux\u64cd\u4f5c\u7cfb\u7edf\uff0c\u672c\u6587\u4ee5Suse 11.3\u64cd\u4f5c\u7cfb\u7edf\u4e3a\u4f8b\uff08\u4e0b\u6587\u4e2d\u4e0d\u505a\u7279\u6b8a\u8bf4\u660e\u5747\u4e3aSuse 11.3\u64cd\u4f5c\u7cfb\u7edf\uff09\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\u64cd\u4f5c\u7cfb\u7edf\u5747\u4f1a\u9ed8\u8ba4\u643a\u5e26\uff0c\u53ef\u901a\u8fc7gcc -v\u547d\u4ee4\u8fdb\u884c\u9a8c\u8bc1\u3002 \u9700\u8981libsasl-dev\u4f9d\u8d56\u5e93\uff0c\u4e00\u822c\u64cd\u4f5c\u7cfb\u7edf\u4e0d\u4f1a\u643a\u5e26\uff0c\u9700\u8981\u4ece\u7f51\u4e0a\u4e0b\u8f7d\uff0c\u53ef\u901a\u8fc7rpm -qa | grep cyrus\u8fdb\u884c\u67e5\u8be2\u3002 \u82e5\u4e0d\u5b58\u5728sasl-dev\u5e93\uff0c\u9700\u8981\u5728\u7f51\u7edc\u4e0a\u4e0b\u8f7d\uff0cSuse 11.3\u64cd\u4f5c\u7cfb\u7edf\u7684\u6b21rpm\u5305\u4e0b\u8f7d\u5730\u5740\u4e3a http://ftp.riken.jp/Linux/cern/slc52/updates/x86_64/RPMS/cyrus-sasl-devel-2.1.22-7.el5_8.1.x86_64.rpm \u4e0b\u8f7d\u540e\uff0c\u6267\u884crpm -ihv cyrus-sasl-devel-2.1.22-7.el5_8.1.x86_64.rpm \u2013nodep \u2013force\u8fdb\u884c\u5b89\u88c5\u3002 \u6ce8\u610f\uff1a\u82e5\u65e0sasl-plain\u3001sasl-devel\u3001sasl-gssapi\u5305\u9700\u8981\u8865\u9f50\u3002 \u82e5\u9700\u8981SSL\u80fd\u529b\uff0c\u9700\u8981\u4e0b\u8f7dlibssl-dev\u4f9d\u8d56\u5e93\uff0c\u65b9\u6cd5\u540c\u4e0a\u3002","title":"1.1\u63d0\u524d\u51c6\u5907\uff1a"},{"location":"Other/librdkafka/#12","text":"\u8bf4\u660e\uff1a\u7531\u4e8e\u5f00\u6e90\u793e\u533a\u7684\u57df\u540d\u4e0d\u652f\u6301\u914d\u7f6e\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u6e90\u7801\u8fdb\u884c\u4fee\u6539\uff0c\u6b64\u4fee\u6539\u4ec5\u9700\u4fee\u6539\u4e00\u6b21\uff0c\u7f16\u8bd1\u540e\u670d\u52a1\u7aef\u7684\u57df\u540d\u53d8\u66f4\uff0c\u4ec5\u9700\u8981\u901a\u8fc7\u4fee\u6539\u914d\u7f6e\u53c2\u6570\u5373\u53ef\uff0c\u65e0\u9700\u518d\u6b21\u7f16\u8bd1\u3002\uff08\u4e00\u5171\u9700\u8981\u4fee\u6539\u4e09\u5904\uff0c\u5f88\u7b80\u5355\uff09 \u7b2c\u4e00\u5904\uff1a \u4fee\u6539\u6587\u4ef6\uff1asrc/rdkafka_conf.h \u4fee\u6539\u884c\u53f7\uff1a223 \u4fee\u6539\u5185\u5bb9\uff1a\u5728rd_kafka_conf_s\u7684\u7ed3\u6784\u4f53\u5185\u7684sasl\u7ed3\u6784\u4f53\u4e2d\u589e\u52a0realm\u7684\u5b57\u7b26\u4e32\u6307\u9488\u3002 char *realm; \u4fee\u6539\u76ee\u7684\uff1a\u589e\u52a0realm\u7684\u914d\u7f6e\u3002 \u4fee\u6539\u793a\u4f8b\uff1a\u5982\u56fe \u7b2c\u4e8c\u5904\uff1a \u4fee\u6539\u6587\u4ef6\uff1a/src/rdkafka_conf.c \u4fee\u6539\u884c\u53f7\uff1a686-688\uff08\u589e\u52a0\u4e09\u884c\uff09 \u4fee\u6539\u5185\u5bb9\uff1a\u5728\u7ed3\u6784\u4f53rd_kafka_properties\u4e2d\u589e\u52a0realm\u7684\u5b9a\u4e49\u3001\u63cf\u8ff0\u3001\u9ed8\u8ba4\u503c\u7b49\u3002 {_RK_GLOBAL, \"sasl.kerberos.realm\", _RK_C_STR, _RK(sasl.realm), \"The realm of kafka cluster.\", .sdef = \"hadoop.com\"}, \u4fee\u6539\u76ee\u7684\uff1a\u589e\u52a0realm\u7684\u9ed8\u8ba4\u503c\u3002 \u4fee\u6539\u793a\u4f8b\uff1a\u5982\u56fe \u7b2c\u4e09\u5904\uff1a \u4fee\u6539\u6587\u4ef6\uff1a/src/rdkafka_sasl.c \u4fee\u6539\u884c\u53f7\uff1a198-200\uff08\u589e\u52a0\u4e09\u884c\uff0c\u5220\u9664\u4e00\u884c\uff09 \u4fee\u6539\u5185\u5bb9\uff1a\u5728\u51fd\u6570rd_kafka_sasl_client_new\u4e2d\u53d8\u66f4hostname\u7684\u8d4b\u503c\u3002 char tmp[2048]; rd_snprintf(tmp, sizeof(tmp), \"hadoop.%s\", rk->rk_conf.sasl.realm); rd_strdupa(&hostname, tmp); \u4fee\u6539\u76ee\u7684\uff1a\u4fee\u6539hostname\u7684\u8d4b\u503c\u4e3a\u57df\u540d\u3002 \u4fee\u6539\u793a\u4f8b\uff1a\u5982\u56fe","title":"1.2\u4fee\u6539\u6e90\u7801\u90e8\u5206\uff1a"},{"location":"Other/librdkafka/#13","text":"\u5c06\u4fee\u6539\u540e\u7684\u4ee3\u7801\u6253\u5305\u6210zip\uff08\u4f8b\u5982librdkafka-master.zip\uff09\u683c\u5f0f\u4e0a\u4f20\u81f3\u7f16\u8bd1\u673a\u5668\u76ee\u5f55\uff0c\u4f8b\u5982/opt/build \u4f7f\u7528unzip librdkafka-master.zip\u89e3\u538b\u5230/opt/build\u5f53\u524d\u76ee\u5f55\u3002 cd /opt/build/librdkafka-master ./configure make make install \u7f16\u8bd1\u5b8c\u6210\u540e\uff0cexamples\u6587\u4ef6\u5939\u5185\u4f1a\u751f\u6210\u5f88\u591a\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u5176\u4e2drdkafka_example\u548crdkafka_example_cpp\u662f\u6211\u4eec\u6240\u9700\u8981\u7684\uff0crdkafka_example\u5bf9\u5e94\u7684\u662fC\u8bed\u8a00\u7248\u672c\u7684\u4f8b\u5b50\uff0crdkafka_example_cpp\u5bf9\u5e94\u7684\u662fC++\u7248\u672c\u7684\u4f8b\u5b50\uff0c\u4e8c\u8005\u4f7f\u7528\u65f6\u53c2\u6570\u76f8\u540c\uff0c\u8c03\u901aC\u8bed\u8a00\u7248\u672c\u7684\uff0cC++\u7248\u672c\u540c\u6837\u53ef\u4ee5\u8fd0\u884c\u3002","title":"1.3\u7f16\u8bd1\uff1a"},{"location":"Other/librdkafka/#14","text":"\u4e0b\u8f7dFusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u5e76\u5b89\u88c5\uff0c\u5047\u8bbe\u5b89\u88c5\u8def\u5f84\u4e3a/opt/client\u76ee\u5f55 \u4e0b\u8f7d\u5177\u6709Kafka\u6743\u9650\u7684\u7528\u6237userA\u7684keytab\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u81f3\u73af\u5883\uff0c\u5047\u8bbe\u4e0a\u4f20\u76ee\u5f55\u4e3a/tmp/user.keytab \u5148\u6d4b\u8bd5\u4e00\u4e0b\u7528\u6237\u7684keytab\u662f\u5426\u6b63\u786e source /opt/client/bigdata_env kinit -k -t \"/tmp/user.keytab\" userA 4. \u5728\u521a\u624d\u7684\u7f16\u8bd1\u73af\u5883\u4e0a\u6267\u884c ./opt/build/librdkafka-master/examples/rdkafka_example -P -t zdtest -b 187.4.64.103:21007 -X sasl.kerberos.service.name=kafka -X sasl.kerberos.principal=userA -X sasl.kerberos.keytab=/tmp/user.keytab -X security.protocol=SASL_PLAINTEXT -X sasl.kerberos.kinit.cmd='/opt/client/KrbClient/kerberos/bin/kinit -k -t \"/tmp/user.keytab\" userA' \u2013X sasl.kerberos.realm=hadoop.com \u5bf9\u5e94\u7684\u53c2\u6570\u8bb2\u89e3\u5982\u4e0b\u56fe\u6240\u793a\uff1a -P \u5bf9\u5e94\u7684\u662fProducer -C\u5bf9\u5e94\u7684\u662fConsumer 187.4.64.103:21007\u4e3a\u670d\u52a1\u5668\u7684brokers\u5730\u5740 -t zdtest\u4e3a\u8981\u5199\u5165\u7684topic\u540d\u79f0 /tmp/user.keytab \u4e3auserA\u7684keytab\u6587\u4ef6\u7684\u8def\u5f84 sasl_plaintext \u4e3a\u5b89\u5168\u8ba4\u8bc1\u7684\u534f\u8bae /opt/client/KrbClient/kerberos/bin/kinit -k -t \"/tmp/user.keytab\" userA\u4e3a\u7528\u6237userA\u7684\u767b\u5f55\u547d\u4ee4 sasl.kerberos.realm=hadoop.com\u4e3a\u96c6\u7fa4\u7684\u57df\u540d\uff0c\u9ed8\u8ba4\u4e3ahadoop.com\u53ef\u4ee5\u4e0d\u7528\u6307\u5b9a\uff0c\u82e5Kafka\u96c6\u7fa4\u57df\u540d\u53d1\u751f\u53d8\u66f4\uff0c\u4ec5\u9700\u53d8\u66f4\u6b21\u540d\u79f0\u5373\u53ef\u3002 \u4ee5\u4e0a\u8fd0\u884c\u7684\u662fproducer \uff0c\u5982\u60f3\u8fd0\u884ccustomer\uff0c\u53ea\u9700\u5c06-P\u4fee\u6539\u4e3a-C\u5373\u53ef","title":"1.4\u4f7f\u7528\uff1a"},{"location":"Other/librdkafka/#15faq","text":"\u9047\u5230\u4ec0\u4e48\u95ee\u9898\uff0c\u53ef\u4ee5\u5728\u6b64\u8865\u5145\u5b8c\u5584\u3002","title":"1.5FAQ"},{"location":"Other/librdkafka/#16","text":"","title":"1.6\u9644\u5f55"},{"location":"Other/librdkafka/#161","text":"C/P \u542b\u4e49\uff1aC = \u751f\u4ea7\u8005, P = \u6d88\u8d39\u8005, * = \u4e8c\u8005\u90fd\u6709","title":"1.6.1\u914d\u7f6e\u8bf4\u660e"},{"location":"Other/librdkafka/#_2","text":"\u5c5e\u6027 C/P \u8303\u56f4 \u9ed8\u8ba4\u503c \u63cf\u8ff0 1 builtin.features * nan gzip, snappy, ssl, safsl, regex, lz4 \u6807\u793a\u8be5librdkafka\u7684\u652f\u6301\u7684\u5185\u5efa\u7279\u6027\u3002\u5e94\u7528\u7a0b\u5e8f\u53ef\u4ee5\u67e5\u770b\u6216\u8bbe\u7f6e\u8fd9\u4e9b\u503c\u6765\u68c0\u67e5\u662f\u5426\u652f\u6301\u8fd9\u4e9b\u7279\u6027\u3002Type: CSV flags 2 client.id * nan rdkafka \u5ba2\u6237\u7aef\u6807\u793a\u3002Type: string 3 metadata.broker.list * nan nan \u521d\u59cb\u5316\u7684broker\u5217\u8868\u3002\u5e94\u7528\u7a0b\u5e8f\u4e5f\u53ef\u4ee5\u4f7f\u7528 rd_kafka_brokers_add() \u5728\u8fd0\u884c\u65f6\u6dfb\u52a0 broker\u3002Type: string 4 bootstrap.servers * nan nan \u53c2\u8003 metadata.broker.list 5 message.max.bytes * 1000 .. 1000000000 1000000 \u6700\u5927\u53d1\u9001\u6d88\u606f\u5927\u5c0f\u3002Type: integer 6 message.copy.max.bytes * 0 .. 1000000000 65535 \u6d88\u606f\u62f7\u8d1d\u5230\u7f13\u5b58\u7684\u6700\u5927\u5927\u5c0f\u3002\u5982\u679c\u6d88\u606f\u5927\u4e8e\u8fd9\u4e2a\u503c\uff0c\u5c06\u4f1a\u6d88\u8017\u66f4\u591a\u7684iovec\u800c\u91c7\u7528\u5f15\u7528\uff08\u96f6\u62f7\u8d1d\uff09\u65b9\u5f0f\u3002Type: integer 7 receive.message.max.bytes * 1000 .. 1000000000 100000000 \u6700\u5927\u63a5\u6536\u6d88\u606f\u5927\u5c0f\u3002\u8fd9\u662f\u4e00\u4e2a\u5b89\u5168\u9884\u9632\u63aa\u65bd\uff0c\u9632\u6b62\u534f\u8bae\u9971\u548c\u65f6\u5185\u5b58\u8017\u5c3d\u3002\u8fd9\u4e2a\u503c\u81f3\u5c11\u4e3a fetch.message.max.bytes * \u6d88\u8d39\u8005\u5206\u533a\u6570 + \u6d88\u606f\u5934\u5927\u5c0f (e.g. 200000 bytes). Type: integer 8 max.in.flight.requests.per.connection * 1 .. 1000000 1000000 \u5ba2\u6237\u7aef\u4fdd\u6301\u7684\u6700\u5927\u53d1\u9001\u8bf7\u6c42\u6570\u3002 \u8be5\u914d\u7f6e\u5e94\u7528\u4e8e\u6bcf\u4e00\u4e2a broker \u8fde\u63a5. Type: integer 9 metadata.request.timeout.ms * 10 .. 900000 60000 \u65e0\u6570\u636e\u8bf7\u6c42\u8d85\u65f6\u65f6\u95f4\uff0c\u6beb\u79d2\u3002 \u9002\u7528\u4e8e metadata \u8bf7\u6c42\u7b49\u3002Type: integer 10 topic.metadata.refresh.interval.ms * -1 .. 3600000 300000 Topic metadata \u5237\u65b0\u95f4\u9694\uff0c\u6beb\u79d2\u3002metadata \u81ea\u52a8\u5237\u65b0\u9519\u8bef\u548c\u8fde\u63a5\u3002\u8bbe\u7f6e\u4e3a -1 \u5173\u95ed\u5237\u65b0\u95f4\u9694\u3002 Type: integer 11 metadata.max.age.ms * nan nan \u53c2\u8003 topic.metadata.refresh.interval.ms 12 topic.metadata.refresh.fast.cnt * 0 .. 1000 10 \u5f53 topic \u4e22\u5931 leader\uff0c metadata \u8bf7\u6c42\u7684\u53d1\u9001\u6b21\u6570\uff0c\u53d1\u9001\u95f4\u9694\u662f topic.metadata.refresh.fast.interval.ms \u800c\u4e0d\u662f topic.metadata.refresh.interval.ms\u3002 \u8be5\u914d\u7f6e\u7528\u4e8e\u5feb\u901f\u4fee\u590dbroker leader\u3002Type: integer 13 topic.metadata.refresh.fast.interval.ms * 1 .. 60000 250 \u53c2\u8003 topic.metadata.refresh.fast.cnt\u3002Type: integer 14 topic.metadata.refresh.sparse * true, false TRUE \u6781\u5c11\u7684 metadata \u8bf7\u6c42 (\u6d88\u8d39\u8005\u7684\u7f51\u7edc\u5e26\u5bbd\u5f88\u5c0f) Type: boolean 15 topic.blacklist * nan nan Topic \u9ed1\u540d\u5355\uff0c\u9017\u53f7\u5206\u9694\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u5217\u8868\uff0c\u5339\u914dtopic\u540d\u5b57\uff0c\u5339\u914d\u5230\u7684 topic \u5982\u679c\u4e0d\u5b58\u5728\uff0c\u5c31\u5728 broker metadata \u4fe1\u606f\u4e2d\u5ffd\u7565\u3002Type: pattern list 16 debug * generic, broker, topic, metadata, queue, msg, protocol, cgrp, security, fetch, feature, all nan \u9017\u53f7\u5206\u9694\u7684\u5217\u8868\uff0c\u63a7\u5236 debug \u4e0a\u4e0b\u6587\u3002\u8c03\u8bd5\u751f\u4ea7\u8005\uff1abroker,topic,msg\u3002\u8c03\u8bd5\u6d88\u8d39\u8005\uff1acgrp,topic,fetch Type: CSV flags 17 socket.timeout.ms * 10 .. 300000 60000 \u7f51\u7edc\u8bf7\u6c42\u8d85\u65f6\u65f6\u95f4\u3002Type: integer 18 socket.blocking.max.ms * 1 .. 60000 100 broker \u5728 socket \u64cd\u4f5c\u65f6\u6700\u5927\u963b\u585e\u65f6\u95f4\u3002\u503c\u8d8a\u4f4e\uff0c\u54cd\u5e94\u8d8a\u5feb\uff0c\u4f46\u4f1a\u7565\u5fae\u63d0\u9ad8CPU\u4f7f\u7528\u7387\u3002Type: integer 19 socket.send.buffer.bytes * 0 .. 100000000 0 Broker socket \u53d1\u9001\u7f13\u51b2\u5927\u5c0f\u3002\u7cfb\u7edf\u9ed8\u8ba4\u4e3a 0\u3002Type: integer 20 socket.receive.buffer.bytes * 0 .. 100000000 0 Broker socket \u63a5\u6536\u7f13\u51b2\u5927\u5c0f\u3002\u7cfb\u7edf\u9ed8\u8ba4\u4e3a 0\u3002Type: integer 21 socket.keepalive.enable * true, false FALSE Broker sockets \u5141\u8bb8 TCP \u4fdd\u6301\u6d3b\u529b (SO_KEEPALIVE)\u3002Type: boolean 22 socket.max.fails * 0 .. 1000000 3 Broker \u5173\u95ed\u8fde\u63a5\u7684\u6700\u5927\u9519\u8bef\u6b21\u6570(e.g., timed out requests)\u30020\u4e0d\u5173\u95ed\u3002\u63d0\u793a\uff1a\u8fde\u63a5\u81ea\u52a8\u91cd\u65b0\u5efa\u7acb\u3002Type: integer 23 broker.address.ttl * 0 .. 86400000 1000 \u4fdd\u5b58 broker \u5730\u5740\u54cd\u5e94\u7ed3\u679c\u7684\u65f6\u95f4 (\u6beb\u79d2)\u3002Type: integer 24 broker.address.family * any, v4, v6 any \u5141\u8bb8\u7684 broker IP \u5730\u5740\u65cf\uff1aany, v4, v6\u3002Type: enum value 25 reconnect.backoff.jitter.ms * 0 .. 3600000 500 \u901a\u8fc7\u8fd9\u4e2a\u503c\u8c03\u8282 broker \u91cd\u8fde\u5c1d\u8bd5 +-50%\u3002Type: integer 26 statistics.interval.ms * 0 .. 86400000 0 librdkafka \u7edf\u8ba1\u95f4\u9694\u3002\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u901a\u8fc7 rd_kafka_conf_set_stats_cb()\u8bbe\u7f6e\u7edf\u8ba1\u7684\u56de\u8c03\u51fd\u6570\u3002\u7c92\u5ea6\u662f 1000ms. 0 \u5173\u95ed\u7edf\u8ba1\u3002Type: integer 27 enabled_events * 0 .. 2147483647 0 \u53c2\u8003 rd_kafka_conf_set_events()\u3002Type: integer 28 error_cb * nan nan \u9519\u8bef\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_error_cb()) Type: pointer 29 throttle_cb * nan nan \u8c03\u8282\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_throttle_cb()) Type: pointer 30 stats_cb * nan nan \u7edf\u8ba1\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_stats_cb()) Type: pointer 31 log_cb * nan nan \u65e5\u5fd7\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_log_cb()) Type: pointer 32 log_level * 0 .. 7 6 \u65e5\u5fd7\u754c\u522b (syslog(3) levels) Type: integer 33 log.thread.name * true, false FALSE \u5728\u65e5\u5fd7\u6d88\u606f\u4e2d\u6253\u5370\u5185\u90e8\u7ebf\u7a0b\u540d\u3002(useful for debugging librdkafka internals) Type: boolean 34 log.connection.close * true, false TRUE \u8bb0\u5f55 broker \u65ad\u5f00\u8fde\u63a5\u3002\u7531\u4e8e\u53d7 0.9 \u7248\u672c broker \u7684 connection.max.idle.ms \u7684\u5f71\u54cd\uff0c\u6700\u597d\u5173\u95ed\u3002Type: boolean 35 socket_cb * nan nan \u4e3aSocket\u521b\u5efa\u56de\u8c03\u51fd\u6570\u63d0\u4f9b\u65e0\u7f1d CLOEXEC Type: pointer 36 open_cb * nan nan \u4e3a\u6587\u4ef6\u6253\u5f00\u56de\u8c03\u51fd\u6570\u63d0\u4f9b\u65e0\u7f1d CLOEXEC Type: pointer 37 opaque * nan nan \u5bf9\u5e94\u7528\u7a0b\u5e8f\u4e0d\u5f00\u653e (set with rd_kafka_conf_set_opaque()) Type: pointer 38 default_topic_conf * nan nan \u9ed8\u8ba4 topic \u914d\u7f6e\uff0c\u7528\u4e8e\u81ea\u52a8\u8ba2\u9605 topics Type: pointer 39 internal.termination.signal * 0 .. 128 0 \u7528\u4e8e librdkafka \u8c03\u7528 rd_kafka_destroy() \u5feb\u901f\u7ec8\u6b62\u7684\u4fe1\u53f7\u3002\u5982\u679c\u6ca1\u6709\u8bbe\u7f6e\u4fe1\u53f7\uff0c \u7ec8\u6b62\u8fc7\u7a0b\u4f1a\u5ef6\u8fdf\u76f4\u5230\u6240\u6709\u5185\u90e8\u7ebf\u7a0b\u7684\u7cfb\u7edf\u8c03\u7528\u8d85\u65f6\u8fd4\u56de\uff0c\u4e14 rd_kafka_wait_destroyed() \u8fd4\u56de true\u3002\u5982\u679c\u8bbe\u7f6e\u4e86\u4fe1\u53f7\uff0c\u5ef6\u8fdf\u4f1a\u6700\u5c0f\u5316\u3002\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u5c4f\u853d\u8be5\u4fe1\u53f7\uff0c\u800c\u4f5c\u4e3a\u5185\u90e8\u4fe1\u53f7\u53e5\u67c4\u3002Type: integer 40 api.version.request * true, false FALSE \u8bf7\u6c42 broker \u652f\u6301\u7684API\u7248\u672c\uff0c\u8c03\u6574\u53ef\u7528\u534f\u8bae\u7279\u6027\u7684\u529f\u80fd\u3002\u5982\u679c\u8bbe\u7f6e\u4e3afalse\uff0c\u5c06\u4f7f\u7528 broker.version.fallback\u8bbe\u7f6e\u7684\u56de\u9000\u7248\u672c\u3002 \u63d0\u793a: \u4f9d\u8d56\u7684 broker \u7248\u672c >=0.10.0\u3002\u5982\u679c broker\uff08\u8001\u7248\u672c\uff09\u4e0d\u652f\u6301\u8be5\u8bf7\u6c42\uff0c\u4f7f\u7528 broker.version.fallback\u8bbe\u7f6e\u7684\u56de\u9000\u7248\u672c\u3002Type: boolean 41 api.version.fallback.ms * 0 .. 604800000 1200000 \u914d\u7f6e ApiVersionRequest \u5931\u8d25\u591a\u957f\u65f6\u95f4\u540e\uff0c\u4f7f\u7528 broker.version.fallback \u56de\u9000\u7248\u672c\u3002\u63d0\u793a: ApiVersionRequest \u53ea\u7528\u65b0\u7684 broker \u80fd\u4f7f\u7528\u3002Type: integer 42 broker.version.fallback * nan 0.9.0 \u8001\u7248\u672c\u7684 broker\uff08<0.10.0\uff09\u4e0d\u652f\u6301\u5ba2\u6237\u7aef\u67e5\u8be2\u652f\u6301\u534f\u8bae\u7279\u6027(ApiVersionRequest, see api.version.request)\uff0c\u6240\u4ee5\u8981\u5ba2\u6237\u7aef\u4e0d\u77e5\u9053\u4ec0\u4e48\u7279\u6027\u53ef\u4ee5\u4f7f\u7528\u3002 \u7528\u6237\u4f7f\u7528\u672c\u5c5e\u6027\u6307\u793a broker \u7248\u672c\uff0c\u5982\u679c ApiVersionRequest \u5931\u8d25\uff08\u6216\u4e0d\u53ef\u7528\uff09\uff0c\u5ba2\u6237\u7aef\u636e\u6b64\u5c5e\u6027\u81ea\u52a8\u8c03\u6574\u7279\u6027\u3002\u4e0e api.version.fallback.ms \u914d\u5408\u4f7f\u7528\u3002\u6709\u6548\u503c\uff1a0.9.0, 0.8.2, 0.8.1, 0.8.0. Type: string 43 security.protocol * plaintext, ssl, sasl_plaintext, sasl_ssl plaintext \u4e0e broker \u901a\u8baf\u7684\u534f\u8bae\u3002Type: enum value 44 ssl.cipher.suites * nan nan \u5bc6\u7801\u5957\u4ef6\u662f\u4e2a\u7ec4\u5408\u4f53\uff0c\u5305\u62ec\u9274\u6743\uff0c\u52a0\u5bc6\uff0c\u8ba4\u8bc1\u548c\u79d8\u94a5\u4ea4\u6362\u7a0b\u5e8f\uff0c\u7528\u4e8e\u7f51\u7edc\u8fde\u63a5\u7684\u5b89\u5168\u8bbe\u7f6e\u4ea4\u6362\uff0c\u4f7f\u7528 TLS \u6216 SSL \u7f51\u7edc\u534f\u8bae\u3002\u67e5\u770b\u624b\u518c ciphers(1) \u548c `SSL_CTX_set_cipher_list(3)\u3002 Type: string 45 ssl.key.location * nan nan \u5ba2\u6237\u7aef\u7684\u79c1\u94a5(PEM)\u8def\u5f84\uff0c\u7528\u4e8e\u9274\u6743\u3002Type: string 46 ssl.key.password * nan nan \u79c1\u94a5\u5bc6\u7801\u3002Type: string 47 ssl.certificate.location * nan nan \u5ba2\u6237\u7aef\u7684\u516c\u94a5(PEM)\u8def\u5f84\uff0c\u7528\u4e8e\u9274\u6743\u3002Type: string 48 ssl.ca.location * nan nan CA \u8bc1\u4e66\u6587\u4ef6\u6216\u8def\u5f84\uff0c\u7528\u4e8e\u6821\u9a8c broker key\u3002Type: string 49 ssl.crl.location * nan nan CRL \u8def\u5f84\uff0c\u7528\u4e8e broker \u7684\u8bc1\u4e66\u6821\u9a8c\u3002Type: string 50 sasl.mechanisms * GSSAPI, PLAIN GSSAPI \u4f7f\u7528 SASL \u673a\u5236\u9274\u6743\u3002 \u652f\u6301\uff1aGSSAPI, PLAIN. \u63d0\u793a: \u53ea\u80fd\u914d\u7f6e\u4e00\u79cd\u673a\u5236\u540d\u3002Type: string 51 sasl.kerberos.service.name * nan kafka Kafka \u8fd0\u884c\u7684 Kerberos \u9996\u8981\u540d\u3002Type: string 52 sasl.kerberos.principal * nan kafkaclient \u5ba2\u6237\u7aef\u7684 Kerberos \u9996\u8981\u540d\u3002Type: string 53 sasl.kerberos.kinit.cmd * nan kinit -S \u201c%{sasl.kerberos.service.name}/%{broker.name}\u201d -k -t \u201c%{sasl.kerberos.keytab}\u201d %{sasl.kerberos.principal} \u5b8c\u6574\u7684 kerberos kinit \u547d\u4ee4\u4e32\uff0c%{config.prop.name} \u66ff\u6362\u4e3a\u4e0e\u914d\u7f6e\u5bf9\u8c61\u4e00\u76f4\u7684\u503c\uff0c%{broker.name} broker \u7684\u4e3b\u673a\u540d\u3002Type: string 54 sasl.kerberos.keytab * nan nan Kerberos keytab \u6587\u4ef6\u7684\u8def\u5f84\u3002\u5982\u679c\u4e0d\u8bbe\u7f6e\uff0c\u5219\u4f7f\u7528\u7cfb\u7edf\u9ed8\u8ba4\u7684\u3002\u63d0\u793a\uff1a\u4e0d\u4f1a\u81ea\u52a8\u4f7f\u7528\uff0c\u5fc5\u987b\u5728 sasl.kerberos.kinit.cmd \u4e2d\u6dfb\u52a0\u5230\u6a21\u677f\uff0c\u5982 ... -t %{sasl.kerberos.keytab}\u3002 Type: string 55 sasl.kerberos.realm * nan hadoop.com \u534e\u4e3a\u5927\u6570\u636e\u4ea7\u54c1\uff08FusionInsight HD\uff09\u4e2d\u7684\u96c6\u7fa4\u57df\u540d 56 sasl.kerberos.min.time.before.relogin * 1 .. 86400000 60000 Key \u6062\u590d\u5c1d\u8bd5\u7684\u6700\u5c0f\u65f6\u95f4\uff0c\u6beb\u79d2\u3002Type: integer 57 sasl.username * nan nan \u4f7f\u7528 PLAIN \u673a\u5236\u65f6\uff0cSASL \u7528\u6237\u540d\u3002Type: string 58 sasl.password * nan nan \u4f7f\u7528 PLAIN \u673a\u5236\u65f6\uff0cSASL \u5bc6\u7801\u3002Type: string 59 group.id * nan nan \u5ba2\u6237\u7aef\u5206\u7ec4\u5b57\u7b26\u4e32\u3002\u540c\u7ec4\u7684\u5ba2\u6237\u7aef\u4f7f\u7528\u76f8\u540c\u7684 group.id\u3002Type: string 60 partition.assignment.strategy * nan range,roundrobin partition \u5206\u914d\u7b56\u7565\uff0c\u5f53\u9009\u4e3e\u7ec4 leader \u65f6\uff0c\u5206\u914d partition \u7ed9\u7ec4\u6210\u5458\u7684\u7b56\u7565\u3002Type: string 61 session.timeout.ms * 1 .. 3600000 30000 \u5ba2\u6237\u7aef\u7ec4\u4f1a\u8bdd\u63a2\u6d4b\u5931\u8d25\u8d85\u5e02\u65f6\u95f4\u3002Type: integer 62 heartbeat.interval.ms * 1 .. 3600000 1000 \u7ec4\u4f1a\u8bdd\u4fdd\u6d3b\u5fc3\u8df3\u95f4\u9694\u3002Type: integer 63 group.protocol.type * nan consumer \u7ec4\u534f\u8bae\u7c7b\u578b\u3002Type: string 64 coordinator.query.interval.ms * 1 .. 3600000 600000 \u591a\u4e45\u67e5\u8be2\u4e00\u6b21\u5f53\u524d\u7684\u5ba2\u6237\u7aef\u7ec4\u534f\u8c03\u4eba\u3002\u5982\u679c\u5f53\u524d\u7684\u5206\u914d\u534f\u8c03\u4eba\u6302\u4e86\uff0c\u4e3a\u4e86\u66f4\u5feb\u7684\u6062\u590d\u534f\u8c03\u4eba\uff0c\u63a2\u6d4b\u65f6\u95f4\u95f4\u9694\u4f1a\u9664\u4ee5 10\u3002Type: integer 65 enable.auto.commit C true, false TRUE \u5728\u540e\u53f0\u5468\u671f\u6027\u7684\u81ea\u52a8\u63d0\u4ea4\u504f\u79fb\u91cf\u3002Type: boolean 66 auto.commit.interval.ms C 0 .. 86400000 5000 \u6d88\u8d39\u8005\u504f\u79fb\u91cf\u63d0\u4ea4\uff08\u5199\u5165\uff09\u5230\u5b58\u50a8\u7684\u9891\u7387\uff0c\u6beb\u79d2\u3002(0 = \u4e0d\u53ef\u7528) Type: integer 67 enable.auto.offset.store C true, false TRUE \u4e3a\u5e94\u7528\u7a0b\u5e8f\u81ea\u52a8\u4fdd\u5b58\u6700\u540e\u6d88\u606f\u7684\u504f\u79fb\u91cf\u3002Type: boolean 68 queued.min.messages C 1 .. 10000000 100000 \u6bcf\u4e00\u4e2a topic+partition\uff0c\u672c\u5730\u6d88\u8d39\u8005\u961f\u5217\u7684\u6700\u5c0f\u6d88\u606f\u6570\u3002Type: integer 69 queued.max.messages.kbytes C 1 .. 1000000000 1000000 \u6bcf\u4e00\u4e2a topic+partition\uff0c\u672c\u5730\u6d88\u8d39\u8005\u961f\u5217\u7684\u6700\u5927\u5927\u5c0f\uff0c\u5355\u4f4dkilobytes\u3002\u8be5\u503c\u5e94\u8be5\u5927\u4e8e fetch.message.max.bytes\u3002Type: integer 70 fetch.wait.max.ms C 0 .. 300000 100 \u4e3a\u5199\u6ee1fetch.min.bytes\uff0cbroker \u7684\u6700\u5927\u7b49\u5f85\u65f6\u95f4\u3002Type: integer 71 fetch.message.max.bytes C 1 .. 1000000000 1048576 \u6bcf\u4e00\u4e2a topic+partition \u521d\u59cb\u5316\u7684\u6700\u5927\u5927\u5c0f\uff08bytes\uff09\u7528\u4e8e\u4ece broker \u8bfb\u6d88\u606f\u3002\u5982\u679c\u5ba2\u6237\u7aef\u9047\u5230\u6d88\u606f\u5927\u4e8e\u8fd9\u4e2a\u503c\uff0c\u4f1a\u9010\u6b65\u6269\u5927\u76f4\u5230\u585e\u4e0b\u8fd9\u4e2a\u6d88\u606f\u3002Type: integer 72 max.partition.fetch.bytes C nan nan \u53c2\u8003 fetch.message.max.bytes 73 fetch.min.bytes C 1 .. 100000000 1 broker \u8bf7\u6c42\u7684\u6700\u5c0f\u6570\u636e\u5927\u5c0f\uff0c\u5355\u4f4dbytes\u3002\u5982\u679c\u8fbe\u5230 fetch.wait.max.ms \u65f6\u95f4\uff0c\u5219\u4e0d\u7ba1\u8fd9\u4e2a\u914d\u7f6e\uff0c\u5c06\u5df2\u6536\u5230\u7684\u6570\u636e\u53d1\u9001\u7ed9\u5ba2\u6237\u7aef\u3002Type: integer 74 fetch.error.backoff.ms C 0 .. 300000 500 \u5bf9\u4e8e topic+partition\uff0c\u5982\u679c\u63a5\u53d7\u9519\u8bef\uff0c\u4e0b\u4e00\u4e2a\u63a5\u53d7\u8bf7\u6c42\u95f4\u9694\u591a\u957f\u65f6\u95f4\u3002Type: integer 75 offset.store.method C none, file, broker broker \u504f\u79fb\u91cf\u5b58\u50a8\u65b9\u5f0f\uff1a\u2019file\u2019 - \u672c\u5730\u6587\u4ef6\u5b58\u50a8 (offset.store.path, et.al), \u2018broker\u2019 - \u5728 broker \u4e0a\u63d0\u4ea4\u5b58\u50a8 (\u8981\u6c42 Apache Kafka 0.8.2 \u6216\u4ee5\u540e\u7248\u672c)\u3002Type: enum value 76 consume_cb C nan nan \u6d88\u606f\u6d88\u8d39\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_consume_cb()) Type: pointer 77 rebalance_cb C nan nan \u6d88\u8d39\u8005\u7ec4\u91cd\u65b0\u5206\u914d\u540e\u8c03\u7528 (\u53c2\u8003 rd_kafka_conf_set_rebalance_cb()) Type: pointer 78 offset_commit_cb C nan nan \u504f\u79fb\u91cf\u63d0\u4ea4\u7ed3\u679c\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_offset_commit_cb()) Type: pointer 79 enable.partition.eof C true, false TRUE \u5f53\u6d88\u8d39\u8005\u5230\u8fbe\u5206\u533a\u7ed3\u5c3e\uff0c\u53d1\u9001 RD_KAFKA_RESP_ERR__PARTITION_EOF \u4e8b\u4ef6\u3002Type: boolean 80 queue.buffering.max.messages P 1 .. 10000000 100000 \u751f\u4ea7\u8005\u961f\u5217\u5141\u8bb8\u7684\u6700\u5927\u6d88\u606f\u6570\u3002Type: integer 81 queue.buffering.max.kbytes P 1 .. 2147483647 4000000 \u751f\u4ea7\u8005\u961f\u5217\u5141\u8bb8\u7684\u6700\u5927\u5927\u5c0f\uff0c\u5355\u4f4dkb\u3002Type: integer 82 queue.buffering.max.ms P 1 .. 900000 1000 \u751f\u4ea7\u8005\u961f\u5217\u7f13\u5b58\u6570\u636e\u7684\u6700\u5927\u65f6\u95f4\uff0c\u6beb\u79d2\u3002Type: integer 83 message.send.max.retries P 0 .. 10000000 2 \u6d88\u606f\u96c6\u53d1\u9001\u5931\u8d25\u91cd\u8bd5\u6b21\u6570\u3002\u63d0\u793a \u91cd\u8bd5\u4f1a\u5bfc\u81f4\u91cd\u6392\u3002Type: integer 84 retries P nan nan \u53c2\u8003 message.send.max.retries 85 retry.backoff.ms P 1 .. 300000 100 \u91cd\u8bd5\u6d88\u606f\u53d1\u9001\u524d\u7684\u8865\u507f\u65f6\u95f4\u3002Type: integer 86 compression.codec P none, gzip, snappy, lz4 none \u538b\u7f29\u6d88\u606f\u96c6\u4f7f\u7528\u7684\u538b\u7f29\u7f16\u89e3\u7801\u5668\u3002\u8fd9\u91cc\u914d\u7f6e\u7684\u662f\u6240\u6709 topic \u7684\u9ed8\u8ba4\u503c\uff0c\u53ef\u80fd\u4f1a\u88ab topic \u4e0a\u7684 compression.codec \u5c5e\u6027\u8986\u76d6\u3002Type: enum value 87 batch.num.messages P 1 .. 1000000 10000 \u4e00\u4e2a\u6d88\u606f\u96c6\u6700\u5927\u6253\u5305\u6d88\u606f\u6570\u91cf\u3002\u6574\u4e2a\u6d88\u606f\u96c6\u7684\u5927\u5c0f\u4ecd\u53d7\u9650\u4e8e message.max.bytes\u3002 Type: integer 88 delivery.report.only.error P true, false FALSE \u53ea\u5bf9\u5931\u8d25\u7684\u6d88\u606f\u63d0\u4f9b\u5206\u53d1\u62a5\u544a\u3002Type: boolean 89 dr_cb P nan nan \u5206\u53d1\u62a5\u544a\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_dr_cb()) Type: pointer 90 dr_msg_cb P nan nan \u5206\u53d1\u62a5\u544a\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_conf_set_dr_msg_cb()) Type: pointer","title":"\u5168\u5c40\u914d\u7f6e\u5c5e\u6027"},{"location":"Other/librdkafka/#topic","text":"\u5c5e\u6027 C/P \u8303\u56f4 \u9ed8\u8ba4\u503c \u63cf\u8ff0 1 request.required.acks P -1 .. 1000 1 \u8fd9\u4e2a\u5b57\u6bb5\u6807\u793a leader broker \u8981\u4ece ISR broker \u63a5\u6536\u591a\u5c11\u4e2a ack\uff0c\u7136\u540e\u624d\u786e\u8ba4\u53d1\u9001\u8bf7\u6c42\uff1a0=\u4e0d\u53d1\u9001\u4efb\u4f55 response/ack \u7ed9\u5ba2\u6237\u7aef, 1=\u53ea\u6709 leader broker \u9700\u8981 ack \u6d88\u606f, -1 or all=broker \u963b\u585e\u76f4\u5230\u6240\u6709\u7684\u540c\u6b65\u5907\u4efd(ISRs)\u6216in.sync.replicas\u8bbe\u7f6e\u7684\u5907\u4efd\u8fd4\u56de\u6d88\u606f\u63d0\u4ea4\u7684\u786e\u8ba4\u5e94\u7b54\u3002Type: integer 2 acks P nan nan \u53c2\u8003 request.required.acks 3 request.timeout.ms P 1 .. 900000 5000 \u751f\u4ea7\u8005\u8bf7\u6c42\u7b49\u5f85\u5e94\u7b54\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u6beb\u79d2\u3002\u8fd9\u4e2a\u503c\u4ec5\u5728 broker \u4e0a\u5f3a\u5236\u6267\u884c\u3002\u53c2\u8003 request.required.acks\uff0c\u4e0d\u80fd\u7b49\u4e8e 0. Type: integer 4 message.timeout.ms P 0 .. 900000 300000 \u672c\u5730\u6d88\u606f\u8d85\u65f6\u65f6\u95f4\u3002\u8fd9\u4e2a\u503c\u4ec5\u5728\u672c\u5730\u5f3a\u5236\u6267\u884c\uff0c\u9650\u5236\u751f\u4ea7\u7684\u6d88\u606f\u7b49\u5f85\u88ab\u6210\u529f\u53d1\u9001\u7684\u7b49\u5f85\u65f6\u95f4\uff0c0 \u662f\u4e0d\u9650\u5236\u3002Type: integer 5 produce.offset.report P true, false FALSE \u62a5\u544a\u751f\u4ea7\u6d88\u606f\u7684\u504f\u79fb\u91cf\u7ed9\u5e94\u7528\u7a0b\u5e8f\u3002\u5e94\u7528\u7a0b\u5e8f\u5fc5\u987b\u4f7f\u7528 dr_msg_cb\u4ece rd_kafka_message_t.offset \u4e2d\u83b7\u53d6\u504f\u79fb\u91cf\u3002Type: boolean 6 partitioner_cb P nan nan \u5206\u533a\u65b9\u6cd5\u56de\u8c03\u51fd\u6570 (\u53c2\u8003 rd_kafka_topic_conf_set_partitioner_cb()) Type: pointer 7 opaque * nan nan \u5e94\u7528\u7a0b\u5e8f\u4e0d\u53ef\u89c1 (\u53c2\u8003 rd_kafka_topic_conf_set_opaque()) Type: pointer 8 compression.codec P none, gzip, snappy, lz4, inherit inherit \u538b\u7f29\u6d88\u606f\u96c6\u7684\u538b\u7f29\u7f16\u89e3\u7801\u5668\u3002Type: enum value 9 auto.commit.enable C true, false TRUE \u5982\u679c\u662f true\uff0c\u5468\u671f\u6027\u7684\u63d0\u4ea4\u6700\u540e\u4e00\u4e2a\u6d88\u606f\u7684\u504f\u79fb\u91cf\u3002\u7528\u4e8e\u5f53\u7a0b\u5e8f\u91cd\u542f\u65f6\u629b\u5f03\u4e0d\u7528\u7684\u6d88\u606f\u3002\u5982\u679c\u662f false\uff0c\u5e94\u7528\u7a0b\u5e8f\u9700\u8981\u8c03\u7528 rd_kafka_offset_store() \u4fdd\u5b58\u504f\u79fb\u91cf (\u53ef\u9009). \u63d0\u793a \u8fd9\u4e2a\u5c5e\u6027\u65f6\u80fd\u7528\u4e8e\u7b80\u5355\u6d88\u8d39\u8005\uff0chigh-level KafkaConsumer \u4f1a\u88ab\u5168\u5c40\u7684 enable.auto.commit\u5c5e\u6027\u66ff\u4ee3\u3002\u63d0\u793a \u76ee\u524d\u6ca1\u6709\u6574\u5408 zookeeper\uff0c\u6839\u636e offset.store.method \u7684\u914d\u7f6e \u504f\u79fb\u91cf\u5c06\u5199\u5165 broker \u6216\u672c\u5730\u6587\u4ef6 file according to offset.store.method. Type: boolean 10 enable.auto.commit C nan nan \u53c2\u8003 auto.commit.enable 11 auto.commit.interval.ms C 10 .. 86400000 60000 \u6d88\u8d39\u8005\u504f\u79fb\u91cf\u63d0\u4ea4\uff08\u5199\u5165\uff09\u5230\u5b58\u50a8\u7684\u9891\u7387\uff0c\u6beb\u79d2\u3002Type: integer 12 auto.offset.reset C smallest, earliest, beginning, largest, latest, end, error largest \u5982\u679c\u504f\u79fb\u91cf\u5b58\u50a8\u8fd8\u6ca1\u6709\u521d\u59cb\u5316\u6216\u504f\u79fb\u91cf\u8d85\u8fc7\u8303\u56f4\u65f6\u7684\u5904\u7406\u65b9\u5f0f\uff1aAction to take when there is no initial offset in offset store or the desired offset is out of range: \u2018smallest\u2019,\u2019earliest\u2019 - \u81ea\u52a8\u91cd\u8bbe\u504f\u79fb\u91cf\u4e3a\u6700\u5c0f\u504f\u79fb\u91cf\uff0c\u2019largest\u2019,\u2019latest\u2019 - \u81ea\u52a8\u91cd\u8bbe\u504f\u79fb\u91cf\u4e3a\u6700\u5927\u504f\u79fb\u91cf\uff0c\u2019error\u2019 - \u901a\u8fc7\u6d88\u8d39\u6d88\u606f\u89e6\u53d1\u4e00\u4e2a\u9519\u8bef\uff0c\u8bf7\u68c0\u67e5 message->err\u3002 Type: enum value 13 offset.store.path C nan . \u5b58\u50a8\u504f\u79fb\u91cf\u7684\u672c\u5730\u6587\u4ef6\u8def\u5f84\u3002\u5982\u679c\u8def\u5f84\u662f\u4e2a\u76ee\u5f55\uff0c\u5728\u76ee\u5f55\u4e0b\u81ea\u52a8\u521b\u5efa\u57fa\u4e8e topic \u548c partition \u7684\u6587\u4ef6\u540d\u3002Type: string 14 offset.store.sync.interval.ms C -1 .. 86400000 -1 \u504f\u79fb\u91cf\u6587\u4ef6 fsync() \u7684\u95f4\u9694\uff0c\u6beb\u79d2\u3002-1 \u4e0d\u540c\u6b65\uff0c0 \u6bcf\u6b21\u5199\u5165\u540e\u7acb\u5373\u540c\u6b65\u3002Type: integer 15 offset.store.method C file, broker broker \u504f\u79fb\u91cf\u5b58\u50a8\u65b9\u5f0f\uff1a\u2019file\u2019 - \u672c\u5730\u6587\u4ef6\u5b58\u50a8 (offset.store.path, et.al), \u2018broker\u2019 - \u5728 broker \u4e0a\u63d0\u4ea4\u5b58\u50a8 (\u8981\u6c42 Apache Kafka 0.8.2 \u6216\u4ee5\u540e\u7248\u672c)\u3002Type: enum value 16 consume.callback.max.messages C 0 .. 1000000 0 \u4e00\u6b21 rd_kafka_consume_callback*() \u8c03\u914d\u7684\u6700\u5927\u6d88\u606f\u6570 (0 = \u65e0\u9650\u5236) Type: integer","title":"Topic\u914d\u7f6e\u5c5e\u6027"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/","text":"\u751f\u6001\u5bf9\u63a5\u5e38\u89c1\u95ee\u9898\u603b\u7ed3 \u00b6 \u9002\u7528\u573a\u666f \u00b6 \u751f\u6001\u5bf9\u63a5\u5e38\u89c1\u95ee\u9898\u603b\u7ed3 1.0 \u2194 FusionInsight HD 6.5 (HDFS) Kerberos\u8ba4\u8bc1\u76f8\u5173\u95ee\u9898 \u00b6 kinit: Cannot contact any KDC for realm 'HADOOP.COM' while getting initial credentials \u00b6 \u62a5\u9519\u539f\u56e0\uff1a \u96c6\u7fa4\u73af\u5883\u53cc\u5e73\u9762\u90e8\u7f72\uff0c\u4f7f\u7528\u5728\u7ba1\u7406\u5e73\u9762\u7f51\u6bb5\u7684\u8df3\u677f\u673a\u5c1d\u8bd5\u5bf9\u63a5\u96c6\u7fa4\u4f1a\u62a5\u7c7b\u4f3c\u95ee\u9898 \u89e3\u51b3\u529e\u6cd5\uff1a \u5c06\u8df3\u677f\u673a\u8f6c\u63a5\u5230\u4e1a\u52a1\u5e73\u9762\u7f51\u6bb5\u4e0a\u8fdb\u884c\u5bf9\u63a5 \u7ecf\u9a8c\u5206\u4eab\uff1a \u5bf9\u63a5\u4e00\u5b9a\u662f\u5728\u4e1a\u52a1\u5e73\u9762\u7f51\u6bb5\u4e0a\u540c\u96c6\u7fa4\u5404\u7ec4\u4ef6\u8fdb\u884c\u4ea4\u4e92\uff0c\u6d4b\u8bd5\u5f00\u59cb\u524d\u8bf7 \u52a1\u5fc5\u5148\u68c0\u67e5\u7f51\u7edc\u5e73\u9762\u72b6\u51b5 \uff0c\u518d\u8fdb\u884c\u5bf9\u63a5\u6d4b\u8bd5 Cannot locate KDC \u00b6 \u62a5\u9519\u7247\u6bb5\uff1a Caused by: KrbException: Cannot locate KDC at sun.security.krb5.Config.getKDCList(Config.java:1084) at sun.security.krb5.KdcComm.send(KdcComm.java:218) at sun.security.krb5.KdcComm.send(KdcComm.java:200) at sun.security.krb5.KrbAsReqBuilder.send(KrbAsReqBuilder.java:316) at sun.security.krb5.KrbAsReqBuilder.action(KrbAsReqBuilder.java:361) at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:776) ... 24 more Caused by: KrbException: Generic error (description in e-text) (60) - Unable to locate KDC for realm HADOOP.COM at sun.security.krb5.Config.getKDCFromDNS(Config.java:1181) at sun.security.krb5.Config.getKDCList(Config.java:1057) \u62a5\u9519\u539f\u56e0\uff1a \u8df3\u677f\u673akrb5.conf\u914d\u7f6e\u6587\u4ef6\u914d\u7f6e\u95ee\u9898\u5bfc\u81f4\u8df3\u677f\u673a\u627e\u4e0d\u5230\u96c6\u7fa4KDC\u670d\u52a1\u5668\u5730\u5740\uff0c\u65e0\u6cd5\u8fdb\u884c\u8ba4\u8bc1 \u89e3\u51b3\u529e\u6cd5\uff1a \u5728\u6b63\u786e\u7684\u914d\u7f6e\u8def\u5f84\u4e0a\u914d\u7f6e\u597dkrb5.conf\u6587\u4ef6 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8df3\u677f\u673a\u6839\u636e\u7cfb\u7edf\u4e0d\u540c\u5206\u4e3awindwos\uff0cLinux\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0bwindows\u4e3b\u673a\u4ece C:\\Windows\\krb5.ini \u6587\u4ef6\u83b7\u53d6KDC\u670d\u52a1\u5668\u4fe1\u606f\uff08krb5.ini\u6587\u4ef6\u5185\u5bb9\u548ckrb5.conf\u6587\u4ef6\u5185\u5bb9\u4e00\u81f4\uff0c\u53ea\u662f\u540e\u7f00\u540d\u4e0d\u540c\uff09\uff0c\u5bf9\u63a5\u6d4b\u8bd5\u524d \u52a1\u5fc5\u68c0\u67e5\u8be5\u6587\u4ef6\u5185\u5bb9 \u9ed8\u8ba4\u60c5\u51b5\u4e0blinux\u4e3b\u673a\u4ece /etc/krb5.conf \u6587\u4ef6\u83b7\u53d6KDC\u670d\u52a1\u5668\u4fe1\u606f\uff0c\u5bf9\u63a5\u6d4b\u8bd5\u524d \u52a1\u5fc5\u68c0\u67e5\u8be5\u6587\u4ef6\u5185\u5bb9 \u5bf9\u63a5\u8fc7\u7a0b\u5982\u679c\u914d\u7f6e\u4e86JVM\u542f\u52a8\u53c2\u6570 -Djava.security.krb5.conf \uff0c\u4e00\u822c\u4f1a\u4ee5\u6539\u542f\u52a8\u53c2\u6570\u914d\u7f6e\u7684krb5.conf\u5177\u4f53\u8def\u5f84\u4e3a\u51c6\uff0c\u5bf9\u63a5\u6d4b\u8bd5\u524d \u52a1\u5fc5\u68c0\u67e5\u8be5\u6587\u4ef6\u5185\u5bb9 Clock skew too great \u00b6 \u62a5\u9519\u7247\u6bb5 GSSException: No valid credentials provided (Mechanism level: Attempt to obtain new INITIATE credentials failed! (null)) . . . Caused by: javax.security.auth.login.LoginException: Clock skew too great GSSException: No valid credentials provided (Mechanism level: Clock skew too great (37) - PROCESS_TGS kinit: krb5_get_init_creds: time skew (343) larger than max (300) \u62a5\u9519\u539f\u56e0\uff1a \u8df3\u677f\u673a\u65f6\u95f4\u4e0e\u96c6\u7fa4\u65f6\u95f4\u76f8\u5dee\u592a\u591a\uff08FusionInsight \u9ed8\u8ba45\u5206\u949f\u4ee5\u5185\uff09 \u89e3\u51b3\u529e\u6cd5\uff1a \u4fee\u6539\u8df3\u677f\u673a\u65f6\u95f4\u4f7f\u4e0e\u96c6\u7fa4\u65f6\u95f4\u76f8\u5dee\u57285\u5206\u949f\u4ee5\u5185 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u62a5\u9519\u5173\u952e\u8bcd\u80fd\u5f88\u5feb\u5b9a\u4f4d\u51fa\u62a5\u9519\u539f\u56e0\uff0c\u4f46\u6709\u65f6\u5e76\u4e0d\u4f1a\u76f4\u63a5\u4ee5 Clock skew too great \u4e3a\u5173\u952e\u8bcd\u62a5\u9519\uff0c\u800c\u662f\u4ee5 GSSException: No valid credentials provided \u5176\u4ed6\u5f62\u5f0f\u62a5\u9519\uff0c\u5c31\u4f1a\u6bd4\u8f83\u5bb9\u6613\u5ffd\u7565\u68c0\u67e5\u65f6\u95f4\u3001\u65f6\u533a\u662f\u5426\u6b63\u786e\u3002\u5bf9\u63a5\u6d4b\u8bd5\u524d\u8bf7 \u52a1\u5fc5\u68c0\u67e5\u8df3\u677f\u673a\u4e0e\u96c6\u7fa4\u65f6\u95f4\u662f\u5426\u5728\u76f8\u5dee\u8303\u56f4\u4ee5\u5185\uff085\u5206\u949f\uff09 \u9519\u8bef\u6765\u6e90\uff1a FI\u4e09\u65b9AD\u670d\u52a1\u914d\u7f6e\u5b8c\u6210\u540e\u4f7f\u7528 hdfs dfs -ls \u547d\u4ee4\u68c0\u67e5\u7ed3\u679c\u62a5\u9519 GSS initiate failed \u6ca1\u6709\u5176\u4ed6\u62a5\u9519\u5173\u952e\u8bcd \u00b6 \u62a5\u9519\u7247\u6bb5 WARN ipc.Client (Client.java:run(676)) - Couldn't setup connection for rm@EXAMPLE.COM to /172.22.97.127:8020 org.apache.hadoop.ipc.RemoteException(javax.security.sasl.SaslException): GSS initiate failed at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375) at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:558) \u8be5\u62a5\u9519\u5173\u952e\u8bcd\u53ea\u662f\u8868\u660e\u8ba4\u8bc1\u5931\u8d25\u7684\u7ed3\u679c\uff0c\u5e76\u770b\u4e0d\u51fa\u6765\u4efb\u4f55\u8fde\u63a5\u5931\u8d25\u7684\u539f\u56e0\uff0c\u9700\u8981\u5728\u62a5\u9519\u65e5\u5fd7\u9644\u8fd1\u627e\u5230 Caused by \u7b49\u5173\u952e\u8bcd\u5b9a\u4f4d\u5230\u5177\u4f53\u8fde\u63a5\u5931\u8d25\u7684\u539f\u56e0\uff0c\u6216\u8005\u6253\u5f00JVM\u62a5\u9519\u65e5\u5fd7\u5f00\u5173 -Dsun.security.krb5.debug=false \uff0c\u7ed3\u5408\u96c6\u7fa4kdc\u670d\u52a1\u65e5\u5fd7 /var/log/Bigdata/kerberos/krb5kdc.log \u5206\u6790\u539f\u56e0 \u9519\u8bef\u6765\u6e90\uff1atensorflow\u5bf9\u63a5\u65f6\u6709\u8fc7\u6539\u62a5\u9519\uff0c\u6253\u5f00JVM\u62a5\u9519\u65e5\u5fd7\u5f00\u5173\u68c0\u67e5\u51fa\u7684\u62a5\u9519\u539f\u56e0 No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) \u00b6 \u62a5\u9519\u7247\u6bb5 javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] \u62a5\u9519\u539f\u56e0\uff1a \u7b80\u8a00\u4e4b\u8ba4\u8bc1\u5931\u8d25 \u53ef\u80fd\u6ca1\u4f7f\u7528 kinit \u9884\u5148\u8ba4\u8bc1 \u53ef\u80fd kinit \u9884\u5148\u8ba4\u8bc1\u540e\uff0c\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e\u8fc7\u671f \u53ef\u80fd\u4f7f\u7528\u7684user.keytab\u8ba4\u8bc1\u6587\u4ef6\u9519\u8bef\uff0c\u548c\u8ba4\u8bc1\u7528\u6237\u4e0d\u5339\u914d\uff0c\u6216\u8005\u8be5\u8ba4\u8bc1\u6587\u4ef6\u4e0d\u5b58\u5728 \u53ef\u80fd\u7f13\u5b58\u7684\u7968\u636e\u88ab\u5176\u4ed6\u7a0b\u5e8f\u66f4\u6539 \u53ef\u80fd\u8ba4\u8bc1\u7528\u6237\u7684\u57df\u540d\uff08Realm\uff09\u548c\u96c6\u7fa4\u7684\u57df\u540d\u4e0d\u4e00\u81f4 \u89e3\u51b3\u529e\u6cd5\uff1a \u786e\u8ba4\u8ba4\u8bc1\u7528\u6237\uff0c\u5bc6\u7801\uff0cuser.keytab\u7684\u6b63\u786e\u6027\uff0c\u786e\u4fdd\u8ba4\u8bc1\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u88ab\u5176\u4ed6\u7684\u4efb\u52a1\u5e72\u6270 \u7ecf\u9a8c\u5206\u4eab\uff1a \u5bf9\u4e8ekeytab\u6587\u4ef6\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\uff0c\u53ef\u4ee5\u5728\u8df3\u677f\u673a\u4e0a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u547d\u4ee4 kinit -kt \u8ba4\u8bc1keytab\u6587\u4ef6 \u8ba4\u8bc1principal \u7684\u65b9\u5f0f\u6765\u9a8c\u8bc1 No JAAS configuration section named 'Client' was found in specified JAAS configuration file \u00b6 \u62a5\u9519\u7247\u6bb5 WARN [2020-08-05 06:54:19,089] org.apache.zookeeper.ClientCnxn: SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/opt/jaas.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. \u62a5\u9519\u539f\u56e0\uff1a jaas.conf\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u7f3a\u5c11Client\u7247\u6bb5 \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5jaas.conf\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u6b63\u786e \u7ecf\u9a8c\u5206\u4eab\uff1a jaas.conf\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684Client\u7247\u6bb5\u662f\u8fde\u63a5Zookeeper\u7ec4\u4ef6\u7684\uff0cKafkaClient\u7247\u6bb5\u662f\u8fde\u63a5Kafka\u5b89\u5168\u6a21\u5f0f\u7684\uff0c\u8bf7\u6839\u636e\u62a5\u9519\u5185\u5bb9\u786e\u8ba4\u76f8\u5173\u7247\u6bb5\u7684\u5185\u5bb9\u6b63\u786e KeeperErrorCode = XXX \u00b6 \u62a5\u9519\u7247\u6bb51(hive). Caused by: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 configs from ZooKeeper at org.apache.hive.jdbc.ZooKeeperHiveClientHelper.configureConnParams(ZooKeeperHiveClientHelper.java:100) at org.apache.hive.jdbc.Utils.configureConnParams(Utils.java:514) at org.apache.hive.jdbc.Utils.parseURL(Utils.java:435) at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:141) ... 34 common frames omitted Caused by: org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hiveserver2 at org.apache.zookeeper.KeeperException.create(KeeperException.java:123) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) \u62a5\u9519\u7247\u6bb52(hbase). org.apache.hadoop.hbase.MasterNotRunningException: org.apache.hadoop.hbase.MasterNotRunningException: Can't get connection to ZooKeeper: KeeperErrorCode = Session expired for /hbase at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:157) at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:4212) at org.apache.hadoop.hbase.client.HBaseAdmin.listTableNames(HBaseAdmin.java:515) \u62a5\u9519\u7247\u6bb53(hetu). Caused by: java.sql.SQLException: KeeperErrorCode = ConnectionLoss for /hetuserver/hsbroker/registry at g5.qry.sql.impl.SQLConnBuilder.build0(SQLConnBuilder.java:69) ~ \u62a5\u9519\u7247\u6bb54(kafka). Causing: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /brokers/topics/hdfs_audit_log_sandbox/partitions at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[eagle-topology-0.5.0-assembly.jar:na] at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[eagle-topology-0.5.0-assembly.jar:na] at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[eagle-topology-0.5.0-assembly.jar:na] \u62a5\u9519\u539f\u56e0\uff1a \u5c1d\u8bd5\u8fde\u63a5Zookeeper\u7ec4\u4ef6\u5931\u8d25\uff0c\u53ef\u80fd\u53d1\u751f\u5728\u8fde\u63a5hive\uff0chbase\uff0chetu\uff0ckafka\u7684\u8fc7\u7a0b\u4e2d JVM\u542f\u52a8\u53c2\u6570\u672a\u914d\u7f6e\u6216\u8005\u5185\u5bb9\u6709\u95ee\u9898 JVM\u542f\u52a8\u53c2\u6570\u5e76\u672a\u5728\u4e09\u65b9\u8f6f\u4ef6\u542f\u52a8\u65f6\u6210\u529f\u5bfc\u5165 JVM\u53c2\u6570\u5bfc\u5165\u7684jaas.conf\u914d\u7f6e\u6587\u4ef6\u4e0d\u5b58\u5728 jaas.conf\u914d\u7f6e\u6587\u4ef6\u4e2d\u914d\u7f6e\u7684keytab\u6587\u4ef6\uff0cprincipal\u914d\u7f6e\u4e0d\u6b63\u786e \u4e09\u65b9\u8f6f\u4ef6\u5339\u914d\u7684zookeeper\u4f9d\u8d56jar\u5305\u7248\u672c\u8f83\u8001\uff0c\u9700\u66f4\u6362\u4e3a\u534e\u4e3a\u7684zookeeper\u4f9d\u8d56jar\u5305\uff0c\u6bd4\u5982 zookeeper-3.5.1.jar \uff08651\u7248\u672c\uff09 \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5JVM\u542f\u52a8\u53c2\u6570\u5185\u5bb9\uff0c\u6bd4\u5982\uff1a -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf \u68c0\u67e5\u4e0a\u8ff0\u914d\u7f6e\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6jaas.conf\uff0ckrb5.conf\u6587\u4ef6\u5185\u5bb9\u65e0\u8bef \u786e\u4fdd\u4e09\u65b9\u8f6f\u4ef6\u542f\u52a8\u65f6\u4e0a\u8ff0JVM\u53c2\u6570\u6210\u529f\u5bfc\u5165\uff0clinux\u53ef\u4f7f\u7528 ps -ef \u547d\u4ee4\u67e5\u770b\uff0cwindows\u53ef\u4f7f\u7528 visualvm \u5de5\u5177\u8fdb\u884c\u76d1\u63a7 \u786e\u4fdd\u5bf9\u63a5\u76f8\u5173zookeeper\u4f9d\u8d56jar\u5305\u66ff\u6362\u4e3a\u534e\u4e3a\u7684zookeeper\u4f9d\u8d56jar\u5305\uff0c\u6bd4\u5982 zookeeper-3.5.1.jar \uff08651\u7248\u672c\uff09 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u95ee\u9898\u4e3a\u5bf9\u63a5\u4e2d\u6700\u4e3a\u5e38\u89c1\u7684\u9519\u8bef\uff0c\u8bf7\u53c2\u8003\u89e3\u51b3\u529e\u6cd5\u7684\u8981\u70b9\uff0c\u9010\u6761\u5206\u6790 GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)] \u00b6 \u62a5\u9519\u7247\u6bb5 ERROR [AdminClient clientId=adminclient-1] Connection to node -2 failed authentication due to: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)]) occurred when evaluating SASL token received from the Kafka Broker. Kafka Client will go to AUTHENTICATION_FAILED state. (org.apache.kafka.clients.NetworkClient:607) [2019-06-26 18:05:06,410] WARN [Principal=developuser@HADOOP.COM]: TGT renewal thread has been interrupted and will exit. (org.apache.kafka.common.security.kerberos.KerberosLogin:192) [2019-06-26 18:05:06,412] ERROR Stopping due to error (org.apache.kafka.connect.cli.ConnectDistributed:112) org.apache.kafka.connect.errors.ConnectException: Failed to connect to and describe Kafka cluster. Check worker's broker connection and security properties. at org.apache.kafka.connect.util.ConnectUtils.lookupKafkaClusterId(ConnectUtils.java:64) at org.apache.kafka.connect.util.ConnectUtils.lookupKafkaClusterId(ConnectUtils.java:45) at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:77) \u62a5\u9519\u539f\u56e0\uff1aFusionInsight\u7684KDC\u670d\u52a1\u5668\u6ca1\u6709\u5bf9\u5e94\u7684\u670d\u52a1\u7aef\u7968\u636e\uff0c\u8be5\u9519\u8bef\u6700\u5e38\u53d1\u751f\u5728\u8fde\u63a5kafka\u5b89\u5168\u6a21\u5f0f\uff0821007\u7aef\u53e3\uff09\uff0c\u4ee5\u53ca\u4e00\u4e9bHTTP\u8bf7\u6c42\u4e2d\uff08SPENGO\uff09\uff0c\u4ee5\u53ca\u6709\u7684\u65f6\u5019\u8fde\u63a5zookeeper\u7684\u65f6\u5019 \u89e3\u51b3\u529e\u6cd5\uff1a \u5bf9\u63a5Kafka\u5b89\u5168\u6a21\u5f0f\u8bf7\u52a1\u5fc5\u66f4\u6362\u5bf9\u5e94\u7684kafka client\u4f9d\u8d56jar\u5305\u4e3a kafka-clients-1.1.0.jar \uff08651\u7248\u672c\uff09\uff0c\u4e0d\u8981\u4f7f\u7528\u5f00\u6e90\u7684\u8be5jar\u5305\uff0c\u534e\u4e3a\u5bf9\u8be5jar\u5305\u6709\u4fee\u6539 \u5982\u679c\u662fHTTP\u8bf7\u6c42\u6bd4\u5982SPENGO\u8ba4\u8bc1\uff0c\u53ef\u4ee5\u8003\u8651\u5728KDC\u670d\u52a1\u5668\u4e2d\u624b\u52a8\u6dfb\u52a0\u76f8\u5173\u7684\u670d\u52a1\u7968 \u5bf9\u63a5zookeeper\u7684\u662f\u6709jvm\u53c2\u6570\u9700\u8981\u5f15\u5165: -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u95ee\u9898\u4e3a\u5bf9\u63a5kafka\u5b89\u5168\u6a21\u5f0f\u4e2d\u6700\u5e38\u89c1\u7684\u95ee\u9898\uff0c\u5173\u952e\u70b9\u5c31\u5728\u4e8e\u9700\u8981\u66ff\u6362\u534e\u4e3a kafka-clients-1.1.0.jar \uff08651\u7248\u672c\uff09\u4f9d\u8d56\u5305\u3002\u5e76\u4e14\u5982\u679c\u9047\u5230\u96c6\u7fa4\u57df\u540d\u66f4\u6539\u7684\u60c5\u51b5\uff0c\u9700\u8981\u60f3\u529e\u6cd5\u5728\u4e09\u65b9\u8f6f\u4ef6\u4e2d\u628a\u57df\u540d\u53c2\u6570 kerberos.domain.name \u8f93\u5165\u8fdb\u53bb\uff0c\u6bd4\u5982 kerberos.domain.name=hadoop.hadoopss.com \u5b9a\u4f4d\u8be5\u95ee\u9898\u6700\u597d\u7684\u529e\u6cd5\u662f\u5728jvm\u53c2\u6570\u4e2d\u52a0\u5165kerberos debug\u65e5\u5fd7\u5f00\u542f\u53c2\u6570 -Dsun.security.krb5.debug=true \uff0c\u5e76\u4e14\u767b\u9646\u96c6\u7fa4KDC\u670d\u52a1\u5668\uff0c\u67e5\u770b\u76f8\u5173\u65e5\u5fd7 /var/log/Bigdata/kerberos/krb5kdc.log \uff0c\u627e\u51fa\u9519\u8bef\u539f\u56e0 ICMP Port Unreachable \u00b6 \u95ee\u9898\u539f\u56e0\uff1a \u4e09\u65b9\u8f6f\u4ef6\u6839\u636e\u5f00\u6e90\u7684\u9ed8\u8ba4\u89c4\u5219\uff0c\u4f7f\u7528udp 88\u7aef\u53e3\u8fdb\u884ckerberos\u8ba4\u8bc1\uff0c\u800cFusionInsight \u4f7f\u7528udp 21732\u7aef\u53e3\u8fdb\u884ckerberos\u8ba4\u8bc1 \u89e3\u51b3\u529e\u6cd5\uff1a\u4f7f\u7528udp\u7aef\u53e3\u66f4\u6539\u5de5\u5177\uff08 https://github.com/troglobit/uredir \uff09\uff0c \u5c06\u8be5\u5de5\u5177\u90e8\u7f72\u5728KDC\u670d\u52a1\u5668\u4e0a\uff0c\u6267\u884c\u547d\u4ee4 ./uredir IP:88 IP:21732 \u8fdb\u884c\u7aef\u53e3\u7ed1\u5b9a \u7ecf\u9a8c\u5206\u4eab\uff1a\u4e00\u822c\u800c\u8a00\u5f00\u6e90kerberos\u5bf9\u63a5\u4f7f\u7528\u7684\u662f88\u7aef\u53e3\uff0c\u53ef\u4ee5\u4f7f\u7528\u6293\u5305\u5de5\u5177\u8fdb\u884c\u68c0\u67e5\uff0c\u6bd4\u5982 windows\u53ef\u4f7f\u7528Microsoft Network Monitor\u5de5\u5177\u68c0\u67e5\uff0clinux\u53ef\u4f7f\u7528tcpdump\u5de5\u5177\u8fdb\u884c\u68c0\u67e5 javax.security.auth.login.LoginException: No password provided \u00b6 \u62a5\u9519\u7247\u6bb5 2015-12-15 17:16:23,517 - WARN [main:SaslServerCallbackHandler@105] - No password found for user: null 2015-12-15 17:16:23,536 - ERROR [main:ZooKeeperServerMain@63] - Unexpected exception, exiting abnormally java.io.IOException: Could not configure server because SASL configuration did not allow the ZooKeeper server to authenticate itself properly: javax.security.auth.login.LoginException: No password provided at org.apache.zookeeper.server.ServerCnxnFactory.configureSaslLogin(ServerCnxnFactory.java:207) at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:87) \u95ee\u9898\u539f\u56e0\uff1a keytab\u6587\u4ef6\u914d\u7f6e\u9519\u8bef keytab\u6587\u4ef6\u4f7f\u7528\u7684principal\u540c\u8ba4\u8bc1\u7528\u6237\u4e0d\u5339\u914d keytab\u6587\u4ef6\u5e76\u6ca1\u6709\u914d\u7f6e\u8fdb\u53bb\uff0c\u4e09\u65b9\u8f6f\u4ef6\u6ca1\u6709\u8bfb\u5230 \u89e3\u51b3\u529e\u6cd5\uff1a \u53c2\u8003\u95ee\u9898\u539f\u56e0\u68c0\u67e5\uff0c\u914d\u7f6e\u6b63\u786e javax.security.auth.login.LoginException: Unable to obtain password from user \u00b6 \u62a5\u9519\u7247\u6bb5 Exception in thread \"main\" java.io.IOException: Login failure for xxx@REALM from keytab /opt/xx.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:962) at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:564) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:154) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:856) at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719) \u95ee\u9898\u539f\u56e0\uff1a\u8ddf\u4e0a\u4e00\u4e2a\u95ee\u9898\u7c7b\u4f3c keytab\u6587\u4ef6\u914d\u7f6e\u9519\u8bef keytab\u6587\u4ef6\u4f7f\u7528\u7684principal\u540c\u8ba4\u8bc1\u7528\u6237\u4e0d\u5339\u914d keytab\u6587\u4ef6\u5e76\u6ca1\u6709\u914d\u7f6e\u8fdb\u53bb\uff0c\u4e09\u65b9\u8f6f\u4ef6\u6ca1\u6709\u8bfb\u5230 \u89e3\u51b3\u529e\u6cd5\uff1a \u53c2\u8003\u95ee\u9898\u539f\u56e0\u68c0\u67e5\uff0c\u914d\u7f6e\u6b63\u786e SIMPLE authentication is not enabled. Available:[TOKEN]\" \u00b6 \u95ee\u9898\u539f\u56e0\uff1a \u6539\u62a5\u9519\u4e00\u822c\u51fa\u73b0\u5728\u5ba2\u6237\u7aef\uff0c\u5ba2\u6237\u7aef\u5c1d\u8bd5\u4f7f\u7528\u666e\u901a\u6a21\u5f0f\uff08SIMPLE\uff09\u8fdb\u884c\u8ba4\u8bc1\uff0c\u4f46\u662f\u670d\u52a1\u7aef\u53ea\u652f\u6301Kerberos\u8ba4\u8bc1\uff08TOKEN\uff09 \u89e3\u51b3\u529e\u6cd5\uff1a \u5728\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u4e2d\u628a hadoop.security.authentication \u6539\u6210 kerberos \uff0c\u6216\u8005\u901a\u8fc7\u522b\u7684\u914d\u7f6e\u65b9\u5f0f\uff0c\u8ba9\u4e09\u65b9\u8f6f\u4ef6\u4f7f\u7528Kerberos\u8fdb\u884c\u8ba4\u8bc1 \u96c6\u7fa4\u914d\u7f6e\u76f8\u5173\u95ee\u9898 \u00b6 java.lang.ClassNotFoundException \u00b6 \u62a5\u9519\u7247\u6bb5 java.lang.IllegalArgumentException: Compression codec com.huawei.hadoop.datasight.io.compress.lzc.ZCodec not found. at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:139) at org.apache.hadoop.io.compress.CompressionCodecFactory.<init>(CompressionCodecFactory.java:180) at org.apache.nifi.processors.hadoop.GetHDFS.processBatchOfFiles(GetHDFS.java:353) \u95ee\u9898\u539f\u56e0\uff1a \u4e09\u65b9\u8f6f\u4ef6\u5bf9\u5e94\u7ec4\u4ef6\u4f9d\u8d56\u7684jar\u5305\u786e\u5b9e \u89e3\u51b3\u529e\u6cd5\uff1a \u5728\u534e\u4e3aFusionInsight\u5ba2\u6237\u7aef\u627e\u5230\u5bf9\u5e94\u7c7b\u7684jar\u5305\uff0c\u5728\u5c06\u4f9d\u8d56\u7684jar\u5305\u5f15\u5165 \u7ecf\u9a8c\u5206\u4eab\uff1a \u4e00\u822c\u901a\u8fc7\u547d\u4ee4 grep -R \"\u7f3a\u5931\u7684\u7c7b\u540d\" /opt/hadoopclient \u5728\u5ba2\u6237\u7aef\u4e2d\u67e5\u627e\u5bf9\u5e94jar\u5305\uff0c\u6bd4\u5982\u8be5\u7247\u6bb5\u7f3a\u5c11\u7684jar\u5305\u4e3a hadoop-plugins-1.0.jar \uff08651\u7248\u672c\uff09 java.lang.NoSuchMethodError \u00b6 \u62a5\u9519\u7247\u6bb5 java.lang.NoSuchMethodError: org.apache.kafka.common.acl.AclBindingFilter.<init>(Lorg/apache/kafka/common/resource/ResourcePatternFilter;Lorg/apache/kafka/common/acl/AccessControlEntryFilter;)V at org.apache.kafka.connect.mirror.MirrorSourceConnector.<clinit>(MirrorSourceConnector.java:67) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) \u95ee\u9898\u539f\u56e0\uff1a \u4e09\u65b9\u8f6f\u4ef6\u76f8\u5173\u7ec4\u4ef6\u7684\u4f9d\u8d56\u5e93\u540c\u534e\u4e3aFusionInsight\u5bf9\u5e94\u7ec4\u4ef6\u7684\u4f9d\u8d56\u5e93\u4e0d\u5339\u914d \u89e3\u51b3\u529e\u6cd5\uff1a \u5982\u679c\u9047\u5230\u7c7b\u4f3c\u62a5\u9519\uff0c\u8981\u5177\u4f53\u5b9a\u4f4d\u5230\u662f\u54ea\u4e9b\u4f9d\u8d56jar\u5305\u9519\u8bef\u662f\u6bd4\u8f83\u56f0\u96be\u7684\uff0c\u6ca1\u6709\u5177\u4f53\u7684\u89e3\u51b3\u529e\u6cd5\u3002\u5728\u5bf9\u63a5\u8fc7\u7a0b\u4e2d\u5c1d\u8bd5\u52a0\u5165\u81ea\u5b9a\u4e49\u7684\u4f9d\u8d56\u5e93\uff0c\u6216\u8005\u9009\u53d6\u548cFusionInsight\u73b0\u7f51\u7248\u672c\u76f8\u8fd1\u7684\u5f00\u6e90\u7248\u672c\u4f9d\u8d56\u5e93\u8fdb\u884c\u5bf9\u63a5 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u62a5\u9519\u539f\u56e0\u5c31\u662f\u4f9d\u8d56jar\u5305\u7684\u4e0d\u5339\u914d\uff0c\u9047\u5230\u8be5\u62a5\u9519\u662f\u5f88\u96be\u5b9a\u4f4d\u5230\u5177\u4f53\u4f9d\u8d56jar\u5305\u7684\uff0c\u9700\u8981\u6839\u636e\u89e3\u51b3\u529e\u6cd5\u7684\u601d\u8def\u5c1d\u8bd5\u4ece\u5176\u4ed6\u7684\u601d\u8def\u6765\u89e3\u51b3 java.net.UnknownHostException: hacluster \u00b6 \u62a5\u9519\u7247\u6bb5 java.lang.IllegalArgumentException: java.net.UnknownHostException: hacluster at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:444) at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:132) at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:355) \u95ee\u9898\u539f\u56e0: \u4e09\u65b9\u8f6f\u4ef6\u4e0d\u652f\u6301HDFS\u9ad8\u53ef\u7528\u6027\uff0c\u6216\u8005\u9ad8\u53ef\u7528\u6027\u6ca1\u6709\u914d\u7f6e\u6b63\u786e \u89e3\u51b3\u529e\u6cd5\uff1a - \u5982\u679c\u4e09\u65b9\u8f6f\u4ef6\u4e0d\u652f\u6301\u9ad8\u53ef\u7528\u6027\u914d\u7f6e\uff0c\u4fee\u6539\u5bfc\u5165\u7684hdfs\u914d\u7f6e\u6587\u4ef6core-site.xml\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u9879\u5982\u4e0b\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://\u4e3bnamenode\u8282\u70b9ip:21005</value> </property> - \u53c2\u8003\u4e09\u65b9\u8f6f\u4ef6\u6587\u6863\uff0c\u914d\u7f6e\u9ad8\u53ef\u7528\u6027 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u62a5\u9519\u539f\u56e0\u5c31\u662f\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u9879\u6709\u95ee\u9898\uff0c\u6bd4\u5982core-site.xml\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684fs.defaultFS\uff0c\u6709\u65f6\u5728\u5bf9\u63a5hbase\u7684\u65f6\u5019\u4e5f\u4f1a\u6709\u7c7b\u4f3c\u95ee\u9898\u51fa\u73b0\uff0c\u53ef\u4ee5\u5728hbase-site.xml\u914d\u7f6e\u6587\u4ef6\u4e2d\u67e5\u627e\u76f8\u5173\u914d\u7f6e\u9879\u8fdb\u884c\u66f4\u6539 Cannot modify XXX at runtime. It is not in list of params that are allowed to be modified at runtime \u00b6 \u95ee\u9898\u539f\u56e0: hive\u767d\u540d\u5355\u6ca1\u6709\u5f00\u901a\u8be5\u76f8\u5173\u53c2\u6570 \u89e3\u51b3\u529e\u6cd5\uff1a \u767b\u9646FusionInsight Manager -> \u96c6\u7fa4 -> Hive -> \u5168\u90e8\u914d\u7f6e -> hive.security.authorization.sqlstd.confwhitelist.append -> \u6dfb\u52a0\u62a5\u9519\u7f3a\u5931\u7684\u767d\u540d\u5355 \u4f8b\u5982\uff1a \u62a5\u9519\u63d0\u793a Cannot modify mapred.job.name at runtime. \u5bf9\u5e94\u7684\u5728 hive.security.authorization.sqlstd.confwhitelist.append \u914d\u7f6e\u9879\u7684\u672b\u5c3e\u6dfb\u52a0\u5185\u5bb9\uff1a |mapred\\.job\\.name \u8bf4\u660e\uff1ahive.security.authorization.sqlstd.confwhitelist.append\u8fd9\u4e2a\u53c2\u6570\u4e2d\u7684\u5185\u5bb9\u4ee5 | \u5206\u9694\uff0c\u586b\u5199\u7684\u5185\u5bb9\u9700\u8981\u7528\u8f6c\u4e49\u5b57\u7b26 \\ Permission denied \u00b6 \u62a5\u9519\u7247\u6bb5 HiveAccessControlException Permission denied: Principal [name=developuser, type=USER] does not have following privileges for operation CREATETABLE [[OBJECT OWNERSHIP] on Object [type=DFS_URI, name=hdfs://hacluster/user/hive/warehouse/sdc_drift_example03]] \u95ee\u9898\u539f\u56e0\uff1a \u8ba4\u8bc1\u7528\u6237\u5728FusionInsight\u4e0a\u6ca1\u6709\u5bf9\u5e94\u7684\u6743\u9650 \u89e3\u51b3\u529e\u6cd5\uff1a \u4ed4\u7ec6\u67e5\u770b\u62a5\u9519\uff0c\u660e\u786e\u54ea\u4e2a\u7528\u6237\uff0c\u5728\u4ec0\u4e48\u5bf9\u8c61\u4e0a\uff0c\u6ca1\u6709\u4ec0\u4e48\u6743\u9650\uff0c\u518d\u53bbFusionInsight Manager\u4e0a\u914d\u7f6e\u76f8\u5173\u6743\u9650\u3002 \u5177\u4f53\u53c2\u8003\u300a\u4ea7\u54c1\u6587\u6863\u300b -> \u300a\u5e94\u7528\u5f00\u53d1\u6307\u5357\u300b -> \u300a\u5b89\u5168\u6a21\u5f0f\u300b -> \u300a\u5b89\u5168\u8ba4\u8bc1\u300b -> \u300a\u51c6\u5907\u5f00\u53d1\u7528\u6237\u300b\u76f8\u5173\u7ae0\u8282 Failed to connect to driver at xxx:\u7aef\u53e3 retrying \u00b6 \u8bf4\u660e\uff1a \u8be5\u62a5\u9519\u51fa\u73b0\u7684\u573a\u666f\u4e3a\u540cspark2x\u7ec4\u4ef6\u5bf9\u63a5\u4e2d\u4efb\u52a1\u5931\u8d25\uff0c\u62a5\u9519\u51fa\u73b0\u7684\u4f4d\u7f6e\u5728FusionInsight Yarn\u4e0a\u67e5\u770b\u76f8\u5173\u5931\u8d25\u4efb\u52a1\u7684container\u65e5\u5fd7stout\u4e2d \u95ee\u9898\u539f\u56e0\uff1a \u4f7f\u7528yarn-client\u6a21\u5f0f\u63d0\u4ea4spark\u4efb\u52a1\uff0c\u96c6\u7fa4\u4e0d\u77e5\u9053driver\u7684\u4e3b\u673a\u540d \u89e3\u51b3\u529e\u6cd5\uff1a \u5728\u5bf9\u5e94\u96c6\u7fa4worker\u7684\u8282\u70b9\u4e0a\uff0c\u4fee\u6539 /etc/hosts \u914d\u7f6e\u6587\u4ef6\uff0c\u5c06\u63d0\u4ea4\u4efb\u52a1\u7684\u4e3b\u673a\u540d\u52a0\u8fdb\u53bb","title":"1.0 <--> 6.5"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#_1","text":"","title":"\u751f\u6001\u5bf9\u63a5\u5e38\u89c1\u95ee\u9898\u603b\u7ed3"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#_2","text":"\u751f\u6001\u5bf9\u63a5\u5e38\u89c1\u95ee\u9898\u603b\u7ed3 1.0 \u2194 FusionInsight HD 6.5 (HDFS)","title":"\u9002\u7528\u573a\u666f"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#kerberos","text":"","title":"Kerberos\u8ba4\u8bc1\u76f8\u5173\u95ee\u9898"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#kinit-cannot-contact-any-kdc-for-realm-hadoopcom-while-getting-initial-credentials","text":"\u62a5\u9519\u539f\u56e0\uff1a \u96c6\u7fa4\u73af\u5883\u53cc\u5e73\u9762\u90e8\u7f72\uff0c\u4f7f\u7528\u5728\u7ba1\u7406\u5e73\u9762\u7f51\u6bb5\u7684\u8df3\u677f\u673a\u5c1d\u8bd5\u5bf9\u63a5\u96c6\u7fa4\u4f1a\u62a5\u7c7b\u4f3c\u95ee\u9898 \u89e3\u51b3\u529e\u6cd5\uff1a \u5c06\u8df3\u677f\u673a\u8f6c\u63a5\u5230\u4e1a\u52a1\u5e73\u9762\u7f51\u6bb5\u4e0a\u8fdb\u884c\u5bf9\u63a5 \u7ecf\u9a8c\u5206\u4eab\uff1a \u5bf9\u63a5\u4e00\u5b9a\u662f\u5728\u4e1a\u52a1\u5e73\u9762\u7f51\u6bb5\u4e0a\u540c\u96c6\u7fa4\u5404\u7ec4\u4ef6\u8fdb\u884c\u4ea4\u4e92\uff0c\u6d4b\u8bd5\u5f00\u59cb\u524d\u8bf7 \u52a1\u5fc5\u5148\u68c0\u67e5\u7f51\u7edc\u5e73\u9762\u72b6\u51b5 \uff0c\u518d\u8fdb\u884c\u5bf9\u63a5\u6d4b\u8bd5","title":"kinit: Cannot contact any KDC for realm 'HADOOP.COM' while getting initial credentials"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#cannot-locate-kdc","text":"\u62a5\u9519\u7247\u6bb5\uff1a Caused by: KrbException: Cannot locate KDC at sun.security.krb5.Config.getKDCList(Config.java:1084) at sun.security.krb5.KdcComm.send(KdcComm.java:218) at sun.security.krb5.KdcComm.send(KdcComm.java:200) at sun.security.krb5.KrbAsReqBuilder.send(KrbAsReqBuilder.java:316) at sun.security.krb5.KrbAsReqBuilder.action(KrbAsReqBuilder.java:361) at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:776) ... 24 more Caused by: KrbException: Generic error (description in e-text) (60) - Unable to locate KDC for realm HADOOP.COM at sun.security.krb5.Config.getKDCFromDNS(Config.java:1181) at sun.security.krb5.Config.getKDCList(Config.java:1057) \u62a5\u9519\u539f\u56e0\uff1a \u8df3\u677f\u673akrb5.conf\u914d\u7f6e\u6587\u4ef6\u914d\u7f6e\u95ee\u9898\u5bfc\u81f4\u8df3\u677f\u673a\u627e\u4e0d\u5230\u96c6\u7fa4KDC\u670d\u52a1\u5668\u5730\u5740\uff0c\u65e0\u6cd5\u8fdb\u884c\u8ba4\u8bc1 \u89e3\u51b3\u529e\u6cd5\uff1a \u5728\u6b63\u786e\u7684\u914d\u7f6e\u8def\u5f84\u4e0a\u914d\u7f6e\u597dkrb5.conf\u6587\u4ef6 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8df3\u677f\u673a\u6839\u636e\u7cfb\u7edf\u4e0d\u540c\u5206\u4e3awindwos\uff0cLinux\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0bwindows\u4e3b\u673a\u4ece C:\\Windows\\krb5.ini \u6587\u4ef6\u83b7\u53d6KDC\u670d\u52a1\u5668\u4fe1\u606f\uff08krb5.ini\u6587\u4ef6\u5185\u5bb9\u548ckrb5.conf\u6587\u4ef6\u5185\u5bb9\u4e00\u81f4\uff0c\u53ea\u662f\u540e\u7f00\u540d\u4e0d\u540c\uff09\uff0c\u5bf9\u63a5\u6d4b\u8bd5\u524d \u52a1\u5fc5\u68c0\u67e5\u8be5\u6587\u4ef6\u5185\u5bb9 \u9ed8\u8ba4\u60c5\u51b5\u4e0blinux\u4e3b\u673a\u4ece /etc/krb5.conf \u6587\u4ef6\u83b7\u53d6KDC\u670d\u52a1\u5668\u4fe1\u606f\uff0c\u5bf9\u63a5\u6d4b\u8bd5\u524d \u52a1\u5fc5\u68c0\u67e5\u8be5\u6587\u4ef6\u5185\u5bb9 \u5bf9\u63a5\u8fc7\u7a0b\u5982\u679c\u914d\u7f6e\u4e86JVM\u542f\u52a8\u53c2\u6570 -Djava.security.krb5.conf \uff0c\u4e00\u822c\u4f1a\u4ee5\u6539\u542f\u52a8\u53c2\u6570\u914d\u7f6e\u7684krb5.conf\u5177\u4f53\u8def\u5f84\u4e3a\u51c6\uff0c\u5bf9\u63a5\u6d4b\u8bd5\u524d \u52a1\u5fc5\u68c0\u67e5\u8be5\u6587\u4ef6\u5185\u5bb9","title":"Cannot locate KDC"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#clock-skew-too-great","text":"\u62a5\u9519\u7247\u6bb5 GSSException: No valid credentials provided (Mechanism level: Attempt to obtain new INITIATE credentials failed! (null)) . . . Caused by: javax.security.auth.login.LoginException: Clock skew too great GSSException: No valid credentials provided (Mechanism level: Clock skew too great (37) - PROCESS_TGS kinit: krb5_get_init_creds: time skew (343) larger than max (300) \u62a5\u9519\u539f\u56e0\uff1a \u8df3\u677f\u673a\u65f6\u95f4\u4e0e\u96c6\u7fa4\u65f6\u95f4\u76f8\u5dee\u592a\u591a\uff08FusionInsight \u9ed8\u8ba45\u5206\u949f\u4ee5\u5185\uff09 \u89e3\u51b3\u529e\u6cd5\uff1a \u4fee\u6539\u8df3\u677f\u673a\u65f6\u95f4\u4f7f\u4e0e\u96c6\u7fa4\u65f6\u95f4\u76f8\u5dee\u57285\u5206\u949f\u4ee5\u5185 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u62a5\u9519\u5173\u952e\u8bcd\u80fd\u5f88\u5feb\u5b9a\u4f4d\u51fa\u62a5\u9519\u539f\u56e0\uff0c\u4f46\u6709\u65f6\u5e76\u4e0d\u4f1a\u76f4\u63a5\u4ee5 Clock skew too great \u4e3a\u5173\u952e\u8bcd\u62a5\u9519\uff0c\u800c\u662f\u4ee5 GSSException: No valid credentials provided \u5176\u4ed6\u5f62\u5f0f\u62a5\u9519\uff0c\u5c31\u4f1a\u6bd4\u8f83\u5bb9\u6613\u5ffd\u7565\u68c0\u67e5\u65f6\u95f4\u3001\u65f6\u533a\u662f\u5426\u6b63\u786e\u3002\u5bf9\u63a5\u6d4b\u8bd5\u524d\u8bf7 \u52a1\u5fc5\u68c0\u67e5\u8df3\u677f\u673a\u4e0e\u96c6\u7fa4\u65f6\u95f4\u662f\u5426\u5728\u76f8\u5dee\u8303\u56f4\u4ee5\u5185\uff085\u5206\u949f\uff09 \u9519\u8bef\u6765\u6e90\uff1a FI\u4e09\u65b9AD\u670d\u52a1\u914d\u7f6e\u5b8c\u6210\u540e\u4f7f\u7528 hdfs dfs -ls \u547d\u4ee4\u68c0\u67e5\u7ed3\u679c\u62a5\u9519","title":"Clock skew too great"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#gss-initiate-failed","text":"\u62a5\u9519\u7247\u6bb5 WARN ipc.Client (Client.java:run(676)) - Couldn't setup connection for rm@EXAMPLE.COM to /172.22.97.127:8020 org.apache.hadoop.ipc.RemoteException(javax.security.sasl.SaslException): GSS initiate failed at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375) at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:558) \u8be5\u62a5\u9519\u5173\u952e\u8bcd\u53ea\u662f\u8868\u660e\u8ba4\u8bc1\u5931\u8d25\u7684\u7ed3\u679c\uff0c\u5e76\u770b\u4e0d\u51fa\u6765\u4efb\u4f55\u8fde\u63a5\u5931\u8d25\u7684\u539f\u56e0\uff0c\u9700\u8981\u5728\u62a5\u9519\u65e5\u5fd7\u9644\u8fd1\u627e\u5230 Caused by \u7b49\u5173\u952e\u8bcd\u5b9a\u4f4d\u5230\u5177\u4f53\u8fde\u63a5\u5931\u8d25\u7684\u539f\u56e0\uff0c\u6216\u8005\u6253\u5f00JVM\u62a5\u9519\u65e5\u5fd7\u5f00\u5173 -Dsun.security.krb5.debug=false \uff0c\u7ed3\u5408\u96c6\u7fa4kdc\u670d\u52a1\u65e5\u5fd7 /var/log/Bigdata/kerberos/krb5kdc.log \u5206\u6790\u539f\u56e0 \u9519\u8bef\u6765\u6e90\uff1atensorflow\u5bf9\u63a5\u65f6\u6709\u8fc7\u6539\u62a5\u9519\uff0c\u6253\u5f00JVM\u62a5\u9519\u65e5\u5fd7\u5f00\u5173\u68c0\u67e5\u51fa\u7684\u62a5\u9519\u539f\u56e0","title":"GSS initiate failed \u6ca1\u6709\u5176\u4ed6\u62a5\u9519\u5173\u952e\u8bcd"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#no-valid-credentials-provided-mechanism-level-failed-to-find-any-kerberos-tgt","text":"\u62a5\u9519\u7247\u6bb5 javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] \u62a5\u9519\u539f\u56e0\uff1a \u7b80\u8a00\u4e4b\u8ba4\u8bc1\u5931\u8d25 \u53ef\u80fd\u6ca1\u4f7f\u7528 kinit \u9884\u5148\u8ba4\u8bc1 \u53ef\u80fd kinit \u9884\u5148\u8ba4\u8bc1\u540e\uff0c\u7f13\u5b58\u7684\u8ba4\u8bc1\u7968\u636e\u8fc7\u671f \u53ef\u80fd\u4f7f\u7528\u7684user.keytab\u8ba4\u8bc1\u6587\u4ef6\u9519\u8bef\uff0c\u548c\u8ba4\u8bc1\u7528\u6237\u4e0d\u5339\u914d\uff0c\u6216\u8005\u8be5\u8ba4\u8bc1\u6587\u4ef6\u4e0d\u5b58\u5728 \u53ef\u80fd\u7f13\u5b58\u7684\u7968\u636e\u88ab\u5176\u4ed6\u7a0b\u5e8f\u66f4\u6539 \u53ef\u80fd\u8ba4\u8bc1\u7528\u6237\u7684\u57df\u540d\uff08Realm\uff09\u548c\u96c6\u7fa4\u7684\u57df\u540d\u4e0d\u4e00\u81f4 \u89e3\u51b3\u529e\u6cd5\uff1a \u786e\u8ba4\u8ba4\u8bc1\u7528\u6237\uff0c\u5bc6\u7801\uff0cuser.keytab\u7684\u6b63\u786e\u6027\uff0c\u786e\u4fdd\u8ba4\u8bc1\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u88ab\u5176\u4ed6\u7684\u4efb\u52a1\u5e72\u6270 \u7ecf\u9a8c\u5206\u4eab\uff1a \u5bf9\u4e8ekeytab\u6587\u4ef6\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\uff0c\u53ef\u4ee5\u5728\u8df3\u677f\u673a\u4e0a\u5b89\u88c5\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u547d\u4ee4 kinit -kt \u8ba4\u8bc1keytab\u6587\u4ef6 \u8ba4\u8bc1principal \u7684\u65b9\u5f0f\u6765\u9a8c\u8bc1","title":"No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#no-jaas-configuration-section-named-client-was-found-in-specified-jaas-configuration-file","text":"\u62a5\u9519\u7247\u6bb5 WARN [2020-08-05 06:54:19,089] org.apache.zookeeper.ClientCnxn: SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/opt/jaas.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it. \u62a5\u9519\u539f\u56e0\uff1a jaas.conf\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u7f3a\u5c11Client\u7247\u6bb5 \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5jaas.conf\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u6b63\u786e \u7ecf\u9a8c\u5206\u4eab\uff1a jaas.conf\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684Client\u7247\u6bb5\u662f\u8fde\u63a5Zookeeper\u7ec4\u4ef6\u7684\uff0cKafkaClient\u7247\u6bb5\u662f\u8fde\u63a5Kafka\u5b89\u5168\u6a21\u5f0f\u7684\uff0c\u8bf7\u6839\u636e\u62a5\u9519\u5185\u5bb9\u786e\u8ba4\u76f8\u5173\u7247\u6bb5\u7684\u5185\u5bb9\u6b63\u786e","title":"No JAAS configuration section named 'Client' was found in specified JAAS configuration file"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#keepererrorcode-xxx","text":"\u62a5\u9519\u7247\u6bb51(hive). Caused by: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 configs from ZooKeeper at org.apache.hive.jdbc.ZooKeeperHiveClientHelper.configureConnParams(ZooKeeperHiveClientHelper.java:100) at org.apache.hive.jdbc.Utils.configureConnParams(Utils.java:514) at org.apache.hive.jdbc.Utils.parseURL(Utils.java:435) at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:141) ... 34 common frames omitted Caused by: org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hiveserver2 at org.apache.zookeeper.KeeperException.create(KeeperException.java:123) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) \u62a5\u9519\u7247\u6bb52(hbase). org.apache.hadoop.hbase.MasterNotRunningException: org.apache.hadoop.hbase.MasterNotRunningException: Can't get connection to ZooKeeper: KeeperErrorCode = Session expired for /hbase at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:157) at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:4212) at org.apache.hadoop.hbase.client.HBaseAdmin.listTableNames(HBaseAdmin.java:515) \u62a5\u9519\u7247\u6bb53(hetu). Caused by: java.sql.SQLException: KeeperErrorCode = ConnectionLoss for /hetuserver/hsbroker/registry at g5.qry.sql.impl.SQLConnBuilder.build0(SQLConnBuilder.java:69) ~ \u62a5\u9519\u7247\u6bb54(kafka). Causing: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /brokers/topics/hdfs_audit_log_sandbox/partitions at org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[eagle-topology-0.5.0-assembly.jar:na] at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[eagle-topology-0.5.0-assembly.jar:na] at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[eagle-topology-0.5.0-assembly.jar:na] \u62a5\u9519\u539f\u56e0\uff1a \u5c1d\u8bd5\u8fde\u63a5Zookeeper\u7ec4\u4ef6\u5931\u8d25\uff0c\u53ef\u80fd\u53d1\u751f\u5728\u8fde\u63a5hive\uff0chbase\uff0chetu\uff0ckafka\u7684\u8fc7\u7a0b\u4e2d JVM\u542f\u52a8\u53c2\u6570\u672a\u914d\u7f6e\u6216\u8005\u5185\u5bb9\u6709\u95ee\u9898 JVM\u542f\u52a8\u53c2\u6570\u5e76\u672a\u5728\u4e09\u65b9\u8f6f\u4ef6\u542f\u52a8\u65f6\u6210\u529f\u5bfc\u5165 JVM\u53c2\u6570\u5bfc\u5165\u7684jaas.conf\u914d\u7f6e\u6587\u4ef6\u4e0d\u5b58\u5728 jaas.conf\u914d\u7f6e\u6587\u4ef6\u4e2d\u914d\u7f6e\u7684keytab\u6587\u4ef6\uff0cprincipal\u914d\u7f6e\u4e0d\u6b63\u786e \u4e09\u65b9\u8f6f\u4ef6\u5339\u914d\u7684zookeeper\u4f9d\u8d56jar\u5305\u7248\u672c\u8f83\u8001\uff0c\u9700\u66f4\u6362\u4e3a\u534e\u4e3a\u7684zookeeper\u4f9d\u8d56jar\u5305\uff0c\u6bd4\u5982 zookeeper-3.5.1.jar \uff08651\u7248\u672c\uff09 \u89e3\u51b3\u529e\u6cd5\uff1a \u68c0\u67e5JVM\u542f\u52a8\u53c2\u6570\u5185\u5bb9\uff0c\u6bd4\u5982\uff1a -Djava.security.auth.login.config=/opt/jaas.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf \u68c0\u67e5\u4e0a\u8ff0\u914d\u7f6e\u76f8\u5173\u7684\u914d\u7f6e\u6587\u4ef6jaas.conf\uff0ckrb5.conf\u6587\u4ef6\u5185\u5bb9\u65e0\u8bef \u786e\u4fdd\u4e09\u65b9\u8f6f\u4ef6\u542f\u52a8\u65f6\u4e0a\u8ff0JVM\u53c2\u6570\u6210\u529f\u5bfc\u5165\uff0clinux\u53ef\u4f7f\u7528 ps -ef \u547d\u4ee4\u67e5\u770b\uff0cwindows\u53ef\u4f7f\u7528 visualvm \u5de5\u5177\u8fdb\u884c\u76d1\u63a7 \u786e\u4fdd\u5bf9\u63a5\u76f8\u5173zookeeper\u4f9d\u8d56jar\u5305\u66ff\u6362\u4e3a\u534e\u4e3a\u7684zookeeper\u4f9d\u8d56jar\u5305\uff0c\u6bd4\u5982 zookeeper-3.5.1.jar \uff08651\u7248\u672c\uff09 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u95ee\u9898\u4e3a\u5bf9\u63a5\u4e2d\u6700\u4e3a\u5e38\u89c1\u7684\u9519\u8bef\uff0c\u8bf7\u53c2\u8003\u89e3\u51b3\u529e\u6cd5\u7684\u8981\u70b9\uff0c\u9010\u6761\u5206\u6790","title":"KeeperErrorCode = XXX"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#gssexception-no-valid-credentials-provided-mechanism-level-server-not-found-in-kerberos-database-7-looking_up_server","text":"\u62a5\u9519\u7247\u6bb5 ERROR [AdminClient clientId=adminclient-1] Connection to node -2 failed authentication due to: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)]) occurred when evaluating SASL token received from the Kafka Broker. Kafka Client will go to AUTHENTICATION_FAILED state. (org.apache.kafka.clients.NetworkClient:607) [2019-06-26 18:05:06,410] WARN [Principal=developuser@HADOOP.COM]: TGT renewal thread has been interrupted and will exit. (org.apache.kafka.common.security.kerberos.KerberosLogin:192) [2019-06-26 18:05:06,412] ERROR Stopping due to error (org.apache.kafka.connect.cli.ConnectDistributed:112) org.apache.kafka.connect.errors.ConnectException: Failed to connect to and describe Kafka cluster. Check worker's broker connection and security properties. at org.apache.kafka.connect.util.ConnectUtils.lookupKafkaClusterId(ConnectUtils.java:64) at org.apache.kafka.connect.util.ConnectUtils.lookupKafkaClusterId(ConnectUtils.java:45) at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:77) \u62a5\u9519\u539f\u56e0\uff1aFusionInsight\u7684KDC\u670d\u52a1\u5668\u6ca1\u6709\u5bf9\u5e94\u7684\u670d\u52a1\u7aef\u7968\u636e\uff0c\u8be5\u9519\u8bef\u6700\u5e38\u53d1\u751f\u5728\u8fde\u63a5kafka\u5b89\u5168\u6a21\u5f0f\uff0821007\u7aef\u53e3\uff09\uff0c\u4ee5\u53ca\u4e00\u4e9bHTTP\u8bf7\u6c42\u4e2d\uff08SPENGO\uff09\uff0c\u4ee5\u53ca\u6709\u7684\u65f6\u5019\u8fde\u63a5zookeeper\u7684\u65f6\u5019 \u89e3\u51b3\u529e\u6cd5\uff1a \u5bf9\u63a5Kafka\u5b89\u5168\u6a21\u5f0f\u8bf7\u52a1\u5fc5\u66f4\u6362\u5bf9\u5e94\u7684kafka client\u4f9d\u8d56jar\u5305\u4e3a kafka-clients-1.1.0.jar \uff08651\u7248\u672c\uff09\uff0c\u4e0d\u8981\u4f7f\u7528\u5f00\u6e90\u7684\u8be5jar\u5305\uff0c\u534e\u4e3a\u5bf9\u8be5jar\u5305\u6709\u4fee\u6539 \u5982\u679c\u662fHTTP\u8bf7\u6c42\u6bd4\u5982SPENGO\u8ba4\u8bc1\uff0c\u53ef\u4ee5\u8003\u8651\u5728KDC\u670d\u52a1\u5668\u4e2d\u624b\u52a8\u6dfb\u52a0\u76f8\u5173\u7684\u670d\u52a1\u7968 \u5bf9\u63a5zookeeper\u7684\u662f\u6709jvm\u53c2\u6570\u9700\u8981\u5f15\u5165: -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u95ee\u9898\u4e3a\u5bf9\u63a5kafka\u5b89\u5168\u6a21\u5f0f\u4e2d\u6700\u5e38\u89c1\u7684\u95ee\u9898\uff0c\u5173\u952e\u70b9\u5c31\u5728\u4e8e\u9700\u8981\u66ff\u6362\u534e\u4e3a kafka-clients-1.1.0.jar \uff08651\u7248\u672c\uff09\u4f9d\u8d56\u5305\u3002\u5e76\u4e14\u5982\u679c\u9047\u5230\u96c6\u7fa4\u57df\u540d\u66f4\u6539\u7684\u60c5\u51b5\uff0c\u9700\u8981\u60f3\u529e\u6cd5\u5728\u4e09\u65b9\u8f6f\u4ef6\u4e2d\u628a\u57df\u540d\u53c2\u6570 kerberos.domain.name \u8f93\u5165\u8fdb\u53bb\uff0c\u6bd4\u5982 kerberos.domain.name=hadoop.hadoopss.com \u5b9a\u4f4d\u8be5\u95ee\u9898\u6700\u597d\u7684\u529e\u6cd5\u662f\u5728jvm\u53c2\u6570\u4e2d\u52a0\u5165kerberos debug\u65e5\u5fd7\u5f00\u542f\u53c2\u6570 -Dsun.security.krb5.debug=true \uff0c\u5e76\u4e14\u767b\u9646\u96c6\u7fa4KDC\u670d\u52a1\u5668\uff0c\u67e5\u770b\u76f8\u5173\u65e5\u5fd7 /var/log/Bigdata/kerberos/krb5kdc.log \uff0c\u627e\u51fa\u9519\u8bef\u539f\u56e0","title":"GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)]"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#icmp-port-unreachable","text":"\u95ee\u9898\u539f\u56e0\uff1a \u4e09\u65b9\u8f6f\u4ef6\u6839\u636e\u5f00\u6e90\u7684\u9ed8\u8ba4\u89c4\u5219\uff0c\u4f7f\u7528udp 88\u7aef\u53e3\u8fdb\u884ckerberos\u8ba4\u8bc1\uff0c\u800cFusionInsight \u4f7f\u7528udp 21732\u7aef\u53e3\u8fdb\u884ckerberos\u8ba4\u8bc1 \u89e3\u51b3\u529e\u6cd5\uff1a\u4f7f\u7528udp\u7aef\u53e3\u66f4\u6539\u5de5\u5177\uff08 https://github.com/troglobit/uredir \uff09\uff0c \u5c06\u8be5\u5de5\u5177\u90e8\u7f72\u5728KDC\u670d\u52a1\u5668\u4e0a\uff0c\u6267\u884c\u547d\u4ee4 ./uredir IP:88 IP:21732 \u8fdb\u884c\u7aef\u53e3\u7ed1\u5b9a \u7ecf\u9a8c\u5206\u4eab\uff1a\u4e00\u822c\u800c\u8a00\u5f00\u6e90kerberos\u5bf9\u63a5\u4f7f\u7528\u7684\u662f88\u7aef\u53e3\uff0c\u53ef\u4ee5\u4f7f\u7528\u6293\u5305\u5de5\u5177\u8fdb\u884c\u68c0\u67e5\uff0c\u6bd4\u5982 windows\u53ef\u4f7f\u7528Microsoft Network Monitor\u5de5\u5177\u68c0\u67e5\uff0clinux\u53ef\u4f7f\u7528tcpdump\u5de5\u5177\u8fdb\u884c\u68c0\u67e5","title":"ICMP Port Unreachable"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#javaxsecurityauthloginloginexception-no-password-provided","text":"\u62a5\u9519\u7247\u6bb5 2015-12-15 17:16:23,517 - WARN [main:SaslServerCallbackHandler@105] - No password found for user: null 2015-12-15 17:16:23,536 - ERROR [main:ZooKeeperServerMain@63] - Unexpected exception, exiting abnormally java.io.IOException: Could not configure server because SASL configuration did not allow the ZooKeeper server to authenticate itself properly: javax.security.auth.login.LoginException: No password provided at org.apache.zookeeper.server.ServerCnxnFactory.configureSaslLogin(ServerCnxnFactory.java:207) at org.apache.zookeeper.server.NIOServerCnxnFactory.configure(NIOServerCnxnFactory.java:87) \u95ee\u9898\u539f\u56e0\uff1a keytab\u6587\u4ef6\u914d\u7f6e\u9519\u8bef keytab\u6587\u4ef6\u4f7f\u7528\u7684principal\u540c\u8ba4\u8bc1\u7528\u6237\u4e0d\u5339\u914d keytab\u6587\u4ef6\u5e76\u6ca1\u6709\u914d\u7f6e\u8fdb\u53bb\uff0c\u4e09\u65b9\u8f6f\u4ef6\u6ca1\u6709\u8bfb\u5230 \u89e3\u51b3\u529e\u6cd5\uff1a \u53c2\u8003\u95ee\u9898\u539f\u56e0\u68c0\u67e5\uff0c\u914d\u7f6e\u6b63\u786e","title":"javax.security.auth.login.LoginException: No password provided"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#javaxsecurityauthloginloginexception-unable-to-obtain-password-from-user","text":"\u62a5\u9519\u7247\u6bb5 Exception in thread \"main\" java.io.IOException: Login failure for xxx@REALM from keytab /opt/xx.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:962) at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:564) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:154) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by: javax.security.auth.login.LoginException: Unable to obtain password from user at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:856) at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719) \u95ee\u9898\u539f\u56e0\uff1a\u8ddf\u4e0a\u4e00\u4e2a\u95ee\u9898\u7c7b\u4f3c keytab\u6587\u4ef6\u914d\u7f6e\u9519\u8bef keytab\u6587\u4ef6\u4f7f\u7528\u7684principal\u540c\u8ba4\u8bc1\u7528\u6237\u4e0d\u5339\u914d keytab\u6587\u4ef6\u5e76\u6ca1\u6709\u914d\u7f6e\u8fdb\u53bb\uff0c\u4e09\u65b9\u8f6f\u4ef6\u6ca1\u6709\u8bfb\u5230 \u89e3\u51b3\u529e\u6cd5\uff1a \u53c2\u8003\u95ee\u9898\u539f\u56e0\u68c0\u67e5\uff0c\u914d\u7f6e\u6b63\u786e","title":"javax.security.auth.login.LoginException: Unable to obtain password from user"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#simple-authentication-is-not-enabled-availabletoken","text":"\u95ee\u9898\u539f\u56e0\uff1a \u6539\u62a5\u9519\u4e00\u822c\u51fa\u73b0\u5728\u5ba2\u6237\u7aef\uff0c\u5ba2\u6237\u7aef\u5c1d\u8bd5\u4f7f\u7528\u666e\u901a\u6a21\u5f0f\uff08SIMPLE\uff09\u8fdb\u884c\u8ba4\u8bc1\uff0c\u4f46\u662f\u670d\u52a1\u7aef\u53ea\u652f\u6301Kerberos\u8ba4\u8bc1\uff08TOKEN\uff09 \u89e3\u51b3\u529e\u6cd5\uff1a \u5728\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u4e2d\u628a hadoop.security.authentication \u6539\u6210 kerberos \uff0c\u6216\u8005\u901a\u8fc7\u522b\u7684\u914d\u7f6e\u65b9\u5f0f\uff0c\u8ba9\u4e09\u65b9\u8f6f\u4ef6\u4f7f\u7528Kerberos\u8fdb\u884c\u8ba4\u8bc1","title":"SIMPLE authentication is not enabled. Available:[TOKEN]\""},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#_3","text":"","title":"\u96c6\u7fa4\u914d\u7f6e\u76f8\u5173\u95ee\u9898"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#javalangclassnotfoundexception","text":"\u62a5\u9519\u7247\u6bb5 java.lang.IllegalArgumentException: Compression codec com.huawei.hadoop.datasight.io.compress.lzc.ZCodec not found. at org.apache.hadoop.io.compress.CompressionCodecFactory.getCodecClasses(CompressionCodecFactory.java:139) at org.apache.hadoop.io.compress.CompressionCodecFactory.<init>(CompressionCodecFactory.java:180) at org.apache.nifi.processors.hadoop.GetHDFS.processBatchOfFiles(GetHDFS.java:353) \u95ee\u9898\u539f\u56e0\uff1a \u4e09\u65b9\u8f6f\u4ef6\u5bf9\u5e94\u7ec4\u4ef6\u4f9d\u8d56\u7684jar\u5305\u786e\u5b9e \u89e3\u51b3\u529e\u6cd5\uff1a \u5728\u534e\u4e3aFusionInsight\u5ba2\u6237\u7aef\u627e\u5230\u5bf9\u5e94\u7c7b\u7684jar\u5305\uff0c\u5728\u5c06\u4f9d\u8d56\u7684jar\u5305\u5f15\u5165 \u7ecf\u9a8c\u5206\u4eab\uff1a \u4e00\u822c\u901a\u8fc7\u547d\u4ee4 grep -R \"\u7f3a\u5931\u7684\u7c7b\u540d\" /opt/hadoopclient \u5728\u5ba2\u6237\u7aef\u4e2d\u67e5\u627e\u5bf9\u5e94jar\u5305\uff0c\u6bd4\u5982\u8be5\u7247\u6bb5\u7f3a\u5c11\u7684jar\u5305\u4e3a hadoop-plugins-1.0.jar \uff08651\u7248\u672c\uff09","title":"java.lang.ClassNotFoundException"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#javalangnosuchmethoderror","text":"\u62a5\u9519\u7247\u6bb5 java.lang.NoSuchMethodError: org.apache.kafka.common.acl.AclBindingFilter.<init>(Lorg/apache/kafka/common/resource/ResourcePatternFilter;Lorg/apache/kafka/common/acl/AccessControlEntryFilter;)V at org.apache.kafka.connect.mirror.MirrorSourceConnector.<clinit>(MirrorSourceConnector.java:67) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) \u95ee\u9898\u539f\u56e0\uff1a \u4e09\u65b9\u8f6f\u4ef6\u76f8\u5173\u7ec4\u4ef6\u7684\u4f9d\u8d56\u5e93\u540c\u534e\u4e3aFusionInsight\u5bf9\u5e94\u7ec4\u4ef6\u7684\u4f9d\u8d56\u5e93\u4e0d\u5339\u914d \u89e3\u51b3\u529e\u6cd5\uff1a \u5982\u679c\u9047\u5230\u7c7b\u4f3c\u62a5\u9519\uff0c\u8981\u5177\u4f53\u5b9a\u4f4d\u5230\u662f\u54ea\u4e9b\u4f9d\u8d56jar\u5305\u9519\u8bef\u662f\u6bd4\u8f83\u56f0\u96be\u7684\uff0c\u6ca1\u6709\u5177\u4f53\u7684\u89e3\u51b3\u529e\u6cd5\u3002\u5728\u5bf9\u63a5\u8fc7\u7a0b\u4e2d\u5c1d\u8bd5\u52a0\u5165\u81ea\u5b9a\u4e49\u7684\u4f9d\u8d56\u5e93\uff0c\u6216\u8005\u9009\u53d6\u548cFusionInsight\u73b0\u7f51\u7248\u672c\u76f8\u8fd1\u7684\u5f00\u6e90\u7248\u672c\u4f9d\u8d56\u5e93\u8fdb\u884c\u5bf9\u63a5 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u62a5\u9519\u539f\u56e0\u5c31\u662f\u4f9d\u8d56jar\u5305\u7684\u4e0d\u5339\u914d\uff0c\u9047\u5230\u8be5\u62a5\u9519\u662f\u5f88\u96be\u5b9a\u4f4d\u5230\u5177\u4f53\u4f9d\u8d56jar\u5305\u7684\uff0c\u9700\u8981\u6839\u636e\u89e3\u51b3\u529e\u6cd5\u7684\u601d\u8def\u5c1d\u8bd5\u4ece\u5176\u4ed6\u7684\u601d\u8def\u6765\u89e3\u51b3","title":"java.lang.NoSuchMethodError"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#javanetunknownhostexception-hacluster","text":"\u62a5\u9519\u7247\u6bb5 java.lang.IllegalArgumentException: java.net.UnknownHostException: hacluster at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:444) at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:132) at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:355) \u95ee\u9898\u539f\u56e0: \u4e09\u65b9\u8f6f\u4ef6\u4e0d\u652f\u6301HDFS\u9ad8\u53ef\u7528\u6027\uff0c\u6216\u8005\u9ad8\u53ef\u7528\u6027\u6ca1\u6709\u914d\u7f6e\u6b63\u786e \u89e3\u51b3\u529e\u6cd5\uff1a - \u5982\u679c\u4e09\u65b9\u8f6f\u4ef6\u4e0d\u652f\u6301\u9ad8\u53ef\u7528\u6027\u914d\u7f6e\uff0c\u4fee\u6539\u5bfc\u5165\u7684hdfs\u914d\u7f6e\u6587\u4ef6core-site.xml\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u9879\u5982\u4e0b\uff1a <property> <name>fs.defaultFS</name> <value>hdfs://\u4e3bnamenode\u8282\u70b9ip:21005</value> </property> - \u53c2\u8003\u4e09\u65b9\u8f6f\u4ef6\u6587\u6863\uff0c\u914d\u7f6e\u9ad8\u53ef\u7528\u6027 \u7ecf\u9a8c\u5206\u4eab\uff1a \u8be5\u62a5\u9519\u539f\u56e0\u5c31\u662f\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u9879\u6709\u95ee\u9898\uff0c\u6bd4\u5982core-site.xml\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684fs.defaultFS\uff0c\u6709\u65f6\u5728\u5bf9\u63a5hbase\u7684\u65f6\u5019\u4e5f\u4f1a\u6709\u7c7b\u4f3c\u95ee\u9898\u51fa\u73b0\uff0c\u53ef\u4ee5\u5728hbase-site.xml\u914d\u7f6e\u6587\u4ef6\u4e2d\u67e5\u627e\u76f8\u5173\u914d\u7f6e\u9879\u8fdb\u884c\u66f4\u6539","title":"java.net.UnknownHostException: hacluster"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#cannot-modify-xxx-at-runtime-it-is-not-in-list-of-params-that-are-allowed-to-be-modified-at-runtime","text":"\u95ee\u9898\u539f\u56e0: hive\u767d\u540d\u5355\u6ca1\u6709\u5f00\u901a\u8be5\u76f8\u5173\u53c2\u6570 \u89e3\u51b3\u529e\u6cd5\uff1a \u767b\u9646FusionInsight Manager -> \u96c6\u7fa4 -> Hive -> \u5168\u90e8\u914d\u7f6e -> hive.security.authorization.sqlstd.confwhitelist.append -> \u6dfb\u52a0\u62a5\u9519\u7f3a\u5931\u7684\u767d\u540d\u5355 \u4f8b\u5982\uff1a \u62a5\u9519\u63d0\u793a Cannot modify mapred.job.name at runtime. \u5bf9\u5e94\u7684\u5728 hive.security.authorization.sqlstd.confwhitelist.append \u914d\u7f6e\u9879\u7684\u672b\u5c3e\u6dfb\u52a0\u5185\u5bb9\uff1a |mapred\\.job\\.name \u8bf4\u660e\uff1ahive.security.authorization.sqlstd.confwhitelist.append\u8fd9\u4e2a\u53c2\u6570\u4e2d\u7684\u5185\u5bb9\u4ee5 | \u5206\u9694\uff0c\u586b\u5199\u7684\u5185\u5bb9\u9700\u8981\u7528\u8f6c\u4e49\u5b57\u7b26 \\","title":"Cannot modify XXX at runtime. It is not in list of params that are allowed to be modified at runtime"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#permission-denied","text":"\u62a5\u9519\u7247\u6bb5 HiveAccessControlException Permission denied: Principal [name=developuser, type=USER] does not have following privileges for operation CREATETABLE [[OBJECT OWNERSHIP] on Object [type=DFS_URI, name=hdfs://hacluster/user/hive/warehouse/sdc_drift_example03]] \u95ee\u9898\u539f\u56e0\uff1a \u8ba4\u8bc1\u7528\u6237\u5728FusionInsight\u4e0a\u6ca1\u6709\u5bf9\u5e94\u7684\u6743\u9650 \u89e3\u51b3\u529e\u6cd5\uff1a \u4ed4\u7ec6\u67e5\u770b\u62a5\u9519\uff0c\u660e\u786e\u54ea\u4e2a\u7528\u6237\uff0c\u5728\u4ec0\u4e48\u5bf9\u8c61\u4e0a\uff0c\u6ca1\u6709\u4ec0\u4e48\u6743\u9650\uff0c\u518d\u53bbFusionInsight Manager\u4e0a\u914d\u7f6e\u76f8\u5173\u6743\u9650\u3002 \u5177\u4f53\u53c2\u8003\u300a\u4ea7\u54c1\u6587\u6863\u300b -> \u300a\u5e94\u7528\u5f00\u53d1\u6307\u5357\u300b -> \u300a\u5b89\u5168\u6a21\u5f0f\u300b -> \u300a\u5b89\u5168\u8ba4\u8bc1\u300b -> \u300a\u51c6\u5907\u5f00\u53d1\u7528\u6237\u300b\u76f8\u5173\u7ae0\u8282","title":"Permission denied"},{"location":"Other/%E5%AF%B9%E6%8E%A5%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%931.0/#failed-to-connect-to-driver-at-xxx-retrying","text":"\u8bf4\u660e\uff1a \u8be5\u62a5\u9519\u51fa\u73b0\u7684\u573a\u666f\u4e3a\u540cspark2x\u7ec4\u4ef6\u5bf9\u63a5\u4e2d\u4efb\u52a1\u5931\u8d25\uff0c\u62a5\u9519\u51fa\u73b0\u7684\u4f4d\u7f6e\u5728FusionInsight Yarn\u4e0a\u67e5\u770b\u76f8\u5173\u5931\u8d25\u4efb\u52a1\u7684container\u65e5\u5fd7stout\u4e2d \u95ee\u9898\u539f\u56e0\uff1a \u4f7f\u7528yarn-client\u6a21\u5f0f\u63d0\u4ea4spark\u4efb\u52a1\uff0c\u96c6\u7fa4\u4e0d\u77e5\u9053driver\u7684\u4e3b\u673a\u540d \u89e3\u51b3\u529e\u6cd5\uff1a \u5728\u5bf9\u5e94\u96c6\u7fa4worker\u7684\u8282\u70b9\u4e0a\uff0c\u4fee\u6539 /etc/hosts \u914d\u7f6e\u6587\u4ef6\uff0c\u5c06\u63d0\u4ea4\u4efb\u52a1\u7684\u4e3b\u673a\u540d\u52a0\u8fdb\u53bb","title":"Failed to connect to driver at xxx:\u7aef\u53e3 retrying"},{"location":"Other/BestPractises/FusionInsight%E8%BF%81%E7%A7%BBMongoDB%E6%95%B0%E6%8D%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","text":"Mongo\u6570\u636e\u8fc1\u79fb\u5230hadoop\u6700\u4f73\u5b9e\u8df5 \u00b6 \u6982\u8ff0 \u00b6 \u4eceMongo\u540c\u6b65\u6570\u636e\u5230FusionInsight\u5e38\u7528\u7684\u51e0\u79cd\u65b9\u5f0f\u5b58\u5728\u7684\u6311\u6218\uff1a MongoDB Connector for Hadoop \u8be5\u65b9\u5f0f\u6700\u5927\u7684\u95ee\u9898\u5728\u4e8e\u9700\u8981\u5728hive\u8fde\u63a5MongoDB\u65f6\u660e\u6587\u5b58\u50a8MongoDB\u7684\u94fe\u63a5\u4e32\u548c\u5bc6\u7801\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5728\u91cd\u89c6\u6570\u636e\u5b89\u5168\u7684\u516c\u53f8\uff0c\u4e0d\u7b26\u5408\u5b89\u5168\u8981\u6c42\u3002 \u901a\u8fc7\u4e0b\u8f7dMongo\u7684BSON\u6587\u4ef6\uff0c\u7136\u540e\u52a0\u8f7d\u5230Hadoop \u8be5\u65b9\u5f0f\u5bf9\u6e90\u7aef\u670d\u52a1\u5668\u9020\u6210\u8f83\u5927\u6027\u80fd\u538b\u529b\uff0c\u4e14\u6587\u4ef6\u9700\u8981\u4e2d\u8f6c\uff0c\u64cd\u4f5c\u8fc7\u7a0b\u6bd4\u8f83\u590d\u6742\u3002 \u4f7f\u7528ROMA FDI \u6027\u80fd\u8f83\u5dee,ROMA\u5b9a\u4f4d\u5e76\u4e0d\u662f\u9ad8\u6027\u80fd\u7684ETL\u5de5\u5177\uff0c\u4e0d\u9002\u5408\u5927\u6279\u91cf\u6570\u636e\u540c\u6b65\uff0c\u4e14\u4e0d\u80fd\u652f\u6301\u5d4c\u5957\u7684\u6587\u6863\u3002 \u63a8\u8350\u65b9\u6848 \u00b6 \u65b9\u6848\u793a\u610f\u56fe\u5982\u4e0b\uff1a \u8be5\u5b9e\u8df5\u6765\u81ea\u4e8e\u534e\u4e3a\u6d88\u8d39\u8005BG\uff0c\u4f7f\u752816\u5e76\u53d1\uff0c\u540c\u6b65\u901f\u5ea6\u53ef\u4ee5\u8fbe\u5230500\u4e07\u884c/\u5206\uff0c\u7406\u8bba\u4e0a\u5e76\u53d1\u7ed3\u70b9\u589e\u52a0\uff0c\u6027\u80fd\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u3002 \u9996\u5148\u5c06MongoDB\u5bc6\u7801\u5b58\u653e\u5230hadoop credential provider\u4e2d\uff0c\u907f\u514d\u4e86\u5bc6\u7801\u660e\u6587\u663e\u793a\u7684\u95ee\u9898\u3002 \u901a\u8fc7MongoDB Connector for Apache Spark\u5bf9\u63a5MongoDB,\u8bfb\u53d6MongoDB\u6570\u636e\u4e3aSpark DataFrame\uff0c\u5229\u7528Spark SQL\u5199\u5165\u5230Hive\u8868\u4e2d\u3002Spark SQL\u57fa\u4e8e\u5185\u5b58\u5904\u7406\u548c\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u63d2\u5165\u6027\u80fd\u9ad8\u3002 \u6838\u5fc3\u7ec4\u4ef6\u4e3aMongoDB Connector for Apache Spark\uff0c \u70b9\u51fb\u67e5\u770b\u76f8\u5173\u4ecb\u7ecd ,\u7ed3\u5408CBG\u573a\u666f\u9700\u6c42\uff0c\u7f16\u5199\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u901a\u8fc7\u7b80\u5355\u914d\u7f6e\u5373\u53ef\u5b9e\u73b0MongoDB\u5411Hive\u7684\u6570\u636e\u540c\u6b65\u3002 \u6ce8\u610f\u5bf9\u5e94\u7684\u914d\u5957\u7248\u672c\uff0c\u534e\u4e3aFusionInsight Spark2x\u4f7f\u7528\u7684Spark\u7248\u672c\u4e3a2.3.2\uff0c\u5bf9\u5e94\u7684MongoDB Connector for spark \u7248\u672c\u4e3a2.3.3 \u64cd\u4f5c\u6b65\u9aa4 \u00b6 \u73af\u5883\u51c6\u5907 \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5199\u5165Mongo\u5bc6\u7801\u5230Hadoop credential\u4e2d. export HADOOP_CREDSTORE_PASSWORD=huawei #\u53ef\u81ea\u5b9a\u4e49 hadoop credential create mongodb20201 -provider jceks://hdfs/mongo2020/secret20201.jceks #hdfs\u5b58\u653ekeystore\u7684\u8def\u5f84\uff0c\u53ef\u81ea\u5b9a\u4e49 \u4e0b\u8f7djar\u5305\u5e76\u4e0a\u4f20\u5230hadoop client\u7aef\uff0c\u5982/root\u8def\u5f84\u4e0b mongo-spark-connector_2.11-2.3.2.jar \u70b9\u51fb\u4e0b\u8f7d mongo-java-driver-3.8.2.jar \u70b9\u51fb\u4e0b\u8f7d \u521b\u5efa\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6,\u5982/root/conf.properties userPrincipal=developuser #kerberos\u8ba4\u8bc1\u7528\u6237 userKeytabPath=/confAndCredentials/user.keytab #Keytab\u6587\u4ef6\u8def\u5f84 krb5ConfPath=/confAndCredentials/krb5.conf #krb5.conf\u6587\u4ef6 mongoUsername=mongodb2020 #mongodb mongoHost=172.16.13.112 #mongodb\u670d\u52a1\u5668\u5730\u5740 mongoPort=27017 #mongodb\u670d\u52a1\u7aef\u53e3 mongoDB=sparkdb #mongodb\u5f85\u540c\u6b65\u6570\u636e\u5e93\u540d\u79f0 mongoCollections=colls #mongodb\u5f85\u540c\u6b65\u6570\u636e\u96c6\u540d\u79f0 conditions= #\u8fc7\u6ee4\u6761\u4ef6\uff0c\u5982_id<xxx jceksPath=jceks://hdfs/mongo2020/secret2020.jceks #jceks\u8def\u5f84 MongoColumns=_id,title,description,likes,by #\u9700\u8981\u540c\u6b65\u7684mongodb\u7684\u5217 hiveDB=sparkdb #\u9700\u8981\u5199\u5165\u7684hive\u6570\u636e\u5e93\u7684\u540d\u79f0 hiveTable=mongotable #\u9700\u8981\u5199\u5165\u7684hive\u6570\u636e\u8868\u7684\u540d\u79f0 partColumn= #hive\u8868\u5206\u533a\u5217\u540d\u79f0\uff0c\u5982\u679c\u4e3a\u7a7a\uff0c\u8868\u793a\u65e0\u5206\u533a\u5217 partValue=2020 #\u5206\u533a\u5217\u503c \u83b7\u53d6kerberos\u8ba4\u8bc1\u6587\u4ef6\uff0c\u7528\u4e8espark2x\u7684kerberos\u8ba4\u8bc1 \u4f7f\u7528\u5f00\u53d1\u8005\u8d26\u53f7\u767b\u5f55FusionInsight Manager\uff0c\u70b9\u53f3\u4e0a\u89d2 \u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u636e \uff0c\u5c06\u83b7\u53d6\u7684krb5.conf\u548cuser.keytab\u6587\u4ef6\u4e0a\u4f20\u5230hadoop client\u7aef\uff0c\u5982/confAndCredentials/\u8def\u5f84\u4e0b\u3002 \u521b\u5efalog4j\u914d\u7f6e\u6587\u4ef6\uff0c\u5982/root/log4j.properties # This sets the global logging level and specifies the appenders log4j.rootLogger=INFO, theConsoleAppender # settings for the console appender log4j.appender.theConsoleAppender=org.apache.log4j.ConsoleAppender log4j.appender.theConsoleAppender.layout=org.apache.log4j.PatternLayout \u7f16\u8bd1jar\u5de5\u5177\u5305 \u00b6 \u4f7f\u7528intellij idea \u521b\u5efajava\u5de5\u7a0b\uff0c\u540d\u79f0\u4e3aMongoToHive \u5728src\u6587\u4ef6\u5939\u4e0b\uff0c\u7c98\u8d34LoginUtil.java (\u4e0b\u8f7d) \u548cMongoToHadoop.java (\u4e0b\u8f7d) \u6587\u4ef6 \u5728 File -> project structure -> libirary \u4e2d\u6dfb\u52a0spark2x\u793a\u4f8b\u5de5\u7a0b\u4e0b\u7684\u5168\u90e8jar\u5305( FusionInsight_Cluster_1_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars )\uff0c\u4ee5\u53camongo spark connector\u548cmongo-jar-driver jar\u5305\u3002 \u5728 File -> project structure -> Artifacts -> jar -> Form Modules with dependencies -> OK \u6700\u7ec8\u914d\u7f6e\u5982\u4e0b\uff1a \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 build -> build Artifacts -> build ,\u5b8c\u6210\u6253\u5305 \u4e0a\u4f20\u751f\u6210\u7684jar\u5305\u5230hadoop client \u4e0a\uff0c\u5982\u653e\u5728/root\u76ee\u5f55\u4e0b\u3002 \u6267\u884c\u8fc1\u79fb\u547d\u4ee4 \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5b9e\u73b0\u6570\u636e\u8fc1\u79fb export HADOOP_CREDSTORE_PASSWORD=huawei source /opt/hadoopclient/bigdata_env kinit developuser spark-submit --class MongoToHadoop --master yarn --deploy-mode client --jars /root/mongo-spark-connector_2.11-2.3.2.jar,/root/mongo-java-driver-3.8.2.jar --driver-java-options \"-Dlog4j.configuration=file:/root/log4j.properties\" --num-executors 8 /root/mongoToHive.jar /root/conf.properties append \u90e8\u5206\u53c2\u6570\u8bf4\u660e\uff1a * --num-executors \u6307\u5b9aspark\u5e76\u53d1\u6570 * /root/mongoToHive.jar \u8868\u793a\u6253\u5305\u7684jar\u5305\u8def\u5f84 * /root/conf.properties \u8868\u793a\u8fc1\u79fb\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 * append \u8868\u793a\u6570\u636e\u63d2\u5165\u5230hive\u8868\u4e2d\u7684\u6a21\u5f0f\uff0c\u5305\u62ecoverwrite\u548cappend\u4e24\u79cd\u6a21\u5f0f\uff0coverwrite\u4f1a\u5148truncate\u76ee\u6807\u8868\u540e\u518d\u63d2\u5165,append\u662f\u76f4\u63a5\u63d2\u5165\u65b9\u5f0f\uff0c\u5982\u679c\u662f\u5206\u533a\u8868\uff0c\u5219\u53ea\u4f1a\u91cd\u5199\u5f53\u524d\u5206\u533a\u7684\u6570\u636e\uff0c\u4e0d\u4f1a\u91cd\u5199\u5176\u4ed6\u5206\u533a\u6570\u636e\u3002 \u6267\u884c\u5b8c\u6210\u540e\uff0c\u663e\u793a\u5982\u4e0b\u5185\u5bb9\uff1a","title":"Mongo\u6570\u636e\u8fc1\u79fb\u5230hadoop\u6700\u4f73\u5b9e\u8df5"},{"location":"Other/BestPractises/FusionInsight%E8%BF%81%E7%A7%BBMongoDB%E6%95%B0%E6%8D%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#mongohadoop","text":"","title":"Mongo\u6570\u636e\u8fc1\u79fb\u5230hadoop\u6700\u4f73\u5b9e\u8df5"},{"location":"Other/BestPractises/FusionInsight%E8%BF%81%E7%A7%BBMongoDB%E6%95%B0%E6%8D%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_1","text":"\u4eceMongo\u540c\u6b65\u6570\u636e\u5230FusionInsight\u5e38\u7528\u7684\u51e0\u79cd\u65b9\u5f0f\u5b58\u5728\u7684\u6311\u6218\uff1a MongoDB Connector for Hadoop \u8be5\u65b9\u5f0f\u6700\u5927\u7684\u95ee\u9898\u5728\u4e8e\u9700\u8981\u5728hive\u8fde\u63a5MongoDB\u65f6\u660e\u6587\u5b58\u50a8MongoDB\u7684\u94fe\u63a5\u4e32\u548c\u5bc6\u7801\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5728\u91cd\u89c6\u6570\u636e\u5b89\u5168\u7684\u516c\u53f8\uff0c\u4e0d\u7b26\u5408\u5b89\u5168\u8981\u6c42\u3002 \u901a\u8fc7\u4e0b\u8f7dMongo\u7684BSON\u6587\u4ef6\uff0c\u7136\u540e\u52a0\u8f7d\u5230Hadoop \u8be5\u65b9\u5f0f\u5bf9\u6e90\u7aef\u670d\u52a1\u5668\u9020\u6210\u8f83\u5927\u6027\u80fd\u538b\u529b\uff0c\u4e14\u6587\u4ef6\u9700\u8981\u4e2d\u8f6c\uff0c\u64cd\u4f5c\u8fc7\u7a0b\u6bd4\u8f83\u590d\u6742\u3002 \u4f7f\u7528ROMA FDI \u6027\u80fd\u8f83\u5dee,ROMA\u5b9a\u4f4d\u5e76\u4e0d\u662f\u9ad8\u6027\u80fd\u7684ETL\u5de5\u5177\uff0c\u4e0d\u9002\u5408\u5927\u6279\u91cf\u6570\u636e\u540c\u6b65\uff0c\u4e14\u4e0d\u80fd\u652f\u6301\u5d4c\u5957\u7684\u6587\u6863\u3002","title":"\u6982\u8ff0"},{"location":"Other/BestPractises/FusionInsight%E8%BF%81%E7%A7%BBMongoDB%E6%95%B0%E6%8D%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_2","text":"\u65b9\u6848\u793a\u610f\u56fe\u5982\u4e0b\uff1a \u8be5\u5b9e\u8df5\u6765\u81ea\u4e8e\u534e\u4e3a\u6d88\u8d39\u8005BG\uff0c\u4f7f\u752816\u5e76\u53d1\uff0c\u540c\u6b65\u901f\u5ea6\u53ef\u4ee5\u8fbe\u5230500\u4e07\u884c/\u5206\uff0c\u7406\u8bba\u4e0a\u5e76\u53d1\u7ed3\u70b9\u589e\u52a0\uff0c\u6027\u80fd\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u3002 \u9996\u5148\u5c06MongoDB\u5bc6\u7801\u5b58\u653e\u5230hadoop credential provider\u4e2d\uff0c\u907f\u514d\u4e86\u5bc6\u7801\u660e\u6587\u663e\u793a\u7684\u95ee\u9898\u3002 \u901a\u8fc7MongoDB Connector for Apache Spark\u5bf9\u63a5MongoDB,\u8bfb\u53d6MongoDB\u6570\u636e\u4e3aSpark DataFrame\uff0c\u5229\u7528Spark SQL\u5199\u5165\u5230Hive\u8868\u4e2d\u3002Spark SQL\u57fa\u4e8e\u5185\u5b58\u5904\u7406\u548c\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u63d2\u5165\u6027\u80fd\u9ad8\u3002 \u6838\u5fc3\u7ec4\u4ef6\u4e3aMongoDB Connector for Apache Spark\uff0c \u70b9\u51fb\u67e5\u770b\u76f8\u5173\u4ecb\u7ecd ,\u7ed3\u5408CBG\u573a\u666f\u9700\u6c42\uff0c\u7f16\u5199\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u901a\u8fc7\u7b80\u5355\u914d\u7f6e\u5373\u53ef\u5b9e\u73b0MongoDB\u5411Hive\u7684\u6570\u636e\u540c\u6b65\u3002 \u6ce8\u610f\u5bf9\u5e94\u7684\u914d\u5957\u7248\u672c\uff0c\u534e\u4e3aFusionInsight Spark2x\u4f7f\u7528\u7684Spark\u7248\u672c\u4e3a2.3.2\uff0c\u5bf9\u5e94\u7684MongoDB Connector for spark \u7248\u672c\u4e3a2.3.3","title":"\u63a8\u8350\u65b9\u6848"},{"location":"Other/BestPractises/FusionInsight%E8%BF%81%E7%A7%BBMongoDB%E6%95%B0%E6%8D%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_3","text":"","title":"\u64cd\u4f5c\u6b65\u9aa4"},{"location":"Other/BestPractises/FusionInsight%E8%BF%81%E7%A7%BBMongoDB%E6%95%B0%E6%8D%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_4","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5199\u5165Mongo\u5bc6\u7801\u5230Hadoop credential\u4e2d. export HADOOP_CREDSTORE_PASSWORD=huawei #\u53ef\u81ea\u5b9a\u4e49 hadoop credential create mongodb20201 -provider jceks://hdfs/mongo2020/secret20201.jceks #hdfs\u5b58\u653ekeystore\u7684\u8def\u5f84\uff0c\u53ef\u81ea\u5b9a\u4e49 \u4e0b\u8f7djar\u5305\u5e76\u4e0a\u4f20\u5230hadoop client\u7aef\uff0c\u5982/root\u8def\u5f84\u4e0b mongo-spark-connector_2.11-2.3.2.jar \u70b9\u51fb\u4e0b\u8f7d mongo-java-driver-3.8.2.jar \u70b9\u51fb\u4e0b\u8f7d \u521b\u5efa\u8fde\u63a5\u914d\u7f6e\u6587\u4ef6,\u5982/root/conf.properties userPrincipal=developuser #kerberos\u8ba4\u8bc1\u7528\u6237 userKeytabPath=/confAndCredentials/user.keytab #Keytab\u6587\u4ef6\u8def\u5f84 krb5ConfPath=/confAndCredentials/krb5.conf #krb5.conf\u6587\u4ef6 mongoUsername=mongodb2020 #mongodb mongoHost=172.16.13.112 #mongodb\u670d\u52a1\u5668\u5730\u5740 mongoPort=27017 #mongodb\u670d\u52a1\u7aef\u53e3 mongoDB=sparkdb #mongodb\u5f85\u540c\u6b65\u6570\u636e\u5e93\u540d\u79f0 mongoCollections=colls #mongodb\u5f85\u540c\u6b65\u6570\u636e\u96c6\u540d\u79f0 conditions= #\u8fc7\u6ee4\u6761\u4ef6\uff0c\u5982_id<xxx jceksPath=jceks://hdfs/mongo2020/secret2020.jceks #jceks\u8def\u5f84 MongoColumns=_id,title,description,likes,by #\u9700\u8981\u540c\u6b65\u7684mongodb\u7684\u5217 hiveDB=sparkdb #\u9700\u8981\u5199\u5165\u7684hive\u6570\u636e\u5e93\u7684\u540d\u79f0 hiveTable=mongotable #\u9700\u8981\u5199\u5165\u7684hive\u6570\u636e\u8868\u7684\u540d\u79f0 partColumn= #hive\u8868\u5206\u533a\u5217\u540d\u79f0\uff0c\u5982\u679c\u4e3a\u7a7a\uff0c\u8868\u793a\u65e0\u5206\u533a\u5217 partValue=2020 #\u5206\u533a\u5217\u503c \u83b7\u53d6kerberos\u8ba4\u8bc1\u6587\u4ef6\uff0c\u7528\u4e8espark2x\u7684kerberos\u8ba4\u8bc1 \u4f7f\u7528\u5f00\u53d1\u8005\u8d26\u53f7\u767b\u5f55FusionInsight Manager\uff0c\u70b9\u53f3\u4e0a\u89d2 \u4e0b\u8f7d\u8ba4\u8bc1\u51ed\u636e \uff0c\u5c06\u83b7\u53d6\u7684krb5.conf\u548cuser.keytab\u6587\u4ef6\u4e0a\u4f20\u5230hadoop client\u7aef\uff0c\u5982/confAndCredentials/\u8def\u5f84\u4e0b\u3002 \u521b\u5efalog4j\u914d\u7f6e\u6587\u4ef6\uff0c\u5982/root/log4j.properties # This sets the global logging level and specifies the appenders log4j.rootLogger=INFO, theConsoleAppender # settings for the console appender log4j.appender.theConsoleAppender=org.apache.log4j.ConsoleAppender log4j.appender.theConsoleAppender.layout=org.apache.log4j.PatternLayout","title":"\u73af\u5883\u51c6\u5907"},{"location":"Other/BestPractises/FusionInsight%E8%BF%81%E7%A7%BBMongoDB%E6%95%B0%E6%8D%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#jar","text":"\u4f7f\u7528intellij idea \u521b\u5efajava\u5de5\u7a0b\uff0c\u540d\u79f0\u4e3aMongoToHive \u5728src\u6587\u4ef6\u5939\u4e0b\uff0c\u7c98\u8d34LoginUtil.java (\u4e0b\u8f7d) \u548cMongoToHadoop.java (\u4e0b\u8f7d) \u6587\u4ef6 \u5728 File -> project structure -> libirary \u4e2d\u6dfb\u52a0spark2x\u793a\u4f8b\u5de5\u7a0b\u4e0b\u7684\u5168\u90e8jar\u5305( FusionInsight_Cluster_1_Services_ClientConfig\\Spark2x\\FusionInsight-Spark2x-2.3.2.tar.gz\\spark\\jars )\uff0c\u4ee5\u53camongo spark connector\u548cmongo-jar-driver jar\u5305\u3002 \u5728 File -> project structure -> Artifacts -> jar -> Form Modules with dependencies -> OK \u6700\u7ec8\u914d\u7f6e\u5982\u4e0b\uff1a \u70b9\u51fb\u83dc\u5355\u680f\u4e2d\u7684 build -> build Artifacts -> build ,\u5b8c\u6210\u6253\u5305 \u4e0a\u4f20\u751f\u6210\u7684jar\u5305\u5230hadoop client \u4e0a\uff0c\u5982\u653e\u5728/root\u76ee\u5f55\u4e0b\u3002","title":"\u7f16\u8bd1jar\u5de5\u5177\u5305"},{"location":"Other/BestPractises/FusionInsight%E8%BF%81%E7%A7%BBMongoDB%E6%95%B0%E6%8D%AE%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_5","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5b9e\u73b0\u6570\u636e\u8fc1\u79fb export HADOOP_CREDSTORE_PASSWORD=huawei source /opt/hadoopclient/bigdata_env kinit developuser spark-submit --class MongoToHadoop --master yarn --deploy-mode client --jars /root/mongo-spark-connector_2.11-2.3.2.jar,/root/mongo-java-driver-3.8.2.jar --driver-java-options \"-Dlog4j.configuration=file:/root/log4j.properties\" --num-executors 8 /root/mongoToHive.jar /root/conf.properties append \u90e8\u5206\u53c2\u6570\u8bf4\u660e\uff1a * --num-executors \u6307\u5b9aspark\u5e76\u53d1\u6570 * /root/mongoToHive.jar \u8868\u793a\u6253\u5305\u7684jar\u5305\u8def\u5f84 * /root/conf.properties \u8868\u793a\u8fc1\u79fb\u914d\u7f6e\u6587\u4ef6\u8def\u5f84 * append \u8868\u793a\u6570\u636e\u63d2\u5165\u5230hive\u8868\u4e2d\u7684\u6a21\u5f0f\uff0c\u5305\u62ecoverwrite\u548cappend\u4e24\u79cd\u6a21\u5f0f\uff0coverwrite\u4f1a\u5148truncate\u76ee\u6807\u8868\u540e\u518d\u63d2\u5165,append\u662f\u76f4\u63a5\u63d2\u5165\u65b9\u5f0f\uff0c\u5982\u679c\u662f\u5206\u533a\u8868\uff0c\u5219\u53ea\u4f1a\u91cd\u5199\u5f53\u524d\u5206\u533a\u7684\u6570\u636e\uff0c\u4e0d\u4f1a\u91cd\u5199\u5176\u4ed6\u5206\u533a\u6570\u636e\u3002 \u6267\u884c\u5b8c\u6210\u540e\uff0c\u663e\u793a\u5982\u4e0b\u5185\u5bb9\uff1a","title":"\u6267\u884c\u8fc1\u79fb\u547d\u4ee4"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","text":"NIFI\u4ecb\u7ecd \u00b6 NIFI\u662f\u4e00\u4e2a\u6613\u7528\u3001\u53ef\u6269\u5c55\u3001\u9ad8\u53ef\u9760\u7684\u6570\u636e\u5904\u7406\u548c\u5206\u53d1\u7cfb\u7edf\u3002\u63d0\u4f9b\u56fe\u5f62\u5316\u754c\u9762\u548c\u5927\u91cf\u5185\u7f6e\u6570\u636e\u5904\u7406\u5668\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u8def\u7531\u3001\u8f6c\u6362\u3002NIFI\u5728\u534e\u4e3aFusionInsight\u89e3\u51b3\u65b9\u6848\u4e2d\u5b9a\u4f4d\u4e3a\u6d41\u5f0f\u6570\u636e\u96c6\u6210\u5de5\u5177\uff0c\u5728\u529f\u80fd\u4e0a\u53ef\u66ff\u4ee3Flume\u3002 NIFI\u4e3b\u8981\u4ef7\u503c\u7279\u6027\u5982\u4e0b\uff1a \u57fa\u4e8eweb\u754c\u9762\u7684\u56fe\u5f62\u5316\u5de5\u5177\uff0c\u652f\u6301\u62d6\u62c9\u62fd\u7684\u5f62\u5f0f\u5feb\u901f\u5b9e\u73b0\u6570\u636e\u5904\u7406\u3002 \u9ad8\u53ef\u9760 \u6570\u636e\u4f20\u8f93\u9ad8\u53ef\u9760\uff0c\u786e\u4fdd\u6570\u636e\u4f20\u8f93 \u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf \u52a8\u6001\u4f18\u5148\u7ea7\u8c03\u6574 \u56de\u538b \u6570\u636e\u8840\u7f18\uff0c\u5b9e\u73b0\u6570\u636e\u5168\u94fe\u8def\u7684\u8ffd\u8e2a\u80fd\u529b \u9ad8\u6269\u5c55 \u652f\u6301\u81ea\u5b9a\u4e49\u5f00\u53d1processor \u652f\u6301\u5feb\u901f\u5f00\u53d1\u548c\u6d4b\u8bd5 \u9ad8\u5b89\u5168 \u652f\u6301SSL\u3001SSH\u3001HTTPS \u7b49\u7c7b\u578b\u7684\u6570\u636e\u52a0\u5bc6\u4f20\u8f93 \u652f\u6301\u591a\u79df\u6237\u7684\u9274\u6743 NIFI\u67b6\u6784 NIFI\u8fd0\u884c\u5728JVM\u4e2d\uff0c\u4e3b\u8981\u7ec4\u4ef6\u5982\u4e0b\uff1a Web Server\uff1a\u4e3b\u8981\u7528\u4e8e\u63d0\u4f9b\u57fa\u4e8ehttp\u534f\u8bae\u7684\u547d\u4ee4\u548c\u63a7\u5236API Flow Controller: \u662fOperation\u7684\u5927\u8111\uff0c\u63d0\u4f9bextentions\u8fd0\u884c\u7684\u7ebf\u7a0b\uff0c\u5e76\u4e14\u8c03\u5ea6extensions\u4f55\u65f6\u63a5\u6536\u5165\u548c\u5904\u7406\u6570\u636e\u3002 FlowFile Repository: NiFi\u7528\u4e8e\u8ffd\u8e2a\u6570\u636e\u6d41\u4e2dflowfile\u7684\u72b6\u6001\u3002\u9ed8\u8ba4\u65b9\u6cd5\u662f\u4f7f\u7528Write-Ahead Log\u6280\u672f(\u7b80\u5355\u666e\u53ca\u4e0b\uff0cWAL\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u5728\u6570\u636e\u5199\u5165\u5e93\u4e4b\u524d\uff0c\u5148\u5199\u5165\u5230\u65e5\u5fd7.\u518d\u5c06\u65e5\u5fd7\u8bb0\u5f55\u53d8\u66f4\u5230\u5b58\u50a8\u5668\u4e2d\u3002)\u5199\u5230\u6307\u5b9a\u78c1\u76d8\u76ee\u5f55\u3002 Content Repository: \u7ed9\u5b9aFlowFile\u7684\u5b9e\u9645\u5185\u5bb9\u5b57\u8282\u5b58\u50a8\u7684\u5730\u65b9\u3002Content Repository\u7684\u5b9e\u73b0\u662f\u53ef\u63d2\u62d4\u7684\u3002\u9ed8\u8ba4\u65b9\u6cd5\u662f\u4e00\u79cd\u76f8\u5f53\u7b80\u5355\u7684\u673a\u5236\uff0c\u5b83\u5c06\u6570\u636e\u5757\u5b58\u50a8\u5728\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a\u6587\u4ef6\u7cfb\u7edf\u5b58\u50a8\u4f4d\u7f6e\uff0c\u4ee5\u4fbf\u83b7\u5f97\u4e0d\u540c\u7684\u7269\u7406\u5206\u533a\u4ee5\u51cf\u5c11\u4efb\u4f55\u5355\u4e2a\u5377\u4e0a\u7684\u4e89\u7528\u3002(\u6240\u4ee5\u73af\u5883\u6700\u4f73\u5b9e\u8df5\u65f6\u53ef\u914d\u7f6e\u591a\u4e2a\u76ee\u5f55\uff0c\u6302\u8f7d\u4e0d\u540c\u78c1\u76d8\uff0c\u63d0\u9ad8IO)\u3002 Provenance Repository: \u5b58\u50a8\u6240\u6709\u4e8b\u4ef6\u6570\u636e\u7684\u5730\u65b9\u3002Provenance Repository\u7684\u5b9e\u73b0\u662f\u53ef\u63d2\u62d4\u7684\uff0c\u9ed8\u8ba4\u5b9e\u73b0\u662f\u4f7f\u7528\u4e00\u4e2a\u6216\u591a\u4e2a\u7269\u7406\u78c1\u76d8\u5377\u3002\u5728\u6bcf\u4e2a\u4f4d\u7f6e\u5185\u7684\u4e8b\u4ef6\u6570\u636e\u90fd\u662f\u88ab\u7d22\u5f15\u5e76\u53ef\u641c\u7d22\u7684\u3002 Minifi\u4ecb\u7ecd \u00b6 MiNiFi\u662fNiFi\u7684\u5b50\u9879\u76ee\uff0c\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684agent\uff0c\u4e3b\u8981\u7528\u4e8e\u6570\u636e\u6536\u96c6\u3002\u4e3b\u8981\u6709\u4ee5\u4e0b\u51e0\u4e2a\u7279\u5f81\uff1a \u8f7b\u91cf\u7ea7\uff0c\u8f6f\u4ef6\u5305\u6bd4\u8f83\u5c0f\uff0c\u8d44\u6e90\u5360\u7528\u5c11 \u96c6\u4e2d\u7ba1\u7406agent \u751f\u6210\u6570\u636e\u8840\u7f18 \u548cNiFi\u96c6\u6210\u7528\u4e8e\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002 NiFi\u5b89\u88c5 \u00b6 NIFI\u96c6\u7fa4\u5b89\u88c5 \u00b6 NIFI\u652f\u6301\u5355\u673a\u4ee5\u53ca\u96c6\u7fa4\u6a21\u5f0f\u5b89\u88c5 \uff0c\u63a8\u8350\u4f7f\u7528\u96c6\u7fa4\u6a21\u5f0f\u5b89\u88c5\uff0c\u63d0\u4f9b\u9ad8\u53ef\u9760\u6027\u4ee5\u53ca\u66f4\u9ad8\u7684\u6570\u636e\u5904\u7406\u548c\u5206\u53d1\u80fd\u529b\u3002 \u4eceNiFi 1.0\u7248\u672c\u5f00\u59cb\uff0c\u91c7\u7528\u4e86zero-master\u96f6\u4e3b\u7fa4\u96c6\u6a21\u5f0f\u3002 NiFi\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u90fd\u5bf9\u6570\u636e\u6267\u884c\u76f8\u540c\u7684\u4efb\u52a1\uff0c\u4f46\u662f\u6bcf\u4e2a\u8282\u70b9\u64cd\u4f5c\u7684\u662f\u4e0d\u540c\u7684\u6570\u636e\u96c6\u3002 Apache ZooKeeper\u9009\u62e9\u4e00\u4e2a\u8282\u70b9\u4f5c\u4e3a\u96c6\u7fa4\u534f\u8c03\u5668coordinator\uff0c\u7531ZooKeeper\u81ea\u52a8\u5904\u7406\u6545\u969c\u8f6c\u79fb\u3002 \u6240\u6709\u7fa4\u96c6\u8282\u70b9\u5747\u5411\u7fa4\u96c6\u534f\u8c03\u5668\u62a5\u544a\u5fc3\u8df3\u548c\u72b6\u6001\u4fe1\u606f\u3002 \u96c6\u7fa4\u534f\u8c03\u5668\u6839\u636e\u8282\u70b9\u7684\u72b6\u6001\u4ee5\u8fde\u63a5\u6216\u8005\u65ad\u5f00\u8282\u70b9\u3002\u6b64\u5916\uff0c\u6bcf\u4e2a\u7fa4\u96c6\u90fd\u6709\u4e00\u4e2a\u4e3b\u8282\u70b9master\uff0c\u8be5\u8282\u70b9\u7531ZooKeeper\u9009\u62e9\u3002 \u53ef\u4ee5\u901a\u8fc7\u4efb\u4f55\u8282\u70b9\u7684\u7528\u6237\u754c\u9762\u8bbf\u95ee\u96c6\u7fa4\u7ba1\u7406\u5668\uff0c\u9488\u5bf9\u96c6\u7fa4\u7684\u4efb\u4f55\u66f4\u6539\u90fd\u5c06\u590d\u5236\u5230\u96c6\u7fa4\u4e2d\u7684\u6240\u6709\u8282\u70b9\uff0c\u8fd9\u79cd\u67b6\u6784\u5141\u8bb8\u4efb\u4f55\u4e00\u4e2a\u8282\u70b9\u6210\u4e3a\u96c6\u7fa4\u7684\u8bbf\u95ee\u5165\u53e3\u3002 \u73af\u5883\u51c6\u5907 \u672c\u6b21\u6d4b\u8bd5\u4f7f\u7528\u7684\u7cfb\u7edf\u53ca\u8f6f\u4ef6\u7248\u672c\u5982\u4e0b\uff1a zookeeper: apache-zookeeper-3.5.5 \u70b9\u51fb\u4e0b\u8f7d NiFi: 1.10.0 \u70b9\u51fb\u4e0b\u8f7d JDK 1.8 \u70b9\u51fb\u4e0b\u8f7d \u81f3\u5c113\u53f0vm: CENTOS 7.3 64bit \u786c\u76d8\u4e0d\u4f4e\u4e8e100GB vm1~vm3: 172.16.13.120/122/123 MiNiFi\uff1ajava\u7248\u672c 0.5.0 \u70b9\u51fb\u4e0b\u8f7d jdk\u5b89\u88c5 \u00b6 \u4e0a\u4f20\u5df2\u4e0b\u8f7djdk\u6587\u4ef6\u52303\u4e2a\u8282\u70b9\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b8c\u6210jdk\u5b89\u88c5\uff1a rpm -ivh jdk-8u231-linux-x64.rpm zookeeper\u5b89\u88c5 \u00b6 * \u4e0a\u4f20\u4e0b\u8f7d\u7684zookeeper\u8f6f\u4ef6apache-zookeeper-3.5.5-bin.tar.gz\u5230vm1\u7684/opt\u76ee\u5f55\u4e0b\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u89e3\u538b\u6587\u4ef6\uff0c\u5e76\u62f7\u8d1d\u5230\u53e6\u59162\u4e2a\u8282\u70b9\u3002 ``` cd /opt tar -zxvf apache-zookeeper-3.5.5-bin.tar.gz mv apache-zookeeper-3.5.5-bin zookeeper-3.5.5 scp -r zookeeper-3.5.5/ root@172.16.13.122:/opt/ scp -r zookeeper-3.5.5/ root@172.16.13.123:/opt/ ``` * zookeeper\u914d\u7f6e \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u751f\u6210zookeeper\u914d\u7f6e\u6587\u4ef6zoo.cfg ``` cp zoo_sample.cfg zoo.cfg ``` \u4fee\u6539zoo.cfg\u6587\u4ef6\u5185\u5bb9\uff0c\u53c2\u8003\u5982\u4e0b\uff1a ``` # The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes. dataDir= /zookeeperData # the port at which the clients will connect clientPort=2181 # the maximum number of client connections. # increase this if you need to handle more clients #maxClientCnxns=60 # # Be sure to read the maintenance section of the # administrator guide before turning on autopurge. # # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance # # The number of snapshots to retain in dataDir #autopurge.snapRetainCount=3 # Purge task interval in hours # Set to \"0\" to disable auto purge feature #autopurge.purgeInterval=1 server.1=172.16.13.120:2888:3888 server.2=172.16.13.122:2888:3888 server.3=172.16.13.123:2888:3888 ``` 3\u53f0\u670d\u52a1\u5668\u7684zoo.cfg\u914d\u7f6e\u6587\u4ef6\u76f8\u540c\u3002 \u5728zoo.cfg \u914d\u7f6e\u6587\u4ef6dataDir\u8bbe\u7f6e\u7684\u8def\u5f84\u4e0b\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efamyid\u6587\u4ef6\uff1a ``` mkdir /zookeeperData cd /zookeeperData echo \"1\" > myid ``` \u5728vm2\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efamyid\u6587\u4ef6 ``` mkdir /zookeeperData cd /zookeeperData echo \"2\" > myid ``` \u5728vm3\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efamyid\u6587\u4ef6 ``` mkdir /zookeeperData cd /zookeeperData echo \"3\" > myid ``` * zookeeper \u542f\u52a8 \u5206\u522b\u57283\u4e2a\u8282\u70b9,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8zookeeper ``` cd /opt/zookeeper-3.5.5/bin/ ./zkServer.sh start ``` * zookeeper \u9a8c\u8bc1 \u5206\u522b\u57283\u4e2a\u8282\u70b9\u4e0a\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u67e5\u770bzookeeper\u72b6\u6001\u53ca\u89d2\u8272 ``` cd /opt/zookeeper-3.5.5/bin ./zkServer.sh status ``` \u7ed3\u679c\u5982\u4e0b\u56fe\uff1a ![](assets/NIFI-347f0.png) \u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u7684\u89d2\u8272\u4e3aleader,\u53e6\u5916\u4e24\u4e2a\u8282\u70b9\u7684\u89d2\u8272\u4e3afollower\u3002 NiFi\u5b89\u88c5 \u00b6 \u4e0a\u4f20\u5df2\u4e0b\u8f7d\u7684nifi-1.10.0-bin.tar.gz\u6587\u4ef6\u5230/opt\u76ee\u5f55\u4e0b\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u89e3\u538b\u7f29\u5b89\u88c5\u6587\u4ef6\u3002 cd /opt tar -zxvf nifi-1.10.0-bin.tar.gz \u5728\u4e09\u4e2a\u8282\u70b9\u4e0a\u5206\u522b\u4fee\u6539/opt/nifi-1.10.0/conf/nifi.properties\u914d\u7f6e\u6587\u4ef6\uff0c\u5305\u62ec\u4ee5\u4e0b\u5185\u5bb9,\u5176\u4e2dIP\u5730\u5740\u4e3a\u5bf9\u5e94\u7ed3\u70b9\u7684IP\u3002 # web properties # nifi.web.war.directory=./lib nifi.web.http.host=172.16.13.120 nifi.web.http.port=18001 # cluster node properties (only configure for cluster nodes) # nifi.cluster.is.node=true nifi.cluster.node.address=172.16.13.120 nifi.cluster.node.protocol.port=28001 nifi.cluster.load.balance.port=16342 # zookeeper properties, used for cluster management # nifi.zookeeper.connect.string=172.16.13.120:2181,172.16.13.122:2181,172.16.13.123:2181 nifi.zookeeper.connect.timeout=3 secs nifi.zookeeper.session.timeout=3 secs nifi.zookeeper.root.node=/nifi \u4fee\u6539/opt/nifi-1.10.0/conf/state-management.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u5305\u62ec\u4ee5\u4e0b\u5185\u5bb9\uff1a <cluster-provider> <id>zk-provider</id> <class>org.apache.nifi.controller.state.providers.zookeeper.ZooKeeperStateProvider</class> <property name=\"Connect String\">172.16.13.121:2181,172.16.13.122:2181,172.16.13.123:2181</property> <property name=\"Root Node\">/nifi</property> <property name=\"Session Timeout\">10 seconds</property> <property name=\"Access Control\">Open</property> </cluster-provider> \u4fee\u6539\u5b8c\u6210\u540e\uff0c\u5728\u4e09\u4e2a\u8282\u70b9\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8NiFi. sh /opt/nifi-1.10.0/bin/nifi.sh start * \u767b\u5f55\u9a8c\u8bc1 \u767b\u5f55http://172.16.13.123:18001/nifi\uff0c\u767b\u5f55\u754c\u9762\u5982\u4e0b\u56fe\uff0c\u663e\u793a\u96c6\u7fa4\u4e2d\u67093\u4e2a\u8282\u70b9\u3002 \u70b9\u51fb\u53f3\u4e0a\u89d2\u56fe\u6807\u5c55\u5f00\uff0c\u9009\u62e9 cluster \u67e5\u770b\u96c6\u7fa4\u4fe1\u606f\u5982\u4e0b\uff1a \u6700\u4f73\u5b9e\u8df5 \u00b6 \u573a\u666f\u4e00\uff1a\u65e5\u5fd7\u6570\u636e\u6d41\u5f0f\u91c7\u96c6 \u00b6 \u573a\u666f\u793a\u610f\u56fe\u5982\u4e0b\uff1a \u901a\u8fc7MiNiFie\u4ece\u591a\u4e2a\u670d\u52a1\u5668\u5b9e\u65f6\u91c7\u96c6\u65e5\u5fd7\u6587\u4ef6\uff0c\u6570\u636e\u5b9e\u65f6\u53d1\u9001\u5230NiFi\u4e2d\uff0c\u901a\u8fc7NiFi\u5c06\u6570\u636e\u53d1\u9001\u5230Kafka\u548cHDFS\u4e2d\uff0c\u5206\u522b\u7528\u4e8e\u540e\u7eed\u7684\u6d41\u5f0f\u6570\u636e\u5904\u7406\u4ee5\u53ca\u6279\u5904\u7406\u3002\u57fa\u4e8e\u62d6\u62c9\u62fd\u7684\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5feb\u901f\u5b9e\u73b0\u6570\u636e\u91c7\u96c6\u6d41\u7a0b\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u4ee3\u7801\u5f00\u53d1\u91cf\u3002\u57fa\u4e8eMiNiFi\u7684\u8f7b\u91cf\u7ea7Agent\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u6e90\u7aef\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002 \u73af\u5883\u51c6\u5907 \u00b6 Minifi\u5b89\u88c5 \u00b6 \u51c6\u5907\u4e00\u53f0tomcat\u670d\u52a1\u5668\uff0c\u5728tomcat\u670d\u52a1\u5668\u4e0a\uff0c\u5b89\u88c5MiNiFi\uff0c\u5b9e\u65f6\u91c7\u96c6MiNiFi\u65e5\u5fd7\uff0c\u5c06\u65e5\u5fd7\u4f20\u5165NiFi\u4e2d\uff0c\u901a\u8fc7NiFi\u5199\u5165HDFS\u5f52\u6863\u5e76\u5199\u5165Kafka\u8fdb\u884c\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002 \u53c2\u8003\u4e4b\u524djdk\u5b89\u88c5\u6b65\u9aa4\uff0c\u5728tomcat\u670d\u52a1\u5668\u4e0a\u5b89\u88c5jdk1.8 \u4e0b\u8f7dMiNiFi\uff0c \u70b9\u51fb\u4e0b\u8f7d \u4e0b\u8f7dMiNiFi-toolkit \u8f6c\u6362\u5de5\u5177 \u70b9\u51fb\u4e0b\u8f7d \u5c06\u4e0b\u8f7d\u7684MiNiFi\u53caMiNiFi-toolkit\u4e0a\u4f20\u5230/opt\u8def\u5f84\u4e0b\u3002 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b8c\u6210\u89e3\u538b tar -zxvf minifi-0.5.0-bin.tar.gz tar -zxvf minifi-toolkit-0.5.0-bin.tar.gz tomcat\u5b89\u88c5 \u00b6 \u51c6\u5907\u4e00\u53f0centos \u670d\u52a1\u5668\u7528\u4e8e\u5b89\u88c5tomcat\u53ca\u6d4b\u8bd5\uff0c\u672c\u6b21\u4f7f\u7528\u7684tomcat \u670d\u52a1\u5668ip\u662f172.16.11.121 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e0b\u8f7d\u5e76\u5b89\u88c5tomcat\u670d\u52a1 wget http://mirror-hk.koddos.net/apache/tomcat/tomcat-8/v8.5.50/bin/apache-tomcat-8.5.50.tar.gz mv apache-tomcat-8.5.50.tar.gz /opt/ cd /opt/ tar -zxvf apache-tomcat-8.5.50.tar.gz mv apache-tomcat-8.5.50 tomcat cd tomcat/ sh bin/catalina.sh start * \u901a\u8fc7\u6d4f\u89c8\u5668\u8bbf\u95eehttp://172.16.11.121:8080\uff0c\u8bbf\u95ee\u7ed3\u679c\u5982\u4e0b\uff1a \u5728tomcat\u670d\u52a1\u5668\u6587\u4ef6/opt/tomcat/logs/localhost_access_log.2020-02-04.txt\u4e2d\uff0c\u5305\u62ec\u4e86\u5f53\u524d\u6700\u65b0\u7684\u8bbf\u95ee\u8bb0\u5f55\u3002 NiFi\u914d\u7f6e \u00b6 \u5728NiFi\u96c6\u7fa4\u4e0a\uff0c\u914d\u7f6enifi.properties\u6587\u4ef6\uff0c\u9700\u8981\u914d\u7f6esite to site\u5185\u5bb9\uff0c\u5f00\u542f\u7ad9\u70b9\u63a5\u6536\u670d\u52a1\u548c\u7aef\u53e3\uff0c\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\uff1a # Site to Site properties nifi.remote.input.host=172.16.13.120 nifi.remote.input.secure=false nifi.remote.input.socket.port=10000 nifi.remote.input.http.enabled=true nifi.remote.input.http.transaction.ttl=30 sec nifi.remote.contents.cache.expiration=30 secs \u91cd\u542fNiFi\u670d\u52a1\u3002 \u521b\u5efa\u6570\u636e\u6d41\u53ca\u6a21\u677f \u00b6 \u5728NiFi\u4e2d\uff0c\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\"From MiNiFi\"\u7684input port\uff0c\u540c\u65f6\u521b\u5efa\u4e00\u4e2aFI Hdfs\u4ee5\u53caFI Kafka Processor\uff0c\u5bf9\u63a5FI \u8fc7\u7a0b\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u6307\u5bfc\u3002 \u70b9\u51fb\u67e5\u770b \u521b\u5efa\u6210\u529f\u540e\u5982\u4e0b\u56fe\uff1a \u5728NiFi\u4e2d\uff0c\u521b\u5efa\u4e00\u4e2aProcess\u3000Group\uff0c\u53cc\u51fb\u8fdb\u5165process group\uff0c\u521b\u5efa\u4e00\u4e2atailfile Processor\u548c\u4e00\u4e2aremote process group \uff0c\u5e76\u5efa\u7acb\u8fde\u63a5\uff0c\u5982\u4e0b\u56fe: TailFile\u53c2\u8003\u914d\u7f6e\uff1a Remote Proces Group\u53c2\u8003\u914d\u7f6e\uff1a \u9009\u4e2dProcess group\u4e2d\u7ec4\u4ef6\uff0c\u70b9\u51fb\u5de6\u4fa7\u6309\u94ae\u521b\u5efa\u6a21\u677f\u3002\u8be5\u6a21\u677f\u521b\u5efa\u4ee5\u540e\u5bfc\u5165\u5230MiNiFi\u4e2d\uff0c\u4f5c\u4e3aMiNiFI\u4e2d\u8fd0\u884c\u7684\u914d\u7f6e\u6587\u4ef6\u3002 \u521b\u5efa\u6210\u529f\u540e\uff0c\u5728\u53f3\u4fa7\u83dc\u5355 templates \u4e2d\uff0c\u4e0b\u8f7d\u76f8\u5e94\u7684\u6a21\u677f\u3002 \u4e0a\u4f20\u6a21\u677f\u5230\u5b89\u88c5\u4e86MiNiFi\u7684tomcat\u670d\u52a1\u5668\u4e0a\uff0c \u9700\u8981\u4fee\u6539xml\u5bf9\u5e94\u7684encoding-version\u4e3a1.2,\u6b64\u5904\u7a0b\u5e8f\u6709\u4e2aBug\uff0c\u5982\u679c\u4e0d\u4fee\u6539\uff0c\u540e\u7eed\u8f6c\u6362\u683c\u5f0f\u65f6\u4f1a\u62a5\u9519 \u3002 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06\u5176\u4ecexml\u683c\u5f0f\u8f6c\u6362\u6210yml\u683c\u5f0f\uff0c\u5e76\u62f7\u8d1d\u5230MiNiFi\u7684\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\u4e2d\u3002 sh /opt/minifi-toolkit-0.5.0/bin/config.sh transform nifitemplate.xml config.yml cp config.yml /opt/minifi-0.5.0/conf/ \u542f\u52a8MiNiFi \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8MiNiFi sh /opt/minifi-0.5.0/minifi.sh start \u6570\u636e\u9a8c\u8bc1 \u00b6 \u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u6574\u4f53\u62d3\u6251\u56fe\u5982\u4e0b\uff1a \u901a\u8fc7\u8bbf\u95eetomcat\u9996\u9875\uff0c\u5c06\u5728tomcat\u65e5\u5fd7\u4e2d\u751f\u6210\u4e00\u6761\u8bbf\u95ee\u8bb0\u5f55\u3002\u67e5\u770bhdfs\u4e2d\u662f\u5426\u751f\u6210\u5bf9\u5e94\u7684\u65e5\u5fd7\u6587\u4ef6\u3002\u5982\u4e0b\u56fe\uff1a \u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\uff0c\u67e5\u770bkafka topic\u63a5\u6536\u7684\u6570\u636e\u3002 kafka-console-consumer.sh --topic testtopic -bootstrap-server 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 --consumer.config /opt/hadoopclient/Kafka/kafka/config/consumer.properties \u7ed3\u679c\u5982\u4e0b\u56fe\uff0c\u8bf4\u660eNifi\u5df2\u5c06\u76f8\u5173\u65e5\u5fd7\u5b9e\u65f6\u53d1\u9001\u5230Kafka Topic\u4e2d\u3002 \u573a\u666f\u4e8c: \u533b\u7597\u884c\u4e1aHL7\u683c\u5f0f\u6570\u636e\u96c6\u6210 \u00b6 \u573a\u666f\u4ecb\u7ecd \u00b6 \u672c\u573a\u666f\u6f14\u793a\u5c06HL7\u683c\u5f0f\u6570\u636e\u901a\u8fc7NiFi\u8f6c\u6362\u6210json\u683c\u5f0f\uff0c\u5e76\u5b58\u50a8\u5230hdfs\u4ee5\u53cakafka\u4e2d\uff0c\u7528\u4e8e\u65e5\u5fd7\u5f52\u6863\u3001\u6279\u5904\u7406\u4ee5\u53ca\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002 \u6574\u4f53\u573a\u666f\u5982\u4e0b\u56fe\uff1a \u9996\u5148\u6a21\u62dfHL7\u683c\u5f0f\u6d4b\u8bd5\u6570\u636e\uff0c\u5c06\u6d4b\u8bd5\u6570\u636e\u53d1\u5e03\u5230Kafka\u4e2d\uff0cNiFi\u901a\u8fc7consumeKafka processor\u63a5\u6536\u6d4b\u8bd5\u6570\u636e\uff0c\u5e76\u901a\u8fc7ExtractHL7Attributes processor\u89e3\u6790\u76f8\u5173\u5c5e\u6027\uff0c\u5e76\u5229\u7528NiFi\u5185\u7f6e\u7b97\u5b50\uff0c\u5c06\u63a5\u6536\u7684kafka\u5c06\u6570\u636e\u6700\u7ec8\u8f6c\u6362\u6210json\u683c\u5f0f\uff0c\u5e76\u5b58\u50a8\u5230FusionInsight HDFS\u4e2d\uff0c\u540c\u65f6\u5c06Json\u683c\u5f0f\u7684\u6570\u636e\u53d1\u5f80Kafka\u8fdb\u884c\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002 \u6570\u636e\u6d41\u51c6\u5907 \u00b6 \u521b\u5efakafka topic\u3002 \u5728hadoop client\u673a\u5668\u4e0a\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efatopic kafka-topics.sh --create --zookeeper 172.16.11.21:24002/kafka --partitions 6 --replication-factor 2 --topic h7topic1; kafka-topics.sh --create --zookeeper 172.16.11.21:24002/kafka --partitions 6 --replication-factor 2 --topic h7topic2 NiFi\u6570\u636e\u6d41\u914d\u7f6e\uff1a ConsumeKafka \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a Kafka Brokers: 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 Security Protocol: SASL_PLAINTEXT Kerberos Credentials Service: KeytabCredentialsService Kerberos Service Name: kafka Topic Name: h7topic1 #\u4e4b\u524d\u521b\u5efa\u7684kafka topic\u540d\u79f0 Group ID: Demo #\u53ef\u81ea\u5b9a\u4e49 ReplcaeText \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a Search Value: <cr> Replacement Value: ${literal('\\r\\n')} ExtractHL7Attributes \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a AttributeToJson \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a JoltTransformJson \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a Jolt Specification: { \"OBX_1.UserDefinedAccessChecks\": \"OBX_1.UserDefinedAccessChecks\", \"OBR_1.OrderingProvider.FamilyName\": \"OBR_1.OrderingProvider.FamilyName\", \"MSH.MessageControlID\": \"MSH.MessageControlID\", \"OBX_1.ObservationIdentifier.Text\": \"OBX_1.ObservationIdentifier.Text\", \"MSH.SendingApplication.NamespaceID\": \"MSH.SendingApplication.NamespaceID\", \"OBR_1.UniversalServiceIdentifier.Text\": \"OBR_1.UniversalServiceIdentifier.Text\", \"MSH.ReceivingApplication.NamespaceID\": \"MSH.ReceivingApplication.NamespaceID\", \"MSH.ProcessingID.ProcessingID\": \"MSH.ProcessingID.ProcessingID\", \"uuid\": \"uuid\", \"PID.SSNNumberPatient\": \"PID.SSNNumberPatient\", \"OBR_1.FillerOrderNumber.EntityIdentifier\": \"OBR_1.FillerOrderNumber.EntityIdentifier\", \"path\": \"path\", \"PID.PatientAccountNumber.ID\": \"PID.PatientAccountNumber.ID\", \"PID.DateOfBirth\": \"PID.DateOfBirth\", \"PD1.PatientPrimaryCareProviderNameIDNo.IDNumber\": \"PD1.PatientPrimaryCareProviderNameIDNo.IDNumber\", \"PID.Sex\": \"PID.Sex\", \"MSH.MessageType.MessageType\": \"MSH.MessageType.MessageType\", \"OBX_1.ReferencesRange\": \"OBX_1.ReferencesRange\", \"OBR_1.OrderingProvider.IDNumber\": \"OBR_1.OrderingProvider.IDNumber\", \"PD1.PatientPrimaryCareProviderNameIDNo.FamilyName\": \"PD1.PatientPrimaryCareProviderNameIDNo.FamilyName\", \"OBX_1.Units.NameOfCodingSystem\": \"OBX_1.Units.NameOfCodingSystem\", \"OBX_1.Units.Identifier\": \"OBX_1.Units.Identifier\", \"filename\": \"filename\", \"PID.PatientName.GivenName\": \"PID.PatientName.GivenName\", \"OBX_1.ObservationSubID\": \"OBX_1.ObservationSubID\", \"PD1.PatientPrimaryCareProviderNameIDNo.GivenName\": \"PD1.PatientPrimaryCareProviderNameIDNo.GivenName\", \"OBR_1.PlacerOrderNumber.NamespaceID\": \"OBR_1.PlacerOrderNumber.NamespaceID\", \"MSH.MessageType.TriggerEvent\": \"MSH.MessageType.TriggerEvent\", \"PD1.PatientPrimaryCareProviderNameIDNo.AssigningAuthority\": \"PD1.PatientPrimaryCareProviderNameIDNo.AssigningAuthority\", \"OBR_1.ResultStatus\": \"OBR_1.ResultStatus\", \"PID.PatientName.FamilyName\": \"PID.PatientName.FamilyName\", \"MSH.EncodingCharacters\": \"MSH.EncodingCharacters\", \"MSH.VersionID\": \"MSH.VersionID\", \"kafka.partition\": \"kafka.partition\", \"OBR_1.UniversalServiceIdentifier.Identifier\": \"OBR_1.UniversalServiceIdentifier.Identifier\", \"OBR_1.ObservationDateTime\": \"OBR_1.ObservationDateTime\", \"OBR_1.ScheduledDateTime\": \"OBR_1.ScheduledDateTime\", \"OBX_1.ObservationIdentifier.Identifier\": \"OBX_1.ObservationIdentifier.Identifier\", \"OBR_1.OrderingProvider.GivenName\": \"OBR_1.OrderingProvider.GivenName\", \"OBR_1.SetIDObservationRequest\": \"OBR_1.SetIDObservationRequest\", \"OBR_1.ResultsRptStatusChngDateTime\": \"OBR_1.ResultsRptStatusChngDateTime\", \"OBR_1.PlacerOrderNumber.EntityIdentifier\": \"OBR_1.PlacerOrderNumber.EntityIdentifier\", \"OBX_1.NatureOfAbnormalTest\": \"OBX_1.NatureOfAbnormalTest\", \"OBX_1.SetIDOBX\": \"OBX_1.SetIDOBX\", \"MSH.FieldSeparator\": \"MSH.FieldSeparator\", \"PD1.PatientPrimaryCareProviderNameIDNo.MiddleInitialOrName\": \"PD1.PatientPrimaryCareProviderNameIDNo.MiddleInitialOrName\", \"OBX_1.Units.Text\": \"OBX_1.Units.Text\", \"OBX_1.ValueType\": \"OBX_1.ValueType\", \"kafka.offset\": \"kafka.offset\", \"PID.PatientIDInternalID.ID\": \"PID.PatientIDInternalID.ID\", \"kafka.topic\": \"kafka.topic\", \"OBX_1.ObservationValue\": \"OBX_1.ObservationValue\", \"OBR_1.OrderingProvider.MiddleInitialOrName\": \"OBR_1.OrderingProvider.MiddleInitialOrName\" } PutHDFS \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a hadooop Configuration Resource: /opt/nifi-1.10.0/hdfs-site.xml,/opt/nifi-1.10.0/conf/core-site.xml Kerberos Credentials Service: KeytabCredentialsService Directory: /nifihl7 #s\u53ef\u81ea\u5b9a\u4e49 PublishKafka \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a Kafka Brokers: 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 Security Protocol: SASL_PLAINTEXT Kerberos Credentials Service: KeytabCredentialsService Kerberos Service Name: kafka Topic Name: h7topic2 #\u4e4b\u524d\u521b\u5efa\u7684kafka topic\u540d\u79f0 \u6570\u636e\u6d4b\u8bd5 \u00b6 \u5f00\u542f\u4e24\u4e2ahadoop client\u547d\u4ee4\u884c\u7a97\u53e3\uff0c\u5728\u7a97\u53e3\u4e00\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a bin/kafka-console-producer.sh --broker-list 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 --topic h7topic1 --producer.config config/producer.properties \u5728\u7a97\u53e3\u4e8c\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a kafka-console-consumer.sh --topic h7topic2 -bootstrap-server 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 --consumer.config /opt/hadoopclient/Kafka/kafka/config/consumer.properties \u5728\u7a97\u53e3\u4e00\u4e2d\u7c98\u8d34\u4ee5\u4e0b\u6570\u636e,\u8be5\u6570\u636e\u4e3a\u6a21\u62df\u7684HL7\u6570\u636e\u683c\u5f0f\u3002 MSH|^~\\&|XXXXXX||HealthOrg01||||ORU^R01|Q1111111111111111111|P|2.3|<cr>PID|||000000001||SMITH^JOHN||19700101|M||||||||||999999999999|123456789|<cr>PD1||||1234567890^LAST^FIRST^M^^^^^NPI|<cr>OBR|1|341856649^HNAM_ORDERID|000000000000000000|648088^Basic Metabolic Panel|||20150101000100|||||||||1620^Johnson^John^R||||||20150101000100|||M|||||||||||20150101000100|<cr>OBX|1|NM|GLU^Glucose Lvl|159|mg/dL|65-99^65^99|H|||F|||20150101000100| \u5982\u4e0b\u56fe\uff1a \u5728\u7a97\u53e3\u4e8c\u4e2d\u67e5\u770b\u7ed3\u679c\uff0c\u53d1\u73b0\u5df2\u7ecf\u8bfb\u53d6\u5230\u8f6c\u6362\u6210json\u683c\u5f0f\u7684\u6570\u636e\uff0c\u6570\u636e\u52a0\u8f7d\u5230kafka\u4e2d\u6210\u529f\uff0c\u53ef\u4ee5\u7528\u4e8e\u540e\u7eed\u7684\u6d41\u5f0f\u6570\u636e\u5904\u7406\uff0c\u5982flink\u7b49\u3002 \u67e5\u770b\u5728 hdfs \u4e2d\u5b58\u50a8\u7684\u6587\u4ef6\uff0c\u5982\u4e0b\u56fe\uff0c\u5df2\u7ecf\u67e5\u8be2\u5230\u5bf9\u5e94\u7684\u6587\u4ef6\uff0c\u8be5\u6570\u636e\u53ef\u4ee5\u505a\u4e3a\u539f\u59cb\u6570\u636e\u5f52\u6863\uff0c\u4ee5\u53ca\u6279\u91cf\u6570\u636e\u5206\u6790\u3002 \u573a\u666f\u4e09\uff1aIOT\u573a\u666f\u5b9e\u73b0MQTT\u6570\u636e\u96c6\u6210\u5230Kafka\u548chdfs \u00b6 \u573a\u666f\u4ecb\u7ecd \u00b6 \u672c\u573a\u666f\u6a21\u62dfNiFi\u63a5\u6536IOT MQTT\u683c\u5f0f\u6570\u636e\uff0c\u5e76\u5b58\u50a8\u5230hdfs\u4ee5\u53cakafka\u4e2d\uff0c\u7528\u4e8e\u540e\u7eed\u7684\u6d41\u5f0f\u5904\u7406\u548c\u6570\u636e\u5f52\u6863\u3002 \u573a\u666f\u793a\u610f\u56fe\u5982\u4e0b\uff1a \u73af\u5883\u51c6\u5907 \u00b6 \u672c\u573a\u666f\u901a\u8fc7\u5728Linux Centos\u4e0a\u5b89\u88c5mosquitto\u6a21\u62dfIOT broker\uff0c\u901a\u8fc7IoT Broker\u5c06\u6570\u636e\u8f6c\u53d1\u5230NiFi\uff0c\u7528\u4e8e\u540e\u7eed\u5c06\u6570\u636e\u5b58\u50a8\u5230HDFS\u6216\u8005kafka\uff0c\u7528\u4e8e\u5f52\u6863\u3001\u6279\u5904\u7406\u6216\u8005\u6d41\u5f0f\u8ba1\u7b97\u3002 \u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5728centos\u670d\u52a1\u5668\u4e0a\u5b89\u88c5mqtt: yum -y install eple-release yum -y install mosquitto systemctl start mosquitto systemctl enable mosquitto yum install python-pip pip install paho-mqtt useradd huawei mosquitto_passwd -c /etc/mosquitto/passwd huawei \u4fee\u6539\u914d\u7f6e\u6587\u4ef6/etc/mosquitto/mosquitto.conf\uff0c\u7981\u7528MQTT\u533f\u540d\u767b\u5f55\u3002 allow_anonymous false password_file /etc/mosuitto/passwd \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u91cd\u542fmosquitto\u670d\u52a1 ``` systemctl restart mosquitto ```` \u5728hadoop client\u4e0a\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efakafka topic kafka-topics.sh --create --zookeeper 172.16.11.21:24002/kafka --partitions 6 --replication-factor 2 --topic mqtttopic \u6570\u636e\u6d41\u51c6\u5907 \u00b6 \u521b\u5efa\u5982\u4e0bNiFI\u6570\u636e\u6d41\uff0c\u5982\u4e0b\u56fe\uff1a ConsumeMQTT Processor\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a PutHdfs Processor\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a hadooop Configuration Resource: /opt/nifi-1.10.0/hdfs-site.xml,/opt/nifi-1.10.0/conf/core-site.xml Kerberos Credentials Service: KeytabCredentialsService Directory: /nifimqtt PublishKafka Processor\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\uff1a Kafka Brokers: 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 Security Protocol: SASL_PLAINTEXT Kerberos Credentials Service: KeytabCredentialsService Kerberos Service Name: kafka Topic Name: mqtttopic #\u4e4b\u524d\u521b\u5efa\u7684kafka topic\u540d\u79f0 \u6570\u636e\u9a8c\u8bc1 \u00b6 \u5728hadoop client\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u76d1\u542ckafka topic\u7684\u8f93\u51fa\u3002 kafka-console-consumer.sh --topic testtopic -bootstrap-server 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 --consumer.config /opt/hadoopclient/Kafka/kafka/config/consumer.properties \u5728mqqtt\u5ba2\u6237\u7aef\uff0c\u521b\u5efamqttpublish.py\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a import paho.mqtt.publish as publish import time auth = { 'username': 'huawei', 'password': 'huawei' } for i in range(10): publish.single('test', payload='Test message %d' % i, auth=auth, hostname='172.16.13.112',port=1883) time.sleep(1) \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u53d1\u9001\u6d88\u606f\u5230MQtt Brokers python mqttpublish.py \u67e5\u770bkafka consumer\u663e\u793a\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u5982\u4e0b\u56fe\uff1a \u67e5\u770bhdfs\u6587\u4ef6 mqtt\u6570\u636e\u5df2\u6210\u529f\u540c\u6b65\u5230kafka\u548chdfs\uff0c\u9a8c\u8bc1\u5b8c\u6210\u3002","title":"NIFI\u4ecb\u7ecd"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#nifi","text":"NIFI\u662f\u4e00\u4e2a\u6613\u7528\u3001\u53ef\u6269\u5c55\u3001\u9ad8\u53ef\u9760\u7684\u6570\u636e\u5904\u7406\u548c\u5206\u53d1\u7cfb\u7edf\u3002\u63d0\u4f9b\u56fe\u5f62\u5316\u754c\u9762\u548c\u5927\u91cf\u5185\u7f6e\u6570\u636e\u5904\u7406\u5668\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u8def\u7531\u3001\u8f6c\u6362\u3002NIFI\u5728\u534e\u4e3aFusionInsight\u89e3\u51b3\u65b9\u6848\u4e2d\u5b9a\u4f4d\u4e3a\u6d41\u5f0f\u6570\u636e\u96c6\u6210\u5de5\u5177\uff0c\u5728\u529f\u80fd\u4e0a\u53ef\u66ff\u4ee3Flume\u3002 NIFI\u4e3b\u8981\u4ef7\u503c\u7279\u6027\u5982\u4e0b\uff1a \u57fa\u4e8eweb\u754c\u9762\u7684\u56fe\u5f62\u5316\u5de5\u5177\uff0c\u652f\u6301\u62d6\u62c9\u62fd\u7684\u5f62\u5f0f\u5feb\u901f\u5b9e\u73b0\u6570\u636e\u5904\u7406\u3002 \u9ad8\u53ef\u9760 \u6570\u636e\u4f20\u8f93\u9ad8\u53ef\u9760\uff0c\u786e\u4fdd\u6570\u636e\u4f20\u8f93 \u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf \u52a8\u6001\u4f18\u5148\u7ea7\u8c03\u6574 \u56de\u538b \u6570\u636e\u8840\u7f18\uff0c\u5b9e\u73b0\u6570\u636e\u5168\u94fe\u8def\u7684\u8ffd\u8e2a\u80fd\u529b \u9ad8\u6269\u5c55 \u652f\u6301\u81ea\u5b9a\u4e49\u5f00\u53d1processor \u652f\u6301\u5feb\u901f\u5f00\u53d1\u548c\u6d4b\u8bd5 \u9ad8\u5b89\u5168 \u652f\u6301SSL\u3001SSH\u3001HTTPS \u7b49\u7c7b\u578b\u7684\u6570\u636e\u52a0\u5bc6\u4f20\u8f93 \u652f\u6301\u591a\u79df\u6237\u7684\u9274\u6743 NIFI\u67b6\u6784 NIFI\u8fd0\u884c\u5728JVM\u4e2d\uff0c\u4e3b\u8981\u7ec4\u4ef6\u5982\u4e0b\uff1a Web Server\uff1a\u4e3b\u8981\u7528\u4e8e\u63d0\u4f9b\u57fa\u4e8ehttp\u534f\u8bae\u7684\u547d\u4ee4\u548c\u63a7\u5236API Flow Controller: \u662fOperation\u7684\u5927\u8111\uff0c\u63d0\u4f9bextentions\u8fd0\u884c\u7684\u7ebf\u7a0b\uff0c\u5e76\u4e14\u8c03\u5ea6extensions\u4f55\u65f6\u63a5\u6536\u5165\u548c\u5904\u7406\u6570\u636e\u3002 FlowFile Repository: NiFi\u7528\u4e8e\u8ffd\u8e2a\u6570\u636e\u6d41\u4e2dflowfile\u7684\u72b6\u6001\u3002\u9ed8\u8ba4\u65b9\u6cd5\u662f\u4f7f\u7528Write-Ahead Log\u6280\u672f(\u7b80\u5355\u666e\u53ca\u4e0b\uff0cWAL\u7684\u6838\u5fc3\u601d\u60f3\u662f\uff1a\u5728\u6570\u636e\u5199\u5165\u5e93\u4e4b\u524d\uff0c\u5148\u5199\u5165\u5230\u65e5\u5fd7.\u518d\u5c06\u65e5\u5fd7\u8bb0\u5f55\u53d8\u66f4\u5230\u5b58\u50a8\u5668\u4e2d\u3002)\u5199\u5230\u6307\u5b9a\u78c1\u76d8\u76ee\u5f55\u3002 Content Repository: \u7ed9\u5b9aFlowFile\u7684\u5b9e\u9645\u5185\u5bb9\u5b57\u8282\u5b58\u50a8\u7684\u5730\u65b9\u3002Content Repository\u7684\u5b9e\u73b0\u662f\u53ef\u63d2\u62d4\u7684\u3002\u9ed8\u8ba4\u65b9\u6cd5\u662f\u4e00\u79cd\u76f8\u5f53\u7b80\u5355\u7684\u673a\u5236\uff0c\u5b83\u5c06\u6570\u636e\u5757\u5b58\u50a8\u5728\u6587\u4ef6\u7cfb\u7edf\u4e2d\u3002\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a\u6587\u4ef6\u7cfb\u7edf\u5b58\u50a8\u4f4d\u7f6e\uff0c\u4ee5\u4fbf\u83b7\u5f97\u4e0d\u540c\u7684\u7269\u7406\u5206\u533a\u4ee5\u51cf\u5c11\u4efb\u4f55\u5355\u4e2a\u5377\u4e0a\u7684\u4e89\u7528\u3002(\u6240\u4ee5\u73af\u5883\u6700\u4f73\u5b9e\u8df5\u65f6\u53ef\u914d\u7f6e\u591a\u4e2a\u76ee\u5f55\uff0c\u6302\u8f7d\u4e0d\u540c\u78c1\u76d8\uff0c\u63d0\u9ad8IO)\u3002 Provenance Repository: \u5b58\u50a8\u6240\u6709\u4e8b\u4ef6\u6570\u636e\u7684\u5730\u65b9\u3002Provenance Repository\u7684\u5b9e\u73b0\u662f\u53ef\u63d2\u62d4\u7684\uff0c\u9ed8\u8ba4\u5b9e\u73b0\u662f\u4f7f\u7528\u4e00\u4e2a\u6216\u591a\u4e2a\u7269\u7406\u78c1\u76d8\u5377\u3002\u5728\u6bcf\u4e2a\u4f4d\u7f6e\u5185\u7684\u4e8b\u4ef6\u6570\u636e\u90fd\u662f\u88ab\u7d22\u5f15\u5e76\u53ef\u641c\u7d22\u7684\u3002","title":"NIFI\u4ecb\u7ecd"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#minifi","text":"MiNiFi\u662fNiFi\u7684\u5b50\u9879\u76ee\uff0c\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684agent\uff0c\u4e3b\u8981\u7528\u4e8e\u6570\u636e\u6536\u96c6\u3002\u4e3b\u8981\u6709\u4ee5\u4e0b\u51e0\u4e2a\u7279\u5f81\uff1a \u8f7b\u91cf\u7ea7\uff0c\u8f6f\u4ef6\u5305\u6bd4\u8f83\u5c0f\uff0c\u8d44\u6e90\u5360\u7528\u5c11 \u96c6\u4e2d\u7ba1\u7406agent \u751f\u6210\u6570\u636e\u8840\u7f18 \u548cNiFi\u96c6\u6210\u7528\u4e8e\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002","title":"Minifi\u4ecb\u7ecd"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#nifi_1","text":"","title":"NiFi\u5b89\u88c5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#nifi_2","text":"NIFI\u652f\u6301\u5355\u673a\u4ee5\u53ca\u96c6\u7fa4\u6a21\u5f0f\u5b89\u88c5 \uff0c\u63a8\u8350\u4f7f\u7528\u96c6\u7fa4\u6a21\u5f0f\u5b89\u88c5\uff0c\u63d0\u4f9b\u9ad8\u53ef\u9760\u6027\u4ee5\u53ca\u66f4\u9ad8\u7684\u6570\u636e\u5904\u7406\u548c\u5206\u53d1\u80fd\u529b\u3002 \u4eceNiFi 1.0\u7248\u672c\u5f00\u59cb\uff0c\u91c7\u7528\u4e86zero-master\u96f6\u4e3b\u7fa4\u96c6\u6a21\u5f0f\u3002 NiFi\u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u90fd\u5bf9\u6570\u636e\u6267\u884c\u76f8\u540c\u7684\u4efb\u52a1\uff0c\u4f46\u662f\u6bcf\u4e2a\u8282\u70b9\u64cd\u4f5c\u7684\u662f\u4e0d\u540c\u7684\u6570\u636e\u96c6\u3002 Apache ZooKeeper\u9009\u62e9\u4e00\u4e2a\u8282\u70b9\u4f5c\u4e3a\u96c6\u7fa4\u534f\u8c03\u5668coordinator\uff0c\u7531ZooKeeper\u81ea\u52a8\u5904\u7406\u6545\u969c\u8f6c\u79fb\u3002 \u6240\u6709\u7fa4\u96c6\u8282\u70b9\u5747\u5411\u7fa4\u96c6\u534f\u8c03\u5668\u62a5\u544a\u5fc3\u8df3\u548c\u72b6\u6001\u4fe1\u606f\u3002 \u96c6\u7fa4\u534f\u8c03\u5668\u6839\u636e\u8282\u70b9\u7684\u72b6\u6001\u4ee5\u8fde\u63a5\u6216\u8005\u65ad\u5f00\u8282\u70b9\u3002\u6b64\u5916\uff0c\u6bcf\u4e2a\u7fa4\u96c6\u90fd\u6709\u4e00\u4e2a\u4e3b\u8282\u70b9master\uff0c\u8be5\u8282\u70b9\u7531ZooKeeper\u9009\u62e9\u3002 \u53ef\u4ee5\u901a\u8fc7\u4efb\u4f55\u8282\u70b9\u7684\u7528\u6237\u754c\u9762\u8bbf\u95ee\u96c6\u7fa4\u7ba1\u7406\u5668\uff0c\u9488\u5bf9\u96c6\u7fa4\u7684\u4efb\u4f55\u66f4\u6539\u90fd\u5c06\u590d\u5236\u5230\u96c6\u7fa4\u4e2d\u7684\u6240\u6709\u8282\u70b9\uff0c\u8fd9\u79cd\u67b6\u6784\u5141\u8bb8\u4efb\u4f55\u4e00\u4e2a\u8282\u70b9\u6210\u4e3a\u96c6\u7fa4\u7684\u8bbf\u95ee\u5165\u53e3\u3002 \u73af\u5883\u51c6\u5907 \u672c\u6b21\u6d4b\u8bd5\u4f7f\u7528\u7684\u7cfb\u7edf\u53ca\u8f6f\u4ef6\u7248\u672c\u5982\u4e0b\uff1a zookeeper: apache-zookeeper-3.5.5 \u70b9\u51fb\u4e0b\u8f7d NiFi: 1.10.0 \u70b9\u51fb\u4e0b\u8f7d JDK 1.8 \u70b9\u51fb\u4e0b\u8f7d \u81f3\u5c113\u53f0vm: CENTOS 7.3 64bit \u786c\u76d8\u4e0d\u4f4e\u4e8e100GB vm1~vm3: 172.16.13.120/122/123 MiNiFi\uff1ajava\u7248\u672c 0.5.0 \u70b9\u51fb\u4e0b\u8f7d","title":"NIFI\u96c6\u7fa4\u5b89\u88c5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#jdk","text":"\u4e0a\u4f20\u5df2\u4e0b\u8f7djdk\u6587\u4ef6\u52303\u4e2a\u8282\u70b9\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b8c\u6210jdk\u5b89\u88c5\uff1a rpm -ivh jdk-8u231-linux-x64.rpm","title":"jdk\u5b89\u88c5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#zookeeper","text":"* \u4e0a\u4f20\u4e0b\u8f7d\u7684zookeeper\u8f6f\u4ef6apache-zookeeper-3.5.5-bin.tar.gz\u5230vm1\u7684/opt\u76ee\u5f55\u4e0b\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u89e3\u538b\u6587\u4ef6\uff0c\u5e76\u62f7\u8d1d\u5230\u53e6\u59162\u4e2a\u8282\u70b9\u3002 ``` cd /opt tar -zxvf apache-zookeeper-3.5.5-bin.tar.gz mv apache-zookeeper-3.5.5-bin zookeeper-3.5.5 scp -r zookeeper-3.5.5/ root@172.16.13.122:/opt/ scp -r zookeeper-3.5.5/ root@172.16.13.123:/opt/ ``` * zookeeper\u914d\u7f6e \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u751f\u6210zookeeper\u914d\u7f6e\u6587\u4ef6zoo.cfg ``` cp zoo_sample.cfg zoo.cfg ``` \u4fee\u6539zoo.cfg\u6587\u4ef6\u5185\u5bb9\uff0c\u53c2\u8003\u5982\u4e0b\uff1a ``` # The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. # do not use /tmp for storage, /tmp here is just # example sakes. dataDir= /zookeeperData # the port at which the clients will connect clientPort=2181 # the maximum number of client connections. # increase this if you need to handle more clients #maxClientCnxns=60 # # Be sure to read the maintenance section of the # administrator guide before turning on autopurge. # # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance # # The number of snapshots to retain in dataDir #autopurge.snapRetainCount=3 # Purge task interval in hours # Set to \"0\" to disable auto purge feature #autopurge.purgeInterval=1 server.1=172.16.13.120:2888:3888 server.2=172.16.13.122:2888:3888 server.3=172.16.13.123:2888:3888 ``` 3\u53f0\u670d\u52a1\u5668\u7684zoo.cfg\u914d\u7f6e\u6587\u4ef6\u76f8\u540c\u3002 \u5728zoo.cfg \u914d\u7f6e\u6587\u4ef6dataDir\u8bbe\u7f6e\u7684\u8def\u5f84\u4e0b\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efamyid\u6587\u4ef6\uff1a ``` mkdir /zookeeperData cd /zookeeperData echo \"1\" > myid ``` \u5728vm2\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efamyid\u6587\u4ef6 ``` mkdir /zookeeperData cd /zookeeperData echo \"2\" > myid ``` \u5728vm3\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u521b\u5efamyid\u6587\u4ef6 ``` mkdir /zookeeperData cd /zookeeperData echo \"3\" > myid ``` * zookeeper \u542f\u52a8 \u5206\u522b\u57283\u4e2a\u8282\u70b9,\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8zookeeper ``` cd /opt/zookeeper-3.5.5/bin/ ./zkServer.sh start ``` * zookeeper \u9a8c\u8bc1 \u5206\u522b\u57283\u4e2a\u8282\u70b9\u4e0a\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u67e5\u770bzookeeper\u72b6\u6001\u53ca\u89d2\u8272 ``` cd /opt/zookeeper-3.5.5/bin ./zkServer.sh status ``` \u7ed3\u679c\u5982\u4e0b\u56fe\uff1a ![](assets/NIFI-347f0.png) \u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u7684\u89d2\u8272\u4e3aleader,\u53e6\u5916\u4e24\u4e2a\u8282\u70b9\u7684\u89d2\u8272\u4e3afollower\u3002","title":"zookeeper\u5b89\u88c5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#nifi_3","text":"\u4e0a\u4f20\u5df2\u4e0b\u8f7d\u7684nifi-1.10.0-bin.tar.gz\u6587\u4ef6\u5230/opt\u76ee\u5f55\u4e0b\uff0c\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u89e3\u538b\u7f29\u5b89\u88c5\u6587\u4ef6\u3002 cd /opt tar -zxvf nifi-1.10.0-bin.tar.gz \u5728\u4e09\u4e2a\u8282\u70b9\u4e0a\u5206\u522b\u4fee\u6539/opt/nifi-1.10.0/conf/nifi.properties\u914d\u7f6e\u6587\u4ef6\uff0c\u5305\u62ec\u4ee5\u4e0b\u5185\u5bb9,\u5176\u4e2dIP\u5730\u5740\u4e3a\u5bf9\u5e94\u7ed3\u70b9\u7684IP\u3002 # web properties # nifi.web.war.directory=./lib nifi.web.http.host=172.16.13.120 nifi.web.http.port=18001 # cluster node properties (only configure for cluster nodes) # nifi.cluster.is.node=true nifi.cluster.node.address=172.16.13.120 nifi.cluster.node.protocol.port=28001 nifi.cluster.load.balance.port=16342 # zookeeper properties, used for cluster management # nifi.zookeeper.connect.string=172.16.13.120:2181,172.16.13.122:2181,172.16.13.123:2181 nifi.zookeeper.connect.timeout=3 secs nifi.zookeeper.session.timeout=3 secs nifi.zookeeper.root.node=/nifi \u4fee\u6539/opt/nifi-1.10.0/conf/state-management.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u5305\u62ec\u4ee5\u4e0b\u5185\u5bb9\uff1a <cluster-provider> <id>zk-provider</id> <class>org.apache.nifi.controller.state.providers.zookeeper.ZooKeeperStateProvider</class> <property name=\"Connect String\">172.16.13.121:2181,172.16.13.122:2181,172.16.13.123:2181</property> <property name=\"Root Node\">/nifi</property> <property name=\"Session Timeout\">10 seconds</property> <property name=\"Access Control\">Open</property> </cluster-provider> \u4fee\u6539\u5b8c\u6210\u540e\uff0c\u5728\u4e09\u4e2a\u8282\u70b9\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8NiFi. sh /opt/nifi-1.10.0/bin/nifi.sh start * \u767b\u5f55\u9a8c\u8bc1 \u767b\u5f55http://172.16.13.123:18001/nifi\uff0c\u767b\u5f55\u754c\u9762\u5982\u4e0b\u56fe\uff0c\u663e\u793a\u96c6\u7fa4\u4e2d\u67093\u4e2a\u8282\u70b9\u3002 \u70b9\u51fb\u53f3\u4e0a\u89d2\u56fe\u6807\u5c55\u5f00\uff0c\u9009\u62e9 cluster \u67e5\u770b\u96c6\u7fa4\u4fe1\u606f\u5982\u4e0b\uff1a","title":"NiFi\u5b89\u88c5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_1","text":"","title":"\u6700\u4f73\u5b9e\u8df5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_2","text":"\u573a\u666f\u793a\u610f\u56fe\u5982\u4e0b\uff1a \u901a\u8fc7MiNiFie\u4ece\u591a\u4e2a\u670d\u52a1\u5668\u5b9e\u65f6\u91c7\u96c6\u65e5\u5fd7\u6587\u4ef6\uff0c\u6570\u636e\u5b9e\u65f6\u53d1\u9001\u5230NiFi\u4e2d\uff0c\u901a\u8fc7NiFi\u5c06\u6570\u636e\u53d1\u9001\u5230Kafka\u548cHDFS\u4e2d\uff0c\u5206\u522b\u7528\u4e8e\u540e\u7eed\u7684\u6d41\u5f0f\u6570\u636e\u5904\u7406\u4ee5\u53ca\u6279\u5904\u7406\u3002\u57fa\u4e8e\u62d6\u62c9\u62fd\u7684\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5feb\u901f\u5b9e\u73b0\u6570\u636e\u91c7\u96c6\u6d41\u7a0b\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u4ee3\u7801\u5f00\u53d1\u91cf\u3002\u57fa\u4e8eMiNiFi\u7684\u8f7b\u91cf\u7ea7Agent\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u6e90\u7aef\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002","title":"\u573a\u666f\u4e00\uff1a\u65e5\u5fd7\u6570\u636e\u6d41\u5f0f\u91c7\u96c6"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_3","text":"","title":"\u73af\u5883\u51c6\u5907"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#minifi_1","text":"\u51c6\u5907\u4e00\u53f0tomcat\u670d\u52a1\u5668\uff0c\u5728tomcat\u670d\u52a1\u5668\u4e0a\uff0c\u5b89\u88c5MiNiFi\uff0c\u5b9e\u65f6\u91c7\u96c6MiNiFi\u65e5\u5fd7\uff0c\u5c06\u65e5\u5fd7\u4f20\u5165NiFi\u4e2d\uff0c\u901a\u8fc7NiFi\u5199\u5165HDFS\u5f52\u6863\u5e76\u5199\u5165Kafka\u8fdb\u884c\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002 \u53c2\u8003\u4e4b\u524djdk\u5b89\u88c5\u6b65\u9aa4\uff0c\u5728tomcat\u670d\u52a1\u5668\u4e0a\u5b89\u88c5jdk1.8 \u4e0b\u8f7dMiNiFi\uff0c \u70b9\u51fb\u4e0b\u8f7d \u4e0b\u8f7dMiNiFi-toolkit \u8f6c\u6362\u5de5\u5177 \u70b9\u51fb\u4e0b\u8f7d \u5c06\u4e0b\u8f7d\u7684MiNiFi\u53caMiNiFi-toolkit\u4e0a\u4f20\u5230/opt\u8def\u5f84\u4e0b\u3002 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b8c\u6210\u89e3\u538b tar -zxvf minifi-0.5.0-bin.tar.gz tar -zxvf minifi-toolkit-0.5.0-bin.tar.gz","title":"Minifi\u5b89\u88c5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#tomcat","text":"\u51c6\u5907\u4e00\u53f0centos \u670d\u52a1\u5668\u7528\u4e8e\u5b89\u88c5tomcat\u53ca\u6d4b\u8bd5\uff0c\u672c\u6b21\u4f7f\u7528\u7684tomcat \u670d\u52a1\u5668ip\u662f172.16.11.121 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e0b\u8f7d\u5e76\u5b89\u88c5tomcat\u670d\u52a1 wget http://mirror-hk.koddos.net/apache/tomcat/tomcat-8/v8.5.50/bin/apache-tomcat-8.5.50.tar.gz mv apache-tomcat-8.5.50.tar.gz /opt/ cd /opt/ tar -zxvf apache-tomcat-8.5.50.tar.gz mv apache-tomcat-8.5.50 tomcat cd tomcat/ sh bin/catalina.sh start * \u901a\u8fc7\u6d4f\u89c8\u5668\u8bbf\u95eehttp://172.16.11.121:8080\uff0c\u8bbf\u95ee\u7ed3\u679c\u5982\u4e0b\uff1a \u5728tomcat\u670d\u52a1\u5668\u6587\u4ef6/opt/tomcat/logs/localhost_access_log.2020-02-04.txt\u4e2d\uff0c\u5305\u62ec\u4e86\u5f53\u524d\u6700\u65b0\u7684\u8bbf\u95ee\u8bb0\u5f55\u3002","title":"tomcat\u5b89\u88c5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#nifi_4","text":"\u5728NiFi\u96c6\u7fa4\u4e0a\uff0c\u914d\u7f6enifi.properties\u6587\u4ef6\uff0c\u9700\u8981\u914d\u7f6esite to site\u5185\u5bb9\uff0c\u5f00\u542f\u7ad9\u70b9\u63a5\u6536\u670d\u52a1\u548c\u7aef\u53e3\uff0c\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\uff1a # Site to Site properties nifi.remote.input.host=172.16.13.120 nifi.remote.input.secure=false nifi.remote.input.socket.port=10000 nifi.remote.input.http.enabled=true nifi.remote.input.http.transaction.ttl=30 sec nifi.remote.contents.cache.expiration=30 secs \u91cd\u542fNiFi\u670d\u52a1\u3002","title":"NiFi\u914d\u7f6e"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_4","text":"\u5728NiFi\u4e2d\uff0c\u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a\"From MiNiFi\"\u7684input port\uff0c\u540c\u65f6\u521b\u5efa\u4e00\u4e2aFI Hdfs\u4ee5\u53caFI Kafka Processor\uff0c\u5bf9\u63a5FI \u8fc7\u7a0b\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u6307\u5bfc\u3002 \u70b9\u51fb\u67e5\u770b \u521b\u5efa\u6210\u529f\u540e\u5982\u4e0b\u56fe\uff1a \u5728NiFi\u4e2d\uff0c\u521b\u5efa\u4e00\u4e2aProcess\u3000Group\uff0c\u53cc\u51fb\u8fdb\u5165process group\uff0c\u521b\u5efa\u4e00\u4e2atailfile Processor\u548c\u4e00\u4e2aremote process group \uff0c\u5e76\u5efa\u7acb\u8fde\u63a5\uff0c\u5982\u4e0b\u56fe: TailFile\u53c2\u8003\u914d\u7f6e\uff1a Remote Proces Group\u53c2\u8003\u914d\u7f6e\uff1a \u9009\u4e2dProcess group\u4e2d\u7ec4\u4ef6\uff0c\u70b9\u51fb\u5de6\u4fa7\u6309\u94ae\u521b\u5efa\u6a21\u677f\u3002\u8be5\u6a21\u677f\u521b\u5efa\u4ee5\u540e\u5bfc\u5165\u5230MiNiFi\u4e2d\uff0c\u4f5c\u4e3aMiNiFI\u4e2d\u8fd0\u884c\u7684\u914d\u7f6e\u6587\u4ef6\u3002 \u521b\u5efa\u6210\u529f\u540e\uff0c\u5728\u53f3\u4fa7\u83dc\u5355 templates \u4e2d\uff0c\u4e0b\u8f7d\u76f8\u5e94\u7684\u6a21\u677f\u3002 \u4e0a\u4f20\u6a21\u677f\u5230\u5b89\u88c5\u4e86MiNiFi\u7684tomcat\u670d\u52a1\u5668\u4e0a\uff0c \u9700\u8981\u4fee\u6539xml\u5bf9\u5e94\u7684encoding-version\u4e3a1.2,\u6b64\u5904\u7a0b\u5e8f\u6709\u4e2aBug\uff0c\u5982\u679c\u4e0d\u4fee\u6539\uff0c\u540e\u7eed\u8f6c\u6362\u683c\u5f0f\u65f6\u4f1a\u62a5\u9519 \u3002 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06\u5176\u4ecexml\u683c\u5f0f\u8f6c\u6362\u6210yml\u683c\u5f0f\uff0c\u5e76\u62f7\u8d1d\u5230MiNiFi\u7684\u914d\u7f6e\u6587\u4ef6\u76ee\u5f55\u4e2d\u3002 sh /opt/minifi-toolkit-0.5.0/bin/config.sh transform nifitemplate.xml config.yml cp config.yml /opt/minifi-0.5.0/conf/ \u542f\u52a8MiNiFi \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u542f\u52a8MiNiFi sh /opt/minifi-0.5.0/minifi.sh start","title":"\u521b\u5efa\u6570\u636e\u6d41\u53ca\u6a21\u677f"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_5","text":"\u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u6574\u4f53\u62d3\u6251\u56fe\u5982\u4e0b\uff1a \u901a\u8fc7\u8bbf\u95eetomcat\u9996\u9875\uff0c\u5c06\u5728tomcat\u65e5\u5fd7\u4e2d\u751f\u6210\u4e00\u6761\u8bbf\u95ee\u8bb0\u5f55\u3002\u67e5\u770bhdfs\u4e2d\u662f\u5426\u751f\u6210\u5bf9\u5e94\u7684\u65e5\u5fd7\u6587\u4ef6\u3002\u5982\u4e0b\u56fe\uff1a \u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\uff0c\u67e5\u770bkafka topic\u63a5\u6536\u7684\u6570\u636e\u3002 kafka-console-consumer.sh --topic testtopic -bootstrap-server 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 --consumer.config /opt/hadoopclient/Kafka/kafka/config/consumer.properties \u7ed3\u679c\u5982\u4e0b\u56fe\uff0c\u8bf4\u660eNifi\u5df2\u5c06\u76f8\u5173\u65e5\u5fd7\u5b9e\u65f6\u53d1\u9001\u5230Kafka Topic\u4e2d\u3002","title":"\u6570\u636e\u9a8c\u8bc1"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#hl7","text":"","title":"\u573a\u666f\u4e8c: \u533b\u7597\u884c\u4e1aHL7\u683c\u5f0f\u6570\u636e\u96c6\u6210"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_6","text":"\u672c\u573a\u666f\u6f14\u793a\u5c06HL7\u683c\u5f0f\u6570\u636e\u901a\u8fc7NiFi\u8f6c\u6362\u6210json\u683c\u5f0f\uff0c\u5e76\u5b58\u50a8\u5230hdfs\u4ee5\u53cakafka\u4e2d\uff0c\u7528\u4e8e\u65e5\u5fd7\u5f52\u6863\u3001\u6279\u5904\u7406\u4ee5\u53ca\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002 \u6574\u4f53\u573a\u666f\u5982\u4e0b\u56fe\uff1a \u9996\u5148\u6a21\u62dfHL7\u683c\u5f0f\u6d4b\u8bd5\u6570\u636e\uff0c\u5c06\u6d4b\u8bd5\u6570\u636e\u53d1\u5e03\u5230Kafka\u4e2d\uff0cNiFi\u901a\u8fc7consumeKafka processor\u63a5\u6536\u6d4b\u8bd5\u6570\u636e\uff0c\u5e76\u901a\u8fc7ExtractHL7Attributes processor\u89e3\u6790\u76f8\u5173\u5c5e\u6027\uff0c\u5e76\u5229\u7528NiFi\u5185\u7f6e\u7b97\u5b50\uff0c\u5c06\u63a5\u6536\u7684kafka\u5c06\u6570\u636e\u6700\u7ec8\u8f6c\u6362\u6210json\u683c\u5f0f\uff0c\u5e76\u5b58\u50a8\u5230FusionInsight HDFS\u4e2d\uff0c\u540c\u65f6\u5c06Json\u683c\u5f0f\u7684\u6570\u636e\u53d1\u5f80Kafka\u8fdb\u884c\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002","title":"\u573a\u666f\u4ecb\u7ecd"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_7","text":"\u521b\u5efakafka topic\u3002 \u5728hadoop client\u673a\u5668\u4e0a\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efatopic kafka-topics.sh --create --zookeeper 172.16.11.21:24002/kafka --partitions 6 --replication-factor 2 --topic h7topic1; kafka-topics.sh --create --zookeeper 172.16.11.21:24002/kafka --partitions 6 --replication-factor 2 --topic h7topic2 NiFi\u6570\u636e\u6d41\u914d\u7f6e\uff1a ConsumeKafka \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a Kafka Brokers: 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 Security Protocol: SASL_PLAINTEXT Kerberos Credentials Service: KeytabCredentialsService Kerberos Service Name: kafka Topic Name: h7topic1 #\u4e4b\u524d\u521b\u5efa\u7684kafka topic\u540d\u79f0 Group ID: Demo #\u53ef\u81ea\u5b9a\u4e49 ReplcaeText \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a Search Value: <cr> Replacement Value: ${literal('\\r\\n')} ExtractHL7Attributes \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a AttributeToJson \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a JoltTransformJson \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a Jolt Specification: { \"OBX_1.UserDefinedAccessChecks\": \"OBX_1.UserDefinedAccessChecks\", \"OBR_1.OrderingProvider.FamilyName\": \"OBR_1.OrderingProvider.FamilyName\", \"MSH.MessageControlID\": \"MSH.MessageControlID\", \"OBX_1.ObservationIdentifier.Text\": \"OBX_1.ObservationIdentifier.Text\", \"MSH.SendingApplication.NamespaceID\": \"MSH.SendingApplication.NamespaceID\", \"OBR_1.UniversalServiceIdentifier.Text\": \"OBR_1.UniversalServiceIdentifier.Text\", \"MSH.ReceivingApplication.NamespaceID\": \"MSH.ReceivingApplication.NamespaceID\", \"MSH.ProcessingID.ProcessingID\": \"MSH.ProcessingID.ProcessingID\", \"uuid\": \"uuid\", \"PID.SSNNumberPatient\": \"PID.SSNNumberPatient\", \"OBR_1.FillerOrderNumber.EntityIdentifier\": \"OBR_1.FillerOrderNumber.EntityIdentifier\", \"path\": \"path\", \"PID.PatientAccountNumber.ID\": \"PID.PatientAccountNumber.ID\", \"PID.DateOfBirth\": \"PID.DateOfBirth\", \"PD1.PatientPrimaryCareProviderNameIDNo.IDNumber\": \"PD1.PatientPrimaryCareProviderNameIDNo.IDNumber\", \"PID.Sex\": \"PID.Sex\", \"MSH.MessageType.MessageType\": \"MSH.MessageType.MessageType\", \"OBX_1.ReferencesRange\": \"OBX_1.ReferencesRange\", \"OBR_1.OrderingProvider.IDNumber\": \"OBR_1.OrderingProvider.IDNumber\", \"PD1.PatientPrimaryCareProviderNameIDNo.FamilyName\": \"PD1.PatientPrimaryCareProviderNameIDNo.FamilyName\", \"OBX_1.Units.NameOfCodingSystem\": \"OBX_1.Units.NameOfCodingSystem\", \"OBX_1.Units.Identifier\": \"OBX_1.Units.Identifier\", \"filename\": \"filename\", \"PID.PatientName.GivenName\": \"PID.PatientName.GivenName\", \"OBX_1.ObservationSubID\": \"OBX_1.ObservationSubID\", \"PD1.PatientPrimaryCareProviderNameIDNo.GivenName\": \"PD1.PatientPrimaryCareProviderNameIDNo.GivenName\", \"OBR_1.PlacerOrderNumber.NamespaceID\": \"OBR_1.PlacerOrderNumber.NamespaceID\", \"MSH.MessageType.TriggerEvent\": \"MSH.MessageType.TriggerEvent\", \"PD1.PatientPrimaryCareProviderNameIDNo.AssigningAuthority\": \"PD1.PatientPrimaryCareProviderNameIDNo.AssigningAuthority\", \"OBR_1.ResultStatus\": \"OBR_1.ResultStatus\", \"PID.PatientName.FamilyName\": \"PID.PatientName.FamilyName\", \"MSH.EncodingCharacters\": \"MSH.EncodingCharacters\", \"MSH.VersionID\": \"MSH.VersionID\", \"kafka.partition\": \"kafka.partition\", \"OBR_1.UniversalServiceIdentifier.Identifier\": \"OBR_1.UniversalServiceIdentifier.Identifier\", \"OBR_1.ObservationDateTime\": \"OBR_1.ObservationDateTime\", \"OBR_1.ScheduledDateTime\": \"OBR_1.ScheduledDateTime\", \"OBX_1.ObservationIdentifier.Identifier\": \"OBX_1.ObservationIdentifier.Identifier\", \"OBR_1.OrderingProvider.GivenName\": \"OBR_1.OrderingProvider.GivenName\", \"OBR_1.SetIDObservationRequest\": \"OBR_1.SetIDObservationRequest\", \"OBR_1.ResultsRptStatusChngDateTime\": \"OBR_1.ResultsRptStatusChngDateTime\", \"OBR_1.PlacerOrderNumber.EntityIdentifier\": \"OBR_1.PlacerOrderNumber.EntityIdentifier\", \"OBX_1.NatureOfAbnormalTest\": \"OBX_1.NatureOfAbnormalTest\", \"OBX_1.SetIDOBX\": \"OBX_1.SetIDOBX\", \"MSH.FieldSeparator\": \"MSH.FieldSeparator\", \"PD1.PatientPrimaryCareProviderNameIDNo.MiddleInitialOrName\": \"PD1.PatientPrimaryCareProviderNameIDNo.MiddleInitialOrName\", \"OBX_1.Units.Text\": \"OBX_1.Units.Text\", \"OBX_1.ValueType\": \"OBX_1.ValueType\", \"kafka.offset\": \"kafka.offset\", \"PID.PatientIDInternalID.ID\": \"PID.PatientIDInternalID.ID\", \"kafka.topic\": \"kafka.topic\", \"OBX_1.ObservationValue\": \"OBX_1.ObservationValue\", \"OBR_1.OrderingProvider.MiddleInitialOrName\": \"OBR_1.OrderingProvider.MiddleInitialOrName\" } PutHDFS \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a hadooop Configuration Resource: /opt/nifi-1.10.0/hdfs-site.xml,/opt/nifi-1.10.0/conf/core-site.xml Kerberos Credentials Service: KeytabCredentialsService Directory: /nifihl7 #s\u53ef\u81ea\u5b9a\u4e49 PublishKafka \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a Kafka Brokers: 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 Security Protocol: SASL_PLAINTEXT Kerberos Credentials Service: KeytabCredentialsService Kerberos Service Name: kafka Topic Name: h7topic2 #\u4e4b\u524d\u521b\u5efa\u7684kafka topic\u540d\u79f0","title":"\u6570\u636e\u6d41\u51c6\u5907"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_8","text":"\u5f00\u542f\u4e24\u4e2ahadoop client\u547d\u4ee4\u884c\u7a97\u53e3\uff0c\u5728\u7a97\u53e3\u4e00\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a bin/kafka-console-producer.sh --broker-list 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 --topic h7topic1 --producer.config config/producer.properties \u5728\u7a97\u53e3\u4e8c\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a kafka-console-consumer.sh --topic h7topic2 -bootstrap-server 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 --consumer.config /opt/hadoopclient/Kafka/kafka/config/consumer.properties \u5728\u7a97\u53e3\u4e00\u4e2d\u7c98\u8d34\u4ee5\u4e0b\u6570\u636e,\u8be5\u6570\u636e\u4e3a\u6a21\u62df\u7684HL7\u6570\u636e\u683c\u5f0f\u3002 MSH|^~\\&|XXXXXX||HealthOrg01||||ORU^R01|Q1111111111111111111|P|2.3|<cr>PID|||000000001||SMITH^JOHN||19700101|M||||||||||999999999999|123456789|<cr>PD1||||1234567890^LAST^FIRST^M^^^^^NPI|<cr>OBR|1|341856649^HNAM_ORDERID|000000000000000000|648088^Basic Metabolic Panel|||20150101000100|||||||||1620^Johnson^John^R||||||20150101000100|||M|||||||||||20150101000100|<cr>OBX|1|NM|GLU^Glucose Lvl|159|mg/dL|65-99^65^99|H|||F|||20150101000100| \u5982\u4e0b\u56fe\uff1a \u5728\u7a97\u53e3\u4e8c\u4e2d\u67e5\u770b\u7ed3\u679c\uff0c\u53d1\u73b0\u5df2\u7ecf\u8bfb\u53d6\u5230\u8f6c\u6362\u6210json\u683c\u5f0f\u7684\u6570\u636e\uff0c\u6570\u636e\u52a0\u8f7d\u5230kafka\u4e2d\u6210\u529f\uff0c\u53ef\u4ee5\u7528\u4e8e\u540e\u7eed\u7684\u6d41\u5f0f\u6570\u636e\u5904\u7406\uff0c\u5982flink\u7b49\u3002 \u67e5\u770b\u5728 hdfs \u4e2d\u5b58\u50a8\u7684\u6587\u4ef6\uff0c\u5982\u4e0b\u56fe\uff0c\u5df2\u7ecf\u67e5\u8be2\u5230\u5bf9\u5e94\u7684\u6587\u4ef6\uff0c\u8be5\u6570\u636e\u53ef\u4ee5\u505a\u4e3a\u539f\u59cb\u6570\u636e\u5f52\u6863\uff0c\u4ee5\u53ca\u6279\u91cf\u6570\u636e\u5206\u6790\u3002","title":"\u6570\u636e\u6d4b\u8bd5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#iotmqttkafkahdfs","text":"","title":"\u573a\u666f\u4e09\uff1aIOT\u573a\u666f\u5b9e\u73b0MQTT\u6570\u636e\u96c6\u6210\u5230Kafka\u548chdfs"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_9","text":"\u672c\u573a\u666f\u6a21\u62dfNiFi\u63a5\u6536IOT MQTT\u683c\u5f0f\u6570\u636e\uff0c\u5e76\u5b58\u50a8\u5230hdfs\u4ee5\u53cakafka\u4e2d\uff0c\u7528\u4e8e\u540e\u7eed\u7684\u6d41\u5f0f\u5904\u7406\u548c\u6570\u636e\u5f52\u6863\u3002 \u573a\u666f\u793a\u610f\u56fe\u5982\u4e0b\uff1a","title":"\u573a\u666f\u4ecb\u7ecd"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_10","text":"\u672c\u573a\u666f\u901a\u8fc7\u5728Linux Centos\u4e0a\u5b89\u88c5mosquitto\u6a21\u62dfIOT broker\uff0c\u901a\u8fc7IoT Broker\u5c06\u6570\u636e\u8f6c\u53d1\u5230NiFi\uff0c\u7528\u4e8e\u540e\u7eed\u5c06\u6570\u636e\u5b58\u50a8\u5230HDFS\u6216\u8005kafka\uff0c\u7528\u4e8e\u5f52\u6863\u3001\u6279\u5904\u7406\u6216\u8005\u6d41\u5f0f\u8ba1\u7b97\u3002 \u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5728centos\u670d\u52a1\u5668\u4e0a\u5b89\u88c5mqtt: yum -y install eple-release yum -y install mosquitto systemctl start mosquitto systemctl enable mosquitto yum install python-pip pip install paho-mqtt useradd huawei mosquitto_passwd -c /etc/mosquitto/passwd huawei \u4fee\u6539\u914d\u7f6e\u6587\u4ef6/etc/mosquitto/mosquitto.conf\uff0c\u7981\u7528MQTT\u533f\u540d\u767b\u5f55\u3002 allow_anonymous false password_file /etc/mosuitto/passwd \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u91cd\u542fmosquitto\u670d\u52a1 ``` systemctl restart mosquitto ```` \u5728hadoop client\u4e0a\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u521b\u5efakafka topic kafka-topics.sh --create --zookeeper 172.16.11.21:24002/kafka --partitions 6 --replication-factor 2 --topic mqtttopic","title":"\u73af\u5883\u51c6\u5907"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_11","text":"\u521b\u5efa\u5982\u4e0bNiFI\u6570\u636e\u6d41\uff0c\u5982\u4e0b\u56fe\uff1a ConsumeMQTT Processor\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a PutHdfs Processor\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a hadooop Configuration Resource: /opt/nifi-1.10.0/hdfs-site.xml,/opt/nifi-1.10.0/conf/core-site.xml Kerberos Credentials Service: KeytabCredentialsService Directory: /nifimqtt PublishKafka Processor\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\uff1a Kafka Brokers: 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 Security Protocol: SASL_PLAINTEXT Kerberos Credentials Service: KeytabCredentialsService Kerberos Service Name: kafka Topic Name: mqtttopic #\u4e4b\u524d\u521b\u5efa\u7684kafka topic\u540d\u79f0","title":"\u6570\u636e\u6d41\u51c6\u5907"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90NiFi%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_12","text":"\u5728hadoop client\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u76d1\u542ckafka topic\u7684\u8f93\u51fa\u3002 kafka-console-consumer.sh --topic testtopic -bootstrap-server 172.16.11.21:21007,172.16.11.22:21007,172.16.11.23:21007 --consumer.config /opt/hadoopclient/Kafka/kafka/config/consumer.properties \u5728mqqtt\u5ba2\u6237\u7aef\uff0c\u521b\u5efamqttpublish.py\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a import paho.mqtt.publish as publish import time auth = { 'username': 'huawei', 'password': 'huawei' } for i in range(10): publish.single('test', payload='Test message %d' % i, auth=auth, hostname='172.16.13.112',port=1883) time.sleep(1) \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u53d1\u9001\u6d88\u606f\u5230MQtt Brokers python mqttpublish.py \u67e5\u770bkafka consumer\u663e\u793a\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u5982\u4e0b\u56fe\uff1a \u67e5\u770bhdfs\u6587\u4ef6 mqtt\u6570\u636e\u5df2\u6210\u529f\u540c\u6b65\u5230kafka\u548chdfs\uff0c\u9a8c\u8bc1\u5b8c\u6210\u3002","title":"\u6570\u636e\u9a8c\u8bc1"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","text":"FusionInsight\u96c6\u6210ROMA\u5b9e\u8df5 \u00b6 ROMA\u4ecb\u7ecd \u00b6 ROMA\u7b80\u4ecb \u00b6 ROMA\u662f\u591a\u4e91\u65f6\u4ee3\u7684\u878d\u5408\u96c6\u6210\u5e73\u53f0\uff0c\u6e90\u81ea\u534e\u4e3a10+\u5e74\u6570\u5b57\u5316\u8f6c\u578b\u5b9e\u8df5\uff0c\u805a\u7126A\uff08Application\uff09\u3001B\uff08Business\uff09\u3001C\uff08Cloud\uff09\u3001D\uff08Device\uff09\u56db\u7c7b\u96c6\u6210\u573a\u666f\uff0c\u63d0\u4f9b\u5feb\u901f\u3001\u7b80\u5355\u7684\u6d88\u606f\u3001\u6570\u636e\u3001\u670d\u52a1\u3001\u8bbe\u5907\u96c6\u6210\u80fd\u529b\uff0c\u7b80\u5316\u4f01\u4e1a\u4e0a\u4e91\uff0c\u652f\u6301\u4e91\u4e0a\u4e91\u4e0b\u3001\u8de8\u533a\u57df\u96c6\u6210\uff0c\u6253\u901aIT\u4e0eOT\uff0c\u8fde\u63a5\u4f01\u4e1a\u4e0e\u751f\u6001\u4f19\u4f34\uff0c\u52a9\u529b\u884c\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\u3002 ROMA\u81f4\u529b\u4e8e\u89e3\u51b3\u884c\u4e1a\u5728\u5411\u6570\u5b57\u5316\u8f6c\u578b\u7684\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u96be\u9898\uff1a \u7f3a\u5c11\u7edf\u4e00\u7684\u8bbe\u5907\u4fe1\u606f\u96c6\u6210\u9014\u5f84\uff1b \u6570\u636e\u683c\u5f0f\u591a\u6837\u5316\uff0c\u96be\u4ee5\u4f20\u8f93\u548c\u96c6\u6210\uff1b \u7f3a\u5c11\u4e0e\u5408\u4f5c\u4f19\u4f34\u5206\u4eab\u6570\u636e\u548c\u540e\u7aef\u670d\u52a1\u7684\u4fbf\u6377\u9014\u5f84\uff1b \u7f3a\u5c11\u4e91\u4e0a\u4e91\u4e0b\u8de8\u7f51\u7edc\u7684\u5b89\u5168\u4fe1\u606f\u901a\u9053\u3002 ROMA\u5b9a\u4f4d \u00b6 ROMA\u5b9a\u4f4d\u4e3a\u4fbf\u6377\u7684\u96c6\u6210\u5e73\u53f0\uff0c\u63d0\u4f9b\u6570\u636e\u63a5\u5165\u3001\u6570\u636e\u96c6\u6210\u4ee5\u53ca\u6570\u636e\u670d\u52a1API\u6784\u5efa\u7684\u80fd\u529b\u3002ROMA\u4e0d\u662f\u4f20\u7edf\u7684ETL\u5de5\u5177\uff0c\u63d0\u4f9b\u76f8\u6bd4ETL\u5de5\u5177\u66f4\u4e3a\u4e30\u5bcc\u7684\u6570\u636e\u6e90\u652f\u6301\u7c7b\u578b\u4ee5\u53ca\u66f4\u4e3a\u4e30\u5bcc\u7684\u529f\u80fd\u3002ROMA\u4e0d\u662f\u5927\u6570\u636e\u5e73\u53f0\uff0cROMA\u652f\u6301\u5c06\u6765\u81ea\u4e0d\u540c\u7cfb\u7edf\u7684\u591a\u79cd\u7c7b\u578b\u53ca\u683c\u5f0f\u7684\u6570\u636e\u540c\u6b65\u5230FusionInsight\uff0c\u5b9e\u73b0IT\u548cOT\u6570\u636e\u6c47\u805a\uff0c\u7ecf\u8fc7FusionInsight\u6570\u636e\u5206\u6790\u540e\u7684\u6570\u636e\u5bfc\u5165\u5230\u6570\u636e\u96c6\u5e02\uff0cROMA\u901a\u8fc7APIC LiveData \u5bf9\u5916\u63d0\u4f9bRestful\u7684\u6570\u636e\u670d\u52a1\u63a5\u53e3\u3002 ROMA\u7ec4\u4ef6\u53ca\u529f\u80fd\u4ecb\u7ecd \u00b6 ROMA \u4e3b\u8981\u7531\u56db\u5927\u7ec4\u4ef6\u7ec4\u6210\uff1a FDI: \u65e8\u5728\u89e3\u51b3\u591a\u79cd\u6570\u636e\u6e90\u7684\u5feb\u901f\u7075\u6d3b\u96c6\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u4efb\u610f\u65f6\u95f4\u3001\u4efb\u610f\u5730\u70b9\u3001\u4efb\u610f\u7cfb\u7edf\u4e4b\u95f4\u5b9e\u73b0\u5b9e\u65f6\u6570\u636e\u8ba2\u9605\u548c\u5b9a\u65f6\u589e\u91cf\u6570\u636e\u8fc1\u79fb\u3002FDI\u662fFusionInsight\u6570\u636e\u96c6\u6210\u4f7f\u7528\u7684\u6700\u4e3b\u8981\u7684\u529f\u80fd\u7ec4\u4ef6\uff0c\u901a\u8fc7FDI\u5b9e\u73b0\u591a\u79cd\u6570\u636e\u6e90\u5b9e\u65f6\u6216\u8005\u589e\u91cf\u540c\u6b65\u5230\u6570\u636e\u6e56\u3002 LINK:\u8bbe\u5907\u96c6\u6210\u670d\u52a1\uff0c\u4f7f\u7528MQTT\u6807\u51c6\u534f\u8bae\u8fde\u63a5\u8bbe\u5907\uff0c\u5b9e\u73b0\u8bbe\u5907\u5feb\u901f\u63a5\u5165\u3001\u6570\u636e\u91c7\u96c6\u7b49\u7269\u8054\u7f51\u5e94\u7528\u3002\u7528\u6237\u53ef\u4ee5\u5728\u63a7\u5236\u53f0\u914d\u7f6e\u89c4\u5219\u5f15\u64ce\uff0c\u5b9e\u73b0\u8bbe\u5907\u5c06Topic\u7ea7\u522b\u7684\u6d88\u606f\u8f6c\u53d1\u5230\u4e0d\u540c\u7684\u670d\u52a1\u4e2d\uff0c\u5982\u6d88\u606f\u961f\u5217\u670d\u52a1\uff08MQS\uff09\u548cFI KAFKA\u7b49\u3002\u540c\u65f6\u652f\u6301\u914d\u7f6e\u4f7f\u7528\u7c7bSQL\u7684\u89c4\u5219\u8bed\u8a00\u5bf9\u8f6c\u53d1\u7684\u6d88\u606f\u8fdb\u884c\u5904\u7406\u548c\u7b5b\u9009\uff0c\u6ee1\u8db3\u4e0d\u540c\u4e1a\u52a1\u5bf9\u8f6c\u53d1\u6570\u636e\u5185\u5bb9\u7684\u8981\u6c42\uff0c\u5b9e\u73b0\u4e1a\u52a1\u903b\u8f91\u4e0e\u5e94\u7528\u7a0b\u5e8f\u7684\u4f4e\u8026\u5408\u3002 \u670d\u52a1\u96c6\u6210\uff1aAPI\u96c6\u6210\u670d\u52a1\u5b9e\u73b0API\u5f00\u53d1\u7f16\u6392\uff0c\u652f\u6301\u51fd\u6570API\u4ee5\u53ca\u6570\u636eAPI\uff0c\u652f\u6301\u901a\u8fc7\u7f16\u5199SQL\u811a\u672c\u7684\u65b9\u5f0f\uff0c\u5c06\u6570\u636e\u5e93\u63d0\u4f9b\u7684\u6570\u636e\u670d\u52a1\u8f6c\u6362\u4e3aREST API\u7684\u80fd\u529b\u3002 MQS\u6d88\u606f\u96c6\u6210\u670d\u52a1\uff1a\u63d0\u4f9b\u4e86\u6d88\u606f\u7ba1\u7406\u80fd\u529b ROMA FDI\u652f\u6301\u7684\u6570\u636e\u6e90 \u00b6 ROMA FDI\u652f\u6301\u7684\u6570\u636e\u6e90\u5982\u4e0b\uff1a ROMA\u5b89\u88c5\u914d\u7f6e \u00b6 \u7248\u672c\u914d\u5957\u5173\u7cfb \u00b6 FusionInsight 6.5.1\u914d\u5957\u7684ROMA\u7248\u672c\u4e3a ROMA 20.0.RC1 \u5b89\u88c5\u6307\u5bfc\u6587\u6863 \u00b6 \u53c2\u8003\u4ee5\u4e0b\u94fe\u63a5\u8bbf\u95eeROMA\u4ea7\u54c1\u6587\u6863\uff1a \u70b9\u51fb\u8bbf\u95ee \u5b89\u88c5\u89c4\u5212 \u00b6 \u5b89\u88c5\u89c4\u5212\u53ef\u53c2\u8003ROMA\u6587\u6863\u7ae0\u8282\uff1a\u201c\u5b89\u88c5\u4e0e\u8c03\u6d4b\u201d->\u201c\u8f6f\u4ef6\u5b89\u88c5\u201d->\u201c\u5b89\u88c5\u6982\u8ff0\u201d->\u201d\u5b89\u88c5\u89c4\u5212\u201d\u3002ROMA\u5b89\u88c5\u90e8\u7f72\u67093\u79cd\u7ec4\u7f51\u6a21\u5f0f\uff0c\u533a\u522b\u548c\u9009\u7528\u5efa\u8bae\u5982\u4e0b\uff1a \u90e8\u7f72\u6a21\u5f0f \u90e8\u7f72\u65b9\u6848\u63cf\u8ff0 \u9009\u7528\u5efa\u8bae 8VM\u57fa\u7840\u7ec4\u7f51 \u6700\u5c0f\u57fa\u7840\u90e8\u7f72\u6a21\u5f0f\uff0c\u5305\u542bFoundation\u3001APIC\u3001FDI\u3001MQS\u3001\u7edf\u4e00\u8fd0\u7ef4\u7b49\u57fa\u7840\u80fd\u529b\uff0cFDI-READER\u548cFDI-WRITER\u91c7\u7528\u5355\u673a\u90e8\u7f72\u3002 \u9002\u7528\u4e8e\u6d4b\u8bd5\u73af\u5883 13VM\u9ad8\u53ef\u9760\u7ec4\u7f51 \u76f8\u6bd4\u57fa\u7840\u7ec4\u7f51\uff0cFDI-READER\u548cFDI-WRITER\u5404\u4f7f\u75282\u53f0VM\u8fdb\u884c\u96c6\u7fa4\u90e8\u7f72\u6a21\u5f0f\uff0c\u63d0\u4f9b\u8d1f\u8f7d\u5747\u8861\u80fd\u529b\uff0c\u652f\u6301\u66f4\u9ad8\u7684\u6570\u636e\u8bbf\u95ee\u80fd\u529b\u3002\u6ce8\uff1a\u4e0d\u5305\u62ecLink\u7ec4\u4ef6\u3002 \u9002\u7528\u4e8e\u4e0d\u9700\u8981Link\u7ec4\u4ef6\u7684\u751f\u4ea7\u73af\u5883 20VM\u9ad8\u53ef\u9760\u7ec4\u7f51 \u76f8\u6bd413VM\u9ad8\u53ef\u9760\u7ec4\u7f51\u6a21\u5f0f\uff0c\u589e\u52a0\u4e867\u4e2a\u8282\u70b9\u7528\u4e8e\u90e8\u7f72LINK\u670d\u52a1 \u9700\u8981\u4f7f\u7528LINK\u7ec4\u4ef6\u7684\u751f\u7acb\u73af\u5883\u9009\u7528\u8be5\u65b9\u6848 \u5b89\u88c5\u6ce8\u610f\u4e8b\u9879 \u00b6 \u53c2\u8003ROMA\u4ea7\u54c1\u6587\u6863 \u5b89\u88c5\u8c03\u6d4b \u7ae0\u8282\u8fdb\u884c\u5b89\u88c5\u3002 \u5b89\u88c5\u6ce8\u610f\u4e8b\u9879\uff1a ROMA\u4ea7\u54c1\u6587\u6863 \u5b89\u88c5\u8c03\u6d4b -> \u8f6f\u4ef6\u5b89\u88c5 -> \u5b89\u88c5ROMA \u7ae0\u8282\u6b65\u9aa4 12 \u4e2d\uff0c\u63d0\u4f9b\u4e86\u914d\u7f6eFusionInsight 6.5.1 \u4f9d\u8d56\u5305\u7684\u6307\u5bfc\u3002FI Hive\u3001FI HDFS\u53caFI Kafka \u9700\u8981\u7684\u4f9d\u8d56\u5305\u89c1ROMA\u5b89\u88c5\u6587\u6863 \u5b89\u88c5\u4e0e\u8c03\u6d4b -> \u8f6f\u4ef6\u5b89\u88c5 -> \u5b89\u88c5\u524d\u51c6\u5907 -> \u83b7\u53d6\u8f6f\u4ef6\u5305 \u3002 \u5176\u4e2dfihdfsreader/fihdfswriter\u4f9d\u8d56\u7684FusionInsight HD 6.5.1 \u6e05\u5355\u5982\u4e0b,\u76f8\u6bd4ROMA\u4ea7\u54c1\u6587\u6863 \u4e2d\u6d89\u53ca\u65b0\u589e\u90e8\u5206\u89c1\u659c\u4f53\u5185\u5bb9\uff1a commons-collections-3.2.2.jar commons-configuration2-2.1.1.jar hadoop-auth-3.1.1.jar hadoop-common-3.1.1.jar hadoop-hdfs-3.1.1.jar hadoop-hdfs-client-3.1.1.jar hadoop-hdfs-httpfs-3.1.1.jar htrace-core4-4.1.0-incubating.jar protobuf-java-2.5.0.jar re2j-1.1.jar stax2-api-3.1.4.jar woodstox-core-5.0.3.jar zookeeper-3.5.1.jar hadoop-mapreduce-client-core-3.1.1.jar fihivereader/fihiverwriter\u4f9d\u8d56\u7684FusionInsight HD 6.5.1 \u6e05\u5355\u5982\u4e0b,\u76f8\u6bd4ROMA\u4ea7\u54c1\u6587\u6863 \u4e2d\u6d89\u53ca\u65b0\u589e\u90e8\u5206\u89c1\u659c\u4f53\u5185\u5bb9\uff1a commons-configuration2-2.1.1.jar hadoop-auth-3.1.1.jar hadoop-common-3.1.1.jar hadoop-hdfs-3.1.1.jar hadoop-hdfs-client-3.1.1.jar hadoop-hdfs-httpfs-3.1.1.jar hive-common-3.1.0.jar hive-jdbc-3.1.0.jar hive-metastore-3.1.0.jar hive-serde-3.1.0.jar hive-service-3.1.0.jar hive-service-rpc-3.1.0.jar hive-shims-common-3.1.0.jar hive-standalone-metastore-3.1.0.jar htrace-core4-4.1.0-incubating.jar protobuf-java-2.5.0.jar re2j-1.1.jar stax2-api-3.1.4.jar woodstox-core-5.0.3.jar commons-collections-3.2.2.jar hive-exec-3.1.0.jar zookeeper-3.5.1.jar commons-collections-3.2.2.jar hadoop-mapreduce-client-core-3.1.1.jar \u5bf9\u5e94jar\u5305\u5728FusionInsight 6.5.1\u4e2d\u7684\u4f4d\u7f6e\u5982\u4e0b\u8868\uff1a jar\u6587\u4ef6\u540d \u83b7\u53d6\u8def\u5f84 hive-exec-3.1.0.jar /opt/hadoopclient/Hive/Beeline/lib/hive-exec-3.1.0.jar zookeeper-3.5.1.jar /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar commons-collections-3.2.2.jar /opt/hadoopclient/HDFS/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar hadoop-mapreduce-client-core-3.1.1.jar /opt/hadoopclient/Hive/Beeline/lib/hadoop-mapreduce-client-core-3.1.1.jar \u66ff\u6362FusionInsight\u4f7f\u7528\u7684guava\u7248\u672c,\u4eceguava 28\u66ff\u6362\u6210guava 26\u7248\u672c. guava 26\u7248\u672c\u4e0b\u8f7d\u5730\u5740\uff1a \u70b9\u51fb\u4e0b\u8f7d ,\u4e0b\u8f7d\u540e\u653e\u5230VM5 fihivereader/fihdfsreader/fikafkareader\u4ee5\u53caVM6 fihivewriter/fihdfswriter/fikafkawriter\u76ee\u5f55\u4e0b\uff0c\u5220\u9664\u539fguava-28 jar\u5305\u3002 \u4fee\u6539guava-26.0-jre.jar\u7684\u6240\u6709\u8005\u53ca\u6743\u9650\uff1a chown romafdi:users guava-26.0-jre.jar chmod 755 guava-26.0-jre.jar \u91cd\u542ffi reader/writer\u670d\u52a1\u4f7fjar\u5305\u66ff\u6362\u751f\u6548\uff0c\u4ee5\u4e0b\u4e3a\u66ff\u6362hivewriter guava\u7248\u672c\u540e\u91cd\u542f\u670d\u52a1\u7684\u547d\u4ee4\uff0c\u91cd\u542fhdfs/hive/kafa\u7684reader\u548cwriter\u7684\u547d\u4ee4\u7c7b\u4f3c\uff0c\u66ff\u6362 tomcat-fihivewriter \u4e3a\u5bf9\u5e94\u8def\u5f84\u5373\u53ef\u3002 su - romafdi cd /data01/fdi/tomcat-fihivewriter/bin sh catalina.sh stop;sh catalina.sh star ROMA\u5bf9\u63a5FusionInsight\u914d\u7f6e\u6307\u5bfc \u00b6 ROMA FDI\u5bf9\u63a5HIVE \u00b6 ROMA FDI\u6dfb\u52a0HIVE\u6570\u636e\u6e90\u53c2\u6570\u8bf4\u660e table th:first-of-type{width:100px;} | \u53c2\u6570\u540d\u79f0 | \u914d\u7f6e\u53c2\u8003 | | :------------- | :------------- | | \u5e94\u7528\u540d\u79f0 | \u4e0b\u62c9\u9009\u9879\u4e2d\u9009\u62e9\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u8bf4\u660e\uff1a \u82e5\u6ca1\u6709\u5e94\u7528\u540d\u53ef\u4ee5\u5173\u8054\uff0c\u5219\u5728ROMA Portal\u83dc\u5355\u680f\uff0c\u9009\u62e9\u201c\u5f53\u524d\u5e94\u7528\u540d > \u5e94\u7528\u6ce8\u518c\u201d\uff0c\u6ce8\u518c\u7528\u6237\u7684\u5e94 \u7528\u540d\u3002 |\u6570\u636e\u6e90\u540d\u79f0|\u7528\u6237\u81ea\u5b9a\u4e49\u3002\u4f8b\u5982\u201croma_fihive_test\u201d\u3002\u6570\u636e\u6e90\u6dfb\u52a0\u6210\u529f\u540e\uff0c\u5728\u521b\u5efa\u96c6\u6210\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u5728\u4e0b\u62c9\u9009\u9879\u4e2d\u81ea\u52a8\u5173\u8054\u5230\u6570\u636e\u6e90\u540d\u79f0\u3002| \u6570\u636e\u6e90\u7c7b\u578b| \u6570\u636e\u5e93\u7684\u7c7b\u578b\uff0c\u9009\u62e9\u201cFI Hive\u201d\u3002| \u5730\u533a | \u6570\u636e\u5e93\u6240\u5728\u7684\u5730\u57df\uff0c\u4f8b\u5982\u201cshenzhen\u201d\u3002| HDFS\u76ee\u5f55| \u4e34\u65f6HDFS\u6587\u4ef6\u5b58\u653e\u8def\u5f84\u3002 \u4f8b\u5982\u201c/user/ico_ipass/\u201d\u3002| FI HD\u7528\u6237\u540d | \u7528\u6237\u8ba4\u8bc1\u540d\u79f0\u3002\u4f8b\u5982\u201cioc_ipass\u201d\u3002 | \u7248\u672c\u53f7 | FI HD\u7684\u7248\u672c\u53f7\u3002\u4f8b\u5982\u201cFI HD V100R002C80U20\u201d\u3002| \u4e0a\u4f20\u914d\u7f6e\u6587\u4ef6 | \u5305\u542b\u7528\u6237\u8ba4\u8bc1\u6587\u4ef6\uff08krb5.conf\u3001user.keytab\uff09\u3001Hive\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff08fihiveclient.properties\uff09\u548cHDFS\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff08core-site.xml\u3001hdfs-site.xml\uff09\u3002\u5c065\u4e2a\u6587\u4ef6\u4e00\u8d77\u76f4\u63a5\u538b\u7f29\u6210ZIP\u5305\uff0c\u4f8b\u5982\u201cFIHive_dqf_ioc_ipass.zip\u201d\u3002 \u7528\u6237\u8ba4\u8bc1\u6587\u4ef6\u7684\u4e0b\u8f7d\u65b9\u6cd5\uff1a \u767b\u5f55FusionInsight Manager\u3002 \u5355\u51fb\u201c\u7cfb\u7edf\u8bbe\u7f6e\u201d\uff0c\u8fdb\u5165\u7cfb\u7edf\u8bbe\u7f6e\u9875\u9762\u3002 \u9009\u62e9\u201c\u7cfb\u7edf\u8bbe\u7f6e > \u914d\u7f6e > \u6743\u9650\u914d\u7f6e > \u7528\u6237\u7ba1\u7406\u201d\u3002 \u5728\u5bf9\u5e94\u7528\u6237\u7684\u6700\u53f3\u4fa7\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u4e0b\u8f7d\u7528\u6237\u51ed\u636e Hive\u3001HDFS\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u7684\u4e0b\u8f7d\u65b9\u6cd5\uff1a \u767b\u5f55FusionInsight Manager\u3002 \u5355\u51fb\u201c\u670d\u52a1\u7ba1\u7406\u201d\uff0c\u8fdb\u5165\u670d\u52a1\u9875\u9762\u3002 \u5355\u51fb\u9700\u8981\u7684FI\u670d\u52a1\uff0c\u8fdb\u5165\u201c\u670d\u52a1 > Hive \u670d\u52a1\u72b6\u6001\u201d\u9875\u9762\u3002 \u5355\u51fb\u201c\u4e0b\u8f7d\u5ba2\u6237\u7aef\u201d\uff0c\u8fdb\u5165\u201c\u4e0b\u8f7d\u5ba2\u6237\u7aef\u201d\u9875\u9762\u3002 \u8bbe\u7f6e\u201c\u5ba2\u6237\u7aef\u7c7b\u578b\u201d\u4e3a\u201c\u4ec5\u914d\u7f6e\u6587\u4ef6\u201d\u3002 \u5355\u51fb\u201c\u786e\u5b9a\u201d\u3002 \u4eceFi Hive\u4e0b\u8f7d\u6587\u4ef6\u4e2d\u83b7\u53d6\u5230hiveclient.properties\u6587\u4ef6\uff0c\u5e76\u6539\u6587\u4ef6\u540d\u4e3afihiveclient.properties\u3002\u4eceHDFS\u4e0b\u8f7d\u6587\u4ef6\u4e2d\u83b7\u53d6core-site.xml\u3001hdfs-site.xml\u6587\u4ef6\u3002| \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a ROMA FDI\u5bf9\u63a5HDFS \u00b6 \u53c2\u6570\u540d\u79f0 \u914d\u7f6e\u53c2\u8003 \u5e94\u7528\u540d\u79f0 \u4e0b\u62c9\u9009\u9879\u4e2d\u9009\u62e9\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u82e5\u6ca1\u6709\u5e94\u7528\u540d\u53ef\u4ee5\u5173\u8054\uff0c\u5219\u5728ROMA Portal\u83dc\u5355\u680f\uff0c\u9009\u62e9\u201c\u5f53\u524d\u5e94\u7528\u540d > \u5e94\u7528\u6ce8\u518c\u201d\uff0c\u6ce8\u518c\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u6570\u636e\u6e90\u540d\u79f0 \u7528\u6237\u81ea\u5b9a\u4e49\u3002\u4f8b\u5982\u201croma_fihdfs_test\u201d\u3002 \u6570\u636e\u6e90\u6dfb\u52a0\u6210\u529f\u540e\uff0c\u5728\u521b\u5efa\u96c6\u6210\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u5728\u4e0b\u62c9\u9009\u9879\u4e2d\u81ea\u52a8\u5173\u8054\u5230\u6570\u636e\u6e90\u540d\u79f0\u3002 \u6570\u636e\u6e90\u7c7b\u578b \u6570\u636e\u5e93\u7684\u7c7b\u578b\uff0c\u9009\u62e9\u201cFIHDFS\u201d\u3002 Region \u6570\u636e\u5e93\u6240\u5728\u7684\u5730\u57df\uff0c\u4f8b\u5982\u201cshenzhen\u201d hdfsUrl \u4e34\u65f6HDFS\u6587\u4ef6\u5b58\u653e\u8def\u5f84\u3002 \u4f8b\u5982\u201c/user/ico_ipass/\u201d\u3002 principlename \u7528\u6237\u8ba4\u8bc1\u540d\u79f0\u3002\u4f8b\u5982\u201ceip_fdi_hdfs\u201d\u3002 \u7248\u672c\u53f7 FI HD\u7684\u7248\u672c\u53f7\u3002\u4f8b\u5982\u201cFI HD V100R002C80U20\u201d,\u6ce8:\u7531\u4e8e\u540e\u7aef\u5b9e\u9645\u4f7f\u7528\u7684\u662fFI 6.5.1 jar\u6587\u4ef6,\u9009\u62e9\u8be5\u7248\u672c\u53ef\u4ee5\u652f\u6301\u5bf9\u63a5FI 6.5.1 \u4e0a\u4f20\u914d\u7f6e\u6587\u4ef6 \u5305\u542b\u7528\u6237\u8ba4\u8bc1\u6587\u4ef6\uff08krb5.conf\u3001user.keytab\uff09\u548cHDFS\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff08core-site.xml\u3001hdfs-site.xml\uff09\u3002\u5c064\u4e2a\u6587\u4ef6\u4e00\u8d77\u76f4\u63a5\u538b\u7f29\u6210ZIP\u5305\uff0c\u4f8b\u5982\u201ceip_fdi_hdfs.zip\u201d\u3002\u83b7\u53d6\u65b9\u6cd5\u548cFI Hive\u7c7b\u4f3c\u3002 \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a ROMA FDI\u5bf9\u63a5KAFKA \u00b6 \u53c2\u6570\u540d\u79f0 \u5982\u4f55\u914d\u7f6e \u5e94\u7528\u540d\u79f0 \u4e0b\u62c9\u9009\u9879\u4e2d\u9009\u62e9\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u8bf4\u660e\uff1a \u82e5\u6ca1\u6709\u5e94\u7528\u540d\u53ef\u4ee5\u5173\u8054\uff0c\u5219\u5728ROMA Portal\u83dc\u5355\u680f\uff0c\u9009\u62e9\u201c\u5f53\u524d\u5e94\u7528\u540d > \u5e94\u7528\u6ce8\u518c\u201d\uff0c\u6ce8\u518c\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u6570\u636e\u6e90\u540d\u79f0 \u7528\u6237\u81ea\u5b9a\u4e49\u3002\u4f8b\u5982\u201croma_fikafka_test\u201d\u3002 \u6570\u636e\u6e90\u6dfb\u52a0\u6210\u529f\u540e\uff0c\u5728\u521b\u5efa\u96c6\u6210\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u5728\u4e0b\u62c9\u9009\u9879\u4e2d\u81ea\u52a8\u5173\u8054\u5230\u6570\u636e\u6e90\u540d\u79f0\u3002 \u6570\u636e\u6e90\u7c7b\u578b \u6570\u636e\u5e93\u7684\u7c7b\u578b\uff0c\u9009\u62e9\u201cFI Kafka\u201d\u3002 Region \u6570\u636e\u5e93\u6240\u5728\u7684\u5730\u57df\uff0c\u4f8b\u5982\u201cshenzhen\u201d\u3002 \u8bc1\u4e66\u540d\u79f0 \u8bbf\u95eeKafka\u670d\u52a1\u5668\u7684\u8bc1\u4e66\u540d\uff08\u5373\u8bbf\u95eekafka\u673a\u5668\u7684\u673a\u673a\u7528\u6237\u540d\uff09\u3002\u4f8b\u5982\u201cioc\u201d\u3002 -------- ------------------------------------------------------------------- \u4e0a\u4f20\u914d\u7f6e\u6587\u4ef6 \u5305\u542b\u7528\u6237\u8ba4\u8bc1\u6587\u4ef6\uff08krb5.conf\u3001user.keytab\uff09\u548c\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff08fikafka-producer.properties\u3001fikafka-consumer.properties\uff09\u3002\u4f8b\u5982\u201cFIKafka_ioc.zip\u201d\u3002\u83b7\u53d6\u65b9\u6cd5\u548cFI Hive\u7c7b\u4f3c\uff0c\u4e0b\u8f7d\u540e\u9700\u89e3\u538b\u914d\u7f6e\u6587\u4ef6\uff0c\u91cd\u547d\u540dproducer.properties\u3001consumer.properties \u4e3afikafka-producer.properties\u3001fikafka-consumer.properties,\u4ece\u201cfikafka-producer.properties\u201d\u6587\u4ef6\u4e2d\u62f7\u8d1d\u5982\u4e0b\u5185\u5bb9\u5230\u201cfikafka-consumer.properties\u201d\u6587\u4ef6\u4e2d\uff1abootstrap.servers = 192.168.1.178:21007,192.168.1.125:21007,192.168.1.243:21007,192.168.1.133:21007 ROMA \u5bf9\u63a5FusionInsight HDFS\u3001Hive\u53caKafka\u793a\u4f8b\u573a\u666f \u00b6 \u793a\u4f8b\u573a\u666f\u8bf4\u660e \u00b6 \u793a\u4f8b\u76ee\u7684 \u901a\u8fc7\u7aef\u5230\u7aef\u7684\u529f\u80fd\u793a\u4f8b\uff0c\u8ba9\u7528\u6237\u4e86\u89e3ROMA\u7684\u4e3b\u8981\u529f\u80fd\u53ca\u4f7f\u7528\u573a\u666f\u3002\u4e3b\u8981\u793a\u4f8b\u4ee5\u4e0b\u51e0\u65b9\u9762\u7684\u80fd\u529b\uff1a ROMA\u652f\u6301IOT\u63a5\u5165\u7684\u80fd\u529b\uff1aLINK\u7ec4\u4ef6\u652f\u6301\u63a5\u5165MQTT\u534f\u8bae\u8bbe\u5907\u3002 ROMA\u652f\u6301\u591a\u79cd\u6570\u636e\u6e90\u63a5\u5165\u5230 FusionInsight HIVE/HDFS, \u5305\u62ecAPI\u3001IOT\u3001MQS\u3001jdbc\u7b49\u3002 \u901a\u8fc7ROMA LiveData API\u670d\u52a1\uff0c\u793a\u4f8b\u5feb\u901f\u5f00\u653e\u6570\u636e\u670d\u52a1\u63a5\u53e3\u80fd\u529b\u3002 \u901a\u8fc7\u4e0d\u540c\u5e94\u7528\u95f4\u7684\u6570\u636e\u6c47\u805a\u5230\u6570\u636e\u6e56\uff0c\u793a\u4f8bROMA\u8de8\u7cfb\u7edf\u96c6\u6210\u7684\u80fd\u529b\u3002 ROMA FDI\u7f16\u6392\u670d\u52a1\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u8f6c\u6362\u7684\u80fd\u529b\u3002 \u793a\u4f8b\u573a\u666f \u793a\u4f8b\u573a\u666f\u6a21\u62df\u4e86\u7528\u7535\u7ba1\u7406\u7cfb\u7edf\u548c\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7ROMA\u5206\u522b\u83b7\u53d6\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u7528\u6237\u4fe1\u606f\u548c\u7528\u7535\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u7535\u8868\u4fe1\u606f\u5e76\u5199\u5165\u5230HIVE\u4e2d\uff0c\u7ecf\u8fc7Hive\u5206\u6790\u540e\uff0c\u5c06\u6570\u636e\u5bfc\u5165\u5230gauss\u6570\u636e\u5e93\uff0c\u5e76\u4ee5REST API\u63a5\u53e3\u65b9\u5f0f\u63d0\u4f9b\u7528\u7535\u6570\u636e\u67e5\u8be2\u670d\u52a1\u3002 \u4e3b\u8981\u6b65\u9aa4\uff1a \u901a\u8fc7\u6a21\u62df\u7535\u8868\uff08MQTT\u534f\u8bae IOT\u8bbe\u5907\uff09\uff0c\u88abLink\u63a5\u7ba1\u5e76\u901a\u8fc7ROMA MQS \u6d88\u606f\u670d\u52a1\u5b9e\u65f6\u53d1\u9001\u6d88\u606f\uff0c\u6d88\u606f\u6570\u636e\u901a\u8fc7ROMA FDI\u8f6c\u6362\u5199\u5165\u5230 HIVE\u8868\u4e2d\u3002\u5728\u63d2\u5165HIVE\u524d\uff0c\u901a\u8fc7FDI\u7f16\u6392\u63d2\u4ef6\u589e\u52a0\u7528\u7535\u6570\u636e\u4e0a\u62a5\u65f6\u95f4\u3002 \u6784\u9020txt\u6587\u4ef6\u683c\u5f0f\u7684\u7535\u8868\u5386\u53f2\u6570\u636e\u5e76\u901a\u8fc7FDI\u5bfc\u5165HIVE\u4e2d\u3002 \u6784\u9020\u5f00\u653e\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684Mysql \u6570\u636e\u4e3aREST API\uff0c\u901a\u8fc7FDI\u5bfc\u5165Hive\u4e2d\u3002 \u5728Hive\u8868\u4e2d\u8fdb\u884c\u5206\u6790\u4ee5\u540e\uff0c\u5c06\u6570\u636e\u5b58\u5165GaussDB\u3002 \u901a\u8fc7ROMA LiveData \uff0c\u5f00\u653e\u6570\u636e\u67e5\u8be2Restful API\u3002 \u793a\u4f8b\u51c6\u5907\u8fc7\u7a0b \u00b6 \u6d4b\u8bd5\u6570\u636e\u51c6\u5907 \u00b6 *\u3000\u6a21\u62df\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u6570\u636e\u5e93\u4e2d\u7684\u5ba2\u6237\u4fe1\u606f\u3002\u51c6\u5907\u4e00\u53f0mysql\u670d\u52a1\u5668\uff0c\u521b\u5efacustomer_info\u8868\uff0c\u5305\u62ec\u7528\u6237ID\uff0c\u7528\u6237\u59d3\u540d\uff0c\u7528\u6237\u5bb6\u5ead\u4f4f\u5740\uff0c\u7528\u6237\u7535\u8868\u8bbe\u5907ID\u3002 ``` create database roma_customer; use roma_customer; create table customer_info(id int primary key auto_increment, user_name varchar(20), address varchar(50), device_id varchar(10)); insert into customer_info(user_name,address,device_id) values('matt','dongguan b4','meter02'); insert into customer_info(user_name,address,device_id) values('justin','shenzhen e1','meter02'); insert into customer_info(user_name,address,device_id) values('haoxi','chengdu u9','meter03'); ``` \u5728\u652f\u6301sftp\u7684linux\u670d\u52a1\u5668\u4e0a\u51c6\u5907\u7535\u8868\u6570\u636e\uff0c\u4ee5txt\u6587\u4ef6\u683c\u5f0f\u5b58\u653e\uff0c\u5404\u5217\u6570\u636e\u901a\u8fc7\u5206\u53f7 ; \u5206\u9694\uff0c\u4e3b\u8981\u5305\u62ec\u8bbe\u5907ID\u5b57\u6bb5\uff0c\u6570\u636e\u91c7\u96c6\u65e5\u671f\uff0c\u91c7\u96c6\u503c\u3002 mkdir /elechistorydata vi /elechistorydata/history.txt \u63d2\u5165\u4ee5\u4e0b\u5185\u5bb9\uff1a meter01;20190101;101 meter01;20190115;115 meter01;20190130;130 meter01;20190201;201 meter01;20190215;215 meter01;20190229;229 meter01;20190301;301 meter01;20190315;315 meter01;20190330;330 meter01;20190401;401 meter01;20190415;415 meter01;20190430;430 meter02;20190101;101 meter02;20190115;115 meter02;20190130;130 meter02;20190201;201 meter02;20190215;215 meter02;20190228;229 meter02;20190301;301 meter02;20190315;315 meter02;20190330;330 meter02;20190401;401 meter02;20190415;415 meter02;20190430;430 meter03;20190101;101 meter03;20190115;115 meter03;20190130;130 meter03;20190201;201 meter03;20190215;215 meter03;20190229;229 meter03;20190301;301 meter03;20190315;315 meter03;20190330;330 meter03;20190401;401 meter03;20190415;415 meter03;20190430;430 \u521b\u5efahive table\u7528\u4ee5\u5b58\u653e\u7535\u8868\u4e0a\u62a5\u6570\u636e\u3002 create database elect; use elect; create table elect_table(device_id string,upload_date date, elect_value int); \u521b\u5efa\u5f53\u6708\u6d4b\u8bd5\u6570\u636e\uff0c\u5728FI \u5ba2\u6237\u7aef\u4e0a\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4: beeline -e \"insert into elect.elect_table values('meter01',date_sub(current_date,dayofmonth(current_date)-1),500)\"; beeline -e \"insert into elect.elect_table values('meter02',date_sub(current_date,dayofmonth(current_date)-1),500)\"; beeline -e \"insert into elect.elect_table values('meter03',date_sub(current_date,dayofmonth(current_date)-1),500)\"; \u521b\u5efahive table\u7528\u4ee5\u5b58\u653e\u5ba2\u6237\u4fe1\u606f use elect; create table customer_info(id int,user_name varchar(20),address varchar(20), device_id varchar(20)); \u53d1\u5e03\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u4e3a\u6570\u636eAPI\u670d\u52a1 \u00b6 \u6ce8\u518c\u5e94\u7528 \u00b6 \u70b9\u51fb\u5de6\u4e0a\u89d2\u5e94\u7528\u5217\u8868\u4e0b\u62c9\u6761\uff0c\u70b9\u51fb \u5e94\u7528\u6ce8\u518c \u3002 \u5728\u5f39\u51fa\u9875\u9762\u8f93\u5165\u5e94\u7528\u6ce8\u518c\u4fe1\u606f,\u70b9\u51fb \u63d0\u4ea4 \u5b8c\u6210\u6ce8\u518c\u3002\u5206\u522b\u6ce8\u518cCRMusers\u6570\u636e\u6e90\u548cElectricity.name\u5e94\u7528\u3002 \u521b\u5efaLiveData\u6570\u636e\u6e90 \u00b6 \u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > Live Data > \u8d44\u6e90\u7ba1\u7406 > \u6570\u636e\u6e90\u7ba1\u7406\u201d\u3002\u5355\u51fb\u65b0\u5efa\uff0c\u521b\u5efa\u6570\u636e\u6e90\uff0c\u53c2\u8003\u914d\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a \u70b9\u51fb \u8fde\u63a5\u6d4b\u8bd5 \uff0c\u9a8c\u8bc1\u662f\u5426\u8fde\u901a\u3002\u9a8c\u8bc1\u6210\u529f\u540e\uff0c\u70b9\u51fb \u63d0\u4ea4 \uff0c\u5b8c\u6210\u6570\u636e\u6e90\u521b\u5efa\u3002 \u521b\u5efa/\u6d4b\u8bd5/\u90e8\u7f72/\u6388\u6743API \u00b6 \u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > Live Data > API\u8bbe\u8ba1\u201d\u3002\u5355\u51fb\u201c\u65b0\u5efa\u201d\uff0c\u8bbe\u7f6eAPI\u57fa\u672c\u4fe1\u606f\u3002\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a \u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u63d0\u4ea4\uff0c\u9009\u62e9 \u8fdb\u5165\u5f00\u53d1 \u6a21\u5f0f\uff0c\u9009\u62e9 \u6570\u636e\u5f00\u53d1 \uff0c\u9009\u62e9 \u6570\u636eAPI \u3002 \u6570\u636e\u6e90\u4fe1\u606f\u53c2\u8003\u5982\u4e0b\u914d\u7f6e\uff1a \u70b9\u51fb\u63d0\u4ea4\uff0c\u9009\u62e9 \u8fdb\u5165\u6d4b\u8bd5 \uff0c\u4fdd\u6301\u9ed8\u8ba4\u503c\uff0c\u70b9\u51fb\u6d4b\u8bd5API\uff0c\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\u3002 \u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > Live Data > API\u90e8\u7f72\u201d\uff0c\u9009\u62e9\u5bf9\u5e94API,\u53f3\u4fa7\u70b9\u51fb\u90e8\u7f72\u56fe\u6807\uff0c\u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e\uff0c\u5b8c\u6210\u90e8\u7f72\u3002 \u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > API \u7f51\u5173 > API\u6388\u6743\u201d\u3002\u9009\u62e9\u5bf9\u5e94API,\u70b9\u51fb\u6388\u6743\u56fe\u6807\uff0c\u6388\u6743\u7ed9\u7528\u7535\u7ba1\u7406\u7cfb\u7edf\u5e94\u7528\uff0c\u5982Electricity.name \u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > API \u7f51\u5173 > API\u6d4b\u8bd5\u201d\u3002\u5728\u201cdata_api\u201dAPI\u7684\u6700\u53f3\u4fa7\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u8c03\u6d4b\u56fe\u6807\uff0c\u8fdb\u5165\u201cAPI\u6d4b\u8bd5\u201d\u9875\u9762\u3002\u201c\u8bf7\u6c42\u8def\u5f84\u201d\u5373API\u7684\u5bf9\u5916\u53d1\u5e03\u5730\u5740\u3002 \u96c6\u6210API\u6570\u636e\u5230 Hive\u8868\u4e2d\u3002 \u00b6 \u521b\u5efaAPI\u6570\u636e\u6e90 \u00b6 \u65b0\u589e\u52a0API\u6570\u636e\u6e90\uff0c\u5185\u5bb9\u53c2\u8003\u4e0b\u56fe\uff1a \u521b\u5efaAPI\u6570\u636e\u540c\u6b65\u5230hive\u7684\u4efb\u52a1 \u00b6 \u5728\u5de6\u4fa7\u5bfc\u822a\u4e2d\uff0c\u9009\u62e9FDI \u4efb\u52a1\u7ba1\u7406 -> \u521b\u5efa\u4efb\u52a1 -> \u8868\u5355\u6a21\u5f0f \uff0c \u9009\u62e9\u5bf9\u5e94\u7684API\u6570\u636e\u6e90\u548chivewriter\uff0c\u586b\u5165\u76f8\u5173\u7684\u53c2\u6570\u4fe1\u606f\uff1a \u6e90\u7aef\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a \u5176\u4e2d\u6d89\u53cametaData\u683c\u5f0f\u8f6c\u6362\u7684\u5185\u5bb9\uff0c\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u5185\u5bb9\uff1a [{\"name\":\"id\",\"type\":\"integer\",\"path\":\"retJSON.result[i].id\",\"format\":\"\"},{\"name\":\"user_name\",\"type\":\"string\",\"path\":\"retJSON.result[i]..user_name\",\"format\":\"\"},{\"name\":\"address\",\"type\":\"string\",\"path\":\"retJSON.result[i].address\",\"format\":\"\"},{\"name\":\"device_id\",\"type\":\"string\",\"path\":\"retJSON.result[i].device_id\",\"format\":\"\"}] \u76ee\u6807\u7aef\u914d\u7f6e\u4fe1\u606f\u53c2\u8003\u4e0b\u56fe\uff1a \u6620\u5c04\u5173\u7cfb\u5982\u4e0b\uff1a \u914d\u7f6e\u6210\u529f\u540e\uff0c\u70b9\u51fb\u4fdd\u5b58\uff0c\u4fdd\u5b58\u6210\u529f\u540e\uff0c\u542f\u52a8\u4efb\u52a1\u3002\u67e5\u770b\u4efb\u52a1\u8be6\u7ec6\uff0c\u6765\u81ea\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u7684\u6570\u636e\u5df2\u7ecf\u901a\u8fc7API\u548cFDI\uff0c\u5b9e\u73b0\u6570\u636e\u540c\u6b65\u5230Hive\u8868\u4e2d\u3002 \u4f7f\u7528FDI\u52a0\u8f7d\u5386\u53f2\u6570\u636e\u5230hive\u8868\u4e2d \u00b6 \u5b8c\u6210\u6dfb\u52a0HIVE\u548cHDFS\u6570\u636e\u6e90\u540e\uff0c\u5728Roma FDI\u4e2d\u521b\u5efa\u4efb\u52a1\uff0c\u5982\u679c\u6570\u636e\u91cf\u8f83\u5c11\u53ef\u76f4\u63a5\u4eceftp\u5bfc\u5165FI Hive\uff0c\u5982\u679c\u6570\u636e\u91cf\u8f83\u5927\uff0c\u9700\u8981\u4f7f\u7528FusionInsight Loader\u6216\u8005toolkits\u5de5\u5177\u5c06\u6570\u636e\u6279\u91cf\u5bfc\u5165FI\u4e2d\u3002 \u521b\u5efaFTP\u6570\u636e\u6e90,\u53ef\u53c2\u8003\u5982\u4e0b\u914d\u7f6e \u521b\u5efa\u8fc1\u79fb\u4efb\u52a1 \u9009\u62e9\u5b9a\u65f6\u4efb\u52a1\u7c7b\u578b,\u6e90\u7aef\u914d\u7f6e\u53c2\u8003\u5982\u4e0b: \u76ee\u6807\u7aef\u548c\u6620\u5c04\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe,\u70b9\u51fb\u4fdd\u5b58\u5b8c\u6210\u914d\u7f6e. \u4efb\u52a1\u5217\u8868\u4e2d,\u70b9\u51fb\u624b\u52a8\u8c03\u5ea6,\u89e6\u53d1\u4efb\u52a1,\u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c.\u5982\u4e0b\u56fe\uff0c\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002\u5c06\u6587\u672c\u7c7b\u578b\u5386\u53f2\u6570\u636e\u6210\u529f\u5bfc\u5165\u5230 FI Hive\u4e2d\u3002 \u6a21\u62dfIOT\u8bbe\u5907\uff0c\u5c06\u5b9e\u65f6\u589e\u91cf\u6570\u636e\u540c\u6b65\u5230FI HIVE\u8868\u4e2d \u00b6 MQS\u521b\u5efaTOPIC \u00b6 \u521b\u5efaROMA\u5e73\u53f0\u63a5\u6536\u6d88\u606f\u7684Topic\uff0c\u5e76\u5c06Topic\u53d1\u5e03\u53ca\u8ba2\u9605\u5230\u5e94\u7528\u4e2d\u3002 \u521b\u5efaTopic\u521b\u5efa\u63a5\u6536IOT\u5e73\u53f0\u6d88\u606f\u7684Topic\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u6d88\u606f\u96c6\u6210 > \u6d88\u606f\u961f\u5217\u670d\u52a1 MQS > Topic\u7ba1\u7406\u201d\u3002 \u5355\u51fb\u201c\u521b\u5efaTopic\u201d\uff0c\u8fdb\u5165\u201c\u65b0\u589eTopic\u201d\u9875\u9762,\u521b\u5efa\u540d\u79f0T_ElectricityMeter\u7684Topic\u3002 Topic\u540d\u79f0\uff1aT_ElectricityMeter \u6240\u5c5e\u5e94\u7528\uff1aElectricity.name * \u5355\u51fb\u201c\u4fdd\u5b58\u201d\u3002 \u53d1\u5e03Topic \u00b6 \u53d1\u5e03Topic\u81f3roma.link.test\u5e94\u7528\u4e2d\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u6d88\u606f\u96c6\u6210 > \u6d88\u606f\u961f\u5217\u670d\u52a1 MQS > Topic\u7ba1\u7406\u201d\u3002 \u5728\u201cTopic\u540d\u79f0\u201d\u4e2d\u8f93\u5165\u201cT_ElectricityMeter\u201d\uff0c\u5355\u51fb\u201c\u67e5\u8be2\u201d\u3002 \u5728Topic\u7684\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u201c\u53d1\u5e03\u201d\uff0c\u8fdb\u5165\u201c\u53d1\u5e03\u201d\u9875\u9762\u3002\u914d\u7f6e\u53d1\u5e03\u4fe1\u606f\uff0c\u201c\u53d1\u5e03\u5e94\u7528\u201d\u9009\u62e9\u201cElectricity.name\u201d, \u5355\u51fb\u201c\u53d1\u5e03\u201d\uff0c\u9875\u9762\u63d0\u793a\u201c\u4fdd\u5b58\u4efb\u52a1\u6210\u529f\u201d\u3002 \u8ba2\u9605\u5e94\u7528 \u00b6 \u8ba2\u9605Topic\u81f3roma.link.test\u5e94\u7528\u4e2d\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u6d88\u606f\u96c6\u6210 > \u6d88\u606f\u961f\u5217\u670d\u52a1 MQS > Topic\u7ba1\u7406\u201d\u3002 \u5728\u201cTopic\u540d\u79f0\u201d\u4e2d\u8f93\u5165\u201cT_ElectricityMeter\u201d\uff0c\u5355\u51fb\u201c\u67e5\u8be2\u201d\u3002 \u5728Topic\u7684\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u201c\u8ba2\u9605\u201d\uff0c\u8fdb\u5165\u201c\u8ba2\u9605\u201d\u9875\u9762\u3002 \u914d\u7f6e\u8ba2\u9605\u4fe1\u606f\uff0c\u201c\u8ba2\u9605\u5e94\u7528\u201d\u9009\u62e9\u201cElectricity.name\u201d \u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u9875\u9762\u63d0\u793a\u201c\u4fdd\u5b58\u4efb\u52a1\u6210\u529f\u201d\u3002 LINK\u63a5\u5165\u6a21\u62df\u8bbe\u5907 \u00b6 \u6dfb\u52a0\u5e94\u7528\u5b9e\u4f8b \u00b6 \u6bcf\u4e2a\u90e8\u7f72\u5728ROMA\u670d\u52a1\u5668\u7684LINK Connector\u7ec4\u4ef6\uff0c\u5747\u5728ROMA Portal\u5b58\u5728\u4e00\u4e2aConnector\u5b9e\u4f8b\uff0c\u5728\u521b\u5efa\u8bbe\u5907\u65f6\u9009\u62e9\u8be5\u5b9e\u4f8b\uff0c\u4ee5\u4f7f\u5b9e\u4f53\u8bbe\u5907\u8fde\u63a5\u5230\u8be5Connector\u3002\u7ed9Connector\u5b9e\u4f8b\u6dfb\u52a0\u5e94\u7528\uff08\u53ef\u6dfb\u52a0\u591a\u4e2a\uff09\uff0c\u8be5\u5e94\u7528\u4e0b\u7684\u8bbe\u5907\u53ef\u4ee5\u4f7f\u7528\u8be5Connector\u8fde\u63a5\u8bbe\u5907\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u7cfb\u7edf\u7ba1\u7406 > \u5b9e\u4f8b\u7ba1\u7406\u201d\u3002 \u5728\u5b9e\u4f8b\u7ba1\u7406\u9875\u9762\u4e2d\u67e5\u627e\u5230\u7684\u5bf9\u5e94\u5b9e\u4f8b\u7684\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u201c\u67e5\u770b\u5b9e\u4f8b\u5e94\u7528\u201d\u3002\u5728\u5f39\u51fa\u7684\u201c\u5b9e\u4f8b\u5e94\u7528\u7ba1\u7406\u201d\u9875\u9762\u5355\u51fb\u201c\u6dfb\u52a0\u5b9e\u4f8b\u5e94\u7528\u201d\u3002\u9009\u62e9\u53ef\u4f7f\u7528\u8be5Connector\u5b9e\u4f8b\u7684\u5e94\u7528\uff0c\u672c\u4f8b\u4e2d\u9009\u62e9Electricity.name\u5e94\u7528\u3002 \u521b\u5efa\u6a21\u578b \u00b6 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u7cfb\u7edf\u7ba1\u7406 > \u6a21\u578b\u7ba1\u7406\u201d\uff0c\u8fdb\u5165\u754c\u9762\u3002 \u5355\u51fb\u201c\u521b\u5efa\u6a21\u578b\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u6a21\u578b\u201d\u5bf9\u8bdd\u6846\u3002 \u586b\u5199\u6a21\u578b\u4fe1\u606f\uff0c\u6839\u636e\u672c\u6a21\u578b\u5185\u4f7f\u7528\u7684\u4ea7\u54c1\u3001\u8bbe\u5907\u7528\u9014\u7b49\u81ea\u5b9a\u4e49\u6a21\u578b\u540d\u79f0\uff0c\u4f8b\u5982Voltage\u3002 \u586b\u5199\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u6a21\u578b\u521b\u5efa\u5b8c\u6210\u3002 \u5355\u51fb\u201c\u6a21\u578b\u8be6\u60c5\u201d\uff0c\u65b0\u589e\u4e00\u4e2a\u540d\u4e3astatus\u7684\u670d\u52a1\uff0c\u548c\u540d\u4e3aid\u548cvalue\u7684\u5c5e\u6027\u3002 \u521b\u5efa\u4ea7\u54c1\uff08\u7f51\u5173\u4ea7\u54c1\uff09 \u00b6 \u5728ROMA\u4e0a\u90e8\u83dc\u5355\u680f\uff0c\u9009\u62e9\u8fdb\u5165\u5e94\u7528\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u4ea7\u54c1\u7ba1\u7406 \u201d\u3002 \u5355\u51fb\u201c\u521b\u5efa\u4ea7\u54c1\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u4ea7\u54c1\u201d\u5bf9\u8bdd\u6846\u3002 \u6839\u636e\u4f7f\u7528\u7684\u5b9e\u4f53\u7f51\u5173\u8bbe\u5907\u7684\u4fe1\u606f\u586b\u5199\u4ea7\u54c1\u4fe1\u606f\u3002 \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe: \u521b\u5efa\u4ea7\u54c1\uff08\u666e\u901a\u4ea7\u54c1\uff09 \u00b6 \u5728ROMA Portal\u4e2d\u521b\u5efa\u666e\u901a\u4ea7\u54c1\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u4ea7\u54c1\u7ba1\u7406 \u201d\u3002 \u5355\u51fb\u201c\u521b\u5efa\u4ea7\u54c1\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u4ea7\u54c1\u201d\u5bf9\u8bdd\u6846\u3002\u586b\u5199\u4ea7\u54c1\u4fe1\u606f\uff0c\u7269\u6a21\u578b\u9009\u62e9\u524d\u9762\u521b\u5efa\u201c\u73af\u5883\u76d1\u6d4b\u6a21\u578b\u201d\uff0c\u5982\u4e0b\u56fe\u3002 \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe: \u586b\u5199\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u4ea7\u54c1\u521b\u5efa\u5b8c\u6210\u3002 \u521b\u5efa\u7f51\u5173\u8bbe\u5907 \u00b6 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907 \u96c6\u6210LINK > \u8bbe\u5907\u7ba1\u7406 \u201d\u3002 \u5355\u51fb\u201c\u521b\u5efa\u8bbe\u5907\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u8bbe\u5907\u201d\u5bf9\u8bdd\u6846\u3002 \u586b\u5199\u8bbe\u5907\u4fe1\u606f\uff0c\u4ea7\u54c1\u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u201c\u73af\u4fdd\u201d\uff0c\u8bbe\u5907\u6807\u7b7e\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u586b\u5199\u76f8\u5173\u8054\u7684\u3002 \u586b\u5199\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u8bbe\u5907\u521b\u5efa\u5b8c\u6210\u3002 \u5728\u201c\u8bbe\u5907\u8be6\u60c5\u201d\u9875\u9762\uff0c\u9009\u62e9\u201c\u5b50\u8bbe\u5907\u7ba1\u7406\u201d\u9875\u7b7e\uff0c\u53ef\u4ee5\u770b\u5230\u7f51\u5173\u5b50\u8bbe\u5907\u5217\u8868\u8fd8\u6ca1\u6709\u5b50\u8bbe\u5907\u3002 \u6a21\u62df\u4e0a\u7ebf\u7f51\u5173\u548c\u7535\u5b50\u8bbe\u5907 \u00b6 \u4e0b\u8f7d\u7f51\u5173\u8bbe\u5907\u6a21\u62df\u7a0b\u5e8f,\u4e0b\u8f7d\u5730\u5740: \u70b9\u51fb\u4e0b\u8f7d \u542f\u52a8Eclipse\u7a0b\u5e8f\u3002 \u5bfc\u5165Demo\u5de5\u7a0b\u3002 \u5728\u83dc\u5355\u680f\u4e2d\u4f9d\u6b21\u9009\u62e9\u201cFile > Import > General > Existing Projects into Workspace\u201d\uff0c\u5355\u51fb\u201cNext\u201d\uff0c\u5bfc\u5165\u672c\u5730Demo\u5de5\u7a0b\u3002 \u5bfc\u5165\u540e\u76ee\u5f55\u683c\u5f0f\u5982\u4e0b: \u5728RUN Configuration\u4e2d\u914d\u7f6e\u542f\u52a8\u4e3b\u51fd\u6570,\u53c2\u8003\u5982\u4e0b\u914d\u7f6e: \u4fee\u6539GatawayStub.java\u7684start\u65b9\u6cd5\u5185\u5bb9,\u4e3b\u8981\u66ff\u6362\u5185\u5bb9\u5305\u62ecDeviceInfo\u4e2d\u7684\u914d\u7f6e\u4fe1\u606f,\u4ee5\u53ca\u8bbe\u5907ID. ``` DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); childDevice.setDeviceIdentifier(\"meter01\"); ``` ``` public void start() { // TODO Auto-generated method stub //\u5206\u522b\u66ff\u6362\u5982\u4e0b\u52a0\u9ed1\u5b57\u6bb5\u4e3a\u666e\u901a\u4ea7\u54c1\u7684\u5382\u5546ID\uff0c\u8bbe\u5907\u7c7b\u578b\uff0c\u8bbe\u5907\u578b\u53f7\uff0c\u5382\u5546\u540d\u79f0\u3002 DeviceInfo childDevice = new DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); //\u66ff\u6362meter01\u4e3a\u5b50\u8bbe\u59071\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier( \"meter01\"); context.registerDevice(childDevice, resp -> { if (resp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to register device:{}\", resp.getMessage()); return; } DeviceStatus deviceStatus1 = new DeviceStatus(); //\u66ff\u6362meter01\u4e3a\u5b50\u8bbe\u59071\u7684\u540d\u79f0\u3002 deviceStatus1.setDeviceIdentifier(\"meter01\"); deviceStatus1.setConnectStatus(DeviceConnStatus.online); context.changeDeviceConnectStatus(deviceStatus1, onlineResp -> { if (onlineResp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to change device state:{}\", resp.getMessage()); return; } // schduler.scheduleAtFixedRate(() -> { // JSONObject report = new JSONObject(); // report.put(\"id\", random.nextInt()); // report.put(\"value\", dateFormater.format(new Date())); // context.propertyReport(childDevice, \"status\", report); // }, 1, 10, TimeUnit.SECONDS); }); }); // TODO Auto-generated method stub // DeviceInfo childDevice = new DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); //\u66ff\u6362meter02\u4e3a\u5b50\u8bbe\u59072\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter02\"); context.registerDevice(childDevice, resp -> { if (resp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to register device:{}\", resp.getMessage()); return; } DeviceStatus deviceStatus2 = new DeviceStatus(); //\u66ff\u6362meter02\u4e3a\u5b50\u8bbe\u59072\u7684\u540d\u79f0\u3002 deviceStatus2.setDeviceIdentifier(\"meter02\"); deviceStatus2.setConnectStatus(DeviceConnStatus.online); context.changeDeviceConnectStatus(deviceStatus2, onlineResp -> { if (onlineResp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to change device state:{}\", resp.getMessage()); return; } // schduler.scheduleAtFixedRate(() -> { // JSONObject report = new JSONObject(); // report.put(\"id\", random.nextInt()); // report.put(\"value\", dateFormater.format(new Date())); // context.propertyReport(childDevice, \"status\", report); // }, 1, 10, TimeUnit.SECONDS); }); }); // TODO Auto-generated method stub // DeviceInfo childDevice = new DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); //\u66ff\u6362meter03\u4e3a\u5b50\u8bbe\u59073\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter03\"); context.registerDevice(childDevice, resp -> { if (resp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to register device:{}\", resp.getMessage()); return; } DeviceStatus deviceStatus3 = new DeviceStatus(); //\u66ff\u6362meter03\u4e3a\u5b50\u8bbe\u59073\u7684\u540d\u79f0\u3002 deviceStatus3.setDeviceIdentifier(\"meter03\"); deviceStatus3.setConnectStatus(DeviceConnStatus.online); context.changeDeviceConnectStatus(deviceStatus3, onlineResp -> { if (onlineResp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to change device state:{}\", resp.getMessage()); return; } // schduler.scheduleAtFixedRate(() -> { // JSONObject report = new JSONObject(); // report.put(\"id\", random.nextInt()); // report.put(\"value\", dateFormater.format(new Date())); // context.propertyReport(childDevice, \"status\", report); // }, 1, 10, TimeUnit.SECONDS); }); }); } ``` \u4fee\u6539iotagent.properties\u914d\u7f6e\u53c2\u6570,\u914d\u7f6e\u53c2\u8003\u5982\u4e0b: \u53f3\u952e\u5355\u51fb\u201cGatewaySdkDemo\u201d\uff0c\u9009\u62e9\u201cRun as> Java Application\u201d\u3002 \u5355\u51fb\u201cOK\u201d\uff0c\u7b49\u5f85\u6267\u884c\u3002\u6267\u884c\u6210\u529f\u540e\u5982\u4e0b\u56fe\uff1a \u5728ROMA Portal\u67e5\u770b\u7f51\u5173\u8bbe\u5907\u548c\u7f51\u5173\u5b50\u8bbe\u5907\u662f\u5426\u5df2\u7ecf\u4e0a\u7ebf\u3002 \u914d\u7f6e\u89c4\u5219\u5f15\u64ce \u00b6 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u89c4\u5219\u5f15\u64ce \u201d\u3002 \u5355\u51fb\u201c\u521b\u5efa\u89c4\u5219\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u89c4\u5219\u201d\u5bf9\u8bdd\u6846\u3002\u81ea\u5b9a\u4e49\u586b\u89c4\u5219\u540d\u79f0\uff0c\u4f8b\u5982meter_rule_01\u3002 \u5728\u89c4\u5219\u5f15\u64ce\u5217\u8868\uff0c\u5355\u51fb\u201c\u64cd\u4f5c\u201d\u5217\u7684\u201c\u7ba1\u7406\u201d\u3002\u5728\u5f39\u51fa\u7684\u201c\u89c4\u5219\u8be6\u60c5\u201d\u9875\u9762\u7684\u201c\u6570\u636e\u6e90\u7aef\u201d\u533a\u57df\uff0c\u5355\u51fb\u201c\u65b0\u589e\u201d\uff0c\u914d\u7f6e\u89c4\u5219\u5f15\u64ce\u7684\u6570\u636e\u6e90\u7aef\u3002 \u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\u3002 \u589e\u52a0\u76ee\u6807\u7aef\uff1a \u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\u3002\u89c4\u5219\u5f15\u64ce\u914d\u7f6e\u5b8c\u6210\u3002 \u53c2\u8003\u4ee5\u4e0a\u6b65\u9aa4\u914d\u7f6e\u89c4\u5219\u5f15\u64cemeter_rule_02\u548cmeter_rule_03\u3002 \u6a21\u62df\u53d1\u9001\u6d88\u606f \u00b6 \u6253\u5f00Eclipse\u7a0b\u5e8f\u3002 \u4fee\u6539GatawayStub.java\u6587\u4ef6\u4e2d\u7684start\u65b9\u6cd5\u5982\u4e0b\uff1a ``` public void start() { //\u5206\u522b\u66ff\u6362\u5982\u4e0b\u52a0\u9ed1\u5b57\u6bb5\u4e3a\u666e\u901a\u4ea7\u54c1\u5382\u5546ID\uff0c\u8bbe\u5907\u7c7b\u578b\uff0c\u8bbe\u5907\u578b\u53f7\uff0c\u5382\u5546\u540d\u79f0\u3002 DeviceInfo childDevice = new DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); //\u66ff\u6362meter01\u4e3a\u5b50\u8bbe\u59071\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter01\"); JSONObject report1 = new JSONObject(); //\u66ff\u6362\u4e3a\u8bbe\u5907\u540d\u79f0\u548c\u5177\u4f53\u7684\u7535\u538b\u503c\u3002 report1.put(\"id\", \"meter01\"); report1.put(\"value\", \"700\"); context.propertyReport(childDevice, \"status\", report1); //\u66ff\u6362meter02\u4e3a\u5b50\u8bbe\u59072\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter02\"); JSONObject report2 = new JSONObject(); //\u66ff\u6362\u4e3a\u8bbe\u5907\u540d\u79f0\u548c\u5177\u4f53\u7684\u7535\u538b\u503c\u3002 report2.put(\"id\", \"meter02\"); report2.put(\"value\", \"710\"); context.propertyReport(childDevice, \"status\", report2); //\u66ff\u6362meter03\u4e3a\u5b50\u8bbe\u59073\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter03\"); JSONObject report3 = new JSONObject(); //\u66ff\u6362\u4e3a\u8bbe\u5907\u540d\u79f0\u548c\u5177\u4f53\u7684\u7535\u538b\u503c\u3002 report3.put(\"id\", \"meter03\"); report3.put(\"value\", \"720\"); context.propertyReport(childDevice, \"status\", report3); }\u53f3\u952e\u5355\u51fb\u201cGatewaySdkDemo\u201d\uff0c\u9009\u62e9\u201cRun as> Java Application\u201d\u3002 ``` \u5355\u51fb\u201cOK\u201d\uff0c\u7b49\u5f85\u6267\u884c\u3002\u6267\u884c\u5b8c\u6210\u540e\u3002\u5c06\u53d1\u9001\u4e09\u4e2a\u7535\u8868\u7684\u6570\u636e\u5230ROMA\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u8bbe\u5907\u7ba1\u7406 \u201d\u3002 \u5206\u522b\u5728\u5b50\u8bbe\u5907meter01\u3001meter02\u3001meter03\u7684\u8bbe\u5907\u8be6\u60c5\u4e2d\u9009\u62e9\u201c\u8bbe\u5907\u65e5\u5fd7 > \u6d88\u606f\u5185\u5bb9\u67e5\u8be2\u201d\u67e5\u770b\u6536\u5230\u7684\u6d88\u606f\u5185\u5bb9\u3002 \u67e5\u770bMQS\u4e2d\u53d1\u9001\u7684\u6d88\u606f \u00b6 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u6d88\u606f\u96c6\u6210 > \u6d88\u606f\u961f\u5217\u670d\u52a1MQS > \u6d88\u606f\u67e5\u8be2 \u201d\u3002 \u5728\u201cTopic\u540d\u79f0\u201d\u4e2d\u8f93\u5165\u201cT_ElectricityMeter\u201d\uff0c\u5355\u51fb\u201c\u67e5\u8be2\u201d\u3002 \u5728\u5bf9\u5e94Topic\u7684\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u201c\u5185\u5bb9\u201d\u3002\u67e5\u770b\u63a5\u6536\u5230\u7684\u6d88\u606f\u5185\u5bb9\u3002 FDI\u91c7\u96c6\u5b9e\u65f6\u7528\u7535\u6570\u636e \u00b6 \u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u5df2\u5b8c\u6210\u6a21\u62dfIOT\u8bbe\u5907\u5b9e\u65f6\u53d1\u9001\u6570\u636e\u5230MQS\u6d88\u606f\u670d\u52a1\uff0c\u4f46\u53d1\u9001\u7684\u6570\u636e\u53ea\u67092\u4e2a\u5b57\u6bb5\uff0c\u76f8\u6bd4hive\u4e2d\u7684\u8868\uff0c\u7f3a\u5c11\u4e00\u4e2a\u65f6\u95f4\u5b57\u6bb5\uff0c\u672c\u793a\u4f8b\u901a\u8fc7FDI\u7f16\u6392\u7279\u6027\uff0c\u5b9e\u73b0\u6570\u636e\u4eceMQS\u4e2d\u62bd\u53d6\u540e\u52a0\u8f7d\u5230hive\u8868\u524d\uff0c\u589e\u52a0\u5f53\u524d\u7cfb\u7edf\u65f6\u95f4\u5b57\u6bb5\u3002 \u521b\u5efaFDI\u4efb\u52a1\uff0c\u9009\u62e9\u7f16\u6392\u6a21\u5f0f\uff0c\u9009\u7528\u5b9e\u65f6\u540c\u6b65\u6a21\u5f0f \u8bfb\u8282\u70b9\u62d6\u9009MQS\u5230\u7f16\u5e03,\u5199\u8282\u70b9\u62d6\u9009FI Hive\uff0c\u753b\u5e03\u4e0a\u5b8c\u6210\u8fde\u7ebf\uff0c\u914d\u7f6e\u8bfb\u5199\u8282\u70b9\u4fe1\u606f\u3002 \u914d\u7f6e\u8f6c\u6362\u8282\u70b9\u4fe1\u606f\uff0c\u9009\u62e9\u811a\u672c\u6a21\u5f0f\u3002\u6dfb\u52a0\u5982\u4e0b\u4ee3\u7801\uff0c\u5b8c\u6210\u4e86\u6e90\u548c\u76ee\u6807\u7aef\u6570\u636e\u5b57\u6bb5\u7684\u6620\u5c04\uff0c\u5e76\u589e\u52a0\u4e86\u65f6\u95f4\u5b57\u6bb5\uff0c\u8bb0\u5f55\u4e86\u6bcf\u4e00\u6761\u8bb0\u5f55\u4ea7\u751f\u7684\u65f6\u95f4\u3002 targetObj = {}; targetObj.id = sourceObj.id; targetObj.value = sourceObj.value; var date = new Date(); var year = date.getFullYear().toString(); var mon = (date.getMonth()+1).toString(); var day = date.getDate().toString(); if(parseInt(mon) < 10){ mon = \"0\" + mon; } if(day < 10){ day = \"0\" + day; } targetObj.upload_date = year+\"-\"+mon+\"-\"+day; return targetObj; \u70b9\u51fb\u4fdd\u5b58\uff0c\u5b8c\u6210\u4efb\u52a1\u914d\u7f6e\u3002 \u53c2\u8003\u6a21\u62df\u53d1\u9001\u6d88\u606f\u6b65\u9aa4\u901a\u8fc7eclipse\u53d1\u9001\u6d4b\u8bd5\u6d88\u606f\u3002 \u67e5\u770bhive\u8868\uff0c\u6570\u636e\u5df2\u6210\u529f\u63d2\u5165\uff0c\u5982\u4e0b\u8868\u6240\u793a\uff0c2020-01-08\u4e3a\u65b0\u63d2\u5165\u76843\u6761\u5b9e\u65f6\u91c7\u96c6\u6570\u636e\u3002 \u81f3\u6b64\uff0c\u5b8c\u6210\u4e86\u5b9e\u65f6IOT\u8bbe\u5907\u6570\u636e\u5b9e\u65f6\u540c\u6b65\u5230FI hive\u7684\u6a21\u62df\u3002 \u5728Hive\u8868\u6267\u884c\u6570\u636e\u5206\u6790\uff0c\u5e76\u5c06\u7ed3\u679c\u5bfc\u5165\u5230mysql \u00b6 \u5728Hive beeline\u6a21\u5f0f\u4e0b\uff0c\u521b\u5efashell\u811a\u672c\u5e76\u6267\u884c\uff0c\u5b8c\u6210\u6570\u636e\u5206\u6790\u3002 touch elect_anaysis.sh chmod a+x elect_anaysis.sh \u811a\u672c\u5185\u5bb9\u5982\u4e0b\uff1a #!/bin/bash beeline -e \"create table elect.analysis(name string,year int,month int,value int);\" for i in 2019 2020 do for j in 1 2 3 4 do sql=\"insert into elect.analysis(name,year,month,value) select a.user_name,$i,$j,max(b.elect_value)-min(b.elect_value) from elect.customer_info a, elect.elect_table b where a.device_id=b.device_id and year(b.upload_date) = $i and month(b.upload_date) = $j group by a.user_name;\" beeline -e \"$sql\" done done beeline -e \"select * from elect.analysis\"; \u5728mysql\u6570\u636e\u5e93\u4e2d\u521b\u5efa\u8868,\u7528\u6765\u5b58\u653e\u7ecf\u8fc7hive\u5206\u6790\u8fc7\u7684\u6570\u636e\u3002\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u5982\u679c\u6570\u636e\u91cf\u8f83\u5927\uff0c\u63a8\u8350\u5c06Hive\u5206\u6790\u8fc7\u7684\u6570\u636e\u5b58\u653e\u5230GaussDB A\u4e2d\u3002 create table user_elects(user_name varchar(20),year int,month int,elect_value int); * \u521b\u5efaFDI\u4efb\u52a1\uff0c\u5b9a\u671f\u540c\u6b65hive\u5206\u6790\u8868\u4e2d\u7684\u7ed3\u679c\u5230mysql\u8868\u4e2d\u3002\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe: \u4fdd\u5b58\u540e\uff0c\u624b\u52a8\u8c03\u5ea6\u4efb\u52a1\uff0c\u5b8c\u6210\u6570\u636e\u540c\u6b65\uff0c\u540c\u6b65\u5b8c\u6210\u540e\u5728mysql\u4e2d\u67e5\u770b\u6570\u636e\u3002 \u5982\u56fe\u6240\u793a\uff0c\u53ef\u4ee5\u67e5\u770b\u5230\u7528\u6237\u6bcf\u6708\u7684\u7528\u7535\u60c5\u51b5\u3002 \u53ef\u4ee5\u901a\u8fc7\u4e3a\u8868\u521b\u5efa\u7d22\u5f15\u7b49\uff0c\u52a0\u901f\u67e5\u8be2\u901f\u5ea6\u3002 create index index_uym on user_elects(user_name,year,month); \u901a\u8fc7LiveData API\uff0c\u5f00\u653e\u6570\u636e\u670d\u52a1\u3002 \u00b6 \u53c2\u8003\u7ae0\u8282 \u53d1\u5e03\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u4e3a\u6570\u636eAPI\u670d\u52a1 \u8fc7\u7a0b\uff0c\u5bf9\u5916\u63d0\u4f9b\u6570\u636e\u67e5\u8be2APi,\u5305\u62ec\u901a\u8fc7\u59d3\u540d\u67e5\u5386\u53f2\u6570\u636e\uff0c\u4ee5\u53ca\u901a\u8fc7\u59d3\u540d\u6216\u8005\u59d3\u540d+\u5e74+\u6708\u7ec4\u5408\u67e5\u8be2\u7684\u63a5\u53e3\u3002 \u5982\u4e0b\u4e3a\u7528\u4f8b\u9a8c\u8bc1\u7ed3\u679c\uff1a \u67e5\u8be2\u7528\u6237\u6240\u6709\u7684\u7528\u7535\u8bb0\u5f55\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u67e5\u8be2\u7528\u6237\u57282020\u5e741\u6708\u7684\u7528\u7535\u8bb0\u5f55\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u81f3\u6b64\uff0c\u793a\u4f8b\u9a8c\u8bc1\u5b8c\u6210\uff0c\u901a\u8fc7ROMA\u91c7\u96c6\u7535\u8868\u8bb0\u5f55\uff0c\u5e76\u7ed3\u5408\u5386\u53f2\u6570\u636e\u548c\u5b9e\u65f6\u6570\u636e\uff0c\u901a\u8fc7FI HIVE\u8fdb\u884c\u5206\u6790\u7edf\u8ba1\u7528\u6237\u6bcf\u6708\u7528\u7535\u60c5\u51b5\uff0c\u5e76\u901a\u8fc7livedata API\u5f00\u653e\u4e86\u6570\u636e\u67e5\u8be2\u63a5\u53e3\u3002","title":"FusionInsight\u96c6\u6210ROMA\u5b9e\u8df5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#fusioninsightroma","text":"","title":"FusionInsight\u96c6\u6210ROMA\u5b9e\u8df5"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma","text":"","title":"ROMA\u4ecb\u7ecd"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma_1","text":"ROMA\u662f\u591a\u4e91\u65f6\u4ee3\u7684\u878d\u5408\u96c6\u6210\u5e73\u53f0\uff0c\u6e90\u81ea\u534e\u4e3a10+\u5e74\u6570\u5b57\u5316\u8f6c\u578b\u5b9e\u8df5\uff0c\u805a\u7126A\uff08Application\uff09\u3001B\uff08Business\uff09\u3001C\uff08Cloud\uff09\u3001D\uff08Device\uff09\u56db\u7c7b\u96c6\u6210\u573a\u666f\uff0c\u63d0\u4f9b\u5feb\u901f\u3001\u7b80\u5355\u7684\u6d88\u606f\u3001\u6570\u636e\u3001\u670d\u52a1\u3001\u8bbe\u5907\u96c6\u6210\u80fd\u529b\uff0c\u7b80\u5316\u4f01\u4e1a\u4e0a\u4e91\uff0c\u652f\u6301\u4e91\u4e0a\u4e91\u4e0b\u3001\u8de8\u533a\u57df\u96c6\u6210\uff0c\u6253\u901aIT\u4e0eOT\uff0c\u8fde\u63a5\u4f01\u4e1a\u4e0e\u751f\u6001\u4f19\u4f34\uff0c\u52a9\u529b\u884c\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\u3002 ROMA\u81f4\u529b\u4e8e\u89e3\u51b3\u884c\u4e1a\u5728\u5411\u6570\u5b57\u5316\u8f6c\u578b\u7684\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u96be\u9898\uff1a \u7f3a\u5c11\u7edf\u4e00\u7684\u8bbe\u5907\u4fe1\u606f\u96c6\u6210\u9014\u5f84\uff1b \u6570\u636e\u683c\u5f0f\u591a\u6837\u5316\uff0c\u96be\u4ee5\u4f20\u8f93\u548c\u96c6\u6210\uff1b \u7f3a\u5c11\u4e0e\u5408\u4f5c\u4f19\u4f34\u5206\u4eab\u6570\u636e\u548c\u540e\u7aef\u670d\u52a1\u7684\u4fbf\u6377\u9014\u5f84\uff1b \u7f3a\u5c11\u4e91\u4e0a\u4e91\u4e0b\u8de8\u7f51\u7edc\u7684\u5b89\u5168\u4fe1\u606f\u901a\u9053\u3002","title":"ROMA\u7b80\u4ecb"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma_2","text":"ROMA\u5b9a\u4f4d\u4e3a\u4fbf\u6377\u7684\u96c6\u6210\u5e73\u53f0\uff0c\u63d0\u4f9b\u6570\u636e\u63a5\u5165\u3001\u6570\u636e\u96c6\u6210\u4ee5\u53ca\u6570\u636e\u670d\u52a1API\u6784\u5efa\u7684\u80fd\u529b\u3002ROMA\u4e0d\u662f\u4f20\u7edf\u7684ETL\u5de5\u5177\uff0c\u63d0\u4f9b\u76f8\u6bd4ETL\u5de5\u5177\u66f4\u4e3a\u4e30\u5bcc\u7684\u6570\u636e\u6e90\u652f\u6301\u7c7b\u578b\u4ee5\u53ca\u66f4\u4e3a\u4e30\u5bcc\u7684\u529f\u80fd\u3002ROMA\u4e0d\u662f\u5927\u6570\u636e\u5e73\u53f0\uff0cROMA\u652f\u6301\u5c06\u6765\u81ea\u4e0d\u540c\u7cfb\u7edf\u7684\u591a\u79cd\u7c7b\u578b\u53ca\u683c\u5f0f\u7684\u6570\u636e\u540c\u6b65\u5230FusionInsight\uff0c\u5b9e\u73b0IT\u548cOT\u6570\u636e\u6c47\u805a\uff0c\u7ecf\u8fc7FusionInsight\u6570\u636e\u5206\u6790\u540e\u7684\u6570\u636e\u5bfc\u5165\u5230\u6570\u636e\u96c6\u5e02\uff0cROMA\u901a\u8fc7APIC LiveData \u5bf9\u5916\u63d0\u4f9bRestful\u7684\u6570\u636e\u670d\u52a1\u63a5\u53e3\u3002","title":"ROMA\u5b9a\u4f4d"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma_3","text":"ROMA \u4e3b\u8981\u7531\u56db\u5927\u7ec4\u4ef6\u7ec4\u6210\uff1a FDI: \u65e8\u5728\u89e3\u51b3\u591a\u79cd\u6570\u636e\u6e90\u7684\u5feb\u901f\u7075\u6d3b\u96c6\u6210\u80fd\u529b\uff0c\u5b9e\u73b0\u4efb\u610f\u65f6\u95f4\u3001\u4efb\u610f\u5730\u70b9\u3001\u4efb\u610f\u7cfb\u7edf\u4e4b\u95f4\u5b9e\u73b0\u5b9e\u65f6\u6570\u636e\u8ba2\u9605\u548c\u5b9a\u65f6\u589e\u91cf\u6570\u636e\u8fc1\u79fb\u3002FDI\u662fFusionInsight\u6570\u636e\u96c6\u6210\u4f7f\u7528\u7684\u6700\u4e3b\u8981\u7684\u529f\u80fd\u7ec4\u4ef6\uff0c\u901a\u8fc7FDI\u5b9e\u73b0\u591a\u79cd\u6570\u636e\u6e90\u5b9e\u65f6\u6216\u8005\u589e\u91cf\u540c\u6b65\u5230\u6570\u636e\u6e56\u3002 LINK:\u8bbe\u5907\u96c6\u6210\u670d\u52a1\uff0c\u4f7f\u7528MQTT\u6807\u51c6\u534f\u8bae\u8fde\u63a5\u8bbe\u5907\uff0c\u5b9e\u73b0\u8bbe\u5907\u5feb\u901f\u63a5\u5165\u3001\u6570\u636e\u91c7\u96c6\u7b49\u7269\u8054\u7f51\u5e94\u7528\u3002\u7528\u6237\u53ef\u4ee5\u5728\u63a7\u5236\u53f0\u914d\u7f6e\u89c4\u5219\u5f15\u64ce\uff0c\u5b9e\u73b0\u8bbe\u5907\u5c06Topic\u7ea7\u522b\u7684\u6d88\u606f\u8f6c\u53d1\u5230\u4e0d\u540c\u7684\u670d\u52a1\u4e2d\uff0c\u5982\u6d88\u606f\u961f\u5217\u670d\u52a1\uff08MQS\uff09\u548cFI KAFKA\u7b49\u3002\u540c\u65f6\u652f\u6301\u914d\u7f6e\u4f7f\u7528\u7c7bSQL\u7684\u89c4\u5219\u8bed\u8a00\u5bf9\u8f6c\u53d1\u7684\u6d88\u606f\u8fdb\u884c\u5904\u7406\u548c\u7b5b\u9009\uff0c\u6ee1\u8db3\u4e0d\u540c\u4e1a\u52a1\u5bf9\u8f6c\u53d1\u6570\u636e\u5185\u5bb9\u7684\u8981\u6c42\uff0c\u5b9e\u73b0\u4e1a\u52a1\u903b\u8f91\u4e0e\u5e94\u7528\u7a0b\u5e8f\u7684\u4f4e\u8026\u5408\u3002 \u670d\u52a1\u96c6\u6210\uff1aAPI\u96c6\u6210\u670d\u52a1\u5b9e\u73b0API\u5f00\u53d1\u7f16\u6392\uff0c\u652f\u6301\u51fd\u6570API\u4ee5\u53ca\u6570\u636eAPI\uff0c\u652f\u6301\u901a\u8fc7\u7f16\u5199SQL\u811a\u672c\u7684\u65b9\u5f0f\uff0c\u5c06\u6570\u636e\u5e93\u63d0\u4f9b\u7684\u6570\u636e\u670d\u52a1\u8f6c\u6362\u4e3aREST API\u7684\u80fd\u529b\u3002 MQS\u6d88\u606f\u96c6\u6210\u670d\u52a1\uff1a\u63d0\u4f9b\u4e86\u6d88\u606f\u7ba1\u7406\u80fd\u529b","title":"ROMA\u7ec4\u4ef6\u53ca\u529f\u80fd\u4ecb\u7ecd"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma-fdi","text":"ROMA FDI\u652f\u6301\u7684\u6570\u636e\u6e90\u5982\u4e0b\uff1a","title":"ROMA FDI\u652f\u6301\u7684\u6570\u636e\u6e90"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma_4","text":"","title":"ROMA\u5b89\u88c5\u914d\u7f6e"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_1","text":"FusionInsight 6.5.1\u914d\u5957\u7684ROMA\u7248\u672c\u4e3a ROMA 20.0.RC1","title":"\u7248\u672c\u914d\u5957\u5173\u7cfb"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_2","text":"\u53c2\u8003\u4ee5\u4e0b\u94fe\u63a5\u8bbf\u95eeROMA\u4ea7\u54c1\u6587\u6863\uff1a \u70b9\u51fb\u8bbf\u95ee","title":"\u5b89\u88c5\u6307\u5bfc\u6587\u6863"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_3","text":"\u5b89\u88c5\u89c4\u5212\u53ef\u53c2\u8003ROMA\u6587\u6863\u7ae0\u8282\uff1a\u201c\u5b89\u88c5\u4e0e\u8c03\u6d4b\u201d->\u201c\u8f6f\u4ef6\u5b89\u88c5\u201d->\u201c\u5b89\u88c5\u6982\u8ff0\u201d->\u201d\u5b89\u88c5\u89c4\u5212\u201d\u3002ROMA\u5b89\u88c5\u90e8\u7f72\u67093\u79cd\u7ec4\u7f51\u6a21\u5f0f\uff0c\u533a\u522b\u548c\u9009\u7528\u5efa\u8bae\u5982\u4e0b\uff1a \u90e8\u7f72\u6a21\u5f0f \u90e8\u7f72\u65b9\u6848\u63cf\u8ff0 \u9009\u7528\u5efa\u8bae 8VM\u57fa\u7840\u7ec4\u7f51 \u6700\u5c0f\u57fa\u7840\u90e8\u7f72\u6a21\u5f0f\uff0c\u5305\u542bFoundation\u3001APIC\u3001FDI\u3001MQS\u3001\u7edf\u4e00\u8fd0\u7ef4\u7b49\u57fa\u7840\u80fd\u529b\uff0cFDI-READER\u548cFDI-WRITER\u91c7\u7528\u5355\u673a\u90e8\u7f72\u3002 \u9002\u7528\u4e8e\u6d4b\u8bd5\u73af\u5883 13VM\u9ad8\u53ef\u9760\u7ec4\u7f51 \u76f8\u6bd4\u57fa\u7840\u7ec4\u7f51\uff0cFDI-READER\u548cFDI-WRITER\u5404\u4f7f\u75282\u53f0VM\u8fdb\u884c\u96c6\u7fa4\u90e8\u7f72\u6a21\u5f0f\uff0c\u63d0\u4f9b\u8d1f\u8f7d\u5747\u8861\u80fd\u529b\uff0c\u652f\u6301\u66f4\u9ad8\u7684\u6570\u636e\u8bbf\u95ee\u80fd\u529b\u3002\u6ce8\uff1a\u4e0d\u5305\u62ecLink\u7ec4\u4ef6\u3002 \u9002\u7528\u4e8e\u4e0d\u9700\u8981Link\u7ec4\u4ef6\u7684\u751f\u4ea7\u73af\u5883 20VM\u9ad8\u53ef\u9760\u7ec4\u7f51 \u76f8\u6bd413VM\u9ad8\u53ef\u9760\u7ec4\u7f51\u6a21\u5f0f\uff0c\u589e\u52a0\u4e867\u4e2a\u8282\u70b9\u7528\u4e8e\u90e8\u7f72LINK\u670d\u52a1 \u9700\u8981\u4f7f\u7528LINK\u7ec4\u4ef6\u7684\u751f\u7acb\u73af\u5883\u9009\u7528\u8be5\u65b9\u6848","title":"\u5b89\u88c5\u89c4\u5212"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_4","text":"\u53c2\u8003ROMA\u4ea7\u54c1\u6587\u6863 \u5b89\u88c5\u8c03\u6d4b \u7ae0\u8282\u8fdb\u884c\u5b89\u88c5\u3002 \u5b89\u88c5\u6ce8\u610f\u4e8b\u9879\uff1a ROMA\u4ea7\u54c1\u6587\u6863 \u5b89\u88c5\u8c03\u6d4b -> \u8f6f\u4ef6\u5b89\u88c5 -> \u5b89\u88c5ROMA \u7ae0\u8282\u6b65\u9aa4 12 \u4e2d\uff0c\u63d0\u4f9b\u4e86\u914d\u7f6eFusionInsight 6.5.1 \u4f9d\u8d56\u5305\u7684\u6307\u5bfc\u3002FI Hive\u3001FI HDFS\u53caFI Kafka \u9700\u8981\u7684\u4f9d\u8d56\u5305\u89c1ROMA\u5b89\u88c5\u6587\u6863 \u5b89\u88c5\u4e0e\u8c03\u6d4b -> \u8f6f\u4ef6\u5b89\u88c5 -> \u5b89\u88c5\u524d\u51c6\u5907 -> \u83b7\u53d6\u8f6f\u4ef6\u5305 \u3002 \u5176\u4e2dfihdfsreader/fihdfswriter\u4f9d\u8d56\u7684FusionInsight HD 6.5.1 \u6e05\u5355\u5982\u4e0b,\u76f8\u6bd4ROMA\u4ea7\u54c1\u6587\u6863 \u4e2d\u6d89\u53ca\u65b0\u589e\u90e8\u5206\u89c1\u659c\u4f53\u5185\u5bb9\uff1a commons-collections-3.2.2.jar commons-configuration2-2.1.1.jar hadoop-auth-3.1.1.jar hadoop-common-3.1.1.jar hadoop-hdfs-3.1.1.jar hadoop-hdfs-client-3.1.1.jar hadoop-hdfs-httpfs-3.1.1.jar htrace-core4-4.1.0-incubating.jar protobuf-java-2.5.0.jar re2j-1.1.jar stax2-api-3.1.4.jar woodstox-core-5.0.3.jar zookeeper-3.5.1.jar hadoop-mapreduce-client-core-3.1.1.jar fihivereader/fihiverwriter\u4f9d\u8d56\u7684FusionInsight HD 6.5.1 \u6e05\u5355\u5982\u4e0b,\u76f8\u6bd4ROMA\u4ea7\u54c1\u6587\u6863 \u4e2d\u6d89\u53ca\u65b0\u589e\u90e8\u5206\u89c1\u659c\u4f53\u5185\u5bb9\uff1a commons-configuration2-2.1.1.jar hadoop-auth-3.1.1.jar hadoop-common-3.1.1.jar hadoop-hdfs-3.1.1.jar hadoop-hdfs-client-3.1.1.jar hadoop-hdfs-httpfs-3.1.1.jar hive-common-3.1.0.jar hive-jdbc-3.1.0.jar hive-metastore-3.1.0.jar hive-serde-3.1.0.jar hive-service-3.1.0.jar hive-service-rpc-3.1.0.jar hive-shims-common-3.1.0.jar hive-standalone-metastore-3.1.0.jar htrace-core4-4.1.0-incubating.jar protobuf-java-2.5.0.jar re2j-1.1.jar stax2-api-3.1.4.jar woodstox-core-5.0.3.jar commons-collections-3.2.2.jar hive-exec-3.1.0.jar zookeeper-3.5.1.jar commons-collections-3.2.2.jar hadoop-mapreduce-client-core-3.1.1.jar \u5bf9\u5e94jar\u5305\u5728FusionInsight 6.5.1\u4e2d\u7684\u4f4d\u7f6e\u5982\u4e0b\u8868\uff1a jar\u6587\u4ef6\u540d \u83b7\u53d6\u8def\u5f84 hive-exec-3.1.0.jar /opt/hadoopclient/Hive/Beeline/lib/hive-exec-3.1.0.jar zookeeper-3.5.1.jar /opt/hadoopclient/ZooKeeper/zookeeper/zookeeper-3.5.1.jar commons-collections-3.2.2.jar /opt/hadoopclient/HDFS/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar hadoop-mapreduce-client-core-3.1.1.jar /opt/hadoopclient/Hive/Beeline/lib/hadoop-mapreduce-client-core-3.1.1.jar \u66ff\u6362FusionInsight\u4f7f\u7528\u7684guava\u7248\u672c,\u4eceguava 28\u66ff\u6362\u6210guava 26\u7248\u672c. guava 26\u7248\u672c\u4e0b\u8f7d\u5730\u5740\uff1a \u70b9\u51fb\u4e0b\u8f7d ,\u4e0b\u8f7d\u540e\u653e\u5230VM5 fihivereader/fihdfsreader/fikafkareader\u4ee5\u53caVM6 fihivewriter/fihdfswriter/fikafkawriter\u76ee\u5f55\u4e0b\uff0c\u5220\u9664\u539fguava-28 jar\u5305\u3002 \u4fee\u6539guava-26.0-jre.jar\u7684\u6240\u6709\u8005\u53ca\u6743\u9650\uff1a chown romafdi:users guava-26.0-jre.jar chmod 755 guava-26.0-jre.jar \u91cd\u542ffi reader/writer\u670d\u52a1\u4f7fjar\u5305\u66ff\u6362\u751f\u6548\uff0c\u4ee5\u4e0b\u4e3a\u66ff\u6362hivewriter guava\u7248\u672c\u540e\u91cd\u542f\u670d\u52a1\u7684\u547d\u4ee4\uff0c\u91cd\u542fhdfs/hive/kafa\u7684reader\u548cwriter\u7684\u547d\u4ee4\u7c7b\u4f3c\uff0c\u66ff\u6362 tomcat-fihivewriter \u4e3a\u5bf9\u5e94\u8def\u5f84\u5373\u53ef\u3002 su - romafdi cd /data01/fdi/tomcat-fihivewriter/bin sh catalina.sh stop;sh catalina.sh star","title":"\u5b89\u88c5\u6ce8\u610f\u4e8b\u9879"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#romafusioninsight","text":"","title":"ROMA\u5bf9\u63a5FusionInsight\u914d\u7f6e\u6307\u5bfc"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma-fdihive","text":"ROMA FDI\u6dfb\u52a0HIVE\u6570\u636e\u6e90\u53c2\u6570\u8bf4\u660e table th:first-of-type{width:100px;} | \u53c2\u6570\u540d\u79f0 | \u914d\u7f6e\u53c2\u8003 | | :------------- | :------------- | | \u5e94\u7528\u540d\u79f0 | \u4e0b\u62c9\u9009\u9879\u4e2d\u9009\u62e9\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u8bf4\u660e\uff1a \u82e5\u6ca1\u6709\u5e94\u7528\u540d\u53ef\u4ee5\u5173\u8054\uff0c\u5219\u5728ROMA Portal\u83dc\u5355\u680f\uff0c\u9009\u62e9\u201c\u5f53\u524d\u5e94\u7528\u540d > \u5e94\u7528\u6ce8\u518c\u201d\uff0c\u6ce8\u518c\u7528\u6237\u7684\u5e94 \u7528\u540d\u3002 |\u6570\u636e\u6e90\u540d\u79f0|\u7528\u6237\u81ea\u5b9a\u4e49\u3002\u4f8b\u5982\u201croma_fihive_test\u201d\u3002\u6570\u636e\u6e90\u6dfb\u52a0\u6210\u529f\u540e\uff0c\u5728\u521b\u5efa\u96c6\u6210\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u5728\u4e0b\u62c9\u9009\u9879\u4e2d\u81ea\u52a8\u5173\u8054\u5230\u6570\u636e\u6e90\u540d\u79f0\u3002| \u6570\u636e\u6e90\u7c7b\u578b| \u6570\u636e\u5e93\u7684\u7c7b\u578b\uff0c\u9009\u62e9\u201cFI Hive\u201d\u3002| \u5730\u533a | \u6570\u636e\u5e93\u6240\u5728\u7684\u5730\u57df\uff0c\u4f8b\u5982\u201cshenzhen\u201d\u3002| HDFS\u76ee\u5f55| \u4e34\u65f6HDFS\u6587\u4ef6\u5b58\u653e\u8def\u5f84\u3002 \u4f8b\u5982\u201c/user/ico_ipass/\u201d\u3002| FI HD\u7528\u6237\u540d | \u7528\u6237\u8ba4\u8bc1\u540d\u79f0\u3002\u4f8b\u5982\u201cioc_ipass\u201d\u3002 | \u7248\u672c\u53f7 | FI HD\u7684\u7248\u672c\u53f7\u3002\u4f8b\u5982\u201cFI HD V100R002C80U20\u201d\u3002| \u4e0a\u4f20\u914d\u7f6e\u6587\u4ef6 | \u5305\u542b\u7528\u6237\u8ba4\u8bc1\u6587\u4ef6\uff08krb5.conf\u3001user.keytab\uff09\u3001Hive\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff08fihiveclient.properties\uff09\u548cHDFS\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff08core-site.xml\u3001hdfs-site.xml\uff09\u3002\u5c065\u4e2a\u6587\u4ef6\u4e00\u8d77\u76f4\u63a5\u538b\u7f29\u6210ZIP\u5305\uff0c\u4f8b\u5982\u201cFIHive_dqf_ioc_ipass.zip\u201d\u3002 \u7528\u6237\u8ba4\u8bc1\u6587\u4ef6\u7684\u4e0b\u8f7d\u65b9\u6cd5\uff1a \u767b\u5f55FusionInsight Manager\u3002 \u5355\u51fb\u201c\u7cfb\u7edf\u8bbe\u7f6e\u201d\uff0c\u8fdb\u5165\u7cfb\u7edf\u8bbe\u7f6e\u9875\u9762\u3002 \u9009\u62e9\u201c\u7cfb\u7edf\u8bbe\u7f6e > \u914d\u7f6e > \u6743\u9650\u914d\u7f6e > \u7528\u6237\u7ba1\u7406\u201d\u3002 \u5728\u5bf9\u5e94\u7528\u6237\u7684\u6700\u53f3\u4fa7\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u4e0b\u8f7d\u7528\u6237\u51ed\u636e Hive\u3001HDFS\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u7684\u4e0b\u8f7d\u65b9\u6cd5\uff1a \u767b\u5f55FusionInsight Manager\u3002 \u5355\u51fb\u201c\u670d\u52a1\u7ba1\u7406\u201d\uff0c\u8fdb\u5165\u670d\u52a1\u9875\u9762\u3002 \u5355\u51fb\u9700\u8981\u7684FI\u670d\u52a1\uff0c\u8fdb\u5165\u201c\u670d\u52a1 > Hive \u670d\u52a1\u72b6\u6001\u201d\u9875\u9762\u3002 \u5355\u51fb\u201c\u4e0b\u8f7d\u5ba2\u6237\u7aef\u201d\uff0c\u8fdb\u5165\u201c\u4e0b\u8f7d\u5ba2\u6237\u7aef\u201d\u9875\u9762\u3002 \u8bbe\u7f6e\u201c\u5ba2\u6237\u7aef\u7c7b\u578b\u201d\u4e3a\u201c\u4ec5\u914d\u7f6e\u6587\u4ef6\u201d\u3002 \u5355\u51fb\u201c\u786e\u5b9a\u201d\u3002 \u4eceFi Hive\u4e0b\u8f7d\u6587\u4ef6\u4e2d\u83b7\u53d6\u5230hiveclient.properties\u6587\u4ef6\uff0c\u5e76\u6539\u6587\u4ef6\u540d\u4e3afihiveclient.properties\u3002\u4eceHDFS\u4e0b\u8f7d\u6587\u4ef6\u4e2d\u83b7\u53d6core-site.xml\u3001hdfs-site.xml\u6587\u4ef6\u3002| \u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a","title":"ROMA FDI\u5bf9\u63a5HIVE"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma-fdihdfs","text":"\u53c2\u6570\u540d\u79f0 \u914d\u7f6e\u53c2\u8003 \u5e94\u7528\u540d\u79f0 \u4e0b\u62c9\u9009\u9879\u4e2d\u9009\u62e9\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u82e5\u6ca1\u6709\u5e94\u7528\u540d\u53ef\u4ee5\u5173\u8054\uff0c\u5219\u5728ROMA Portal\u83dc\u5355\u680f\uff0c\u9009\u62e9\u201c\u5f53\u524d\u5e94\u7528\u540d > \u5e94\u7528\u6ce8\u518c\u201d\uff0c\u6ce8\u518c\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u6570\u636e\u6e90\u540d\u79f0 \u7528\u6237\u81ea\u5b9a\u4e49\u3002\u4f8b\u5982\u201croma_fihdfs_test\u201d\u3002 \u6570\u636e\u6e90\u6dfb\u52a0\u6210\u529f\u540e\uff0c\u5728\u521b\u5efa\u96c6\u6210\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u5728\u4e0b\u62c9\u9009\u9879\u4e2d\u81ea\u52a8\u5173\u8054\u5230\u6570\u636e\u6e90\u540d\u79f0\u3002 \u6570\u636e\u6e90\u7c7b\u578b \u6570\u636e\u5e93\u7684\u7c7b\u578b\uff0c\u9009\u62e9\u201cFIHDFS\u201d\u3002 Region \u6570\u636e\u5e93\u6240\u5728\u7684\u5730\u57df\uff0c\u4f8b\u5982\u201cshenzhen\u201d hdfsUrl \u4e34\u65f6HDFS\u6587\u4ef6\u5b58\u653e\u8def\u5f84\u3002 \u4f8b\u5982\u201c/user/ico_ipass/\u201d\u3002 principlename \u7528\u6237\u8ba4\u8bc1\u540d\u79f0\u3002\u4f8b\u5982\u201ceip_fdi_hdfs\u201d\u3002 \u7248\u672c\u53f7 FI HD\u7684\u7248\u672c\u53f7\u3002\u4f8b\u5982\u201cFI HD V100R002C80U20\u201d,\u6ce8:\u7531\u4e8e\u540e\u7aef\u5b9e\u9645\u4f7f\u7528\u7684\u662fFI 6.5.1 jar\u6587\u4ef6,\u9009\u62e9\u8be5\u7248\u672c\u53ef\u4ee5\u652f\u6301\u5bf9\u63a5FI 6.5.1 \u4e0a\u4f20\u914d\u7f6e\u6587\u4ef6 \u5305\u542b\u7528\u6237\u8ba4\u8bc1\u6587\u4ef6\uff08krb5.conf\u3001user.keytab\uff09\u548cHDFS\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff08core-site.xml\u3001hdfs-site.xml\uff09\u3002\u5c064\u4e2a\u6587\u4ef6\u4e00\u8d77\u76f4\u63a5\u538b\u7f29\u6210ZIP\u5305\uff0c\u4f8b\u5982\u201ceip_fdi_hdfs.zip\u201d\u3002\u83b7\u53d6\u65b9\u6cd5\u548cFI Hive\u7c7b\u4f3c\u3002 \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a","title":"ROMA FDI\u5bf9\u63a5HDFS"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma-fdikafka","text":"\u53c2\u6570\u540d\u79f0 \u5982\u4f55\u914d\u7f6e \u5e94\u7528\u540d\u79f0 \u4e0b\u62c9\u9009\u9879\u4e2d\u9009\u62e9\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u8bf4\u660e\uff1a \u82e5\u6ca1\u6709\u5e94\u7528\u540d\u53ef\u4ee5\u5173\u8054\uff0c\u5219\u5728ROMA Portal\u83dc\u5355\u680f\uff0c\u9009\u62e9\u201c\u5f53\u524d\u5e94\u7528\u540d > \u5e94\u7528\u6ce8\u518c\u201d\uff0c\u6ce8\u518c\u7528\u6237\u7684\u5e94\u7528\u540d\u3002 \u6570\u636e\u6e90\u540d\u79f0 \u7528\u6237\u81ea\u5b9a\u4e49\u3002\u4f8b\u5982\u201croma_fikafka_test\u201d\u3002 \u6570\u636e\u6e90\u6dfb\u52a0\u6210\u529f\u540e\uff0c\u5728\u521b\u5efa\u96c6\u6210\u4efb\u52a1\u65f6\uff0c\u53ef\u4ee5\u5728\u4e0b\u62c9\u9009\u9879\u4e2d\u81ea\u52a8\u5173\u8054\u5230\u6570\u636e\u6e90\u540d\u79f0\u3002 \u6570\u636e\u6e90\u7c7b\u578b \u6570\u636e\u5e93\u7684\u7c7b\u578b\uff0c\u9009\u62e9\u201cFI Kafka\u201d\u3002 Region \u6570\u636e\u5e93\u6240\u5728\u7684\u5730\u57df\uff0c\u4f8b\u5982\u201cshenzhen\u201d\u3002 \u8bc1\u4e66\u540d\u79f0 \u8bbf\u95eeKafka\u670d\u52a1\u5668\u7684\u8bc1\u4e66\u540d\uff08\u5373\u8bbf\u95eekafka\u673a\u5668\u7684\u673a\u673a\u7528\u6237\u540d\uff09\u3002\u4f8b\u5982\u201cioc\u201d\u3002 -------- ------------------------------------------------------------------- \u4e0a\u4f20\u914d\u7f6e\u6587\u4ef6 \u5305\u542b\u7528\u6237\u8ba4\u8bc1\u6587\u4ef6\uff08krb5.conf\u3001user.keytab\uff09\u548c\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff08fikafka-producer.properties\u3001fikafka-consumer.properties\uff09\u3002\u4f8b\u5982\u201cFIKafka_ioc.zip\u201d\u3002\u83b7\u53d6\u65b9\u6cd5\u548cFI Hive\u7c7b\u4f3c\uff0c\u4e0b\u8f7d\u540e\u9700\u89e3\u538b\u914d\u7f6e\u6587\u4ef6\uff0c\u91cd\u547d\u540dproducer.properties\u3001consumer.properties \u4e3afikafka-producer.properties\u3001fikafka-consumer.properties,\u4ece\u201cfikafka-producer.properties\u201d\u6587\u4ef6\u4e2d\u62f7\u8d1d\u5982\u4e0b\u5185\u5bb9\u5230\u201cfikafka-consumer.properties\u201d\u6587\u4ef6\u4e2d\uff1abootstrap.servers = 192.168.1.178:21007,192.168.1.125:21007,192.168.1.243:21007,192.168.1.133:21007","title":"ROMA FDI\u5bf9\u63a5KAFKA"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#roma-fusioninsight-hdfshivekafka","text":"","title":"ROMA \u5bf9\u63a5FusionInsight HDFS\u3001Hive\u53caKafka\u793a\u4f8b\u573a\u666f"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_5","text":"\u793a\u4f8b\u76ee\u7684 \u901a\u8fc7\u7aef\u5230\u7aef\u7684\u529f\u80fd\u793a\u4f8b\uff0c\u8ba9\u7528\u6237\u4e86\u89e3ROMA\u7684\u4e3b\u8981\u529f\u80fd\u53ca\u4f7f\u7528\u573a\u666f\u3002\u4e3b\u8981\u793a\u4f8b\u4ee5\u4e0b\u51e0\u65b9\u9762\u7684\u80fd\u529b\uff1a ROMA\u652f\u6301IOT\u63a5\u5165\u7684\u80fd\u529b\uff1aLINK\u7ec4\u4ef6\u652f\u6301\u63a5\u5165MQTT\u534f\u8bae\u8bbe\u5907\u3002 ROMA\u652f\u6301\u591a\u79cd\u6570\u636e\u6e90\u63a5\u5165\u5230 FusionInsight HIVE/HDFS, \u5305\u62ecAPI\u3001IOT\u3001MQS\u3001jdbc\u7b49\u3002 \u901a\u8fc7ROMA LiveData API\u670d\u52a1\uff0c\u793a\u4f8b\u5feb\u901f\u5f00\u653e\u6570\u636e\u670d\u52a1\u63a5\u53e3\u80fd\u529b\u3002 \u901a\u8fc7\u4e0d\u540c\u5e94\u7528\u95f4\u7684\u6570\u636e\u6c47\u805a\u5230\u6570\u636e\u6e56\uff0c\u793a\u4f8bROMA\u8de8\u7cfb\u7edf\u96c6\u6210\u7684\u80fd\u529b\u3002 ROMA FDI\u7f16\u6392\u670d\u52a1\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u8f6c\u6362\u7684\u80fd\u529b\u3002 \u793a\u4f8b\u573a\u666f \u793a\u4f8b\u573a\u666f\u6a21\u62df\u4e86\u7528\u7535\u7ba1\u7406\u7cfb\u7edf\u548c\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7ROMA\u5206\u522b\u83b7\u53d6\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u7528\u6237\u4fe1\u606f\u548c\u7528\u7535\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u7535\u8868\u4fe1\u606f\u5e76\u5199\u5165\u5230HIVE\u4e2d\uff0c\u7ecf\u8fc7Hive\u5206\u6790\u540e\uff0c\u5c06\u6570\u636e\u5bfc\u5165\u5230gauss\u6570\u636e\u5e93\uff0c\u5e76\u4ee5REST API\u63a5\u53e3\u65b9\u5f0f\u63d0\u4f9b\u7528\u7535\u6570\u636e\u67e5\u8be2\u670d\u52a1\u3002 \u4e3b\u8981\u6b65\u9aa4\uff1a \u901a\u8fc7\u6a21\u62df\u7535\u8868\uff08MQTT\u534f\u8bae IOT\u8bbe\u5907\uff09\uff0c\u88abLink\u63a5\u7ba1\u5e76\u901a\u8fc7ROMA MQS \u6d88\u606f\u670d\u52a1\u5b9e\u65f6\u53d1\u9001\u6d88\u606f\uff0c\u6d88\u606f\u6570\u636e\u901a\u8fc7ROMA FDI\u8f6c\u6362\u5199\u5165\u5230 HIVE\u8868\u4e2d\u3002\u5728\u63d2\u5165HIVE\u524d\uff0c\u901a\u8fc7FDI\u7f16\u6392\u63d2\u4ef6\u589e\u52a0\u7528\u7535\u6570\u636e\u4e0a\u62a5\u65f6\u95f4\u3002 \u6784\u9020txt\u6587\u4ef6\u683c\u5f0f\u7684\u7535\u8868\u5386\u53f2\u6570\u636e\u5e76\u901a\u8fc7FDI\u5bfc\u5165HIVE\u4e2d\u3002 \u6784\u9020\u5f00\u653e\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684Mysql \u6570\u636e\u4e3aREST API\uff0c\u901a\u8fc7FDI\u5bfc\u5165Hive\u4e2d\u3002 \u5728Hive\u8868\u4e2d\u8fdb\u884c\u5206\u6790\u4ee5\u540e\uff0c\u5c06\u6570\u636e\u5b58\u5165GaussDB\u3002 \u901a\u8fc7ROMA LiveData \uff0c\u5f00\u653e\u6570\u636e\u67e5\u8be2Restful API\u3002","title":"\u793a\u4f8b\u573a\u666f\u8bf4\u660e"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_6","text":"","title":"\u793a\u4f8b\u51c6\u5907\u8fc7\u7a0b"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_7","text":"*\u3000\u6a21\u62df\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u6570\u636e\u5e93\u4e2d\u7684\u5ba2\u6237\u4fe1\u606f\u3002\u51c6\u5907\u4e00\u53f0mysql\u670d\u52a1\u5668\uff0c\u521b\u5efacustomer_info\u8868\uff0c\u5305\u62ec\u7528\u6237ID\uff0c\u7528\u6237\u59d3\u540d\uff0c\u7528\u6237\u5bb6\u5ead\u4f4f\u5740\uff0c\u7528\u6237\u7535\u8868\u8bbe\u5907ID\u3002 ``` create database roma_customer; use roma_customer; create table customer_info(id int primary key auto_increment, user_name varchar(20), address varchar(50), device_id varchar(10)); insert into customer_info(user_name,address,device_id) values('matt','dongguan b4','meter02'); insert into customer_info(user_name,address,device_id) values('justin','shenzhen e1','meter02'); insert into customer_info(user_name,address,device_id) values('haoxi','chengdu u9','meter03'); ``` \u5728\u652f\u6301sftp\u7684linux\u670d\u52a1\u5668\u4e0a\u51c6\u5907\u7535\u8868\u6570\u636e\uff0c\u4ee5txt\u6587\u4ef6\u683c\u5f0f\u5b58\u653e\uff0c\u5404\u5217\u6570\u636e\u901a\u8fc7\u5206\u53f7 ; \u5206\u9694\uff0c\u4e3b\u8981\u5305\u62ec\u8bbe\u5907ID\u5b57\u6bb5\uff0c\u6570\u636e\u91c7\u96c6\u65e5\u671f\uff0c\u91c7\u96c6\u503c\u3002 mkdir /elechistorydata vi /elechistorydata/history.txt \u63d2\u5165\u4ee5\u4e0b\u5185\u5bb9\uff1a meter01;20190101;101 meter01;20190115;115 meter01;20190130;130 meter01;20190201;201 meter01;20190215;215 meter01;20190229;229 meter01;20190301;301 meter01;20190315;315 meter01;20190330;330 meter01;20190401;401 meter01;20190415;415 meter01;20190430;430 meter02;20190101;101 meter02;20190115;115 meter02;20190130;130 meter02;20190201;201 meter02;20190215;215 meter02;20190228;229 meter02;20190301;301 meter02;20190315;315 meter02;20190330;330 meter02;20190401;401 meter02;20190415;415 meter02;20190430;430 meter03;20190101;101 meter03;20190115;115 meter03;20190130;130 meter03;20190201;201 meter03;20190215;215 meter03;20190229;229 meter03;20190301;301 meter03;20190315;315 meter03;20190330;330 meter03;20190401;401 meter03;20190415;415 meter03;20190430;430 \u521b\u5efahive table\u7528\u4ee5\u5b58\u653e\u7535\u8868\u4e0a\u62a5\u6570\u636e\u3002 create database elect; use elect; create table elect_table(device_id string,upload_date date, elect_value int); \u521b\u5efa\u5f53\u6708\u6d4b\u8bd5\u6570\u636e\uff0c\u5728FI \u5ba2\u6237\u7aef\u4e0a\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4: beeline -e \"insert into elect.elect_table values('meter01',date_sub(current_date,dayofmonth(current_date)-1),500)\"; beeline -e \"insert into elect.elect_table values('meter02',date_sub(current_date,dayofmonth(current_date)-1),500)\"; beeline -e \"insert into elect.elect_table values('meter03',date_sub(current_date,dayofmonth(current_date)-1),500)\"; \u521b\u5efahive table\u7528\u4ee5\u5b58\u653e\u5ba2\u6237\u4fe1\u606f use elect; create table customer_info(id int,user_name varchar(20),address varchar(20), device_id varchar(20));","title":"\u6d4b\u8bd5\u6570\u636e\u51c6\u5907"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#api","text":"","title":"\u53d1\u5e03\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u4e3a\u6570\u636eAPI\u670d\u52a1"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_8","text":"\u70b9\u51fb\u5de6\u4e0a\u89d2\u5e94\u7528\u5217\u8868\u4e0b\u62c9\u6761\uff0c\u70b9\u51fb \u5e94\u7528\u6ce8\u518c \u3002 \u5728\u5f39\u51fa\u9875\u9762\u8f93\u5165\u5e94\u7528\u6ce8\u518c\u4fe1\u606f,\u70b9\u51fb \u63d0\u4ea4 \u5b8c\u6210\u6ce8\u518c\u3002\u5206\u522b\u6ce8\u518cCRMusers\u6570\u636e\u6e90\u548cElectricity.name\u5e94\u7528\u3002","title":"\u6ce8\u518c\u5e94\u7528"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#livedata","text":"\u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > Live Data > \u8d44\u6e90\u7ba1\u7406 > \u6570\u636e\u6e90\u7ba1\u7406\u201d\u3002\u5355\u51fb\u65b0\u5efa\uff0c\u521b\u5efa\u6570\u636e\u6e90\uff0c\u53c2\u8003\u914d\u53c2\u8003\u914d\u7f6e\u5982\u4e0b\u56fe\uff1a \u70b9\u51fb \u8fde\u63a5\u6d4b\u8bd5 \uff0c\u9a8c\u8bc1\u662f\u5426\u8fde\u901a\u3002\u9a8c\u8bc1\u6210\u529f\u540e\uff0c\u70b9\u51fb \u63d0\u4ea4 \uff0c\u5b8c\u6210\u6570\u636e\u6e90\u521b\u5efa\u3002","title":"\u521b\u5efaLiveData\u6570\u636e\u6e90"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#api_1","text":"\u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > Live Data > API\u8bbe\u8ba1\u201d\u3002\u5355\u51fb\u201c\u65b0\u5efa\u201d\uff0c\u8bbe\u7f6eAPI\u57fa\u672c\u4fe1\u606f\u3002\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a \u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u70b9\u51fb\u63d0\u4ea4\uff0c\u9009\u62e9 \u8fdb\u5165\u5f00\u53d1 \u6a21\u5f0f\uff0c\u9009\u62e9 \u6570\u636e\u5f00\u53d1 \uff0c\u9009\u62e9 \u6570\u636eAPI \u3002 \u6570\u636e\u6e90\u4fe1\u606f\u53c2\u8003\u5982\u4e0b\u914d\u7f6e\uff1a \u70b9\u51fb\u63d0\u4ea4\uff0c\u9009\u62e9 \u8fdb\u5165\u6d4b\u8bd5 \uff0c\u4fdd\u6301\u9ed8\u8ba4\u503c\uff0c\u70b9\u51fb\u6d4b\u8bd5API\uff0c\u67e5\u770b\u6d4b\u8bd5\u7ed3\u679c\u3002 \u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > Live Data > API\u90e8\u7f72\u201d\uff0c\u9009\u62e9\u5bf9\u5e94API,\u53f3\u4fa7\u70b9\u51fb\u90e8\u7f72\u56fe\u6807\uff0c\u4fdd\u6301\u9ed8\u8ba4\u914d\u7f6e\uff0c\u5b8c\u6210\u90e8\u7f72\u3002 \u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > API \u7f51\u5173 > API\u6388\u6743\u201d\u3002\u9009\u62e9\u5bf9\u5e94API,\u70b9\u51fb\u6388\u6743\u56fe\u6807\uff0c\u6388\u6743\u7ed9\u7528\u7535\u7ba1\u7406\u7cfb\u7edf\u5e94\u7528\uff0c\u5982Electricity.name \u5728ROMA\u5de6\u4fa7\u5bfc\u822a\u6811\u9009\u62e9\u201c\u670d\u52a1\u96c6\u6210 > API \u7f51\u5173 > API\u6d4b\u8bd5\u201d\u3002\u5728\u201cdata_api\u201dAPI\u7684\u6700\u53f3\u4fa7\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u8c03\u6d4b\u56fe\u6807\uff0c\u8fdb\u5165\u201cAPI\u6d4b\u8bd5\u201d\u9875\u9762\u3002\u201c\u8bf7\u6c42\u8def\u5f84\u201d\u5373API\u7684\u5bf9\u5916\u53d1\u5e03\u5730\u5740\u3002","title":"\u521b\u5efa/\u6d4b\u8bd5/\u90e8\u7f72/\u6388\u6743API"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#api-hive","text":"","title":"\u96c6\u6210API\u6570\u636e\u5230 Hive\u8868\u4e2d\u3002"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#api_2","text":"\u65b0\u589e\u52a0API\u6570\u636e\u6e90\uff0c\u5185\u5bb9\u53c2\u8003\u4e0b\u56fe\uff1a","title":"\u521b\u5efaAPI\u6570\u636e\u6e90"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#apihive","text":"\u5728\u5de6\u4fa7\u5bfc\u822a\u4e2d\uff0c\u9009\u62e9FDI \u4efb\u52a1\u7ba1\u7406 -> \u521b\u5efa\u4efb\u52a1 -> \u8868\u5355\u6a21\u5f0f \uff0c \u9009\u62e9\u5bf9\u5e94\u7684API\u6570\u636e\u6e90\u548chivewriter\uff0c\u586b\u5165\u76f8\u5173\u7684\u53c2\u6570\u4fe1\u606f\uff1a \u6e90\u7aef\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe\uff1a \u5176\u4e2d\u6d89\u53cametaData\u683c\u5f0f\u8f6c\u6362\u7684\u5185\u5bb9\uff0c\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u5185\u5bb9\uff1a [{\"name\":\"id\",\"type\":\"integer\",\"path\":\"retJSON.result[i].id\",\"format\":\"\"},{\"name\":\"user_name\",\"type\":\"string\",\"path\":\"retJSON.result[i]..user_name\",\"format\":\"\"},{\"name\":\"address\",\"type\":\"string\",\"path\":\"retJSON.result[i].address\",\"format\":\"\"},{\"name\":\"device_id\",\"type\":\"string\",\"path\":\"retJSON.result[i].device_id\",\"format\":\"\"}] \u76ee\u6807\u7aef\u914d\u7f6e\u4fe1\u606f\u53c2\u8003\u4e0b\u56fe\uff1a \u6620\u5c04\u5173\u7cfb\u5982\u4e0b\uff1a \u914d\u7f6e\u6210\u529f\u540e\uff0c\u70b9\u51fb\u4fdd\u5b58\uff0c\u4fdd\u5b58\u6210\u529f\u540e\uff0c\u542f\u52a8\u4efb\u52a1\u3002\u67e5\u770b\u4efb\u52a1\u8be6\u7ec6\uff0c\u6765\u81ea\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u7684\u6570\u636e\u5df2\u7ecf\u901a\u8fc7API\u548cFDI\uff0c\u5b9e\u73b0\u6570\u636e\u540c\u6b65\u5230Hive\u8868\u4e2d\u3002","title":"\u521b\u5efaAPI\u6570\u636e\u540c\u6b65\u5230hive\u7684\u4efb\u52a1"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#fdihive","text":"\u5b8c\u6210\u6dfb\u52a0HIVE\u548cHDFS\u6570\u636e\u6e90\u540e\uff0c\u5728Roma FDI\u4e2d\u521b\u5efa\u4efb\u52a1\uff0c\u5982\u679c\u6570\u636e\u91cf\u8f83\u5c11\u53ef\u76f4\u63a5\u4eceftp\u5bfc\u5165FI Hive\uff0c\u5982\u679c\u6570\u636e\u91cf\u8f83\u5927\uff0c\u9700\u8981\u4f7f\u7528FusionInsight Loader\u6216\u8005toolkits\u5de5\u5177\u5c06\u6570\u636e\u6279\u91cf\u5bfc\u5165FI\u4e2d\u3002 \u521b\u5efaFTP\u6570\u636e\u6e90,\u53ef\u53c2\u8003\u5982\u4e0b\u914d\u7f6e \u521b\u5efa\u8fc1\u79fb\u4efb\u52a1 \u9009\u62e9\u5b9a\u65f6\u4efb\u52a1\u7c7b\u578b,\u6e90\u7aef\u914d\u7f6e\u53c2\u8003\u5982\u4e0b: \u76ee\u6807\u7aef\u548c\u6620\u5c04\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe,\u70b9\u51fb\u4fdd\u5b58\u5b8c\u6210\u914d\u7f6e. \u4efb\u52a1\u5217\u8868\u4e2d,\u70b9\u51fb\u624b\u52a8\u8c03\u5ea6,\u89e6\u53d1\u4efb\u52a1,\u67e5\u770b\u4efb\u52a1\u6267\u884c\u7ed3\u679c.\u5982\u4e0b\u56fe\uff0c\u4efb\u52a1\u6267\u884c\u6210\u529f\u3002\u5c06\u6587\u672c\u7c7b\u578b\u5386\u53f2\u6570\u636e\u6210\u529f\u5bfc\u5165\u5230 FI Hive\u4e2d\u3002","title":"\u4f7f\u7528FDI\u52a0\u8f7d\u5386\u53f2\u6570\u636e\u5230hive\u8868\u4e2d"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#iotfi-hive","text":"","title":"\u6a21\u62dfIOT\u8bbe\u5907\uff0c\u5c06\u5b9e\u65f6\u589e\u91cf\u6570\u636e\u540c\u6b65\u5230FI HIVE\u8868\u4e2d"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#mqstopic","text":"\u521b\u5efaROMA\u5e73\u53f0\u63a5\u6536\u6d88\u606f\u7684Topic\uff0c\u5e76\u5c06Topic\u53d1\u5e03\u53ca\u8ba2\u9605\u5230\u5e94\u7528\u4e2d\u3002 \u521b\u5efaTopic\u521b\u5efa\u63a5\u6536IOT\u5e73\u53f0\u6d88\u606f\u7684Topic\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u6d88\u606f\u96c6\u6210 > \u6d88\u606f\u961f\u5217\u670d\u52a1 MQS > Topic\u7ba1\u7406\u201d\u3002 \u5355\u51fb\u201c\u521b\u5efaTopic\u201d\uff0c\u8fdb\u5165\u201c\u65b0\u589eTopic\u201d\u9875\u9762,\u521b\u5efa\u540d\u79f0T_ElectricityMeter\u7684Topic\u3002 Topic\u540d\u79f0\uff1aT_ElectricityMeter \u6240\u5c5e\u5e94\u7528\uff1aElectricity.name * \u5355\u51fb\u201c\u4fdd\u5b58\u201d\u3002","title":"MQS\u521b\u5efaTOPIC"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#topic","text":"\u53d1\u5e03Topic\u81f3roma.link.test\u5e94\u7528\u4e2d\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u6d88\u606f\u96c6\u6210 > \u6d88\u606f\u961f\u5217\u670d\u52a1 MQS > Topic\u7ba1\u7406\u201d\u3002 \u5728\u201cTopic\u540d\u79f0\u201d\u4e2d\u8f93\u5165\u201cT_ElectricityMeter\u201d\uff0c\u5355\u51fb\u201c\u67e5\u8be2\u201d\u3002 \u5728Topic\u7684\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u201c\u53d1\u5e03\u201d\uff0c\u8fdb\u5165\u201c\u53d1\u5e03\u201d\u9875\u9762\u3002\u914d\u7f6e\u53d1\u5e03\u4fe1\u606f\uff0c\u201c\u53d1\u5e03\u5e94\u7528\u201d\u9009\u62e9\u201cElectricity.name\u201d, \u5355\u51fb\u201c\u53d1\u5e03\u201d\uff0c\u9875\u9762\u63d0\u793a\u201c\u4fdd\u5b58\u4efb\u52a1\u6210\u529f\u201d\u3002","title":"\u53d1\u5e03Topic"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_9","text":"\u8ba2\u9605Topic\u81f3roma.link.test\u5e94\u7528\u4e2d\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u6d88\u606f\u96c6\u6210 > \u6d88\u606f\u961f\u5217\u670d\u52a1 MQS > Topic\u7ba1\u7406\u201d\u3002 \u5728\u201cTopic\u540d\u79f0\u201d\u4e2d\u8f93\u5165\u201cT_ElectricityMeter\u201d\uff0c\u5355\u51fb\u201c\u67e5\u8be2\u201d\u3002 \u5728Topic\u7684\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u201c\u8ba2\u9605\u201d\uff0c\u8fdb\u5165\u201c\u8ba2\u9605\u201d\u9875\u9762\u3002 \u914d\u7f6e\u8ba2\u9605\u4fe1\u606f\uff0c\u201c\u8ba2\u9605\u5e94\u7528\u201d\u9009\u62e9\u201cElectricity.name\u201d \u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u9875\u9762\u63d0\u793a\u201c\u4fdd\u5b58\u4efb\u52a1\u6210\u529f\u201d\u3002","title":"\u8ba2\u9605\u5e94\u7528"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#link","text":"","title":"LINK\u63a5\u5165\u6a21\u62df\u8bbe\u5907"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_10","text":"\u6bcf\u4e2a\u90e8\u7f72\u5728ROMA\u670d\u52a1\u5668\u7684LINK Connector\u7ec4\u4ef6\uff0c\u5747\u5728ROMA Portal\u5b58\u5728\u4e00\u4e2aConnector\u5b9e\u4f8b\uff0c\u5728\u521b\u5efa\u8bbe\u5907\u65f6\u9009\u62e9\u8be5\u5b9e\u4f8b\uff0c\u4ee5\u4f7f\u5b9e\u4f53\u8bbe\u5907\u8fde\u63a5\u5230\u8be5Connector\u3002\u7ed9Connector\u5b9e\u4f8b\u6dfb\u52a0\u5e94\u7528\uff08\u53ef\u6dfb\u52a0\u591a\u4e2a\uff09\uff0c\u8be5\u5e94\u7528\u4e0b\u7684\u8bbe\u5907\u53ef\u4ee5\u4f7f\u7528\u8be5Connector\u8fde\u63a5\u8bbe\u5907\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u7cfb\u7edf\u7ba1\u7406 > \u5b9e\u4f8b\u7ba1\u7406\u201d\u3002 \u5728\u5b9e\u4f8b\u7ba1\u7406\u9875\u9762\u4e2d\u67e5\u627e\u5230\u7684\u5bf9\u5e94\u5b9e\u4f8b\u7684\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u201c\u67e5\u770b\u5b9e\u4f8b\u5e94\u7528\u201d\u3002\u5728\u5f39\u51fa\u7684\u201c\u5b9e\u4f8b\u5e94\u7528\u7ba1\u7406\u201d\u9875\u9762\u5355\u51fb\u201c\u6dfb\u52a0\u5b9e\u4f8b\u5e94\u7528\u201d\u3002\u9009\u62e9\u53ef\u4f7f\u7528\u8be5Connector\u5b9e\u4f8b\u7684\u5e94\u7528\uff0c\u672c\u4f8b\u4e2d\u9009\u62e9Electricity.name\u5e94\u7528\u3002","title":"\u6dfb\u52a0\u5e94\u7528\u5b9e\u4f8b"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_11","text":"\u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u7cfb\u7edf\u7ba1\u7406 > \u6a21\u578b\u7ba1\u7406\u201d\uff0c\u8fdb\u5165\u754c\u9762\u3002 \u5355\u51fb\u201c\u521b\u5efa\u6a21\u578b\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u6a21\u578b\u201d\u5bf9\u8bdd\u6846\u3002 \u586b\u5199\u6a21\u578b\u4fe1\u606f\uff0c\u6839\u636e\u672c\u6a21\u578b\u5185\u4f7f\u7528\u7684\u4ea7\u54c1\u3001\u8bbe\u5907\u7528\u9014\u7b49\u81ea\u5b9a\u4e49\u6a21\u578b\u540d\u79f0\uff0c\u4f8b\u5982Voltage\u3002 \u586b\u5199\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u6a21\u578b\u521b\u5efa\u5b8c\u6210\u3002 \u5355\u51fb\u201c\u6a21\u578b\u8be6\u60c5\u201d\uff0c\u65b0\u589e\u4e00\u4e2a\u540d\u4e3astatus\u7684\u670d\u52a1\uff0c\u548c\u540d\u4e3aid\u548cvalue\u7684\u5c5e\u6027\u3002","title":"\u521b\u5efa\u6a21\u578b"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_12","text":"\u5728ROMA\u4e0a\u90e8\u83dc\u5355\u680f\uff0c\u9009\u62e9\u8fdb\u5165\u5e94\u7528\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u4ea7\u54c1\u7ba1\u7406 \u201d\u3002 \u5355\u51fb\u201c\u521b\u5efa\u4ea7\u54c1\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u4ea7\u54c1\u201d\u5bf9\u8bdd\u6846\u3002 \u6839\u636e\u4f7f\u7528\u7684\u5b9e\u4f53\u7f51\u5173\u8bbe\u5907\u7684\u4fe1\u606f\u586b\u5199\u4ea7\u54c1\u4fe1\u606f\u3002 \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe:","title":"\u521b\u5efa\u4ea7\u54c1\uff08\u7f51\u5173\u4ea7\u54c1\uff09"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_13","text":"\u5728ROMA Portal\u4e2d\u521b\u5efa\u666e\u901a\u4ea7\u54c1\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u4ea7\u54c1\u7ba1\u7406 \u201d\u3002 \u5355\u51fb\u201c\u521b\u5efa\u4ea7\u54c1\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u4ea7\u54c1\u201d\u5bf9\u8bdd\u6846\u3002\u586b\u5199\u4ea7\u54c1\u4fe1\u606f\uff0c\u7269\u6a21\u578b\u9009\u62e9\u524d\u9762\u521b\u5efa\u201c\u73af\u5883\u76d1\u6d4b\u6a21\u578b\u201d\uff0c\u5982\u4e0b\u56fe\u3002 \u914d\u7f6e\u53c2\u8003\u4e0b\u56fe: \u586b\u5199\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u4ea7\u54c1\u521b\u5efa\u5b8c\u6210\u3002","title":"\u521b\u5efa\u4ea7\u54c1\uff08\u666e\u901a\u4ea7\u54c1\uff09"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_14","text":"\u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907 \u96c6\u6210LINK > \u8bbe\u5907\u7ba1\u7406 \u201d\u3002 \u5355\u51fb\u201c\u521b\u5efa\u8bbe\u5907\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u8bbe\u5907\u201d\u5bf9\u8bdd\u6846\u3002 \u586b\u5199\u8bbe\u5907\u4fe1\u606f\uff0c\u4ea7\u54c1\u9009\u62e9\u524d\u9762\u521b\u5efa\u7684\u201c\u73af\u4fdd\u201d\uff0c\u8bbe\u5907\u6807\u7b7e\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u586b\u5199\u76f8\u5173\u8054\u7684\u3002 \u586b\u5199\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\uff0c\u8bbe\u5907\u521b\u5efa\u5b8c\u6210\u3002 \u5728\u201c\u8bbe\u5907\u8be6\u60c5\u201d\u9875\u9762\uff0c\u9009\u62e9\u201c\u5b50\u8bbe\u5907\u7ba1\u7406\u201d\u9875\u7b7e\uff0c\u53ef\u4ee5\u770b\u5230\u7f51\u5173\u5b50\u8bbe\u5907\u5217\u8868\u8fd8\u6ca1\u6709\u5b50\u8bbe\u5907\u3002","title":"\u521b\u5efa\u7f51\u5173\u8bbe\u5907"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_15","text":"\u4e0b\u8f7d\u7f51\u5173\u8bbe\u5907\u6a21\u62df\u7a0b\u5e8f,\u4e0b\u8f7d\u5730\u5740: \u70b9\u51fb\u4e0b\u8f7d \u542f\u52a8Eclipse\u7a0b\u5e8f\u3002 \u5bfc\u5165Demo\u5de5\u7a0b\u3002 \u5728\u83dc\u5355\u680f\u4e2d\u4f9d\u6b21\u9009\u62e9\u201cFile > Import > General > Existing Projects into Workspace\u201d\uff0c\u5355\u51fb\u201cNext\u201d\uff0c\u5bfc\u5165\u672c\u5730Demo\u5de5\u7a0b\u3002 \u5bfc\u5165\u540e\u76ee\u5f55\u683c\u5f0f\u5982\u4e0b: \u5728RUN Configuration\u4e2d\u914d\u7f6e\u542f\u52a8\u4e3b\u51fd\u6570,\u53c2\u8003\u5982\u4e0b\u914d\u7f6e: \u4fee\u6539GatawayStub.java\u7684start\u65b9\u6cd5\u5185\u5bb9,\u4e3b\u8981\u66ff\u6362\u5185\u5bb9\u5305\u62ecDeviceInfo\u4e2d\u7684\u914d\u7f6e\u4fe1\u606f,\u4ee5\u53ca\u8bbe\u5907ID. ``` DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); childDevice.setDeviceIdentifier(\"meter01\"); ``` ``` public void start() { // TODO Auto-generated method stub //\u5206\u522b\u66ff\u6362\u5982\u4e0b\u52a0\u9ed1\u5b57\u6bb5\u4e3a\u666e\u901a\u4ea7\u54c1\u7684\u5382\u5546ID\uff0c\u8bbe\u5907\u7c7b\u578b\uff0c\u8bbe\u5907\u578b\u53f7\uff0c\u5382\u5546\u540d\u79f0\u3002 DeviceInfo childDevice = new DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); //\u66ff\u6362meter01\u4e3a\u5b50\u8bbe\u59071\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier( \"meter01\"); context.registerDevice(childDevice, resp -> { if (resp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to register device:{}\", resp.getMessage()); return; } DeviceStatus deviceStatus1 = new DeviceStatus(); //\u66ff\u6362meter01\u4e3a\u5b50\u8bbe\u59071\u7684\u540d\u79f0\u3002 deviceStatus1.setDeviceIdentifier(\"meter01\"); deviceStatus1.setConnectStatus(DeviceConnStatus.online); context.changeDeviceConnectStatus(deviceStatus1, onlineResp -> { if (onlineResp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to change device state:{}\", resp.getMessage()); return; } // schduler.scheduleAtFixedRate(() -> { // JSONObject report = new JSONObject(); // report.put(\"id\", random.nextInt()); // report.put(\"value\", dateFormater.format(new Date())); // context.propertyReport(childDevice, \"status\", report); // }, 1, 10, TimeUnit.SECONDS); }); }); // TODO Auto-generated method stub // DeviceInfo childDevice = new DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); //\u66ff\u6362meter02\u4e3a\u5b50\u8bbe\u59072\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter02\"); context.registerDevice(childDevice, resp -> { if (resp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to register device:{}\", resp.getMessage()); return; } DeviceStatus deviceStatus2 = new DeviceStatus(); //\u66ff\u6362meter02\u4e3a\u5b50\u8bbe\u59072\u7684\u540d\u79f0\u3002 deviceStatus2.setDeviceIdentifier(\"meter02\"); deviceStatus2.setConnectStatus(DeviceConnStatus.online); context.changeDeviceConnectStatus(deviceStatus2, onlineResp -> { if (onlineResp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to change device state:{}\", resp.getMessage()); return; } // schduler.scheduleAtFixedRate(() -> { // JSONObject report = new JSONObject(); // report.put(\"id\", random.nextInt()); // report.put(\"value\", dateFormater.format(new Date())); // context.propertyReport(childDevice, \"status\", report); // }, 1, 10, TimeUnit.SECONDS); }); }); // TODO Auto-generated method stub // DeviceInfo childDevice = new DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); //\u66ff\u6362meter03\u4e3a\u5b50\u8bbe\u59073\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter03\"); context.registerDevice(childDevice, resp -> { if (resp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to register device:{}\", resp.getMessage()); return; } DeviceStatus deviceStatus3 = new DeviceStatus(); //\u66ff\u6362meter03\u4e3a\u5b50\u8bbe\u59073\u7684\u540d\u79f0\u3002 deviceStatus3.setDeviceIdentifier(\"meter03\"); deviceStatus3.setConnectStatus(DeviceConnStatus.online); context.changeDeviceConnectStatus(deviceStatus3, onlineResp -> { if (onlineResp.getResult() == ResponseResult.fail) { LOG.error(\"Failed to change device state:{}\", resp.getMessage()); return; } // schduler.scheduleAtFixedRate(() -> { // JSONObject report = new JSONObject(); // report.put(\"id\", random.nextInt()); // report.put(\"value\", dateFormater.format(new Date())); // context.propertyReport(childDevice, \"status\", report); // }, 1, 10, TimeUnit.SECONDS); }); }); } ``` \u4fee\u6539iotagent.properties\u914d\u7f6e\u53c2\u6570,\u914d\u7f6e\u53c2\u8003\u5982\u4e0b: \u53f3\u952e\u5355\u51fb\u201cGatewaySdkDemo\u201d\uff0c\u9009\u62e9\u201cRun as> Java Application\u201d\u3002 \u5355\u51fb\u201cOK\u201d\uff0c\u7b49\u5f85\u6267\u884c\u3002\u6267\u884c\u6210\u529f\u540e\u5982\u4e0b\u56fe\uff1a \u5728ROMA Portal\u67e5\u770b\u7f51\u5173\u8bbe\u5907\u548c\u7f51\u5173\u5b50\u8bbe\u5907\u662f\u5426\u5df2\u7ecf\u4e0a\u7ebf\u3002","title":"\u6a21\u62df\u4e0a\u7ebf\u7f51\u5173\u548c\u7535\u5b50\u8bbe\u5907"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_16","text":"\u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u89c4\u5219\u5f15\u64ce \u201d\u3002 \u5355\u51fb\u201c\u521b\u5efa\u89c4\u5219\u201d\u6309\u94ae\uff0c\u754c\u9762\u5f39\u51fa\u201c\u521b\u5efa\u89c4\u5219\u201d\u5bf9\u8bdd\u6846\u3002\u81ea\u5b9a\u4e49\u586b\u89c4\u5219\u540d\u79f0\uff0c\u4f8b\u5982meter_rule_01\u3002 \u5728\u89c4\u5219\u5f15\u64ce\u5217\u8868\uff0c\u5355\u51fb\u201c\u64cd\u4f5c\u201d\u5217\u7684\u201c\u7ba1\u7406\u201d\u3002\u5728\u5f39\u51fa\u7684\u201c\u89c4\u5219\u8be6\u60c5\u201d\u9875\u9762\u7684\u201c\u6570\u636e\u6e90\u7aef\u201d\u533a\u57df\uff0c\u5355\u51fb\u201c\u65b0\u589e\u201d\uff0c\u914d\u7f6e\u89c4\u5219\u5f15\u64ce\u7684\u6570\u636e\u6e90\u7aef\u3002 \u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\u3002 \u589e\u52a0\u76ee\u6807\u7aef\uff1a \u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u5355\u51fb\u201c\u4fdd\u5b58\u201d\u3002\u89c4\u5219\u5f15\u64ce\u914d\u7f6e\u5b8c\u6210\u3002 \u53c2\u8003\u4ee5\u4e0a\u6b65\u9aa4\u914d\u7f6e\u89c4\u5219\u5f15\u64cemeter_rule_02\u548cmeter_rule_03\u3002","title":"\u914d\u7f6e\u89c4\u5219\u5f15\u64ce"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#_17","text":"\u6253\u5f00Eclipse\u7a0b\u5e8f\u3002 \u4fee\u6539GatawayStub.java\u6587\u4ef6\u4e2d\u7684start\u65b9\u6cd5\u5982\u4e0b\uff1a ``` public void start() { //\u5206\u522b\u66ff\u6362\u5982\u4e0b\u52a0\u9ed1\u5b57\u6bb5\u4e3a\u666e\u901a\u4ea7\u54c1\u5382\u5546ID\uff0c\u8bbe\u5907\u7c7b\u578b\uff0c\u8bbe\u5907\u578b\u53f7\uff0c\u5382\u5546\u540d\u79f0\u3002 DeviceInfo childDevice = new DeviceInfo(null, \"TestId\", \"TestType\", \"TestModel\", \"HUAWEI\"); //\u66ff\u6362meter01\u4e3a\u5b50\u8bbe\u59071\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter01\"); JSONObject report1 = new JSONObject(); //\u66ff\u6362\u4e3a\u8bbe\u5907\u540d\u79f0\u548c\u5177\u4f53\u7684\u7535\u538b\u503c\u3002 report1.put(\"id\", \"meter01\"); report1.put(\"value\", \"700\"); context.propertyReport(childDevice, \"status\", report1); //\u66ff\u6362meter02\u4e3a\u5b50\u8bbe\u59072\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter02\"); JSONObject report2 = new JSONObject(); //\u66ff\u6362\u4e3a\u8bbe\u5907\u540d\u79f0\u548c\u5177\u4f53\u7684\u7535\u538b\u503c\u3002 report2.put(\"id\", \"meter02\"); report2.put(\"value\", \"710\"); context.propertyReport(childDevice, \"status\", report2); //\u66ff\u6362meter03\u4e3a\u5b50\u8bbe\u59073\u7684\u540d\u79f0\u3002 childDevice.setDeviceIdentifier(\"meter03\"); JSONObject report3 = new JSONObject(); //\u66ff\u6362\u4e3a\u8bbe\u5907\u540d\u79f0\u548c\u5177\u4f53\u7684\u7535\u538b\u503c\u3002 report3.put(\"id\", \"meter03\"); report3.put(\"value\", \"720\"); context.propertyReport(childDevice, \"status\", report3); }\u53f3\u952e\u5355\u51fb\u201cGatewaySdkDemo\u201d\uff0c\u9009\u62e9\u201cRun as> Java Application\u201d\u3002 ``` \u5355\u51fb\u201cOK\u201d\uff0c\u7b49\u5f85\u6267\u884c\u3002\u6267\u884c\u5b8c\u6210\u540e\u3002\u5c06\u53d1\u9001\u4e09\u4e2a\u7535\u8868\u7684\u6570\u636e\u5230ROMA\u3002 \u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u8bbe\u5907\u96c6\u6210 > \u8bbe\u5907\u96c6\u6210LINK > \u8bbe\u5907\u7ba1\u7406 \u201d\u3002 \u5206\u522b\u5728\u5b50\u8bbe\u5907meter01\u3001meter02\u3001meter03\u7684\u8bbe\u5907\u8be6\u60c5\u4e2d\u9009\u62e9\u201c\u8bbe\u5907\u65e5\u5fd7 > \u6d88\u606f\u5185\u5bb9\u67e5\u8be2\u201d\u67e5\u770b\u6536\u5230\u7684\u6d88\u606f\u5185\u5bb9\u3002","title":"\u6a21\u62df\u53d1\u9001\u6d88\u606f"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#mqs","text":"\u5728ROMA Portal\u5de6\u4fa7\u5bfc\u822a\u6811\u4e2d\uff0c\u9009\u62e9\u201c\u6d88\u606f\u96c6\u6210 > \u6d88\u606f\u961f\u5217\u670d\u52a1MQS > \u6d88\u606f\u67e5\u8be2 \u201d\u3002 \u5728\u201cTopic\u540d\u79f0\u201d\u4e2d\u8f93\u5165\u201cT_ElectricityMeter\u201d\uff0c\u5355\u51fb\u201c\u67e5\u8be2\u201d\u3002 \u5728\u5bf9\u5e94Topic\u7684\u201c\u64cd\u4f5c\u201d\u5217\uff0c\u5355\u51fb\u201c\u5185\u5bb9\u201d\u3002\u67e5\u770b\u63a5\u6536\u5230\u7684\u6d88\u606f\u5185\u5bb9\u3002","title":"\u67e5\u770bMQS\u4e2d\u53d1\u9001\u7684\u6d88\u606f"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#fdi","text":"\u901a\u8fc7\u4ee5\u4e0a\u6b65\u9aa4\uff0c\u5df2\u5b8c\u6210\u6a21\u62dfIOT\u8bbe\u5907\u5b9e\u65f6\u53d1\u9001\u6570\u636e\u5230MQS\u6d88\u606f\u670d\u52a1\uff0c\u4f46\u53d1\u9001\u7684\u6570\u636e\u53ea\u67092\u4e2a\u5b57\u6bb5\uff0c\u76f8\u6bd4hive\u4e2d\u7684\u8868\uff0c\u7f3a\u5c11\u4e00\u4e2a\u65f6\u95f4\u5b57\u6bb5\uff0c\u672c\u793a\u4f8b\u901a\u8fc7FDI\u7f16\u6392\u7279\u6027\uff0c\u5b9e\u73b0\u6570\u636e\u4eceMQS\u4e2d\u62bd\u53d6\u540e\u52a0\u8f7d\u5230hive\u8868\u524d\uff0c\u589e\u52a0\u5f53\u524d\u7cfb\u7edf\u65f6\u95f4\u5b57\u6bb5\u3002 \u521b\u5efaFDI\u4efb\u52a1\uff0c\u9009\u62e9\u7f16\u6392\u6a21\u5f0f\uff0c\u9009\u7528\u5b9e\u65f6\u540c\u6b65\u6a21\u5f0f \u8bfb\u8282\u70b9\u62d6\u9009MQS\u5230\u7f16\u5e03,\u5199\u8282\u70b9\u62d6\u9009FI Hive\uff0c\u753b\u5e03\u4e0a\u5b8c\u6210\u8fde\u7ebf\uff0c\u914d\u7f6e\u8bfb\u5199\u8282\u70b9\u4fe1\u606f\u3002 \u914d\u7f6e\u8f6c\u6362\u8282\u70b9\u4fe1\u606f\uff0c\u9009\u62e9\u811a\u672c\u6a21\u5f0f\u3002\u6dfb\u52a0\u5982\u4e0b\u4ee3\u7801\uff0c\u5b8c\u6210\u4e86\u6e90\u548c\u76ee\u6807\u7aef\u6570\u636e\u5b57\u6bb5\u7684\u6620\u5c04\uff0c\u5e76\u589e\u52a0\u4e86\u65f6\u95f4\u5b57\u6bb5\uff0c\u8bb0\u5f55\u4e86\u6bcf\u4e00\u6761\u8bb0\u5f55\u4ea7\u751f\u7684\u65f6\u95f4\u3002 targetObj = {}; targetObj.id = sourceObj.id; targetObj.value = sourceObj.value; var date = new Date(); var year = date.getFullYear().toString(); var mon = (date.getMonth()+1).toString(); var day = date.getDate().toString(); if(parseInt(mon) < 10){ mon = \"0\" + mon; } if(day < 10){ day = \"0\" + day; } targetObj.upload_date = year+\"-\"+mon+\"-\"+day; return targetObj; \u70b9\u51fb\u4fdd\u5b58\uff0c\u5b8c\u6210\u4efb\u52a1\u914d\u7f6e\u3002 \u53c2\u8003\u6a21\u62df\u53d1\u9001\u6d88\u606f\u6b65\u9aa4\u901a\u8fc7eclipse\u53d1\u9001\u6d4b\u8bd5\u6d88\u606f\u3002 \u67e5\u770bhive\u8868\uff0c\u6570\u636e\u5df2\u6210\u529f\u63d2\u5165\uff0c\u5982\u4e0b\u8868\u6240\u793a\uff0c2020-01-08\u4e3a\u65b0\u63d2\u5165\u76843\u6761\u5b9e\u65f6\u91c7\u96c6\u6570\u636e\u3002 \u81f3\u6b64\uff0c\u5b8c\u6210\u4e86\u5b9e\u65f6IOT\u8bbe\u5907\u6570\u636e\u5b9e\u65f6\u540c\u6b65\u5230FI hive\u7684\u6a21\u62df\u3002","title":"FDI\u91c7\u96c6\u5b9e\u65f6\u7528\u7535\u6570\u636e"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#hivemysql","text":"\u5728Hive beeline\u6a21\u5f0f\u4e0b\uff0c\u521b\u5efashell\u811a\u672c\u5e76\u6267\u884c\uff0c\u5b8c\u6210\u6570\u636e\u5206\u6790\u3002 touch elect_anaysis.sh chmod a+x elect_anaysis.sh \u811a\u672c\u5185\u5bb9\u5982\u4e0b\uff1a #!/bin/bash beeline -e \"create table elect.analysis(name string,year int,month int,value int);\" for i in 2019 2020 do for j in 1 2 3 4 do sql=\"insert into elect.analysis(name,year,month,value) select a.user_name,$i,$j,max(b.elect_value)-min(b.elect_value) from elect.customer_info a, elect.elect_table b where a.device_id=b.device_id and year(b.upload_date) = $i and month(b.upload_date) = $j group by a.user_name;\" beeline -e \"$sql\" done done beeline -e \"select * from elect.analysis\"; \u5728mysql\u6570\u636e\u5e93\u4e2d\u521b\u5efa\u8868,\u7528\u6765\u5b58\u653e\u7ecf\u8fc7hive\u5206\u6790\u8fc7\u7684\u6570\u636e\u3002\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u5982\u679c\u6570\u636e\u91cf\u8f83\u5927\uff0c\u63a8\u8350\u5c06Hive\u5206\u6790\u8fc7\u7684\u6570\u636e\u5b58\u653e\u5230GaussDB A\u4e2d\u3002 create table user_elects(user_name varchar(20),year int,month int,elect_value int); * \u521b\u5efaFDI\u4efb\u52a1\uff0c\u5b9a\u671f\u540c\u6b65hive\u5206\u6790\u8868\u4e2d\u7684\u7ed3\u679c\u5230mysql\u8868\u4e2d\u3002\u914d\u7f6e\u53c2\u8003\u4e0b\u56fe: \u4fdd\u5b58\u540e\uff0c\u624b\u52a8\u8c03\u5ea6\u4efb\u52a1\uff0c\u5b8c\u6210\u6570\u636e\u540c\u6b65\uff0c\u540c\u6b65\u5b8c\u6210\u540e\u5728mysql\u4e2d\u67e5\u770b\u6570\u636e\u3002 \u5982\u56fe\u6240\u793a\uff0c\u53ef\u4ee5\u67e5\u770b\u5230\u7528\u6237\u6bcf\u6708\u7684\u7528\u7535\u60c5\u51b5\u3002 \u53ef\u4ee5\u901a\u8fc7\u4e3a\u8868\u521b\u5efa\u7d22\u5f15\u7b49\uff0c\u52a0\u901f\u67e5\u8be2\u901f\u5ea6\u3002 create index index_uym on user_elects(user_name,year,month);","title":"\u5728Hive\u8868\u6267\u884c\u6570\u636e\u5206\u6790\uff0c\u5e76\u5c06\u7ed3\u679c\u5bfc\u5165\u5230mysql"},{"location":"Other/BestPractises/FusionInsight%E9%9B%86%E6%88%90Roma%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/#livedata-api","text":"\u53c2\u8003\u7ae0\u8282 \u53d1\u5e03\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u4e3a\u6570\u636eAPI\u670d\u52a1 \u8fc7\u7a0b\uff0c\u5bf9\u5916\u63d0\u4f9b\u6570\u636e\u67e5\u8be2APi,\u5305\u62ec\u901a\u8fc7\u59d3\u540d\u67e5\u5386\u53f2\u6570\u636e\uff0c\u4ee5\u53ca\u901a\u8fc7\u59d3\u540d\u6216\u8005\u59d3\u540d+\u5e74+\u6708\u7ec4\u5408\u67e5\u8be2\u7684\u63a5\u53e3\u3002 \u5982\u4e0b\u4e3a\u7528\u4f8b\u9a8c\u8bc1\u7ed3\u679c\uff1a \u67e5\u8be2\u7528\u6237\u6240\u6709\u7684\u7528\u7535\u8bb0\u5f55\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u67e5\u8be2\u7528\u6237\u57282020\u5e741\u6708\u7684\u7528\u7535\u8bb0\u5f55\uff0c\u7ed3\u679c\u5982\u4e0b\u56fe\uff1a \u81f3\u6b64\uff0c\u793a\u4f8b\u9a8c\u8bc1\u5b8c\u6210\uff0c\u901a\u8fc7ROMA\u91c7\u96c6\u7535\u8868\u8bb0\u5f55\uff0c\u5e76\u7ed3\u5408\u5386\u53f2\u6570\u636e\u548c\u5b9e\u65f6\u6570\u636e\uff0c\u901a\u8fc7FI HIVE\u8fdb\u884c\u5206\u6790\u7edf\u8ba1\u7528\u6237\u6bcf\u6708\u7528\u7535\u60c5\u51b5\uff0c\u5e76\u901a\u8fc7livedata API\u5f00\u653e\u4e86\u6570\u636e\u67e5\u8be2\u63a5\u53e3\u3002","title":"\u901a\u8fc7LiveData API\uff0c\u5f00\u653e\u6570\u636e\u670d\u52a1\u3002"},{"location":"SQL_Analytics/","text":"SQL\u5206\u6790 \u00b6 Apache Drill 1.15.0 \u2194 6.5 1.15.0 \u2194 C80 1.17.0 \u2194 8.0 1.17.0 \u2194 6.5 Apache Kylin 1.6.0 \u2194 C60 2.1.0 \u2194 C70 2.3.1 \u2194 C80 2.6.1 \u2194 6.5 3.0.1 \u2194 6.5 Kyligence Analytics Platform 2.2 \u2194 C60 2.3 \u2194 C60 2.4 \u2194 C70 2.5 \u2194 C70 3.0 \u2194 C80 Presto 0.155 \u2194 C60 0.184 \u2194 C70 0.196 \u2194 C80 0.210 \u2194 C80 0.210 \u2194 6.5 0.210 \u2194 8.0","title":"Index"},{"location":"SQL_Analytics/#sql","text":"Apache Drill 1.15.0 \u2194 6.5 1.15.0 \u2194 C80 1.17.0 \u2194 8.0 1.17.0 \u2194 6.5 Apache Kylin 1.6.0 \u2194 C60 2.1.0 \u2194 C70 2.3.1 \u2194 C80 2.6.1 \u2194 6.5 3.0.1 \u2194 6.5 Kyligence Analytics Platform 2.2 \u2194 C60 2.3 \u2194 C60 2.4 \u2194 C70 2.5 \u2194 C70 3.0 \u2194 C80 Presto 0.155 \u2194 C60 0.184 \u2194 C70 0.196 \u2194 C80 0.210 \u2194 C80 0.210 \u2194 6.5 0.210 \u2194 8.0","title":"SQL\u5206\u6790"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/","text":"ApacheDrill\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Drill 1.15.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive/HBase/Kafka) \u8bf4\u660e \u00b6 ApacheDrill\u5b89\u88c5\u4e3b\u673a\uff1a172.16.2.120 FI HD 6.5.1\u96c6\u7fa4\uff1a 172.16.4.21-23 \u914d\u7f6eOracle JDK \u00b6 \u767b\u9646oracle\u5b98\u7f51\uff0c\u627e\u5230\u5408\u9002\u7cfb\u7edf\u7684JDK\u7248\u672c \u5c06\u5b89\u88c5rpm\u5305\u5bfc\u5165\u81f3drill\u5b89\u88c5\u4e3b\u673a/usr/java\u8def\u5f84\u4e0b\uff0c\u5982\u679c\u6ca1\u6709\u6b64\u8def\u5f84\u9700\u521b\u5efa \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a chmod +x jdk-8u231-linux-x64.rpm rpm -ivh jdk-8u231-linux-x64.rpm \u53c2\u8003\u4e0b\u9762\u914d\u7f6e\uff0c\u5728 ~/.bash_profile \u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\uff1a export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64 export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib PATH=$JAVA_HOME/bin:$PATH:$HOME/bin export PATH export LD_LIBRARY_PATH=/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/libjsig.so:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH \u4f7f\u7528\u547d\u4ee4\u52a0\u8f7d\u5b89\u88c5\u7684jdk\uff1a source ~/.bash_profile \u5b89\u88c5Apache Drill \u00b6 \u4e0b\u8f7dApacheDrill wget http://apache.mirrors.hoobly.com/drill/drill-1.15.0/apache-drill-1.15.0.tar.gz \u6216\u8005\u4ece\u5b98\u7f51\u4e0b\u8f7d: \u5b89\u88c5drill \u5c06\u5b89\u88c5\u5305\u5bfc\u5165/opt/drill\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf apache-drill-1.15.0.tar.gz \u89e3\u538b\u538b\u7f29\u5305 \u542f\u52a8drill cd /opt/drill/apache-drill-1.15.0 bin/drill-embedded \u540c\u65f6\u53ef\u4ee5\u767b\u5f55 172.16.2.120:8047 \u6765\u67e5\u770bwebUI\u754c\u9762 \u5bf9\u63a5HDFS \u00b6 \u786e\u4fddapachedrill\u4e3b\u673a\u4e0e\u5bf9\u63a5\u96c6\u7fa4\u65f6\u95f4\u5dee\u5f02\u5c0f\u4e8e5\u5206\u949f \u5728apachedrill\u4e3b\u673a\u4e0a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef \u627e\u5230 apacchedrill\u5b89\u88c5\u8def\u5f84/bin/drill-config.sh \u914d\u7f6e\u6587\u4ef6\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a CP=\"$CP:/opt/65hadoopclient/HDFS/hadoop/share/hadoop/common/*\" CP=\"$CP:/opt/65hadoopclient/HDFS/hadoop/share/hadoop/common/lib/*\" \u8bf4\u660e\uff1a 1. \u5728\u5982\u56fe\u4f4d\u7f6e\u6dfb\u52a0Class Path\u53d8\u91cf,\u8def\u5f84\u4e3a\u534e\u4e3aFI HD HDFS\u5ba2\u6237\u7aef\u76f8\u5173\u4f9d\u8d56\u7684\u8def\u5f84 2. \u6dfb\u52a0\u4f9d\u8d56\u53c2\u6570\u7684\u4f4d\u7f6e\u8981\u6309\u7167\u622a\u56fe\u6240\u793a\uff0c\u5728 CP=\"$CP:$DRILL_HOME/jars/3rdparty/*\" \u4e4b\u540e` \u767b\u9646drill webUI \u754c\u9762\uff0c\u9009\u62e9Storage,\u521b\u5efa\u65b0\u7684huaweihdfs \u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"file\", \"connection\": \"hdfs://172.16.4.22:25000/\", \"config\": null, \"workspaces\": { \"tmp\": { \"location\": \"/tmp\", \"writable\": true, \"defaultInputFormat\": null, \"allowAccessOutsideWorkspace\": false } }, \"formats\": { \"json\": { \"type\": \"json\", \"extensions\": [ \"json\" ] } }, \"enabled\": true } \u5176\u4e2d172.16.4.22\u4e3a\u96c6\u7fa4namenode\u4e3b\u8282\u70b9 \u51c6\u5907\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u4e3b\u8282\u70b9172.16.4.21:/opt\u8def\u5f84,\u4f7f\u7528\u547d\u4ee4 find /opt -name hdfs.keytab \u67e5\u627ehdfs\u8ba4\u8bc1\u76f8\u5173keytab\u6587\u4ef6 \u5c06hdfs.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230apachedrill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u5728\u5bf9\u63a5\u96c6\u7fa4HDFS\u5ba2\u6237\u7aef\u4e2d\u627e\u5230HDFS\u76f8\u5173core-site.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u5bf9\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539\uff1a \u627e\u5230\u53c2\u6570\u9879fs.defaultFS,\u5c06\u503c\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip+25000\u7684\u5f62\u5f0f\uff1a \u4fdd\u5b58\u4fee\u6539 \u4fee\u6539drill conf\u8def\u5f84\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6drill-override.conf\uff0c\u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58: security.auth.principal: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hdfs.keytab\" \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684/tmp\u8def\u5f84\u4e0b\u521b\u5efajson\u683c\u5f0f\u7684\u6d4b\u8bd5\u6570\u636etest.json \u5185\u5bb9\u5982\u4e0b\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u4f7f\u7528\u547d\u4ee4 !quit \u505c\u6b62drill,\u518d\u91cd\u542fdrill \u5728\u547d\u4ee4\u884c\u4f7f\u7528\u547d\u4ee4 show databases; \u68c0\u67e5\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 select * from huaweihdfs.`tmp`.`test.json`; \u67e5\u627e\u6570\u636e\uff1a \u5bf9\u63a5HIVE \u00b6 \u53c2\u8003\u4e0a\u8ff0\u300a\u5bf9\u63a5HDFS\u300b\u7ae0\u8282\u5b8c\u6210\u5bf9\u63a5drill 1.15.0\u7248\u672c\u4e0eFI HD\u7684\u5bf9\u63a5\uff0c\u56e0\u4e3a\u5bf9\u63a5hdfs\u662f\u8fde\u63a5hive\u7684\u57fa\u7840\uff0c\u6240\u4ee5\u9700\u8981\u5b8c\u6210\u6b64\u6b65\u9aa4 \u5728\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u627e\u5230hdfs-site.xml\u4ee5\u53cayarn-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff08core-site.xml\u914d\u7f6e\u6587\u4ef6\u5728\u5bf9\u63a5HDFS\u7684\u65f6\u5019\u5df2\u7ecf\u5bfc\u5165\uff0c\u5e76\u4e14\u4fee\u6539\u8fc7fs.defaultFS\u914d\u7f6e\u9879\uff09\uff1a \u5e76\u4e14\u5bf9hdfs-site.xml\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539: \u627e\u5230\u914d\u7f6e\u9879dfs.client.failover.proxy.provider.hacluster\uff0c\u5c06\u503c\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value> </property> \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4172.16.4.21\uff0c \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u627e\u5230\u5bf9\u63a5hive\u76f8\u5173\u8ba4\u8bc1keytab\u6587\u4ef6hive.keytab\u5e76\u628a\u6b64\u6587\u4ef6\u4f20\u81f3drill\u4e3b\u673a/opt\u8def\u5f84\u4e0b\uff1a find /opt -name hive.keytab scp /opt/huawei/Bigdata/components/FusionInsight_HD_6.5.1/Hive/hive.keytab root@172.16.2.120:/opt \u767b\u9646drill\u4e3b\u673a\uff0c\u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230drill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff0c\u5982\u679c\u6b64\u6b65\u4e4b\u524d\u505a\u8fc7\u53ef\u4ee5\u4e0d\u505a \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/conf/drill-override.conf\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a drill.exec: { cluster-id: \"drillbits1\", zk.connect: \"localhost:2181\" security.auth.principal: \"hive/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hive.keytab\" sys.store.provider.local.path = \"/home/drill\" } \u91cd\u542fdrill,\u767b\u9646drill WebUI,\u521b\u5efa\u65b0\u7684storage\u540d\u5b57\u4e3ahuaweihive\u5e76enable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hive\", \"configProps\": { \"hive.metastore.uris\": \"thrift://172.16.4.21:21088,thrift://172.16.4.22:21088\", \"hive.metastore.kerberos.principal\": \"hive/hadoop.hadoop.com@HADOOP.COM\", \"hive.metastore.sasl.enabled\": \"true\", \"fs.default.name\": \"hdfs://172.16.4.22:25000\", \"inputDirectories\": \"hdfs://172.16.4.22:25000\" }, \"enabled\": true } \u5176\u4e2dhive.metastore.uris\u53ef\u5728\u96c6\u7fa4hive-site.xml\u6587\u4ef6\u4e2d\u67e5\u5230 \u540e\u53f0\u767b\u9646drill\uff0c\u4f7f\u7528 show databases \u547d\u4ee4\u67e5\u770b\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 use huaweihive; \u4f7f\u7528hive\u8fde\u63a5\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhive\u8868\uff1a \u4f7f\u7528\u67e5\u8be2\u547d\u4ee4\u67e5\u8be2hive\u8868\uff1a \u5bf9\u63a5kafka \u00b6 \u5bf9\u63a5\u53c2\u8003drill\u5b98\u65b9\u6587\u6863\uff1a https://drill.apache.org/docs/kafka-storage-plugin/ \u53ef\u4ee5\u77e5\u9053\u652f\u6301\u7684kafka\u8bfb\u53d6\u6570\u636e\u7ed3\u6784\u53ea\u80fd\u4e3ajson\uff1a \u51c6\u5907topic \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u65b0\u7684topic\uff1a bin/kafka-topics.sh --create --zookeeper 172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/kafka --partitions 1 --replication-factor 1 --topic druidkafka \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u751f\u4ea7\u6570\u636e\uff1a bin/kafka-console-producer.sh --broker-list 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic druidkafka --producer.config config/producer.properties \u8f93\u5165\u4e09\u6761\u6d4b\u8bd5\u6570\u636e\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u542f\u52a8apachedrill,\u767b\u9646webUI,\u70b9\u51fbStorage\u521b\u5efahuaweikafka,\u70b9\u51fbenable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"kafka\", \"kafkaConsumerProps\": { \"key.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"auto.offset.reset\": \"earliest\", \"bootstrap.servers\": \"172.16.4.21:21005,172.16.4.22:21005,172.16.4.23:21005\", \"group.id\": \"drill-query-consumer-1\", \"enable.auto.commit\": \"true\", \"value.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"session.timeout.ms\": \"30000\" }, \"enabled\": true } \u540e\u53f0\u547d\u4ee4\u884c\u8f93\u5165 show databases; \u68c0\u67e5\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 use huaweikafka; \u4f7f\u7528\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 show tables; \u67e5\u770btopic \u8f93\u5165\u547d\u4ee4 select * from druidkafka; \u67e5\u8be2\u521a\u521a\u521b\u5efa\u7684kafka topic \u5bf9\u63a5HBase \u00b6 \u505c\u6b62\u6b63\u5728\u8fd0\u884c\u7684drill \u53c2\u8003\u524d\u6587\u300a\u5bf9\u63a5HDFS\u300b,\u300a\u5bf9\u63a5HIVE\u300b\u7ae0\u8282\uff0c\u6210\u529f\u914d\u7f6e\u597d\u5bf9\u63a5hdfs,hive\u7ec4\u4ef6 \u627e\u5230drill\u5b89\u88c5\u76ee\u5f55\u4e0b./jar/ext/\u8def\u5f84\uff0c\u5c06drill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u91cd\u547d\u540d\u4e3azookeeper-3.4.12.jar.org\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06FI HD\u5ba2\u6237\u7aef\u4e2dzookeeper\u76f8\u5173jar\u5305 zookeeper-3.5.1.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\u3002\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\uff0cdrill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u7248\u672c\u592a\u65e7\uff0c\u5176\u5185\u90e8\u6ca1\u6709\u5b9a\u4e49send4LetterWord\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u5411FI HD zookeeper\u670d\u52a1\u81ea\u52a8\u83b7\u53d6\u8fde\u63a5zookeeper\u7684service principal (zookeeper/hadoop. hadoop.com@HADOOP.COM ) \u5728/opt\u8def\u5f84\u4e0b\u51c6\u5907jass.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff0c\u5176\u4e2d/opt/user.keytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8drill\u4e4b\u524d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u8fdb\u5fc5\u8981\u7684JVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u5b8c\u6210\u4e4b\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5\u662f\u5426\u52a0\u8f7d\u6210\u529f \u5c06HBase\u5ba2\u6237\u7aef\u5305\u542b\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b \u542f\u52a8drill\uff0c\u767b\u9646drill webUI\uff0c\u6dfb\u52a0Storage\u540d\u5b57\u4e3ahuaweihbase\u5e76enable,\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hbase\", \"config\": { \"hbase.zookeeper.quorum\": \"172.16.4.21,172.16.4.22\uff0c172.16.4.23\", \"hbase.zookeeper.property.clientPort\": \"24002\" }, \"size.calculator.enabled\": false, \"enabled\": true } \u767b\u9646drill\u540e\u53f0\uff0c\u4f7f\u7528\u547d\u4ee4 use huaweihbase; \u4f7f\u7528\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhbase\u8868\uff1a \u4f7f\u7528\u547d\u4ee4 select * from ImportTable; \u67e5\u770bhbase\u8868\uff1a","title":"1.15.0 <--> 6.5"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#apachedrillfusioninsight","text":"","title":"ApacheDrill\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#_1","text":"Apache Drill 1.15.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive/HBase/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#_2","text":"ApacheDrill\u5b89\u88c5\u4e3b\u673a\uff1a172.16.2.120 FI HD 6.5.1\u96c6\u7fa4\uff1a 172.16.4.21-23","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#oracle-jdk","text":"\u767b\u9646oracle\u5b98\u7f51\uff0c\u627e\u5230\u5408\u9002\u7cfb\u7edf\u7684JDK\u7248\u672c \u5c06\u5b89\u88c5rpm\u5305\u5bfc\u5165\u81f3drill\u5b89\u88c5\u4e3b\u673a/usr/java\u8def\u5f84\u4e0b\uff0c\u5982\u679c\u6ca1\u6709\u6b64\u8def\u5f84\u9700\u521b\u5efa \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a chmod +x jdk-8u231-linux-x64.rpm rpm -ivh jdk-8u231-linux-x64.rpm \u53c2\u8003\u4e0b\u9762\u914d\u7f6e\uff0c\u5728 ~/.bash_profile \u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\uff1a export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64 export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib PATH=$JAVA_HOME/bin:$PATH:$HOME/bin export PATH export LD_LIBRARY_PATH=/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/libjsig.so:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH \u4f7f\u7528\u547d\u4ee4\u52a0\u8f7d\u5b89\u88c5\u7684jdk\uff1a source ~/.bash_profile","title":"\u914d\u7f6eOracle JDK"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#apache-drill","text":"\u4e0b\u8f7dApacheDrill wget http://apache.mirrors.hoobly.com/drill/drill-1.15.0/apache-drill-1.15.0.tar.gz \u6216\u8005\u4ece\u5b98\u7f51\u4e0b\u8f7d: \u5b89\u88c5drill \u5c06\u5b89\u88c5\u5305\u5bfc\u5165/opt/drill\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf apache-drill-1.15.0.tar.gz \u89e3\u538b\u538b\u7f29\u5305 \u542f\u52a8drill cd /opt/drill/apache-drill-1.15.0 bin/drill-embedded \u540c\u65f6\u53ef\u4ee5\u767b\u5f55 172.16.2.120:8047 \u6765\u67e5\u770bwebUI\u754c\u9762","title":"\u5b89\u88c5Apache Drill"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#hdfs","text":"\u786e\u4fddapachedrill\u4e3b\u673a\u4e0e\u5bf9\u63a5\u96c6\u7fa4\u65f6\u95f4\u5dee\u5f02\u5c0f\u4e8e5\u5206\u949f \u5728apachedrill\u4e3b\u673a\u4e0a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef \u627e\u5230 apacchedrill\u5b89\u88c5\u8def\u5f84/bin/drill-config.sh \u914d\u7f6e\u6587\u4ef6\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a CP=\"$CP:/opt/65hadoopclient/HDFS/hadoop/share/hadoop/common/*\" CP=\"$CP:/opt/65hadoopclient/HDFS/hadoop/share/hadoop/common/lib/*\" \u8bf4\u660e\uff1a 1. \u5728\u5982\u56fe\u4f4d\u7f6e\u6dfb\u52a0Class Path\u53d8\u91cf,\u8def\u5f84\u4e3a\u534e\u4e3aFI HD HDFS\u5ba2\u6237\u7aef\u76f8\u5173\u4f9d\u8d56\u7684\u8def\u5f84 2. \u6dfb\u52a0\u4f9d\u8d56\u53c2\u6570\u7684\u4f4d\u7f6e\u8981\u6309\u7167\u622a\u56fe\u6240\u793a\uff0c\u5728 CP=\"$CP:$DRILL_HOME/jars/3rdparty/*\" \u4e4b\u540e` \u767b\u9646drill webUI \u754c\u9762\uff0c\u9009\u62e9Storage,\u521b\u5efa\u65b0\u7684huaweihdfs \u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"file\", \"connection\": \"hdfs://172.16.4.22:25000/\", \"config\": null, \"workspaces\": { \"tmp\": { \"location\": \"/tmp\", \"writable\": true, \"defaultInputFormat\": null, \"allowAccessOutsideWorkspace\": false } }, \"formats\": { \"json\": { \"type\": \"json\", \"extensions\": [ \"json\" ] } }, \"enabled\": true } \u5176\u4e2d172.16.4.22\u4e3a\u96c6\u7fa4namenode\u4e3b\u8282\u70b9 \u51c6\u5907\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u4e3b\u8282\u70b9172.16.4.21:/opt\u8def\u5f84,\u4f7f\u7528\u547d\u4ee4 find /opt -name hdfs.keytab \u67e5\u627ehdfs\u8ba4\u8bc1\u76f8\u5173keytab\u6587\u4ef6 \u5c06hdfs.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230apachedrill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u5728\u5bf9\u63a5\u96c6\u7fa4HDFS\u5ba2\u6237\u7aef\u4e2d\u627e\u5230HDFS\u76f8\u5173core-site.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u5bf9\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539\uff1a \u627e\u5230\u53c2\u6570\u9879fs.defaultFS,\u5c06\u503c\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip+25000\u7684\u5f62\u5f0f\uff1a \u4fdd\u5b58\u4fee\u6539 \u4fee\u6539drill conf\u8def\u5f84\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6drill-override.conf\uff0c\u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58: security.auth.principal: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hdfs.keytab\" \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684/tmp\u8def\u5f84\u4e0b\u521b\u5efajson\u683c\u5f0f\u7684\u6d4b\u8bd5\u6570\u636etest.json \u5185\u5bb9\u5982\u4e0b\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u4f7f\u7528\u547d\u4ee4 !quit \u505c\u6b62drill,\u518d\u91cd\u542fdrill \u5728\u547d\u4ee4\u884c\u4f7f\u7528\u547d\u4ee4 show databases; \u68c0\u67e5\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 select * from huaweihdfs.`tmp`.`test.json`; \u67e5\u627e\u6570\u636e\uff1a","title":"\u5bf9\u63a5HDFS"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#hive","text":"\u53c2\u8003\u4e0a\u8ff0\u300a\u5bf9\u63a5HDFS\u300b\u7ae0\u8282\u5b8c\u6210\u5bf9\u63a5drill 1.15.0\u7248\u672c\u4e0eFI HD\u7684\u5bf9\u63a5\uff0c\u56e0\u4e3a\u5bf9\u63a5hdfs\u662f\u8fde\u63a5hive\u7684\u57fa\u7840\uff0c\u6240\u4ee5\u9700\u8981\u5b8c\u6210\u6b64\u6b65\u9aa4 \u5728\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u627e\u5230hdfs-site.xml\u4ee5\u53cayarn-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff08core-site.xml\u914d\u7f6e\u6587\u4ef6\u5728\u5bf9\u63a5HDFS\u7684\u65f6\u5019\u5df2\u7ecf\u5bfc\u5165\uff0c\u5e76\u4e14\u4fee\u6539\u8fc7fs.defaultFS\u914d\u7f6e\u9879\uff09\uff1a \u5e76\u4e14\u5bf9hdfs-site.xml\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539: \u627e\u5230\u914d\u7f6e\u9879dfs.client.failover.proxy.provider.hacluster\uff0c\u5c06\u503c\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value> </property> \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4172.16.4.21\uff0c \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u627e\u5230\u5bf9\u63a5hive\u76f8\u5173\u8ba4\u8bc1keytab\u6587\u4ef6hive.keytab\u5e76\u628a\u6b64\u6587\u4ef6\u4f20\u81f3drill\u4e3b\u673a/opt\u8def\u5f84\u4e0b\uff1a find /opt -name hive.keytab scp /opt/huawei/Bigdata/components/FusionInsight_HD_6.5.1/Hive/hive.keytab root@172.16.2.120:/opt \u767b\u9646drill\u4e3b\u673a\uff0c\u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230drill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff0c\u5982\u679c\u6b64\u6b65\u4e4b\u524d\u505a\u8fc7\u53ef\u4ee5\u4e0d\u505a \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/conf/drill-override.conf\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a drill.exec: { cluster-id: \"drillbits1\", zk.connect: \"localhost:2181\" security.auth.principal: \"hive/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hive.keytab\" sys.store.provider.local.path = \"/home/drill\" } \u91cd\u542fdrill,\u767b\u9646drill WebUI,\u521b\u5efa\u65b0\u7684storage\u540d\u5b57\u4e3ahuaweihive\u5e76enable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hive\", \"configProps\": { \"hive.metastore.uris\": \"thrift://172.16.4.21:21088,thrift://172.16.4.22:21088\", \"hive.metastore.kerberos.principal\": \"hive/hadoop.hadoop.com@HADOOP.COM\", \"hive.metastore.sasl.enabled\": \"true\", \"fs.default.name\": \"hdfs://172.16.4.22:25000\", \"inputDirectories\": \"hdfs://172.16.4.22:25000\" }, \"enabled\": true } \u5176\u4e2dhive.metastore.uris\u53ef\u5728\u96c6\u7fa4hive-site.xml\u6587\u4ef6\u4e2d\u67e5\u5230 \u540e\u53f0\u767b\u9646drill\uff0c\u4f7f\u7528 show databases \u547d\u4ee4\u67e5\u770b\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 use huaweihive; \u4f7f\u7528hive\u8fde\u63a5\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhive\u8868\uff1a \u4f7f\u7528\u67e5\u8be2\u547d\u4ee4\u67e5\u8be2hive\u8868\uff1a","title":"\u5bf9\u63a5HIVE"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#kafka","text":"\u5bf9\u63a5\u53c2\u8003drill\u5b98\u65b9\u6587\u6863\uff1a https://drill.apache.org/docs/kafka-storage-plugin/ \u53ef\u4ee5\u77e5\u9053\u652f\u6301\u7684kafka\u8bfb\u53d6\u6570\u636e\u7ed3\u6784\u53ea\u80fd\u4e3ajson\uff1a \u51c6\u5907topic \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u65b0\u7684topic\uff1a bin/kafka-topics.sh --create --zookeeper 172.16.4.21:24002,172.16.4.22:24002,172.16.4.23:24002/kafka --partitions 1 --replication-factor 1 --topic druidkafka \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u751f\u4ea7\u6570\u636e\uff1a bin/kafka-console-producer.sh --broker-list 172.16.4.21:21007,172.16.4.22:21007,172.16.4.23:21007 --topic druidkafka --producer.config config/producer.properties \u8f93\u5165\u4e09\u6761\u6d4b\u8bd5\u6570\u636e\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u542f\u52a8apachedrill,\u767b\u9646webUI,\u70b9\u51fbStorage\u521b\u5efahuaweikafka,\u70b9\u51fbenable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"kafka\", \"kafkaConsumerProps\": { \"key.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"auto.offset.reset\": \"earliest\", \"bootstrap.servers\": \"172.16.4.21:21005,172.16.4.22:21005,172.16.4.23:21005\", \"group.id\": \"drill-query-consumer-1\", \"enable.auto.commit\": \"true\", \"value.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"session.timeout.ms\": \"30000\" }, \"enabled\": true } \u540e\u53f0\u547d\u4ee4\u884c\u8f93\u5165 show databases; \u68c0\u67e5\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 use huaweikafka; \u4f7f\u7528\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 show tables; \u67e5\u770btopic \u8f93\u5165\u547d\u4ee4 select * from druidkafka; \u67e5\u8be2\u521a\u521a\u521b\u5efa\u7684kafka topic","title":"\u5bf9\u63a5kafka"},{"location":"SQL_Analytics/ApacheDrill_1.15.0toHD651/#hbase","text":"\u505c\u6b62\u6b63\u5728\u8fd0\u884c\u7684drill \u53c2\u8003\u524d\u6587\u300a\u5bf9\u63a5HDFS\u300b,\u300a\u5bf9\u63a5HIVE\u300b\u7ae0\u8282\uff0c\u6210\u529f\u914d\u7f6e\u597d\u5bf9\u63a5hdfs,hive\u7ec4\u4ef6 \u627e\u5230drill\u5b89\u88c5\u76ee\u5f55\u4e0b./jar/ext/\u8def\u5f84\uff0c\u5c06drill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u91cd\u547d\u540d\u4e3azookeeper-3.4.12.jar.org\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06FI HD\u5ba2\u6237\u7aef\u4e2dzookeeper\u76f8\u5173jar\u5305 zookeeper-3.5.1.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\u3002\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\uff0cdrill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u7248\u672c\u592a\u65e7\uff0c\u5176\u5185\u90e8\u6ca1\u6709\u5b9a\u4e49send4LetterWord\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u5411FI HD zookeeper\u670d\u52a1\u81ea\u52a8\u83b7\u53d6\u8fde\u63a5zookeeper\u7684service principal (zookeeper/hadoop. hadoop.com@HADOOP.COM ) \u5728/opt\u8def\u5f84\u4e0b\u51c6\u5907jass.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff0c\u5176\u4e2d/opt/user.keytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8drill\u4e4b\u524d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u8fdb\u5fc5\u8981\u7684JVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u5b8c\u6210\u4e4b\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5\u662f\u5426\u52a0\u8f7d\u6210\u529f \u5c06HBase\u5ba2\u6237\u7aef\u5305\u542b\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b \u542f\u52a8drill\uff0c\u767b\u9646drill webUI\uff0c\u6dfb\u52a0Storage\u540d\u5b57\u4e3ahuaweihbase\u5e76enable,\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hbase\", \"config\": { \"hbase.zookeeper.quorum\": \"172.16.4.21,172.16.4.22\uff0c172.16.4.23\", \"hbase.zookeeper.property.clientPort\": \"24002\" }, \"size.calculator.enabled\": false, \"enabled\": true } \u767b\u9646drill\u540e\u53f0\uff0c\u4f7f\u7528\u547d\u4ee4 use huaweihbase; \u4f7f\u7528\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhbase\u8868\uff1a \u4f7f\u7528\u547d\u4ee4 select * from ImportTable; \u67e5\u770bhbase\u8868\uff1a","title":"\u5bf9\u63a5HBase"},{"location":"SQL_Analytics/ApacheDrillto17HD/","text":"ApacheDrill\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 ApacheDrill 1.17.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive/HBase/Kafka) ApacheDrill 1.17.0 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/HBase/Kafka) \u8bf4\u660e \u00b6 ApacheDrill\u5b89\u88c5\u4e3b\u673a\uff1a172.16.9.107 FI HD \u96c6\u7fa4\uff1a 172.16.4.131-133 \u914d\u7f6eOracle JDK \u00b6 \u8bf4\u660e\uff1a\u542f\u52a8drill\u8981\u4f7f\u7528oracle\u7684jdk\uff0c\u5426\u5219\u4f1a\u62a5\u9519 \u767b\u9646oracle\u5b98\u7f51\uff0c\u627e\u5230\u5408\u9002\u7cfb\u7edf\u7684JDK\u7248\u672c \u5c06\u5b89\u88c5rpm\u5305\u5bfc\u5165\u81f3drill\u5b89\u88c5\u4e3b\u673a/usr/java\u8def\u5f84\u4e0b\uff0c\u5982\u679c\u6ca1\u6709\u6b64\u8def\u5f84\u9700\u521b\u5efa \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a chmod +x jdk-8u231-linux-x64.rpm rpm -ivh jdk-8u231-linux-x64.rpm \u53c2\u8003\u4e0b\u9762\u914d\u7f6e\uff0c\u5728 ~/.bash_profile \u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\uff1a export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64 export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib PATH=$JAVA_HOME/bin:$PATH:$HOME/bin export PATH export LD_LIBRARY_PATH=/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/libjsig.so:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH \u4f7f\u7528\u547d\u4ee4\u52a0\u8f7d\u5b89\u88c5\u7684jdk\uff1a source ~/.bash_profile \u5b89\u88c5Apache Drill \u00b6 \u4e0b\u8f7dApacheDrill \u6216\u8005\u4ece\u5b98\u7f51\u4e0b\u8f7d: \u5b89\u88c5drill \u5c06\u5b89\u88c5\u5305\u5bfc\u5165/opt/drill\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf apache-drill-1.17.0.tar.gz \u89e3\u538b\u538b\u7f29\u5305 \u542f\u52a8drill cd /opt/drill/apache-drill-1.17.0 bin/drill-embedded \u540c\u65f6\u53ef\u4ee5\u767b\u5f55 172.16.9.107:8047 \u6765\u67e5\u770bwebUI\u754c\u9762 \u63d0\u793a\uff1a\u5982\u679c\u542f\u52a8\u8fc7\u7a0b\u9047\u5230\u62a5\u9519 Area [/root/drill/udf/registry] must be writable and executable for application user ,\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u66f4\u6539\u6307\u5b9a\u76ee\u5f55\u7684\u6743\u9650 cd root chmod -R 777 drill/ \u5bf9\u63a5HDFS \u00b6 \u786e\u4fddapachedrill\u4e3b\u673a\u4e0e\u5bf9\u63a5\u96c6\u7fa4\u65f6\u95f4\u5dee\u5f02\u5c0f\u4e8e5\u5206\u949f \u5728apachedrill\u4e3b\u673a\u4e0a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef \uff08\u91cd\u8981\uff09\u5728\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u627e\u5230core-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u4e14\u4fee\u6539\u914d\u7f6e\u9879 \u627e\u5230 apacchedrill\u5b89\u88c5\u8def\u5f84/bin/drill-config.sh \u914d\u7f6e\u6587\u4ef6\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a CP=\"$CP:/opt/65hadoopclient/HDFS/hadoop/share/hadoop/common/*\" CP=\"$CP:/opt/65hadoopclient/HDFS/hadoop/share/hadoop/common/lib/*\" \u8bf4\u660e\uff1a 1. \u5728\u5982\u56fe\u4f4d\u7f6e\u6dfb\u52a0Class Path\u53d8\u91cf,\u8def\u5f84\u4e3a\u534e\u4e3aFI HD HDFS\u5ba2\u6237\u7aef\u76f8\u5173\u4f9d\u8d56\u7684\u8def\u5f84 2. \u6dfb\u52a0\u4f9d\u8d56\u53c2\u6570\u7684\u4f4d\u7f6e\u8981\u6309\u7167\u622a\u56fe\u6240\u793a\uff0c\u5728 CP=\"$CP:$DRILL_HOME/jars/3rdparty/*\" \u4e4b\u540e` \u767b\u9646drill webUI \u754c\u9762\uff0c\u9009\u62e9Storage,\u521b\u5efa\u65b0\u7684huaweihdfs \u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"file\", \"connection\": \"hdfs://172.16.4.133:25000/\", \"config\": null, \"workspaces\": { \"tmp\": { \"location\": \"/tmp\", \"writable\": true, \"defaultInputFormat\": null, \"allowAccessOutsideWorkspace\": false } }, \"formats\": { \"json\": { \"type\": \"json\", \"extensions\": [ \"json\" ] } }, \"enabled\": true } \u5176\u4e2d172.16.4.133\u4e3a\u96c6\u7fa4namenode\u4e3b\u8282\u70b9 \u51c6\u5907\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u4e3b\u8282\u70b9172.16.4.131:/opt\u8def\u5f84,\u4f7f\u7528\u547d\u4ee4 find /opt -name hdfs.keytab \u67e5\u627ehdfs\u8ba4\u8bc1\u76f8\u5173keytab\u6587\u4ef6 \u5c06hdfs.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230apachedrill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u5728\u5bf9\u63a5\u96c6\u7fa4HDFS\u5ba2\u6237\u7aef\u4e2d\u627e\u5230HDFS\u76f8\u5173core-site.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u5bf9\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539\uff1a \u627e\u5230\u53c2\u6570\u9879fs.defaultFS,\u5c06\u503c\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip+25000\u7684\u5f62\u5f0f\uff1a \u4fdd\u5b58\u4fee\u6539 \u4fee\u6539drill conf\u8def\u5f84\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6drill-override.conf\uff0c\u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58: security.auth.principal: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hdfs.keytab\" \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684/tmp\u8def\u5f84\u4e0b\u521b\u5efajson\u683c\u5f0f\u7684\u6d4b\u8bd5\u6570\u636etest.json \u5185\u5bb9\u5982\u4e0b\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u4f7f\u7528\u547d\u4ee4 !quit \u505c\u6b62drill,\u518d\u91cd\u542fdrill \u5728\u547d\u4ee4\u884c\u4f7f\u7528\u547d\u4ee4 show databases; \u68c0\u67e5\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 select * from huaweihdfs.`tmp`.`test.json`; \u67e5\u627e\u6570\u636e\uff1a \u5bf9\u63a5HIVE \u00b6 \u53c2\u8003\u4e0a\u8ff0\u300a\u5bf9\u63a5HDFS\u300b\u7ae0\u8282\u5b8c\u6210\u5bf9\u63a5drill 1.17.0\u7248\u672c\u4e0eFI HD\u7684\u5bf9\u63a5\uff0c\u56e0\u4e3a\u5bf9\u63a5hdfs\u662f\u8fde\u63a5hive\u7684\u57fa\u7840\uff0c\u6240\u4ee5\u9700\u8981\u5b8c\u6210\u6b64\u6b65\u9aa4 \u5728\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u627e\u5230hdfs-site.xml\u4ee5\u53cayarn-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff08core-site.xml\u914d\u7f6e\u6587\u4ef6\u5728\u5bf9\u63a5HDFS\u7684\u65f6\u5019\u5df2\u7ecf\u5bfc\u5165\uff0c\u5e76\u4e14\u4fee\u6539\u8fc7fs.defaultFS\u914d\u7f6e\u9879\uff09\uff1a \u5e76\u4e14\u5bf9hdfs-site.xml\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539: \u627e\u5230\u914d\u7f6e\u9879dfs.client.failover.proxy.provider.hacluster\uff0c\u5c06\u503c\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value> </property> \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4172.16.4.131\uff0c \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u627e\u5230\u5bf9\u63a5hive\u76f8\u5173\u8ba4\u8bc1keytab\u6587\u4ef6hive.keytab\u5e76\u628a\u6b64\u6587\u4ef6\u4f20\u81f3drill\u4e3b\u673a/opt\u8def\u5f84\u4e0b\uff1a find /opt -name hive.keytab \u5c06hive.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u767b\u9646drill\u4e3b\u673a\uff0c\u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230drill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff0c\u5982\u679c\u6b64\u6b65\u4e4b\u524d\u505a\u8fc7\u53ef\u4ee5\u4e0d\u505a \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/conf/drill-override.conf\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a drill.exec: { cluster-id: \"drillbits1\", zk.connect: \"localhost:2181\" security.auth.principal: \"hive/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hive.keytab\" sys.store.provider.local.path = \"/home/drill\" } \u91cd\u542fdrill,\u767b\u9646drill WebUI,\u521b\u5efa\u65b0\u7684storage\u540d\u5b57\u4e3ahuaweihive\u5e76enable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hive\", \"configProps\": { \"hive.metastore.uris\": \"thrift://172.16.4.131:21088,thrift://172.16.4.132:21088\", \"hive.metastore.kerberos.principal\": \"hive/hadoop.hadoop.com@HADOOP.COM\", \"hive.metastore.sasl.enabled\": \"true\", \"fs.default.name\": \"hdfs://172.16.4.133:25000\", \"inputDirectories\": \"hdfs://172.16.4.133:25000\" }, \"enabled\": true } \u5176\u4e2dhive.metastore.uris\u53ef\u5728\u96c6\u7fa4hive-site.xml\u6587\u4ef6\u4e2d\u67e5\u5230 \u540e\u53f0\u767b\u9646drill\uff0c\u4f7f\u7528 show databases \u547d\u4ee4\u67e5\u770b\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u8868 use huaweihive.`default`; show tables; \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhive\u8868\uff1a \u4f7f\u7528\u67e5\u8be2\u547d\u4ee4\u67e5\u8be2hive\u8868\uff1a select * from huaweihive.`default`.`iris`; \u5bf9\u63a5kafka \u00b6 \u5bf9\u63a5\u53c2\u8003drill\u5b98\u65b9\u6587\u6863\uff1a https://drill.apache.org/docs/kafka-storage-plugin/ \u53ef\u4ee5\u77e5\u9053\u652f\u6301\u7684kafka\u8bfb\u53d6\u6570\u636e\u7ed3\u6784\u53ea\u80fd\u4e3ajson\uff1a \u51c6\u5907topic \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u65b0\u7684topic\uff1a bin/kafka-topics.sh --create --zookeeper 172.16.4.131:24002,172.16.4.132:24002,172.16.4.133:24002/kafka --partitions 2 --replication-factor 2 --topic druidkafka \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u751f\u4ea7\u6570\u636e\uff1a bin/kafka-console-producer.sh --broker-list 172.16.4.131:21007,172.16.4.132:21007,172.16.4.133:21007 --topic druidkafka --producer.config config/producer.properties \u8f93\u5165\u4e09\u6761\u6d4b\u8bd5\u6570\u636e\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u542f\u52a8apachedrill,\u767b\u9646webUI,\u70b9\u51fbStorage\u521b\u5efahuaweikafka,\u70b9\u51fbenable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"kafka\", \"kafkaConsumerProps\": { \"key.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"auto.offset.reset\": \"earliest\", \"bootstrap.servers\": \"172.16.4.131:21005,172.16.4.132:21005,172.16.4.133:21005\", \"group.id\": \"drill-query-consumer-1\", \"enable.auto.commit\": \"true\", \"value.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"session.timeout.ms\": \"30000\" }, \"enabled\": true } \u540e\u53f0\u547d\u4ee4\u884c\u8f93\u5165 show databases; \u68c0\u67e5\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 use huaweikafka; \u4f7f\u7528\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 show tables; \u67e5\u770btopic \u8f93\u5165\u547d\u4ee4 select * from druidkafka; \u67e5\u8be2\u521a\u521a\u521b\u5efa\u7684kafka topic \u5bf9\u63a5HBase \u00b6 \u505c\u6b62\u6b63\u5728\u8fd0\u884c\u7684drill \u53c2\u8003\u524d\u6587\u300a\u5bf9\u63a5HDFS\u300b,\u300a\u5bf9\u63a5HIVE\u300b\u7ae0\u8282\uff0c\u6210\u529f\u914d\u7f6e\u597d\u5bf9\u63a5hdfs,hive\u7ec4\u4ef6 \u627e\u5230drill\u5b89\u88c5\u76ee\u5f55\u4e0b./jar/ext/\u8def\u5f84\uff0c\u5c06drill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u91cd\u547d\u540d\u4e3azookeeper-3.4.12.jar.org\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06FI HD\u5ba2\u6237\u7aef\u4e2dzookeeper\u76f8\u5173jar\u5305 zookeeper-3.5.1.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\u3002\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\uff0cdrill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u7248\u672c\u592a\u65e7\uff0c\u5176\u5185\u90e8\u6ca1\u6709\u5b9a\u4e49send4LetterWord\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u5411FI HD zookeeper\u670d\u52a1\u81ea\u52a8\u83b7\u53d6\u8fde\u63a5zookeeper\u7684service principal (zookeeper/hadoop. hadoop.com@HADOOP.COM ) \u8bf4\u660e\uff1a\uff08\u91cd\u8981\uff09\u5982\u679c\u5bf9\u63a5\u7684\u662fmrs 8.0\u7248\u672c\uff0c\u5219\u9700\u8981\u66ff\u6362\u7684Jar\u5305\u4e3azookeeper-3.5.6-hw-ei-301001-SNAPSHOT.jar\uff0czookeeper-jute-3.5.6-hw-ei-301001-SNAPSHOT.jar \u5728/opt\u8def\u5f84\u4e0b\u51c6\u5907jass.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff0c\u5176\u4e2d/opt/user.keytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8drill\u4e4b\u524d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u8fdb\u5fc5\u8981\u7684JVM\u53c2\u6570\uff1a export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dsun.security.krb5.debug=false\" \u5b8c\u6210\u4e4b\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5\u662f\u5426\u52a0\u8f7d\u6210\u529f \u5c06HBase\u5ba2\u6237\u7aef\u5305\u542b\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b \u542f\u52a8drill\uff0c\u767b\u9646drill webUI\uff0c\u6dfb\u52a0Storage\u540d\u5b57\u4e3ahuaweihbase\u5e76enable,\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hbase\", \"config\": { \"hbase.zookeeper.quorum\": \"172.16.4.131,172.16.4.132\uff0c172.16.4.133\", \"hbase.zookeeper.property.clientPort\": \"24002\" }, \"size.calculator.enabled\": false, \"enabled\": true } \u767b\u9646drill\u540e\u53f0\uff0c\u4f7f\u7528\u547d\u4ee4 use huaweihbase; \u4f7f\u7528\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhbase\u8868\uff1a \u4f7f\u7528\u547d\u4ee4 select * from test6; \u67e5\u770bhbase\u8868\uff1a FAQ \u00b6 \u95ee\u9898\uff1a\u5bf9\u63a5hdfs\u7684\u65f6\u5019\u9047\u5230\u95ee\u9898\uff0c\u5177\u4f53\u662f\u914d\u7f6e\u597dhuaweihdfs\u4e4b\u540e\u5728\u540e\u53f0\u4f7f\u7528 show databases; \u67e5\u770b\u7684\u65f6\u5019\u62a5\u9519\uff1a Caused by: javax.security.sasl.SaslException: No common protection layer between client and server at com.sun.security.sasl.gsskerb.GssKrb5Client.doFinalHandshake(GssKrb5Client.java:251) \u95ee\u9898\u539f\u56e0\uff1ahadoop.rpc.protection\u7684\u914d\u7f6e\u5b58\u5728\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u7aef\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5 \u53d1\u73b0\u5728\u5bf9\u63a5Hdfs\u7684\u65f6\u5019\u6ca1\u6709\u50cfhive\u90a3\u6837\u5bfc\u5165\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff0c\u6240\u4ee5\u6211\u5c31\u628ahdfs\u7684core-site hdfs-site\u914d\u7f6e\u6587\u4ef6\u5bfc\u5165\u5230drill\u7684conf\u76ee\u5f55\u4e0b\uff0c \u68c0\u67e5hadoop.rpc.protection\u914d\u7f6e\u548c\u96c6\u7fa4\u7684\u5339\u914d\uff0c\u91cd\u542fdrill\u5c31\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898 \u95ee\u9898: \u5bf9\u63a5hbase\u7684\u65f6\u5019\u7167\u4e0a\u9762\u7684\u6587\u6863\u641e\uff0c\u5bf9\u63a5\u4e0d\u4e0a \u539f\u56e0\uff1ahbase-site.xml\u6587\u4ef6\u8981\u653e\u5230conf\u8def\u5f84\u4e0b \u95ee\u9898\uff1a \u8d77drill\u7684\u65f6\u5019\u8d77\u4e0d\u8d77\u6765\uff1a \u89e3\u51b3\u529e\u6cd5\uff1a\u4e0d\u8981\u4f7f\u7528huawei\u7684jdk,\u4f7f\u7528oracle\u7684jdk\u53ef\u4ee5\u8d77","title":"1.17.0 <--> 8.0"},{"location":"SQL_Analytics/ApacheDrillto17HD/#apachedrillfusioninsight","text":"","title":"ApacheDrill\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/ApacheDrillto17HD/#_1","text":"ApacheDrill 1.17.0 \u2194 FusionInsight HD 6.5 (HDFS/Hive/HBase/Kafka) ApacheDrill 1.17.0 \u2194 FusionInsight MRS 8.0 (HDFS/Hive/HBase/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/ApacheDrillto17HD/#_2","text":"ApacheDrill\u5b89\u88c5\u4e3b\u673a\uff1a172.16.9.107 FI HD \u96c6\u7fa4\uff1a 172.16.4.131-133","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/ApacheDrillto17HD/#oracle-jdk","text":"\u8bf4\u660e\uff1a\u542f\u52a8drill\u8981\u4f7f\u7528oracle\u7684jdk\uff0c\u5426\u5219\u4f1a\u62a5\u9519 \u767b\u9646oracle\u5b98\u7f51\uff0c\u627e\u5230\u5408\u9002\u7cfb\u7edf\u7684JDK\u7248\u672c \u5c06\u5b89\u88c5rpm\u5305\u5bfc\u5165\u81f3drill\u5b89\u88c5\u4e3b\u673a/usr/java\u8def\u5f84\u4e0b\uff0c\u5982\u679c\u6ca1\u6709\u6b64\u8def\u5f84\u9700\u521b\u5efa \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5b89\u88c5\uff1a chmod +x jdk-8u231-linux-x64.rpm rpm -ivh jdk-8u231-linux-x64.rpm \u53c2\u8003\u4e0b\u9762\u914d\u7f6e\uff0c\u5728 ~/.bash_profile \u914d\u7f6e\u6587\u4ef6\u6700\u540e\u589e\u52a0\uff1a export JAVA_HOME=/usr/java/jdk1.8.0_231-amd64 export CLASSPATH=$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib PATH=$JAVA_HOME/bin:$PATH:$HOME/bin export PATH export LD_LIBRARY_PATH=/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/server/libjvm.so:/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/usr/java/jdk1.8.0_231-amd64/jre/lib/amd64/libjsig.so:$LD_LIBRARY_PATH export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH \u4f7f\u7528\u547d\u4ee4\u52a0\u8f7d\u5b89\u88c5\u7684jdk\uff1a source ~/.bash_profile","title":"\u914d\u7f6eOracle JDK"},{"location":"SQL_Analytics/ApacheDrillto17HD/#apache-drill","text":"\u4e0b\u8f7dApacheDrill \u6216\u8005\u4ece\u5b98\u7f51\u4e0b\u8f7d: \u5b89\u88c5drill \u5c06\u5b89\u88c5\u5305\u5bfc\u5165/opt/drill\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf apache-drill-1.17.0.tar.gz \u89e3\u538b\u538b\u7f29\u5305 \u542f\u52a8drill cd /opt/drill/apache-drill-1.17.0 bin/drill-embedded \u540c\u65f6\u53ef\u4ee5\u767b\u5f55 172.16.9.107:8047 \u6765\u67e5\u770bwebUI\u754c\u9762 \u63d0\u793a\uff1a\u5982\u679c\u542f\u52a8\u8fc7\u7a0b\u9047\u5230\u62a5\u9519 Area [/root/drill/udf/registry] must be writable and executable for application user ,\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u66f4\u6539\u6307\u5b9a\u76ee\u5f55\u7684\u6743\u9650 cd root chmod -R 777 drill/","title":"\u5b89\u88c5Apache Drill"},{"location":"SQL_Analytics/ApacheDrillto17HD/#hdfs","text":"\u786e\u4fddapachedrill\u4e3b\u673a\u4e0e\u5bf9\u63a5\u96c6\u7fa4\u65f6\u95f4\u5dee\u5f02\u5c0f\u4e8e5\u5206\u949f \u5728apachedrill\u4e3b\u673a\u4e0a\u5b89\u88c5\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef \uff08\u91cd\u8981\uff09\u5728\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u627e\u5230core-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u4e14\u4fee\u6539\u914d\u7f6e\u9879 \u627e\u5230 apacchedrill\u5b89\u88c5\u8def\u5f84/bin/drill-config.sh \u914d\u7f6e\u6587\u4ef6\uff0c\u505a\u5982\u4e0b\u4fee\u6539\uff1a CP=\"$CP:/opt/65hadoopclient/HDFS/hadoop/share/hadoop/common/*\" CP=\"$CP:/opt/65hadoopclient/HDFS/hadoop/share/hadoop/common/lib/*\" \u8bf4\u660e\uff1a 1. \u5728\u5982\u56fe\u4f4d\u7f6e\u6dfb\u52a0Class Path\u53d8\u91cf,\u8def\u5f84\u4e3a\u534e\u4e3aFI HD HDFS\u5ba2\u6237\u7aef\u76f8\u5173\u4f9d\u8d56\u7684\u8def\u5f84 2. \u6dfb\u52a0\u4f9d\u8d56\u53c2\u6570\u7684\u4f4d\u7f6e\u8981\u6309\u7167\u622a\u56fe\u6240\u793a\uff0c\u5728 CP=\"$CP:$DRILL_HOME/jars/3rdparty/*\" \u4e4b\u540e` \u767b\u9646drill webUI \u754c\u9762\uff0c\u9009\u62e9Storage,\u521b\u5efa\u65b0\u7684huaweihdfs \u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"file\", \"connection\": \"hdfs://172.16.4.133:25000/\", \"config\": null, \"workspaces\": { \"tmp\": { \"location\": \"/tmp\", \"writable\": true, \"defaultInputFormat\": null, \"allowAccessOutsideWorkspace\": false } }, \"formats\": { \"json\": { \"type\": \"json\", \"extensions\": [ \"json\" ] } }, \"enabled\": true } \u5176\u4e2d172.16.4.133\u4e3a\u96c6\u7fa4namenode\u4e3b\u8282\u70b9 \u51c6\u5907\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u4e3b\u8282\u70b9172.16.4.131:/opt\u8def\u5f84,\u4f7f\u7528\u547d\u4ee4 find /opt -name hdfs.keytab \u67e5\u627ehdfs\u8ba4\u8bc1\u76f8\u5173keytab\u6587\u4ef6 \u5c06hdfs.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230apachedrill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u5728\u5bf9\u63a5\u96c6\u7fa4HDFS\u5ba2\u6237\u7aef\u4e2d\u627e\u5230HDFS\u76f8\u5173core-site.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u5bf9\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539\uff1a \u627e\u5230\u53c2\u6570\u9879fs.defaultFS,\u5c06\u503c\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip+25000\u7684\u5f62\u5f0f\uff1a \u4fdd\u5b58\u4fee\u6539 \u4fee\u6539drill conf\u8def\u5f84\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6drill-override.conf\uff0c\u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58: security.auth.principal: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hdfs.keytab\" \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684/tmp\u8def\u5f84\u4e0b\u521b\u5efajson\u683c\u5f0f\u7684\u6d4b\u8bd5\u6570\u636etest.json \u5185\u5bb9\u5982\u4e0b\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u4f7f\u7528\u547d\u4ee4 !quit \u505c\u6b62drill,\u518d\u91cd\u542fdrill \u5728\u547d\u4ee4\u884c\u4f7f\u7528\u547d\u4ee4 show databases; \u68c0\u67e5\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 select * from huaweihdfs.`tmp`.`test.json`; \u67e5\u627e\u6570\u636e\uff1a","title":"\u5bf9\u63a5HDFS"},{"location":"SQL_Analytics/ApacheDrillto17HD/#hive","text":"\u53c2\u8003\u4e0a\u8ff0\u300a\u5bf9\u63a5HDFS\u300b\u7ae0\u8282\u5b8c\u6210\u5bf9\u63a5drill 1.17.0\u7248\u672c\u4e0eFI HD\u7684\u5bf9\u63a5\uff0c\u56e0\u4e3a\u5bf9\u63a5hdfs\u662f\u8fde\u63a5hive\u7684\u57fa\u7840\uff0c\u6240\u4ee5\u9700\u8981\u5b8c\u6210\u6b64\u6b65\u9aa4 \u5728\u5bf9\u63a5\u96c6\u7fa4\u5ba2\u6237\u7aef\u4e2d\u627e\u5230hdfs-site.xml\u4ee5\u53cayarn-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff08core-site.xml\u914d\u7f6e\u6587\u4ef6\u5728\u5bf9\u63a5HDFS\u7684\u65f6\u5019\u5df2\u7ecf\u5bfc\u5165\uff0c\u5e76\u4e14\u4fee\u6539\u8fc7fs.defaultFS\u914d\u7f6e\u9879\uff09\uff1a \u5e76\u4e14\u5bf9hdfs-site.xml\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539: \u627e\u5230\u914d\u7f6e\u9879dfs.client.failover.proxy.provider.hacluster\uff0c\u5c06\u503c\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider <property> <name>dfs.client.failover.proxy.provider.hacluster</name> <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value> </property> \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4172.16.4.131\uff0c \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u627e\u5230\u5bf9\u63a5hive\u76f8\u5173\u8ba4\u8bc1keytab\u6587\u4ef6hive.keytab\u5e76\u628a\u6b64\u6587\u4ef6\u4f20\u81f3drill\u4e3b\u673a/opt\u8def\u5f84\u4e0b\uff1a find /opt -name hive.keytab \u5c06hive.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u767b\u9646drill\u4e3b\u673a\uff0c\u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230drill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff0c\u5982\u679c\u6b64\u6b65\u4e4b\u524d\u505a\u8fc7\u53ef\u4ee5\u4e0d\u505a \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/conf/drill-override.conf\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a drill.exec: { cluster-id: \"drillbits1\", zk.connect: \"localhost:2181\" security.auth.principal: \"hive/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hive.keytab\" sys.store.provider.local.path = \"/home/drill\" } \u91cd\u542fdrill,\u767b\u9646drill WebUI,\u521b\u5efa\u65b0\u7684storage\u540d\u5b57\u4e3ahuaweihive\u5e76enable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hive\", \"configProps\": { \"hive.metastore.uris\": \"thrift://172.16.4.131:21088,thrift://172.16.4.132:21088\", \"hive.metastore.kerberos.principal\": \"hive/hadoop.hadoop.com@HADOOP.COM\", \"hive.metastore.sasl.enabled\": \"true\", \"fs.default.name\": \"hdfs://172.16.4.133:25000\", \"inputDirectories\": \"hdfs://172.16.4.133:25000\" }, \"enabled\": true } \u5176\u4e2dhive.metastore.uris\u53ef\u5728\u96c6\u7fa4hive-site.xml\u6587\u4ef6\u4e2d\u67e5\u5230 \u540e\u53f0\u767b\u9646drill\uff0c\u4f7f\u7528 show databases \u547d\u4ee4\u67e5\u770b\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u67e5\u8868 use huaweihive.`default`; show tables; \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhive\u8868\uff1a \u4f7f\u7528\u67e5\u8be2\u547d\u4ee4\u67e5\u8be2hive\u8868\uff1a select * from huaweihive.`default`.`iris`;","title":"\u5bf9\u63a5HIVE"},{"location":"SQL_Analytics/ApacheDrillto17HD/#kafka","text":"\u5bf9\u63a5\u53c2\u8003drill\u5b98\u65b9\u6587\u6863\uff1a https://drill.apache.org/docs/kafka-storage-plugin/ \u53ef\u4ee5\u77e5\u9053\u652f\u6301\u7684kafka\u8bfb\u53d6\u6570\u636e\u7ed3\u6784\u53ea\u80fd\u4e3ajson\uff1a \u51c6\u5907topic \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u65b0\u7684topic\uff1a bin/kafka-topics.sh --create --zookeeper 172.16.4.131:24002,172.16.4.132:24002,172.16.4.133:24002/kafka --partitions 2 --replication-factor 2 --topic druidkafka \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u751f\u4ea7\u6570\u636e\uff1a bin/kafka-console-producer.sh --broker-list 172.16.4.131:21007,172.16.4.132:21007,172.16.4.133:21007 --topic druidkafka --producer.config config/producer.properties \u8f93\u5165\u4e09\u6761\u6d4b\u8bd5\u6570\u636e\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u542f\u52a8apachedrill,\u767b\u9646webUI,\u70b9\u51fbStorage\u521b\u5efahuaweikafka,\u70b9\u51fbenable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"kafka\", \"kafkaConsumerProps\": { \"key.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"auto.offset.reset\": \"earliest\", \"bootstrap.servers\": \"172.16.4.131:21005,172.16.4.132:21005,172.16.4.133:21005\", \"group.id\": \"drill-query-consumer-1\", \"enable.auto.commit\": \"true\", \"value.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"session.timeout.ms\": \"30000\" }, \"enabled\": true } \u540e\u53f0\u547d\u4ee4\u884c\u8f93\u5165 show databases; \u68c0\u67e5\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 use huaweikafka; \u4f7f\u7528\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 show tables; \u67e5\u770btopic \u8f93\u5165\u547d\u4ee4 select * from druidkafka; \u67e5\u8be2\u521a\u521a\u521b\u5efa\u7684kafka topic","title":"\u5bf9\u63a5kafka"},{"location":"SQL_Analytics/ApacheDrillto17HD/#hbase","text":"\u505c\u6b62\u6b63\u5728\u8fd0\u884c\u7684drill \u53c2\u8003\u524d\u6587\u300a\u5bf9\u63a5HDFS\u300b,\u300a\u5bf9\u63a5HIVE\u300b\u7ae0\u8282\uff0c\u6210\u529f\u914d\u7f6e\u597d\u5bf9\u63a5hdfs,hive\u7ec4\u4ef6 \u627e\u5230drill\u5b89\u88c5\u76ee\u5f55\u4e0b./jar/ext/\u8def\u5f84\uff0c\u5c06drill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u91cd\u547d\u540d\u4e3azookeeper-3.4.12.jar.org\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06FI HD\u5ba2\u6237\u7aef\u4e2dzookeeper\u76f8\u5173jar\u5305 zookeeper-3.5.1.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\u3002\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\uff0cdrill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u7248\u672c\u592a\u65e7\uff0c\u5176\u5185\u90e8\u6ca1\u6709\u5b9a\u4e49send4LetterWord\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u5411FI HD zookeeper\u670d\u52a1\u81ea\u52a8\u83b7\u53d6\u8fde\u63a5zookeeper\u7684service principal (zookeeper/hadoop. hadoop.com@HADOOP.COM ) \u8bf4\u660e\uff1a\uff08\u91cd\u8981\uff09\u5982\u679c\u5bf9\u63a5\u7684\u662fmrs 8.0\u7248\u672c\uff0c\u5219\u9700\u8981\u66ff\u6362\u7684Jar\u5305\u4e3azookeeper-3.5.6-hw-ei-301001-SNAPSHOT.jar\uff0czookeeper-jute-3.5.6-hw-ei-301001-SNAPSHOT.jar \u5728/opt\u8def\u5f84\u4e0b\u51c6\u5907jass.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff0c\u5176\u4e2d/opt/user.keytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8drill\u4e4b\u524d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u8fdb\u5fc5\u8981\u7684JVM\u53c2\u6570\uff1a export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com -Dsun.security.krb5.debug=false\" \u5b8c\u6210\u4e4b\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5\u662f\u5426\u52a0\u8f7d\u6210\u529f \u5c06HBase\u5ba2\u6237\u7aef\u5305\u542b\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b \u542f\u52a8drill\uff0c\u767b\u9646drill webUI\uff0c\u6dfb\u52a0Storage\u540d\u5b57\u4e3ahuaweihbase\u5e76enable,\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hbase\", \"config\": { \"hbase.zookeeper.quorum\": \"172.16.4.131,172.16.4.132\uff0c172.16.4.133\", \"hbase.zookeeper.property.clientPort\": \"24002\" }, \"size.calculator.enabled\": false, \"enabled\": true } \u767b\u9646drill\u540e\u53f0\uff0c\u4f7f\u7528\u547d\u4ee4 use huaweihbase; \u4f7f\u7528\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhbase\u8868\uff1a \u4f7f\u7528\u547d\u4ee4 select * from test6; \u67e5\u770bhbase\u8868\uff1a","title":"\u5bf9\u63a5HBase"},{"location":"SQL_Analytics/ApacheDrillto17HD/#faq","text":"\u95ee\u9898\uff1a\u5bf9\u63a5hdfs\u7684\u65f6\u5019\u9047\u5230\u95ee\u9898\uff0c\u5177\u4f53\u662f\u914d\u7f6e\u597dhuaweihdfs\u4e4b\u540e\u5728\u540e\u53f0\u4f7f\u7528 show databases; \u67e5\u770b\u7684\u65f6\u5019\u62a5\u9519\uff1a Caused by: javax.security.sasl.SaslException: No common protection layer between client and server at com.sun.security.sasl.gsskerb.GssKrb5Client.doFinalHandshake(GssKrb5Client.java:251) \u95ee\u9898\u539f\u56e0\uff1ahadoop.rpc.protection\u7684\u914d\u7f6e\u5b58\u5728\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u7aef\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5 \u53d1\u73b0\u5728\u5bf9\u63a5Hdfs\u7684\u65f6\u5019\u6ca1\u6709\u50cfhive\u90a3\u6837\u5bfc\u5165\u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\uff0c\u6240\u4ee5\u6211\u5c31\u628ahdfs\u7684core-site hdfs-site\u914d\u7f6e\u6587\u4ef6\u5bfc\u5165\u5230drill\u7684conf\u76ee\u5f55\u4e0b\uff0c \u68c0\u67e5hadoop.rpc.protection\u914d\u7f6e\u548c\u96c6\u7fa4\u7684\u5339\u914d\uff0c\u91cd\u542fdrill\u5c31\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898 \u95ee\u9898: \u5bf9\u63a5hbase\u7684\u65f6\u5019\u7167\u4e0a\u9762\u7684\u6587\u6863\u641e\uff0c\u5bf9\u63a5\u4e0d\u4e0a \u539f\u56e0\uff1ahbase-site.xml\u6587\u4ef6\u8981\u653e\u5230conf\u8def\u5f84\u4e0b \u95ee\u9898\uff1a \u8d77drill\u7684\u65f6\u5019\u8d77\u4e0d\u8d77\u6765\uff1a \u89e3\u51b3\u529e\u6cd5\uff1a\u4e0d\u8981\u4f7f\u7528huawei\u7684jdk,\u4f7f\u7528oracle\u7684jdk\u53ef\u4ee5\u8d77","title":"FAQ"},{"location":"SQL_Analytics/ApacheKylin3.0.1/","text":"Apache Kylin3.0.1\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 3.0.1 \u2194 FusionInsight HD 6.5 (Hive/HBase/Kafka/Spark) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u5982\u679c\u9700\u8981\u8bfb\u53d6hive\u5206\u533a\u8868\u9700\u8981\u4fee\u6539\u6e90\u7801\uff0c\u672c\u6587\u4e0d\u63d0\u4f9b\u8be5\u65b9\u6cd5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1(\u82e5kylin\u4e3b\u673a\u4e0e\u96c6\u7fa4\u65f6\u95f4\u57285min\u4e4b\u5185\u6b64\u6b65\u9aa4\u53ef\u9009) \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/135_651armhdclient/hadoopclient \u5b89\u88c5JDK1.8\uff08\u53ef\u9009\uff09 rpm -Uvh jdk-8u112-linux-x64.rpm \u4e0b\u8f7dKylin \u00b6 Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.1\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-3.0.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin \u4e0b\u8f7d\u89e3\u538bKylin \u00b6 \u4e0b\u8f7dKylin-3.0.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305: https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-3.0.1/apache-kylin-3.0.1-bin-hbase1x.tar.gz \u4e0a\u4f20apache-kylin-3.0.1-hbase1x-bin.tar.gz\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt/kylin tar -xvf apache-kylin-3.0.1-bin-hbase1x.tar.gz \u914d\u7f6eKylin \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/kylin/apache-kylin-3.0.1-bin-hbase1x \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/135_651armhdclient/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/135_651armhdclient/hadoopclient/Hive/config export HCAT_HOME=/opt/135_651armhdclient/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/135_651armhdclient/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit developuser Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/bin ./check-env.sh \u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879 \u00b6 \u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..*|mapred\\..* \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit developuser beeline \u4fee\u6539kylin.properties\uff1a vi /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.16.10.131:24002,172.16.10.132:24002,172.16.10.133:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/135_651armhdclient/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/135_651armhdclient/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/kylin/apache-kylin-3.0.1-bin-hbase1x/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/kylin/apache-kylin-3.0.1-bin-hbase1x/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84 \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2 Streaming Sample\u7528\u4f8b\uff0cKafka\u666e\u901a\u6a21\u5f0f \u00b6 \u53c2\u8003\u5b98\u65b9\u7f51\u7ad9http://kylin.apache.org/docs30/tutorial/kylin_sample.html \uff08\u91cd\u8981\uff09\u9996\u5148\u4e0b\u8f7d\u96c6\u7fa4\u8ba4\u8bc1\u6587\u4ef6krb5.conf\uff0c \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u8282\u70b9\uff0c\u5c06\u4e0b\u8f7d\u4e0b\u6765\u6709\u6548\u7684krb5.conf\u6587\u4ef6\u653e\u7f6e\u5230\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9\u7684/etc/\u8def\u5f84\u4e0b\uff0c\u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf\" \u52a0\u8f7d\u8ba4\u8bc1\u53c2\u6570 \u4e0b\u56fe\u4ee5\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u4e3a\u4f8b\uff0c\u5176\u4f59\u5404\u8282\u70b9\u53c2\u8003\u76f8\u540c\u547d\u4ee4 \u505c\u6b62\u4e4b\u524d\u8fd0\u884c\u7684Kylin\u4efb\u52a1\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f15\u5165\u5df2\u5b89\u88c5\u7684\u5404\u7ec4\u4ef6\u5ba2\u6237\u7aef\u4f4d\u7f6e ``` export HIVE_CONF=/opt/135_651armhdclient/hadoopclient/Hive/config export HCAT_HOME=/opt/135_651armhdclient/hadoopclient/Hive/HCatalog export KAFKA_HOME=/opt/135_651armhdclient/hadoopclient/Kafka/kafka ```` \u5230kylin webUI\u7684Model -> Data Source -> \u9009\u4e2d\u8868KYLIN_STREAMING_TABLE -> \u9009\u62e9Streaming Cluster -> \u70b9\u51fbEdit \u586b\u5199\u5bf9\u5e94\u96c6\u7fa4kafka\u8fde\u63a5\u4fe1\u606f\uff0c\u70b9\u51fbsave\u4fdd\u5b58\u540e\uff0c\u5728\u70b9\u51fbsubmit \u767b\u9646kylin\u5b89\u88c5\u4e3b\u673a\u540e\u53f0\uff0c\u4fee\u6539bin\u76ee\u5f55\u4e0b\u7684sample-streaming.sh\u6587\u4ef6 \u767b\u9646\u540e\u53f0\u4f7f\u7528\u547d\u4ee4 bin/sample-streaming.sh \u542f\u52a8\u4efb\u52a1 \u56de\u5230Model\uff0c\u9009\u62e9kylin_streaming_cube\uff0c\u70b9\u51fbBuild \u767b\u9646Monitor\u754c\u9762\u68c0\u67e5 \u767b\u9646Model\u754c\u9762\u68c0\u67e5 \u767b\u9646Insight\u754c\u9762\u8f93\u5165SQL\u8bed\u53e5\u67e5\u8be2 \u7528spark\u6784\u5efacube\u7528\u4f8b \u00b6 \u53c2\u8003Kylin\u5b98\u65b9\u6587\u6863\uff1a http://kylin.apache.org/docs30/tutorial/cube_spark.html \u505c\u6b62\u4e4b\u524d\u8fd0\u884c\u7684Kylin\u4efb\u52a1\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f15\u5165\u5df2\u5b89\u88c5\u7684\u5404\u7ec4\u4ef6\u5ba2\u6237\u7aef\u4f4d\u7f6e ``` export HIVE_CONF=/opt/135_651armhdclient/hadoopclient/Hive/config export HCAT_HOME=/opt/135_651armhdclient/hadoopclient/Hive/HCatalog export KAFKA_HOME=/opt/135_651armhdclient/hadoopclient/Kafka/kafka export SPARK_HOME=/opt/135_651armhdclient/hadoopclient/Spark2x/spark ```` \uff08\u53ef\u9009\uff09\u65b0\u5efa\u8def\u5f84 /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/spark/jars \uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5c06\u5df2\u5b89\u88c5\u7684Spark2x\u5ba2\u6237\u7aef\u7684jar\u5305\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u5220\u9664hadoop\u5f00\u5934\u7684jar\u5305 cp /opt/135_651armhdclient/hadoopclient/Spark2x/spark/jars/*.jar /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/spark/jars/ rm -rf hadoop-* \u542f\u52a8Kylin\uff0c\u767b\u9646web UI \u9009\u62e9kylin_sales_cube\u70b9Clone, \u5c06\u65b0\u7684cube\u91cd\u547d\u540d\u4e3akylin_sales_cube_clone_Spark2x \u9009\u62e9kylin_sales_cube_clone_Spark2x\uff0c\u5728Actions\u4e0b\u9009\u62e9Edit \u5728Advanced Setting\u4e0b\u5c06Cube Engine\u9009\u6210Spark \u5728Configuration Overwrites\u68c0\u67e5\u53c2\u6570 \u4fdd\u5b58 Build\u65b0\u521b\u5efa\u7684kylin_sales_cube_clone_Spark2x \u5728Monitor\u4e0b\u68c0\u67e5\u4f5c\u4e1a\u60c5\u51b5 \u53ef\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4yarn\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff1a \u767b\u9646Model\u754c\u9762\u68c0\u67e5 \u767b\u9646Insight\u754c\u9762\u8f93\u5165SQL\u8bed\u53e5\u67e5\u8be2 FAQ \u00b6 \u95ee\u98981\uff1a \u5728\u505abin/sample.sh\u4e4b\u540e\u518dbuild cube\u7684\u65f6\u5019\uff0c\u7b2c\u4e00\u6b65\u9047\u5230\u62a5\u9519\uff1a \u95ee\u9898\u539f\u56e0\uff1a\u6743\u9650\u6ca1\u52a0\u591f\uff0c\u53c2\u8003\u4e4b\u524d\u7684\u5728hive\u52a0\u6743\u9650\uff1a \u589e\u52a0 |mapred\\..* ,\u540c\u6b65\u8be5\u914d\u7f6e\u540e\u91cd\u542fhive\u670d\u52a1\u95ee\u9898\u89e3\u51b3 \u95ee\u98982\uff1a \u5728\u300a\u7528spark\u6784\u5efacube\u7528\u4f8b\u300b\u8fd9\u4e00\u8282\u4e2d\uff0cbuild\u4f5c\u4e1a\u4e4b\u540e\u5728Monitor\u67e5\u770b\u4efb\u52a1\u5931\u8d25 \u9519\u8bef\u5361\u5728\u7b2c7\u6b65 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4YARN\u67e5\u770b\u5931\u8d25\u4efb\u52a1\u65e5\u5fd7\uff1a \u5728container\u4e0b\u67e5\u770b\u5177\u4f53\u62a5\u9519\u65e5\u5fd7\u65e5\u5fd7\uff1a \u89e3\u51b3\u529e\u6cd5\uff1a\u5c06Kylin\u4e3b\u673a\u540d 172-16-2-120\u589e\u52a0\u5230\u5bf9\u63a5\u96c6\u7fa4\u7684/etc/hosts\u914d\u7f6e\u6587\u4ef6\u4e0b\u91cd\u65b0\u8fd0\u884c\u8be5\u4efb\u52a1\u95ee\u9898\u89e3\u51b3","title":"3.0.1 <--> 6.5"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#apache-kylin301fusioninsight-hd","text":"","title":"Apache Kylin3.0.1\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#_1","text":"Apache Kylin 3.0.1 \u2194 FusionInsight HD 6.5 (Hive/HBase/Kafka/Spark)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u5982\u679c\u9700\u8981\u8bfb\u53d6hive\u5206\u533a\u8868\u9700\u8981\u4fee\u6539\u6e90\u7801\uff0c\u672c\u6587\u4e0d\u63d0\u4f9b\u8be5\u65b9\u6cd5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1(\u82e5kylin\u4e3b\u673a\u4e0e\u96c6\u7fa4\u65f6\u95f4\u57285min\u4e4b\u5185\u6b64\u6b65\u9aa4\u53ef\u9009) \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/135_651armhdclient/hadoopclient \u5b89\u88c5JDK1.8\uff08\u53ef\u9009\uff09 rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#kylin","text":"Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.1\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-3.0.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin","title":"\u4e0b\u8f7dKylin"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#kylin_1","text":"\u4e0b\u8f7dKylin-3.0.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305: https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-3.0.1/apache-kylin-3.0.1-bin-hbase1x.tar.gz \u4e0a\u4f20apache-kylin-3.0.1-hbase1x-bin.tar.gz\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt/kylin tar -xvf apache-kylin-3.0.1-bin-hbase1x.tar.gz","title":"\u4e0b\u8f7d\u89e3\u538bKylin"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#kylin_2","text":"","title":"\u914d\u7f6eKylin"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/kylin/apache-kylin-3.0.1-bin-hbase1x \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/135_651armhdclient/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/135_651armhdclient/hadoopclient/Hive/config export HCAT_HOME=/opt/135_651armhdclient/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/135_651armhdclient/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit developuser Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#fusioninsighthive","text":"\u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..*|mapred\\..*","title":"\u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#kylin_3","text":"\u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit developuser beeline \u4fee\u6539kylin.properties\uff1a vi /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.16.10.131:24002,172.16.10.132:24002,172.16.10.133:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/135_651armhdclient/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/135_651armhdclient/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/kylin/apache-kylin-3.0.1-bin-hbase1x/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/kylin/apache-kylin-3.0.1-bin-hbase1x/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#kylin_4","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#_5","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#streaming-samplekafka","text":"\u53c2\u8003\u5b98\u65b9\u7f51\u7ad9http://kylin.apache.org/docs30/tutorial/kylin_sample.html \uff08\u91cd\u8981\uff09\u9996\u5148\u4e0b\u8f7d\u96c6\u7fa4\u8ba4\u8bc1\u6587\u4ef6krb5.conf\uff0c \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u8282\u70b9\uff0c\u5c06\u4e0b\u8f7d\u4e0b\u6765\u6709\u6548\u7684krb5.conf\u6587\u4ef6\u653e\u7f6e\u5230\u96c6\u7fa4\u5404\u4e2a\u8282\u70b9\u7684/etc/\u8def\u5f84\u4e0b\uff0c\u5b8c\u6210\u540e\u4f7f\u7528\u547d\u4ee4 export JAVA_TOOL_OPTIONS=\"-Djava.security.krb5.conf=/etc/krb5.conf\" \u52a0\u8f7d\u8ba4\u8bc1\u53c2\u6570 \u4e0b\u56fe\u4ee5\u5176\u4e2d\u4e00\u4e2a\u8282\u70b9\u4e3a\u4f8b\uff0c\u5176\u4f59\u5404\u8282\u70b9\u53c2\u8003\u76f8\u540c\u547d\u4ee4 \u505c\u6b62\u4e4b\u524d\u8fd0\u884c\u7684Kylin\u4efb\u52a1\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f15\u5165\u5df2\u5b89\u88c5\u7684\u5404\u7ec4\u4ef6\u5ba2\u6237\u7aef\u4f4d\u7f6e ``` export HIVE_CONF=/opt/135_651armhdclient/hadoopclient/Hive/config export HCAT_HOME=/opt/135_651armhdclient/hadoopclient/Hive/HCatalog export KAFKA_HOME=/opt/135_651armhdclient/hadoopclient/Kafka/kafka ```` \u5230kylin webUI\u7684Model -> Data Source -> \u9009\u4e2d\u8868KYLIN_STREAMING_TABLE -> \u9009\u62e9Streaming Cluster -> \u70b9\u51fbEdit \u586b\u5199\u5bf9\u5e94\u96c6\u7fa4kafka\u8fde\u63a5\u4fe1\u606f\uff0c\u70b9\u51fbsave\u4fdd\u5b58\u540e\uff0c\u5728\u70b9\u51fbsubmit \u767b\u9646kylin\u5b89\u88c5\u4e3b\u673a\u540e\u53f0\uff0c\u4fee\u6539bin\u76ee\u5f55\u4e0b\u7684sample-streaming.sh\u6587\u4ef6 \u767b\u9646\u540e\u53f0\u4f7f\u7528\u547d\u4ee4 bin/sample-streaming.sh \u542f\u52a8\u4efb\u52a1 \u56de\u5230Model\uff0c\u9009\u62e9kylin_streaming_cube\uff0c\u70b9\u51fbBuild \u767b\u9646Monitor\u754c\u9762\u68c0\u67e5 \u767b\u9646Model\u754c\u9762\u68c0\u67e5 \u767b\u9646Insight\u754c\u9762\u8f93\u5165SQL\u8bed\u53e5\u67e5\u8be2","title":"Streaming Sample\u7528\u4f8b\uff0cKafka\u666e\u901a\u6a21\u5f0f"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#sparkcube","text":"\u53c2\u8003Kylin\u5b98\u65b9\u6587\u6863\uff1a http://kylin.apache.org/docs30/tutorial/cube_spark.html \u505c\u6b62\u4e4b\u524d\u8fd0\u884c\u7684Kylin\u4efb\u52a1\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5f15\u5165\u5df2\u5b89\u88c5\u7684\u5404\u7ec4\u4ef6\u5ba2\u6237\u7aef\u4f4d\u7f6e ``` export HIVE_CONF=/opt/135_651armhdclient/hadoopclient/Hive/config export HCAT_HOME=/opt/135_651armhdclient/hadoopclient/Hive/HCatalog export KAFKA_HOME=/opt/135_651armhdclient/hadoopclient/Kafka/kafka export SPARK_HOME=/opt/135_651armhdclient/hadoopclient/Spark2x/spark ```` \uff08\u53ef\u9009\uff09\u65b0\u5efa\u8def\u5f84 /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/spark/jars \uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5c06\u5df2\u5b89\u88c5\u7684Spark2x\u5ba2\u6237\u7aef\u7684jar\u5305\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\uff0c\u5e76\u4e14\u5220\u9664hadoop\u5f00\u5934\u7684jar\u5305 cp /opt/135_651armhdclient/hadoopclient/Spark2x/spark/jars/*.jar /opt/kylin/apache-kylin-3.0.1-bin-hbase1x/spark/jars/ rm -rf hadoop-* \u542f\u52a8Kylin\uff0c\u767b\u9646web UI \u9009\u62e9kylin_sales_cube\u70b9Clone, \u5c06\u65b0\u7684cube\u91cd\u547d\u540d\u4e3akylin_sales_cube_clone_Spark2x \u9009\u62e9kylin_sales_cube_clone_Spark2x\uff0c\u5728Actions\u4e0b\u9009\u62e9Edit \u5728Advanced Setting\u4e0b\u5c06Cube Engine\u9009\u6210Spark \u5728Configuration Overwrites\u68c0\u67e5\u53c2\u6570 \u4fdd\u5b58 Build\u65b0\u521b\u5efa\u7684kylin_sales_cube_clone_Spark2x \u5728Monitor\u4e0b\u68c0\u67e5\u4f5c\u4e1a\u60c5\u51b5 \u53ef\u767b\u9646\u5bf9\u63a5\u96c6\u7fa4yarn\u67e5\u770b\u4f5c\u4e1a\u60c5\u51b5\uff1a \u767b\u9646Model\u754c\u9762\u68c0\u67e5 \u767b\u9646Insight\u754c\u9762\u8f93\u5165SQL\u8bed\u53e5\u67e5\u8be2","title":"\u7528spark\u6784\u5efacube\u7528\u4f8b"},{"location":"SQL_Analytics/ApacheKylin3.0.1/#faq","text":"\u95ee\u98981\uff1a \u5728\u505abin/sample.sh\u4e4b\u540e\u518dbuild cube\u7684\u65f6\u5019\uff0c\u7b2c\u4e00\u6b65\u9047\u5230\u62a5\u9519\uff1a \u95ee\u9898\u539f\u56e0\uff1a\u6743\u9650\u6ca1\u52a0\u591f\uff0c\u53c2\u8003\u4e4b\u524d\u7684\u5728hive\u52a0\u6743\u9650\uff1a \u589e\u52a0 |mapred\\..* ,\u540c\u6b65\u8be5\u914d\u7f6e\u540e\u91cd\u542fhive\u670d\u52a1\u95ee\u9898\u89e3\u51b3 \u95ee\u98982\uff1a \u5728\u300a\u7528spark\u6784\u5efacube\u7528\u4f8b\u300b\u8fd9\u4e00\u8282\u4e2d\uff0cbuild\u4f5c\u4e1a\u4e4b\u540e\u5728Monitor\u67e5\u770b\u4efb\u52a1\u5931\u8d25 \u9519\u8bef\u5361\u5728\u7b2c7\u6b65 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4YARN\u67e5\u770b\u5931\u8d25\u4efb\u52a1\u65e5\u5fd7\uff1a \u5728container\u4e0b\u67e5\u770b\u5177\u4f53\u62a5\u9519\u65e5\u5fd7\u65e5\u5fd7\uff1a \u89e3\u51b3\u529e\u6cd5\uff1a\u5c06Kylin\u4e3b\u673a\u540d 172-16-2-120\u589e\u52a0\u5230\u5bf9\u63a5\u96c6\u7fa4\u7684/etc/hosts\u914d\u7f6e\u6587\u4ef6\u4e0b\u91cd\u65b0\u8fd0\u884c\u8be5\u4efb\u52a1\u95ee\u9898\u89e3\u51b3","title":"FAQ"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/","text":"Apache Drill\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Drill 1.15.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/HBase/Kafka) \u8bf4\u660e \u00b6 Apache Drill\u5b89\u88c5\u4e3b\u673a\uff1a172.16.2.123 FI HD V100R002C80SPC200\u96c6\u7fa4\uff1a 172.16.6.10-12 \u5b89\u88c5Apache Drill \u00b6 \u4e0b\u8f7dApache Drill wget http://apache.mirrors.hoobly.com/drill/drill-1.15.0/apache-drill-1.15.0.tar.gz \u6216\u8005\u4ece\u5b98\u7f51\u4e0b\u8f7d: \u5b89\u88c5drill \u5c06\u5b89\u88c5\u5305\u5bfc\u5165/opt/drill\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf apache-drill-1.15.0.tar.gz \u89e3\u538b\u538b\u7f29\u5305 \u542f\u52a8drill cd /opt/drill/apache-drill-1.15.0 bin/drill-embedded \u540c\u65f6\u53ef\u4ee5\u767b\u5f55 172.16.2.123:8047 \u6765\u67e5\u770bwebUI\u754c\u9762 \u5bf9\u63a5HDFS \u00b6 \u786e\u4fddApache Drill\u4e3b\u673a\u4e0e\u5bf9\u63a5\u96c6\u7fa4\u65f6\u95f4\u5dee\u5f02\u5c0f\u4e8e5\u5206\u949f \u767b\u9646drill webUI \u754c\u9762\uff0c\u9009\u62e9Storage,\u521b\u5efa\u65b0\u7684huaweihdfs \u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"file\", \"connection\": \"hdfs://172.16.6.12:25000/\", \"config\": null, \"workspaces\": { \"tmp\": { \"location\": \"/tmp\", \"writable\": true, \"defaultInputFormat\": null, \"allowAccessOutsideWorkspace\": false } }, \"formats\": { \"json\": { \"type\": \"json\", \"extensions\": [ \"json\" ] } }, \"enabled\": true } \u5176\u4e2d172.16.6.12\u4e3a\u96c6\u7fa4namenode\u4e3b\u8282\u70b9 \u51c6\u5907\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u4e3b\u8282\u70b9172.16.6.10:/opt\u8def\u5f84,\u4f7f\u7528\u547d\u4ee4 find /opt -name hdfs.keytab \u67e5\u627ehdfs\u8ba4\u8bc1\u76f8\u5173keytab\u6587\u4ef6 \u5c06hdfs.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230apachedrill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u5728\u5ba2\u6237\u7aef\u4e2d\u627e\u5230HDFS\u76f8\u5173core-site.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u5bf9\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539\uff1a \u627e\u5230\u53c2\u6570\u9879fs.defaultFS,\u5c06\u503c\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip+25000\u7684\u5f62\u5f0f\uff1a \u4fdd\u5b58\u4fee\u6539 \u4fee\u6539drill conf\u8def\u5f84\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6drill-override.conf\uff0c\u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58: security.auth.principal: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hdfs.keytab\" \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684/tmp\u8def\u5f84\u4e0b\u521b\u5efajson\u683c\u5f0f\u7684\u6d4b\u8bd5\u6570\u636etest.json \u5185\u5bb9\u5982\u4e0b\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u4f7f\u7528\u547d\u4ee4 !quit \u505c\u6b62drill,\u518d\u91cd\u542fdrill \u5728\u547d\u4ee4\u884c\u4f7f\u7528\u547d\u4ee4 show databases; \u68c0\u67e5\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 select * from huaweihdfs.`tmp`.`test.json`; \u67e5\u627e\u6570\u636e\uff1a \u5bf9\u63a5HIVE \u00b6 \u8bf4\u660e\uff1aapache drill 1.15\u7248\u672c\u7684hive\u7248\u672c\u4e3a2.3.2, FI HD V100R002C80SPC200\u7248\u672chive\u7248\u672c\u4e3a1.3.0\uff0c\u7248\u672c\u4e0d\u5339\u914d\uff0c\u4e0d\u80fd\u5bf9\u63a5\u6210\u529f\uff0c\u9700\u8981\u4f7f\u7528drill 1.12.0\u7248\u672c\uff0c\u6545\u672c\u8282\u6240\u8ff0\u4f7f\u7528\u7684drill\u7248\u672c\u90fd\u4e3a1.12.0 ApacheDrill 1.12.0\u5bf9\u63a5 FI HD V100R002C80SPC200 Hive\u670d\u52a1\u524d\u63d0\u6761\u4ef6\uff1a \u4e0b\u8f7dApacheDrill 1.12.0\u7684\u5b89\u88c5\u5305apache-drill-1.12.0.tar.gz\uff0c \u53c2\u8003\u4e0a\u8ff0\u300a\u5b89\u88c5Apache Drill\u300b\u7ae0\u8282\u5b8c\u6210drill\u5b89\u88c5 \u53c2\u8003\u4e0a\u8ff0\u300a\u5bf9\u63a5HDFS\u300b\u7ae0\u8282\u5b8c\u6210\u5bf9\u63a5drill 1.12.0\u7248\u672c\u4e0eFI HD\u7684\u5bf9\u63a5\uff0c\u56e0\u4e3a\u5bf9\u63a5hdfs\u662f\u8fde\u63a5hive\u7684\u57fa\u7840\uff0c\u6240\u4ee5\u9700\u8981\u5b8c\u6210\u6b64\u6b65\u9aa4 \u7531\u4e8edrill 1.12.0\u7248\u672c\u652f\u6301\u5bf9\u63a5ha\u6a21\u5f0f\u7684\u5927\u6570\u636e\u96c6\u7fa4\uff0c\u4e3a\u51cf\u5c11\u4f9d\u8d56\u76f8\u5173\u9519\u8bef\u53d1\u751f\uff0c\u66f4\u6539\u5bf9\u63a5FI HD\u96c6\u7fa4HDFS\u670d\u52a1\u4e2ddfs.client.failover.proxy.provider.hacluster\u914d\u7f6e\u9879\u7684\u503c\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\uff0c\u5e76\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u76f8\u5173\u670d\u52a1 \u5b8c\u6210\u540e\u91cd\u65b0\u4e0b\u8f7d\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 \u6ce8\u610f\uff1a\u6b64\u6b65\u9aa4\u4e3a\u66f4\u6539\u96c6\u7fa4HDFS\u914d\u7f6e\uff0c\u5c5e\u4e8e\u9ad8\u5371\u64cd\u4f5c\u3002\u5982\u679c\u4e0d\u80fd\u66f4\u6539\u96c6\u7fa4\u914d\u7f6e\u5219\u76f8\u5e94\u7684\u89c4\u907f\u65b9\u5f0f\u5982\u4e0b\uff08\u53ea\u6539\u5bf9\u63a5\u9700\u8981\u4f7f\u7528\u7684\u914d\u7f6e\u6587\u4ef6hdfs-site.xml\u800c\u4e0d\u5bf9\u96c6\u7fa4\u914d\u7f6e\u505a\u5b9e\u9645\u66f4\u6539\uff09: \u4fee\u6539\u914d\u7f6e\u6587\u4ef6hdfs-site.xml\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u9879dfs.client.failover.proxy.provider.hacluster\u7684\u503c\u6539\u6210org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684\u914d\u7f6e\u6587\u4ef6\u627e\u5230hdfs-site.xml\u4ee5\u53cayarn-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff1a \u5176\u4e2dcore-site.xml\u914d\u7f6e\u6587\u4ef6\u5728\u5bf9\u63a5HDFS\u7684\u65f6\u5019\u5df2\u7ecf\u5bfc\u5165\uff0c\u5e76\u4e14\u4fee\u6539\u8fc7fs.defaultFS\u914d\u7f6e\u9879\uff0c hdfs-site.xml\u6587\u4ef6\u7684dfs.client.failover.proxy.provider.hacluster \u914d\u7f6e\u9879\u505a\u76f8\u5e94\u66f4\u6539\uff0c\u6539\u6210org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u5230drill\u5b89\u88c5\u8def\u5f84/jars\u4e0b\u4f7f\u7528\u547d\u4ee4 mkdir hd_jars \u65b0\u5efa\u4e00\u4e2a\u540d\u4e3ahd_jars\u7684\u8def\u5f84 \u5230FI HD\u5ba2\u6237\u7aef\u4e0b\u627e\u5230Jar\u5305 hadoop-yarn-api-2.7.2.jar\uff0c \u5e76\u5c06\u6b64jar\u5305\u62f7\u8d1d\u5230hd_jars\u8def\u5f84\u4e0b cp /opt/hadoopclient/HDFS/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar /opt/drill/apache-drill-1.12.0/jars/hd_jars/ \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/bin/drill-config.sh\u6587\u4ef6\u5982\u4e0b\uff1a \u5728 3rdparty\u8def\u5f84\u5bfc\u5165\u524d\u52a0\u5165\u4e00\u884c\uff1a CP=\"$CP:$DRILL_HOME/jars/hd_jars/*\" \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4172.16.2.11\uff0c \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u627e\u5230\u5bf9\u63a5hive\u76f8\u5173\u8ba4\u8bc1keytab\u6587\u4ef6hive.keytab\u5e76\u628a\u6b64\u6587\u4ef6\u4f20\u81f3drill\u4e3b\u673a/opt\u8def\u5f84\u4e0b\uff1a find /opt -name hive.keytab scp /opt/huawei/Bigdata/components/FusionInsight_HD_V100R002C80SPC200/Hive/hive.keytab root@172.16.2.123:/opt \u767b\u9646drill\u4e3b\u673a\uff0c\u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230drill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff0c\u5982\u679c\u6b64\u6b65\u4e4b\u524d\u505a\u8fc7\u53ef\u4ee5\u4e0d\u505a \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/conf/drill-override.conf\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a drill.exec: { cluster-id: \"drillbits1\", zk.connect: \"localhost:2181\" security.auth.principal: \"hive/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hive.keytab\" sys.store.provider.local.path = \"/home/drill\" } \u91cd\u542fdrill,\u767b\u9646drill WebUI,\u521b\u5efa\u65b0\u7684storage\u540d\u5b57\u4e3ahuaweihive\u5e76enable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hive\", \"enabled\": true, \"configProps\": { \"hive.metastore.uris\": \"thrift://172.16.6.10:21088,thrift://172.16.6.11:21088\", \"hive.metastore.kerberos.principal\": \"hive/hadoop.hadoop.com@HADOOP.COM\", \"hive.metastore.sasl.enabled\": \"true\", \"fs.default.name\": \"hdfs://172.16.6.12:25000\", \"inputDirectories\": \"hdfs://172.16.6.12:25000\" } } \u5176\u4e2dhive.metastore.uris\u53ef\u5728\u96c6\u7fa4hive-site.xml\u6587\u4ef6\u4e2d\u67e5\u5230 \u540e\u53f0\u767b\u9646drill\uff0c\u4f7f\u7528 show databases \u547d\u4ee4\u67e5\u770b\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 use huaweihive; \u4f7f\u7528hive\u8fde\u63a5\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhive\u8868\uff1a \u4f7f\u7528\u67e5\u8be2\u547d\u4ee4\u67e5\u8be2hive\u8868\uff1a \u5bf9\u63a5kafka \u00b6 \u5bf9\u63a5\u53c2\u8003drill\u5b98\u65b9\u6587\u6863\uff1a https://drill.apache.org/docs/kafka-storage-plugin/ \u53ef\u4ee5\u77e5\u9053\u652f\u6301\u7684kafka\u8bfb\u53d6\u6570\u636e\u7ed3\u6784\u53ea\u80fd\u4e3ajson\uff1a \u51c6\u5907topic \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u65b0\u7684topic\uff1a bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic druidkafka \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u751f\u4ea7\u6570\u636e\uff1a bin/kafka-console-producer.sh --broker-list 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --topic druidkafka --producer.config config/producer.properties \u8f93\u5165\u4e09\u6761\u6d4b\u8bd5\u6570\u636e\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u542f\u52a8apachedrill,\u767b\u9646webUI,\u70b9\u51fbStorage\u521b\u5efahuaweikafka,\u70b9\u51fbenable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"kafka\", \"kafkaConsumerProps\": { \"key.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"auto.offset.reset\": \"earliest\", \"bootstrap.servers\": \"172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005\", \"group.id\": \"drill-query-consumer-1\", \"enable.auto.commit\": \"true\", \"value.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"session.timeout.ms\": \"30000\" }, \"enabled\": true } \u540e\u53f0\u547d\u4ee4\u884c\u8f93\u5165 show databases; \u68c0\u67e5\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 use huaweikafka; \u4f7f\u7528\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 show tables; \u67e5\u770btopic \u8f93\u5165\u547d\u4ee4 select * from druidkafka; \u67e5\u8be2\u521a\u521a\u521b\u5efa\u7684kafka topic \u5bf9\u63a5HBase \u00b6 \u505c\u6b62\u6b63\u5728\u8fd0\u884c\u7684drill \u53c2\u8003\u524d\u6587\u300a\u5bf9\u63a5HDFS\u300b,\u300a\u5bf9\u63a5HIVE\u300b\u7ae0\u8282\uff0c\u6210\u529f\u914d\u7f6e\u597d\u5bf9\u63a5hdfs,hive\u7ec4\u4ef6 \u627e\u5230drill\u5b89\u88c5\u76ee\u5f55\u4e0b./jar/ext/\u8def\u5f84\uff0c\u5c06drill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u91cd\u547d\u540d\u4e3azookeeper-3.4.12.jar.org\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06FI HD\u5ba2\u6237\u7aef\u4e2dzookeeper\u76f8\u5173jar\u5305 zookeeper-3.5.1.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\u3002\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\uff0cdrill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u7248\u672c\u592a\u65e7\uff0c\u5176\u5185\u90e8\u6ca1\u6709\u5b9a\u4e49send4LetterWord\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u5411FI HD zookeeper\u670d\u52a1\u81ea\u52a8\u83b7\u53d6\u8fde\u63a5zookeeper\u7684service principal (zookeeper/hadoop. hadoop.com@HADOOP.COM ) \u5728/opt\u8def\u5f84\u4e0b\u51c6\u5907jass.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff0c\u5176\u4e2d/opt/user.keytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8drill\u4e4b\u524d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u8fdb\u5fc5\u8981\u7684JVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u5b8c\u6210\u4e4b\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u5c06HBase\u5ba2\u6237\u7aef\u5305\u542b\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b \u542f\u52a8drill\uff0c\u767b\u9646drill webUI\uff0c\u6dfb\u52a0Storage\u540d\u5b57\u4e3ahuaweihbase\u5e76enable,\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hbase\", \"config\": { \"hbase.zookeeper.quorum\": \"172.16.6.10,172.16.6.11,172.16.6.12\", \"hbase.zookeeper.property.clientPort\": \"24002\" }, \"size.calculator.enabled\": false, \"enabled\": true } \u767b\u9646drill\u540e\u53f0\uff0c\u4f7f\u7528\u547d\u4ee4 use huaweihbase; \u4f7f\u7528\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhbase\u8868\uff1a \u4f7f\u7528\u547d\u4ee4 select * from ImportTable; \u67e5\u770bhbase\u8868\uff1a","title":"1.15.0 <--> C80"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#apache-drillfusioninsight","text":"","title":"Apache Drill\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#_1","text":"Apache Drill 1.15.0 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/HBase/Kafka)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#_2","text":"Apache Drill\u5b89\u88c5\u4e3b\u673a\uff1a172.16.2.123 FI HD V100R002C80SPC200\u96c6\u7fa4\uff1a 172.16.6.10-12","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#apache-drill","text":"\u4e0b\u8f7dApache Drill wget http://apache.mirrors.hoobly.com/drill/drill-1.15.0/apache-drill-1.15.0.tar.gz \u6216\u8005\u4ece\u5b98\u7f51\u4e0b\u8f7d: \u5b89\u88c5drill \u5c06\u5b89\u88c5\u5305\u5bfc\u5165/opt/drill\u8def\u5f84\u4e0b\uff0c\u4f7f\u7528\u547d\u4ee4 tar -xvf apache-drill-1.15.0.tar.gz \u89e3\u538b\u538b\u7f29\u5305 \u542f\u52a8drill cd /opt/drill/apache-drill-1.15.0 bin/drill-embedded \u540c\u65f6\u53ef\u4ee5\u767b\u5f55 172.16.2.123:8047 \u6765\u67e5\u770bwebUI\u754c\u9762","title":"\u5b89\u88c5Apache Drill"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#hdfs","text":"\u786e\u4fddApache Drill\u4e3b\u673a\u4e0e\u5bf9\u63a5\u96c6\u7fa4\u65f6\u95f4\u5dee\u5f02\u5c0f\u4e8e5\u5206\u949f \u767b\u9646drill webUI \u754c\u9762\uff0c\u9009\u62e9Storage,\u521b\u5efa\u65b0\u7684huaweihdfs \u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"file\", \"connection\": \"hdfs://172.16.6.12:25000/\", \"config\": null, \"workspaces\": { \"tmp\": { \"location\": \"/tmp\", \"writable\": true, \"defaultInputFormat\": null, \"allowAccessOutsideWorkspace\": false } }, \"formats\": { \"json\": { \"type\": \"json\", \"extensions\": [ \"json\" ] } }, \"enabled\": true } \u5176\u4e2d172.16.6.12\u4e3a\u96c6\u7fa4namenode\u4e3b\u8282\u70b9 \u51c6\u5907\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6 \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4\u4e3b\u8282\u70b9172.16.6.10:/opt\u8def\u5f84,\u4f7f\u7528\u547d\u4ee4 find /opt -name hdfs.keytab \u67e5\u627ehdfs\u8ba4\u8bc1\u76f8\u5173keytab\u6587\u4ef6 \u5c06hdfs.keytab\u6587\u4ef6scp\u62f7\u8d1d\u5230apachedrill\u4e3b\u673a/opt\u8def\u5f84\u4e0b \u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230apachedrill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6 \u5728\u5ba2\u6237\u7aef\u4e2d\u627e\u5230HDFS\u76f8\u5173core-site.xml\u914d\u7f6e\u6587\u4ef6\uff0c\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b\uff0c\u5e76\u5bf9\u914d\u7f6e\u6587\u4ef6\u505a\u5982\u4e0b\u4fee\u6539\uff1a \u627e\u5230\u53c2\u6570\u9879fs.defaultFS,\u5c06\u503c\u6539\u4e3anamenode\u4e3b\u8282\u70b9ip+25000\u7684\u5f62\u5f0f\uff1a \u4fdd\u5b58\u4fee\u6539 \u4fee\u6539drill conf\u8def\u5f84\u4e0b\u7684\u914d\u7f6e\u6587\u4ef6drill-override.conf\uff0c\u505a\u5982\u4e0b\u4fee\u6539\u5e76\u4fdd\u5b58: security.auth.principal: \"hdfs/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hdfs.keytab\" \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684/tmp\u8def\u5f84\u4e0b\u521b\u5efajson\u683c\u5f0f\u7684\u6d4b\u8bd5\u6570\u636etest.json \u5185\u5bb9\u5982\u4e0b\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u4f7f\u7528\u547d\u4ee4 !quit \u505c\u6b62drill,\u518d\u91cd\u542fdrill \u5728\u547d\u4ee4\u884c\u4f7f\u7528\u547d\u4ee4 show databases; \u68c0\u67e5\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 select * from huaweihdfs.`tmp`.`test.json`; \u67e5\u627e\u6570\u636e\uff1a","title":"\u5bf9\u63a5HDFS"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#hive","text":"\u8bf4\u660e\uff1aapache drill 1.15\u7248\u672c\u7684hive\u7248\u672c\u4e3a2.3.2, FI HD V100R002C80SPC200\u7248\u672chive\u7248\u672c\u4e3a1.3.0\uff0c\u7248\u672c\u4e0d\u5339\u914d\uff0c\u4e0d\u80fd\u5bf9\u63a5\u6210\u529f\uff0c\u9700\u8981\u4f7f\u7528drill 1.12.0\u7248\u672c\uff0c\u6545\u672c\u8282\u6240\u8ff0\u4f7f\u7528\u7684drill\u7248\u672c\u90fd\u4e3a1.12.0 ApacheDrill 1.12.0\u5bf9\u63a5 FI HD V100R002C80SPC200 Hive\u670d\u52a1\u524d\u63d0\u6761\u4ef6\uff1a \u4e0b\u8f7dApacheDrill 1.12.0\u7684\u5b89\u88c5\u5305apache-drill-1.12.0.tar.gz\uff0c \u53c2\u8003\u4e0a\u8ff0\u300a\u5b89\u88c5Apache Drill\u300b\u7ae0\u8282\u5b8c\u6210drill\u5b89\u88c5 \u53c2\u8003\u4e0a\u8ff0\u300a\u5bf9\u63a5HDFS\u300b\u7ae0\u8282\u5b8c\u6210\u5bf9\u63a5drill 1.12.0\u7248\u672c\u4e0eFI HD\u7684\u5bf9\u63a5\uff0c\u56e0\u4e3a\u5bf9\u63a5hdfs\u662f\u8fde\u63a5hive\u7684\u57fa\u7840\uff0c\u6240\u4ee5\u9700\u8981\u5b8c\u6210\u6b64\u6b65\u9aa4 \u7531\u4e8edrill 1.12.0\u7248\u672c\u652f\u6301\u5bf9\u63a5ha\u6a21\u5f0f\u7684\u5927\u6570\u636e\u96c6\u7fa4\uff0c\u4e3a\u51cf\u5c11\u4f9d\u8d56\u76f8\u5173\u9519\u8bef\u53d1\u751f\uff0c\u66f4\u6539\u5bf9\u63a5FI HD\u96c6\u7fa4HDFS\u670d\u52a1\u4e2ddfs.client.failover.proxy.provider.hacluster\u914d\u7f6e\u9879\u7684\u503c\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\uff0c\u5e76\u70b9\u51fb\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u76f8\u5173\u670d\u52a1 \u5b8c\u6210\u540e\u91cd\u65b0\u4e0b\u8f7d\u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 \u6ce8\u610f\uff1a\u6b64\u6b65\u9aa4\u4e3a\u66f4\u6539\u96c6\u7fa4HDFS\u914d\u7f6e\uff0c\u5c5e\u4e8e\u9ad8\u5371\u64cd\u4f5c\u3002\u5982\u679c\u4e0d\u80fd\u66f4\u6539\u96c6\u7fa4\u914d\u7f6e\u5219\u76f8\u5e94\u7684\u89c4\u907f\u65b9\u5f0f\u5982\u4e0b\uff08\u53ea\u6539\u5bf9\u63a5\u9700\u8981\u4f7f\u7528\u7684\u914d\u7f6e\u6587\u4ef6hdfs-site.xml\u800c\u4e0d\u5bf9\u96c6\u7fa4\u914d\u7f6e\u505a\u5b9e\u9645\u66f4\u6539\uff09: \u4fee\u6539\u914d\u7f6e\u6587\u4ef6hdfs-site.xml\u6587\u4ef6\u4e2d\u7684\u914d\u7f6e\u9879dfs.client.failover.proxy.provider.hacluster\u7684\u503c\u6539\u6210org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u5728\u5bf9\u63a5\u96c6\u7fa4\u7684\u914d\u7f6e\u6587\u4ef6\u627e\u5230hdfs-site.xml\u4ee5\u53cayarn-site.xml\u6587\u4ef6\u5bfc\u5165\u5230drill\u5b89\u88c5\u8def\u5f84/conf\u76ee\u5f55\u4e0b\uff1a \u5176\u4e2dcore-site.xml\u914d\u7f6e\u6587\u4ef6\u5728\u5bf9\u63a5HDFS\u7684\u65f6\u5019\u5df2\u7ecf\u5bfc\u5165\uff0c\u5e76\u4e14\u4fee\u6539\u8fc7fs.defaultFS\u914d\u7f6e\u9879\uff0c hdfs-site.xml\u6587\u4ef6\u7684dfs.client.failover.proxy.provider.hacluster \u914d\u7f6e\u9879\u505a\u76f8\u5e94\u66f4\u6539\uff0c\u6539\u6210org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider \u5230drill\u5b89\u88c5\u8def\u5f84/jars\u4e0b\u4f7f\u7528\u547d\u4ee4 mkdir hd_jars \u65b0\u5efa\u4e00\u4e2a\u540d\u4e3ahd_jars\u7684\u8def\u5f84 \u5230FI HD\u5ba2\u6237\u7aef\u4e0b\u627e\u5230Jar\u5305 hadoop-yarn-api-2.7.2.jar\uff0c \u5e76\u5c06\u6b64jar\u5305\u62f7\u8d1d\u5230hd_jars\u8def\u5f84\u4e0b cp /opt/hadoopclient/HDFS/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.2.jar /opt/drill/apache-drill-1.12.0/jars/hd_jars/ \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/bin/drill-config.sh\u6587\u4ef6\u5982\u4e0b\uff1a \u5728 3rdparty\u8def\u5f84\u5bfc\u5165\u524d\u52a0\u5165\u4e00\u884c\uff1a CP=\"$CP:$DRILL_HOME/jars/hd_jars/*\" \u767b\u9646\u5bf9\u63a5FI HD\u96c6\u7fa4172.16.2.11\uff0c \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u627e\u5230\u5bf9\u63a5hive\u76f8\u5173\u8ba4\u8bc1keytab\u6587\u4ef6hive.keytab\u5e76\u628a\u6b64\u6587\u4ef6\u4f20\u81f3drill\u4e3b\u673a/opt\u8def\u5f84\u4e0b\uff1a find /opt -name hive.keytab scp /opt/huawei/Bigdata/components/FusionInsight_HD_V100R002C80SPC200/Hive/hive.keytab root@172.16.2.123:/opt \u767b\u9646drill\u4e3b\u673a\uff0c\u51c6\u5907\u5bf9\u63a5\u96c6\u7fa4\u76f8\u5173\u7684krb5.conf\u6587\u4ef6\uff08\u53ef\u4ecemanager\u4e0b\u8f7d\uff09\uff0c\u590d\u5236\u5230drill\u4e3b\u673a/etc\u8def\u5f84\u4e0b\u5e76\u8986\u76d6\uff0capachedrill\u9ed8\u8ba4\u4ece/etc/\u8def\u5f84\u4e0b\u8bfb\u53d6krb5.conf\u6587\u4ef6\uff0c\u5982\u679c\u6b64\u6b65\u4e4b\u524d\u505a\u8fc7\u53ef\u4ee5\u4e0d\u505a \u4fee\u6539drill\u5b89\u88c5\u8def\u5f84/conf/drill-override.conf\u914d\u7f6e\u6587\u4ef6\u5982\u4e0b\uff1a drill.exec: { cluster-id: \"drillbits1\", zk.connect: \"localhost:2181\" security.auth.principal: \"hive/hadoop.hadoop.com@HADOOP.COM\" security.auth.keytab: \"/opt/hive.keytab\" sys.store.provider.local.path = \"/home/drill\" } \u91cd\u542fdrill,\u767b\u9646drill WebUI,\u521b\u5efa\u65b0\u7684storage\u540d\u5b57\u4e3ahuaweihive\u5e76enable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hive\", \"enabled\": true, \"configProps\": { \"hive.metastore.uris\": \"thrift://172.16.6.10:21088,thrift://172.16.6.11:21088\", \"hive.metastore.kerberos.principal\": \"hive/hadoop.hadoop.com@HADOOP.COM\", \"hive.metastore.sasl.enabled\": \"true\", \"fs.default.name\": \"hdfs://172.16.6.12:25000\", \"inputDirectories\": \"hdfs://172.16.6.12:25000\" } } \u5176\u4e2dhive.metastore.uris\u53ef\u5728\u96c6\u7fa4hive-site.xml\u6587\u4ef6\u4e2d\u67e5\u5230 \u540e\u53f0\u767b\u9646drill\uff0c\u4f7f\u7528 show databases \u547d\u4ee4\u67e5\u770b\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 use huaweihive; \u4f7f\u7528hive\u8fde\u63a5\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhive\u8868\uff1a \u4f7f\u7528\u67e5\u8be2\u547d\u4ee4\u67e5\u8be2hive\u8868\uff1a","title":"\u5bf9\u63a5HIVE"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#kafka","text":"\u5bf9\u63a5\u53c2\u8003drill\u5b98\u65b9\u6587\u6863\uff1a https://drill.apache.org/docs/kafka-storage-plugin/ \u53ef\u4ee5\u77e5\u9053\u652f\u6301\u7684kafka\u8bfb\u53d6\u6570\u636e\u7ed3\u6784\u53ea\u80fd\u4e3ajson\uff1a \u51c6\u5907topic \u767b\u9646\u5bf9\u63a5\u96c6\u7fa4kafka\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u65b0\u7684topic\uff1a bin/kafka-topics.sh --create --zookeeper 172.16.6.10:24002,172.16.6.11:24002,172.16.6.12:24002/kafka --partitions 1 --replication-factor 1 --topic druidkafka \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u751f\u4ea7\u6570\u636e\uff1a bin/kafka-console-producer.sh --broker-list 172.16.6.10:21007,172.16.6.11:21007,172.16.6.12:21007 --topic druidkafka --producer.config config/producer.properties \u8f93\u5165\u4e09\u6761\u6d4b\u8bd5\u6570\u636e\uff1a { \"_id\" : \"5968dd23fc13ae04d9000001\", \"product_name\" : \"sildenafil citrate\", \"supplier\" : \"Wisozk Inc\", \"quantity\" : 261, \"unit_cost\" : \"$10.47\" } { \"_id\" : \"5968dd23fc13ae04d9000002\", \"product_name\" : \"Mountain Juniperus ashei\", \"supplier\" : \"Keebler-Hilpert\", \"quantity\" : 292, \"unit_cost\" : \"$8.74\" } { \"_id\" : \"5968dd23fc13ae04d9000003\", \"product_name\" : \"Dextromathorphan HBr\", \"supplier\" : \"Schmitt-Weissnat\", \"quantity\" : 211, \"unit_cost\" : \"$20.53\" } \u542f\u52a8apachedrill,\u767b\u9646webUI,\u70b9\u51fbStorage\u521b\u5efahuaweikafka,\u70b9\u51fbenable\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"kafka\", \"kafkaConsumerProps\": { \"key.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"auto.offset.reset\": \"earliest\", \"bootstrap.servers\": \"172.16.6.10:21005,172.16.6.11:21005,172.16.6.12:21005\", \"group.id\": \"drill-query-consumer-1\", \"enable.auto.commit\": \"true\", \"value.deserializer\": \"org.apache.kafka.common.serialization.ByteArrayDeserializer\", \"session.timeout.ms\": \"30000\" }, \"enabled\": true } \u540e\u53f0\u547d\u4ee4\u884c\u8f93\u5165 show databases; \u68c0\u67e5\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 use huaweikafka; \u4f7f\u7528\u6570\u636e\u5e93 \u8f93\u5165\u547d\u4ee4 show tables; \u67e5\u770btopic \u8f93\u5165\u547d\u4ee4 select * from druidkafka; \u67e5\u8be2\u521a\u521a\u521b\u5efa\u7684kafka topic","title":"\u5bf9\u63a5kafka"},{"location":"SQL_Analytics/Apache_Drill_1.15.0/#hbase","text":"\u505c\u6b62\u6b63\u5728\u8fd0\u884c\u7684drill \u53c2\u8003\u524d\u6587\u300a\u5bf9\u63a5HDFS\u300b,\u300a\u5bf9\u63a5HIVE\u300b\u7ae0\u8282\uff0c\u6210\u529f\u914d\u7f6e\u597d\u5bf9\u63a5hdfs,hive\u7ec4\u4ef6 \u627e\u5230drill\u5b89\u88c5\u76ee\u5f55\u4e0b./jar/ext/\u8def\u5f84\uff0c\u5c06drill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u91cd\u547d\u540d\u4e3azookeeper-3.4.12.jar.org\u6ce8\u91ca\u6389\uff0c\u5e76\u5c06FI HD\u5ba2\u6237\u7aef\u4e2dzookeeper\u76f8\u5173jar\u5305 zookeeper-3.5.1.jar\u62f7\u8d1d\u5230\u8be5\u8def\u5f84\u4e0b\u3002\u8fd9\u4e00\u6b65\u7684\u76ee\u7684\u662f\uff0cdrill\u81ea\u5e26\u7684zookeeper-3.4.12.jar\u7248\u672c\u592a\u65e7\uff0c\u5176\u5185\u90e8\u6ca1\u6709\u5b9a\u4e49send4LetterWord\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u5411FI HD zookeeper\u670d\u52a1\u81ea\u52a8\u83b7\u53d6\u8fde\u63a5zookeeper\u7684service principal (zookeeper/hadoop. hadoop.com@HADOOP.COM ) \u5728/opt\u8def\u5f84\u4e0b\u51c6\u5907jass.conf\u6587\u4ef6\uff0c\u5185\u5bb9\u5982\u4e0b\uff0c\u5176\u4e2d/opt/user.keytab\u4e3a\u7528\u6237developuser\u7684\u8ba4\u8bc1\u6587\u4ef6\uff1a Client { com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true principal=\"developuser@HADOOP.COM\" keyTab=\"/opt/user.keytab\" useTicketCache=false storeKey=true debug=true; }; \u542f\u52a8drill\u4e4b\u524d\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u52a0\u8f7d\u8fdb\u5fc5\u8981\u7684JVM\u53c2\u6570\uff1a source /opt/hadoopclient/bigdata_env export JAVA_TOOL_OPTIONS=\"-Xmx512m -Xms64m -Djava.security.auth.login.config=/opt/jaas.conf -Dkerberos.domain.name=hadoop.hadoop.com -Djava.security.krb5.conf=/etc/krb5.conf -Dzookeeper.server.principal=zookeeper/hadoop.hadoop.com\" \u5b8c\u6210\u4e4b\u540e\u4f7f\u7528\u547d\u4ee4 java -version \u68c0\u67e5\u662f\u5426\u52a0\u8f7d\u6210\u529f\uff1a \u5c06HBase\u5ba2\u6237\u7aef\u5305\u542b\u7684\u914d\u7f6e\u6587\u4ef6hbase-site.xml\u62f7\u8d1d\u5230drill\u5b89\u88c5\u8def\u5f84\u4e0b\u7684conf\u76ee\u5f55\u4e0b \u542f\u52a8drill\uff0c\u767b\u9646drill webUI\uff0c\u6dfb\u52a0Storage\u540d\u5b57\u4e3ahuaweihbase\u5e76enable,\u5185\u5bb9\u5982\u4e0b\uff1a { \"type\": \"hbase\", \"config\": { \"hbase.zookeeper.quorum\": \"172.16.6.10,172.16.6.11,172.16.6.12\", \"hbase.zookeeper.property.clientPort\": \"24002\" }, \"size.calculator.enabled\": false, \"enabled\": true } \u767b\u9646drill\u540e\u53f0\uff0c\u4f7f\u7528\u547d\u4ee4 use huaweihbase; \u4f7f\u7528\u6570\u636e\u5e93\uff1a \u4f7f\u7528\u547d\u4ee4 show tables; \u67e5\u770bhbase\u8868\uff1a \u4f7f\u7528\u547d\u4ee4 select * from ImportTable; \u67e5\u770bhbase\u8868\uff1a","title":"\u5bf9\u63a5HBase"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/","text":"Apache Kylin\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 1.6.0 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 162.2.200.200 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b cd FusionInsight_V100R002C60U20_Services_ClientConfig/ ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm \u7f16\u8bd1Kylin \u00b6 \u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-1.6.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u914d\u59571.0.2\u7684HBase\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-1.6.0\u57fa\u4e8eHBase1.0.2\u7248\u672c\u7684\u6e90\u7801 https://github.com/apache/kylin/tree/yang21-hbase102 \u5f97\u5230kylin-yang21-hbase102.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u7f16\u8bd1\u6253\u5305 unzip kylin-yang21-hbase102.zip cd kylin-yang21-hbase102/ sed -i \"s/1.6.0-SNAPSHOT/1.6.0/g\" `grep 1.6.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305 \u542f\u52a8Kylin \u00b6 \u89e3\u538b\u4e8c\u8fdb\u5236\u5305 \u00b6 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-1.6.0-bin.tar.gz -C /opt \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-1.6.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test_cn Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-1.6.0-bin/bin ./check-env.sh \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-1.6.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.hive.client=beeline kylin.hive.beeline.params=-n root -u 'jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-1.6.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u7f16\u8f91find-hive-dependency.sh\uff1a vi /opt/apache-kylin-1.6.0-bin/bin/find-hive-dependency.sh hive_lib=`find -L \"$(dirname $HCAT_HOME)\" -name '*.jar' ! -name '*calcite*' -printf '%p:' | sed 's/:$//'` \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-1.6.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u6784\u5efaCube \u00b6 \u9009\u62e9learn_kylin\u5de5\u7a0b\uff0c\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"1.6.0 <--> C60"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#apache-kylinfusioninsight-hd","text":"","title":"Apache Kylin\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_1","text":"Apache Kylin 1.6.0 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 162.1.115.89 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 162.2.200.200 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b cd FusionInsight_V100R002C60U20_Services_ClientConfig/ ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin","text":"\u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-1.6.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u914d\u59571.0.2\u7684HBase\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-1.6.0\u57fa\u4e8eHBase1.0.2\u7248\u672c\u7684\u6e90\u7801 https://github.com/apache/kylin/tree/yang21-hbase102 \u5f97\u5230kylin-yang21-hbase102.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u7f16\u8bd1\u6253\u5305 unzip kylin-yang21-hbase102.zip cd kylin-yang21-hbase102/ sed -i \"s/1.6.0-SNAPSHOT/1.6.0/g\" `grep 1.6.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305","title":"\u7f16\u8bd1Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_1","text":"","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_4","text":"\u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-1.6.0-bin.tar.gz -C /opt","title":"\u89e3\u538b\u4e8c\u8fdb\u5236\u5305"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_5","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-1.6.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test_cn Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-1.6.0-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_2","text":"\u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-1.6.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.hive.client=beeline kylin.hive.beeline.params=-n root -u 'jdbc:hive2://162.1.93.103:24002,162.1.93.102:24002,162.1.93.101:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-1.6.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u7f16\u8f91find-hive-dependency.sh\uff1a vi /opt/apache-kylin-1.6.0-bin/bin/find-hive-dependency.sh hive_lib=`find -L \"$(dirname $HCAT_HOME)\" -name '*.jar' ! -name '*calcite*' -printf '%p:' | sed 's/:$//'`","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#kylin_3","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-1.6.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#cube","text":"\u9009\u62e9learn_kylin\u5de5\u7a0b\uff0c\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_1.6.0/#_6","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/","text":"Apache Kylin2.1.0\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.1.0 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm \u7f16\u8bd1Kylin \u00b6 \u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-2.1.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-2.1.0\u57fa\u4e8eHBase1.1.1\u7248\u672c\u7684\u6e90\u7801\u7801 https://github.com/apache/kylin/tree/2.1.x \u5f97\u5230kylin-2.1.x.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u4fee\u6539kylin\u6e90\u7801 \u4fee\u6539HiveMRInput.java vi /opt/kylin-2.1.x/source-hive/src/main/java/org/apache/kylin/source/hive/HiveMRInput.java \u4fee\u6539pom.xml vi /opt/kylin-2.1.x/pom.xml \u5c06HBase\u3001Hive\u3001Hadoop\u7248\u672c\u6539\u6210\u4e0eFusionInsight HD\u5bf9\u5e94\u7684\u7248\u672c \u7f16\u8bd1\u6253\u5305 unzip kylin-2.1.x.zip cd kylin-2.1.x sed -i \"s/2.1.0-SNAPSHOT/2.1.0/g\" `grep 2.1.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305 \u542f\u52a8Kylin \u00b6 \u89e3\u538b\u4e8c\u8fdb\u5236\u5305 \u00b6 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-2.1.0-bin.tar.gz -C /opt \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN\\_HOME=/opt/apache-kylin-2.1.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.1.0-bin/bin ./check-env.sh \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.1.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.42.30:24002,172.21.42.31:24002,172.21.42.32:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u9700\u8981\u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.1.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.1.0-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.1.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"2.1.0 <--> C70"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#apache-kylin210fusioninsight-hd","text":"","title":"Apache Kylin2.1.0\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_1","text":"Apache Kylin 2.1.0 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u5b89\u88c5Hadoop client \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u5b89\u88c5\u4e3b\u673a\uff0c\u89e3\u538b ./install.sh /opt/hadoopclient \u5b89\u88c5JDK rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin","text":"\u53ef\u76f4\u63a5\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684Kylin-2.1.0\u4e3b\u7248\u672c\u662f\u57fa\u4e8eHBase1.1.1\u7f16\u8bd1\u7684\uff0c\u800cFusionInsight\u4f7f\u7528\u7684HBase\u7248\u672c\u662f1.0.2\uff0c\u8fd9\u4e24\u4e2a\u7248\u672c\u90e8\u5206\u7c7b\u548c\u65b9\u6cd5\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u91cd\u65b0\u7f16\u8bd1Kylin\u3002 \u4e0b\u8f7dKylin-2.1.0\u57fa\u4e8eHBase1.1.1\u7248\u672c\u7684\u6e90\u7801\u7801 https://github.com/apache/kylin/tree/2.1.x \u5f97\u5230kylin-2.1.x.zip \u5b89\u88c5\u7f16\u8bd1\u5de5\u5177 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u5b89\u88c5git yum install -y git \u5b89\u88c5nodejs\uff1a wget https://nodejs.org/dist/v6.10.0/node-v6.10.0-linux-x64.tar.xz --no-check-certificate tar -xvf node-v6.10.0-linux-x64.tar.xz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin:/opt/node-v6.10.0-linux-x64/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c npm -v \u4fee\u6539kylin\u6e90\u7801 \u4fee\u6539HiveMRInput.java vi /opt/kylin-2.1.x/source-hive/src/main/java/org/apache/kylin/source/hive/HiveMRInput.java \u4fee\u6539pom.xml vi /opt/kylin-2.1.x/pom.xml \u5c06HBase\u3001Hive\u3001Hadoop\u7248\u672c\u6539\u6210\u4e0eFusionInsight HD\u5bf9\u5e94\u7684\u7248\u672c \u7f16\u8bd1\u6253\u5305 unzip kylin-2.1.x.zip cd kylin-2.1.x sed -i \"s/2.1.0-SNAPSHOT/2.1.0/g\" `grep 2.1.0-SNAPSHOT -rl *` sh build/script/package.sh \u7b49\u5f85\u7f16\u8bd1\u5b8c\u6210\uff0c\u5f97\u5230Kylin\u4e8c\u8fdb\u5236\u5b89\u88c5\u5305","title":"\u7f16\u8bd1Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_1","text":"","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_4","text":"\u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u751f\u6210\u7684\u5b89\u88c5\u5305 tar -xzvf apache-kylin-2.1.0-bin.tar.gz -C /opt","title":"\u89e3\u538b\u4e8c\u8fdb\u5236\u5305"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_5","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN\\_HOME=/opt/apache-kylin-2.1.0-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.1.0-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_2","text":"\u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.1.0-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.42.30:24002,172.21.42.31:24002,172.21.42.32:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' \u914d\u7f6e\u83b7\u53d6\u4efb\u52a1\u72b6\u6001\u65f6\u4f7f\u7528kerberos \u9274\u6743\uff1a kylin.job.status.with.kerberos=true \u53bb\u6389\u4e0d\u5141\u8bb8\u4fee\u6539\u7684\u914d\u7f6e FusionInsight\u4e0d\u5141\u8bb8\u4fee\u6539dfs.replication, mapreduce.job.split.metainfo.maxsize\u7684\u53c2\u6570\uff0c\u9700\u8981\u6ce8\u91ca\u6389Kylin\u6240\u6709\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u76f8\u5173\u53c2\u6570\uff0c\u5426\u5219\u6784\u5efaCube\u65f6\u4f1a\u62a5\u5982\u4e0b\u9519\u8bef\uff1a \u9700\u8981\u4fee\u6539\u4ee5\u4e0b\u6587\u4ef6\uff1a kylin_hive_conf.xml kylin_job_conf_inmem.xml kylin_job_conf.xml Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.1.0-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.1.0-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u82e5Kylin\u5b89\u88c5\u5728\u96c6\u7fa4\u8282\u70b9\u4e0a\u4e0d\u4f1a\u6709\u95ee\u9898\uff0c\u5426\u5219\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#kylin_3","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.1.0-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.1.0/#_6","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/","text":"Apache Kylin2.3.1\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.3.1 \u2194 FusionInsight HD V100R002C80SPC100 (HBase/Hive) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/hadoopclient \u5b89\u88c5JDK1.8 rpm -Uvh jdk-8u112-linux-x64.rpm \u4e0b\u8f7dKylin \u00b6 Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-2.3.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin \u4e0b\u8f7d\u89e3\u538bKylin \u00b6 \u4e0b\u8f7dKylin-2.3.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\uff0c http://ftp.cuhk.edu.hk/pub/packages/apache.org/kylin/apache-kylin-2.3.1/apache-kylin-2.3.1-hbase1x-bin.tar.gz \u4e0a\u4f20apache-kylin-2.3.1-hbase1x-bin.tar.gz\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C /opt \u914d\u7f6eKylin \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.3.1-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.3.1-bin/bin ./check-env.sh \u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879 \u00b6 \u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..* \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84 \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"2.3.1 <--> C80"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#apache-kylin231fusioninsight-hd","text":"","title":"Apache Kylin2.3.1\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_1","text":"Apache Kylin 2.3.1 \u2194 FusionInsight HD V100R002C80SPC100 (HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_3","text":"\u4fee\u6539/etc/hosts \u6dfb\u52a0\u672c\u673a\u4e3b\u673a\u540d\u89e3\u6790 172.16.52.86 kylin \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e,\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65 server 172.18.0.18 nomodify notrap nopeer noquery \u542f\u52a8NTP\u670d\u52a1 service ntpd start chkconfig ntpd on \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55 ./install.sh /opt/hadoopclient \u5b89\u88c5JDK1.8 rpm -Uvh jdk-8u112-linux-x64.rpm","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin","text":"Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-2.3.1-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin","title":"\u4e0b\u8f7dKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_1","text":"\u4e0b\u8f7dKylin-2.3.1\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\uff0c http://ftp.cuhk.edu.hk/pub/packages/apache.org/kylin/apache-kylin-2.3.1/apache-kylin-2.3.1-hbase1x-bin.tar.gz \u4e0a\u4f20apache-kylin-2.3.1-hbase1x-bin.tar.gz\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.3.1-hbase1x-bin.tar.gz -C /opt","title":"\u4e0b\u8f7d\u89e3\u538bKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_2","text":"","title":"\u914d\u7f6eKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.3.1-bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit test Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.3.1-bin/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#fusioninsighthive","text":"\u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4e00\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\uff0c\u91cd\u542f\u5f71\u54cd\u7684\u670d\u52a1 |mapreduce\\.job\\..*|dfs\\..*","title":"\u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_3","text":"\u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties \u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4Hive lib\u8def\u5f84\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84\uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#kylin_4","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 System -> Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.3.1/#_5","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/","text":"Apache Kylin2.6.1\u5bf9\u63a5FusionInsight HD \u00b6 \u9002\u7528\u573a\u666f \u00b6 Apache Kylin 2.6.1 \u2194 FusionInsight HD 6.5 (HBase/Hive) \u8bf4\u660e \u00b6 Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5 \u73af\u5883\u51c6\u5907 \u00b6 \u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e\uff0c\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65\u3002 server 172.21.3.101 nomodify notrap nopeer noquery \u8bf4\u660e\uff1a172.21.3.101\u4e3aFusionInsight HD\u5176\u4e2d\u4e00\u4e2a\u96c6\u7fa4\u7684IP\u3002 \u91cd\u542fNTP\u670d\u52a1\uff0c\u5e76\u540c\u6b65\u672c\u673a\u548c\u96c6\u7fa4\u7684\u65f6\u95f4\u3002 systemctl restart ntpd systemctl stop ntpd ntpdate 172.16.4.21 \u8bf4\u660e\uff1a\u9700\u8981\u5148\u505c\u6b62ntpd\u670d\u52a1\u540e\uff0c\u518d\u6267\u884cntpdate\u540c\u6b65\u65f6\u95f4\uff0c\u5426\u5219\u4f1a\u8fd4\u56de\u9519\u8bef\u201cthe NTP socket is in use, exiting\u201d\u3002\u53e6\u5916172.16.4.21\u96c6\u7fa4\u7684nptd\u670d\u52a1\u5fc5\u987b\u662f\u542f\u52a8\u7684\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55\u3002 ./install.sh /opt/hadoopclient \u4e0b\u8f7dKylin \u00b6 Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-**-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u4f8b\u5982apache-kylin-2.6.1-hbase1x-bin.tar.gz\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin\u3002 \u4e0b\u8f7d\u89e3\u538bKylin \u00b6 \u4e0b\u8f7dKylin\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\u3002\u4f8b\u5982Kylin-2.6.1\u4e0b\u8f7d\u8def\u5f84\u4e3a https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-2.6.1/apache-kylin-2.6.1-bin-hbase1x.tar.gz \u3002\u5176\u4ed6\u7248\u672c\u4e0b\u8f7d\u8def\u5f84\u7c7b\u540c\u3002 \u4e0a\u4f20\u5b89\u88c5\u5305\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.6.1-hbase1x-bin.tar.gz \u914d\u7f6eKylin \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf \u00b6 \u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.6.1-bin-hbase1x \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit developuser Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.6.1-bin-hbase1x/bin ./check-env.sh \u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879 \u00b6 \u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\u5e76\u91cd\u542fHive\u670d\u52a1\u4ee5\u53ca\u53d7\u5f71\u54cd\u7684\u670d\u52a1\u3002 |mapreduce\\.job\\..*|dfs\\..* \u4fee\u6539Kylin\u914d\u7f6e \u00b6 \u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff0c\u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389\u3002 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4hive_exec_path\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84 /opt/huawei/Bigdata/ \uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 vi find-hive-dependency.sh \u8bf4\u660e\uff1a\u5728\u7ed9hive_exec_path\u8d4b\u503c\u4e4b\u540e\uff0c\u4f7f\u7528\u4e4b\u524d\uff0c\u589e\u52a0 hive_exec_path=\"$HCAT_HOME\" \u3002\u5426\u5219\u6267\u884c./kylin.sh start\u542f\u52a8kylin\u65f6\u8fd4\u56de\u9519\u8bef\"Couldn't find hive executable jar. Please check if hive executable jar exists in HIVE_LIB folder.\" \u542f\u52a8Kylin \u00b6 \u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646 Demo\u6d4b\u8bd5 \u00b6 \u5bfc\u5165Demo\u6570\u636e \u00b6 \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 Model \u6784\u5efaCube \u00b6 \u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY \u67e5\u8be2\u8868\u6570\u636e \u00b6 \u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2 FAQ \u00b6 \u70b9\u51fbMonitor\u67e5\u770bbuild\u72b6\u6001\u65f6\u8fd4\u56de\u9519\u8beffailed to load jobs \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6784\u5efaCube\u540e\uff0c\u70b9\u51fbMonitor\u67e5\u770bbuild\u72b6\u6001\u65f6\u8fd4\u56de\u9519\u8beffailed to load jobs\u3002 \u67e5\u770b\u65e5\u5fd7/opt/apache-kylin-2.6.3-bin-hbase1x/logs/kylin.log\u9519\u8bef\u5982\u4e0b\uff1a 2019-08-20 21:19:07,958 INFO [http-bio-7070-exec-7] ipc.RpcClientImpl:824 : RPC Server Kerberos principal name for service=ClientService is hbase/hadoop.hadoop.com@HADOOP.COM 2019-08-20 21:19:07,979 DEBUG [http-bio-7070-exec-7] badquery.BadQueryHistoryManager:65 : Loaded 0 Bad Query(s) 2019-08-20 21:19:08,016 ERROR [http-bio-7070-exec-8] controller.BasicController:63 : org.springframework.web.util.NestedServletException: Handler dispatch failed; nested exception is java.lang.NoClassDefFoundError: org/apache/directory/api/util/Strings at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:982) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) ... ... Caused by: java.lang.ClassNotFoundException: org.apache.directory.api.util.Strings at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1928) at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1771) ... 89 more \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u95ee\u9898\u539f\u56e0\uff1a\u7f3a\u5c11\u5305\u542borg.apache.directory.api.util.Strings\u7684jar\u5305\u3002 * \u4ece<http://www.java2s.com/Code/JarDownload/api-util/api-util-1.0.0-m18.jar.zip>\u4e0b\u8f7d\u5305\u542borg.apache.directory.api.util.Strings\u7684jar\u5305\uff0c\u5e76\u653e\u5230/opt/apache-kylin-2.6.3-bin-hbase1x/lib\u76ee\u5f55\u4e0b\u3002 ![](assets/Apache_Kylin_2.6.1/1cde4e36.png) * \u91cd\u542fkylin\u3002 ``` cd /opt/apache-kylin-2.6.3-bin-hbase1x/bin ./kylin.sh stop ./kylin.sh start ```","title":"2.6.1 <--> 6.5"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#apache-kylin261fusioninsight-hd","text":"","title":"Apache Kylin2.6.1\u5bf9\u63a5FusionInsight HD"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_1","text":"Apache Kylin 2.6.1 \u2194 FusionInsight HD 6.5 (HBase/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_2","text":"Apache Kylin\u2122\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5206\u6790\u5f15\u64ce\uff0c\u63d0\u4f9bHadoop\u4e4b\u4e0a\u7684SQL\u67e5\u8be2\u63a5\u53e3\u53ca\u591a\u7ef4\u5206\u6790\uff08OLAP\uff09\u80fd\u529b\u4ee5\u652f\u6301\u8d85\u5927\u89c4\u6a21\u6570\u636e\uff0c\u6700\u521d\u7531eBay Inc. \u5f00\u53d1\u5e76\u8d21\u732e\u81f3\u5f00\u6e90\u793e\u533a\u3002\u5b83\u80fd\u5728\u4e9a\u79d2\u5185\u67e5\u8be2\u5de8\u5927\u7684Hive\u8868\u3002 Apache Kylin\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHBase\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_3","text":"\u914d\u7f6eNTP\u670d\u52a1 \u4f7f\u7528vi /etc/ntp.conf\u589e\u52a0NTP\u670d\u52a1\u7684\u914d\u7f6e\uff0c\u65f6\u95f4\u4e0eFusionInsight\u96c6\u7fa4\u540c\u6b65\u3002 server 172.21.3.101 nomodify notrap nopeer noquery \u8bf4\u660e\uff1a172.21.3.101\u4e3aFusionInsight HD\u5176\u4e2d\u4e00\u4e2a\u96c6\u7fa4\u7684IP\u3002 \u91cd\u542fNTP\u670d\u52a1\uff0c\u5e76\u540c\u6b65\u672c\u673a\u548c\u96c6\u7fa4\u7684\u65f6\u95f4\u3002 systemctl restart ntpd systemctl stop ntpd ntpdate 172.16.4.21 \u8bf4\u660e\uff1a\u9700\u8981\u5148\u505c\u6b62ntpd\u670d\u52a1\u540e\uff0c\u518d\u6267\u884cntpdate\u540c\u6b65\u65f6\u95f4\uff0c\u5426\u5219\u4f1a\u8fd4\u56de\u9519\u8bef\u201cthe NTP socket is in use, exiting\u201d\u3002\u53e6\u5916172.16.4.21\u96c6\u7fa4\u7684nptd\u670d\u52a1\u5fc5\u987b\u662f\u542f\u52a8\u7684\u3002 \u53c2\u8003FusionInsight\u4ea7\u54c1\u6587\u6863\u5728Kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef \u5728FusionInsight Manager\u670d\u52a1\u7ba1\u7406\u9875\u9762\u4e0b\u8f7d\u5ba2\u6237\u7aef\uff0c\u4e0a\u4f20\u5230kylin\u8282\u70b9\u5b89\u88c5FusionInsight\u5ba2\u6237\u7aef\u5230 /opt/hadoopclient \u76ee\u5f55\u3002 ./install.sh /opt/hadoopclient","title":"\u73af\u5883\u51c6\u5907"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin","text":"Fusioninsight\u914d\u5957\u7684HBase\u662f1.3.0\uff0cApache Kylin\u53ef\u76f4\u63a5\u4e0b\u8f7dapache-kylin-**-hbase1x-bin.tar.gz\u4e3b\u7248\u672c\u4e8c\u8fdb\u5236\u5305\uff0c\u4f8b\u5982apache-kylin-2.6.1-hbase1x-bin.tar.gz\uff0c\u65e0\u9700\u7f16\u8bd1Apache kylin\u3002","title":"\u4e0b\u8f7dKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_1","text":"\u4e0b\u8f7dKylin\u57fa\u4e8eHBase1.x\u7248\u672c\u7684\u4e8c\u8fdb\u5236\u5305\u3002\u4f8b\u5982Kylin-2.6.1\u4e0b\u8f7d\u8def\u5f84\u4e3a https://www.apache.org/dyn/closer.cgi/kylin/apache-kylin-2.6.1/apache-kylin-2.6.1-bin-hbase1x.tar.gz \u3002\u5176\u4ed6\u7248\u672c\u4e0b\u8f7d\u8def\u5f84\u7c7b\u540c\u3002 \u4e0a\u4f20\u5b89\u88c5\u5305\u5230Apache kylin\u8282\u70b9\u7684 /opt \u76ee\u5f55 \u89e3\u538b\u4e0a\u4e00\u6b65\u9aa4\u7684\u5b89\u88c5\u5305 cd /opt tar -zxvf apache-kylin-2.6.1-hbase1x-bin.tar.gz","title":"\u4e0b\u8f7d\u89e3\u538bKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_2","text":"","title":"\u914d\u7f6eKylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_4","text":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff1a vi /etc/profile \uff0c\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export KYLIN_HOME=/opt/apache-kylin-2.6.1-bin-hbase1x \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile Kylin\u542f\u52a8\u8fd8\u9700\u8981\u914d\u7f6eHIVE_CONF\u3001HCAT_HOME\uff0c\u4f7f\u7528 vi /opt/hadoopclient/Hive/component_env \uff0c\u5728\u6587\u4ef6\u6700\u540e\u589e\u52a0 export HIVE_CONF=/opt/hadoopclient/Hive/config export HCAT_HOME=/opt/hadoopclient/Hive/HCatalog \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /opt/hadoopclient/bigdata_env \u8fdb\u884ckerberos\u8ba4\u8bc1 kinit developuser Kylin\u68c0\u67e5\u73af\u5883\u8bbe\u7f6e\uff1a cd /opt/apache-kylin-2.6.1-bin-hbase1x/bin ./check-env.sh","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#fusioninsighthive","text":"\u5728hive.security.authorization.sqlstd.confwhitelist.append\u53c2\u6570\u6700\u540e\u8ffd\u52a0\u4ee5\u4e0b\u53c2\u6570\u914d\u7f6e\uff0c\u4fdd\u5b58\u914d\u7f6e\u5e76\u91cd\u542fHive\u670d\u52a1\u4ee5\u53ca\u53d7\u5f71\u54cd\u7684\u670d\u52a1\u3002 |mapreduce\\.job\\..*|dfs\\..*","title":"\u4fee\u6539FusionInsight\u7684Hive\u914d\u7f6e\u9879"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_3","text":"\u83b7\u53d6Hive\u7684JDBC\u5b57\u7b26\u4e32 \u6267\u884cBeeline\u67e5\u770bHive\u7684JDBC\u5b57\u7b26\u4e32 source bigdata_env kinit test beeline \u4fee\u6539kylin.properties\uff0c\u914d\u7f6eHive client\u4f7f\u7528beeline\uff1a vi /opt/apache-kylin-2.3.1-bin/conf/kylin.properties kylin.source.hive.client=beeline kylin.source.hive.beeline-shell=beeline kylin.source.hive.beeline-params=-n root -u 'jdbc:hive2://172.21.3.101:24002,172.21.3.102:24002,172.21.3.103:24002/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;sasl.qop=auth-conf;auth=KERBEROS;principal=hive/hadoop.hadoop.com@HADOOP.COM' JDBC\u5b57\u7b26\u4e32\u4f7f\u7528\u4e0a\u4e00\u6b65\u9aa4\u83b7\u53d6\u7684\u5b57\u7b26\u4e32 \u6ce8\u610f\uff1akylin.source.hive.beeline-params\u53c2\u6570\u91cc\u9762\u539f\u6709\u7684 --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' \u8981\u53bb\u6389\u3002 \u4fee\u6539Hive/HBase\u914d\u7f6e \u5c06/opt/hadoopclient/Hive/config/hivemetastore-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230hive-site.xml \u5c06/opt/hadoopclient/HBase/hbase/conf/hbase-site.xml\u4e2d\u7684\u914d\u7f6e\u5408\u5e76\u5230/opt/apache-kylin-2.3.1-bin/conf/kylin_job_conf.xml Hive lib\u8def\u5f84 kylin\u7684/opt/apache-kylin-2.3.1-bin/bin/find-hive-dependency.sh\u9ed8\u8ba4hive_exec_path\u4e3a\u5927\u6570\u636e\u96c6\u7fa4\u4e2dHive\u7684\u5b89\u88c5\u8def\u5f84 /opt/huawei/Bigdata/ \uff0c\u9700\u8981\u4fee\u6539\u4e3a\u5ba2\u6237\u7aef\u8def\u5f84\u3002 vi find-hive-dependency.sh \u8bf4\u660e\uff1a\u5728\u7ed9hive_exec_path\u8d4b\u503c\u4e4b\u540e\uff0c\u4f7f\u7528\u4e4b\u524d\uff0c\u589e\u52a0 hive_exec_path=\"$HCAT_HOME\" \u3002\u5426\u5219\u6267\u884c./kylin.sh start\u542f\u52a8kylin\u65f6\u8fd4\u56de\u9519\u8bef\"Couldn't find hive executable jar. Please check if hive executable jar exists in HIVE_LIB folder.\"","title":"\u4fee\u6539Kylin\u914d\u7f6e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#kylin_4","text":"\u4f7f\u7528 ./kylin.sh start \u542f\u52a8Kylin \u8f93\u5165\u9ed8\u8ba4\u7528\u6237\u540d\u5bc6\u7801\uff1aADMIN/KYLIN\u767b\u9646","title":"\u542f\u52a8Kylin"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#demo","text":"","title":"Demo\u6d4b\u8bd5"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#demo_1","text":"\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u5bfc\u5165sample\u6570\u636e cd /opt/apache-kylin-2.3.1-bin/bin ./sample.sh \u9009\u62e9\u83dc\u5355 System -> Actions -> Reload Metadata \u9009\u62e9\u83dc\u5355 Model","title":"\u5bfc\u5165Demo\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#cube","text":"\u6784\u5efa\u9ed8\u8ba4\u7684kylin_sales_cube \u9009\u62e9End Data\uff08Exclude\uff09\u65f6\u95f4\uff1a \u70b9\u51fbMonitor\u53ef\u4ee5\u67e5\u770bbuild\u72b6\u6001\uff1a Build\u5b8c\u6210\uff1a Cube\u6784\u5efa\u6210\u529f\uff0c\u72b6\u6001\u53d8\u4e3aREADY","title":"\u6784\u5efaCube"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#_5","text":"\u5728Insight\u9875\u9762\u6267\u884c\u67e5\u8be2","title":"\u67e5\u8be2\u8868\u6570\u636e"},{"location":"SQL_Analytics/Apache_Kylin_2.6.1/#faq","text":"\u70b9\u51fbMonitor\u67e5\u770bbuild\u72b6\u6001\u65f6\u8fd4\u56de\u9519\u8beffailed to load jobs \u3010\u95ee\u9898\u63cf\u8ff0\u3011 \u6784\u5efaCube\u540e\uff0c\u70b9\u51fbMonitor\u67e5\u770bbuild\u72b6\u6001\u65f6\u8fd4\u56de\u9519\u8beffailed to load jobs\u3002 \u67e5\u770b\u65e5\u5fd7/opt/apache-kylin-2.6.3-bin-hbase1x/logs/kylin.log\u9519\u8bef\u5982\u4e0b\uff1a 2019-08-20 21:19:07,958 INFO [http-bio-7070-exec-7] ipc.RpcClientImpl:824 : RPC Server Kerberos principal name for service=ClientService is hbase/hadoop.hadoop.com@HADOOP.COM 2019-08-20 21:19:07,979 DEBUG [http-bio-7070-exec-7] badquery.BadQueryHistoryManager:65 : Loaded 0 Bad Query(s) 2019-08-20 21:19:08,016 ERROR [http-bio-7070-exec-8] controller.BasicController:63 : org.springframework.web.util.NestedServletException: Handler dispatch failed; nested exception is java.lang.NoClassDefFoundError: org/apache/directory/api/util/Strings at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:982) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) ... ... Caused by: java.lang.ClassNotFoundException: org.apache.directory.api.util.Strings at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1928) at org.apache.catalina.loader.WebappClassLoaderBase.loadClass(WebappClassLoaderBase.java:1771) ... 89 more \u3010\u89e3\u51b3\u65b9\u6cd5\u3011 \u95ee\u9898\u539f\u56e0\uff1a\u7f3a\u5c11\u5305\u542borg.apache.directory.api.util.Strings\u7684jar\u5305\u3002 * \u4ece<http://www.java2s.com/Code/JarDownload/api-util/api-util-1.0.0-m18.jar.zip>\u4e0b\u8f7d\u5305\u542borg.apache.directory.api.util.Strings\u7684jar\u5305\uff0c\u5e76\u653e\u5230/opt/apache-kylin-2.6.3-bin-hbase1x/lib\u76ee\u5f55\u4e0b\u3002 ![](assets/Apache_Kylin_2.6.1/1cde4e36.png) * \u91cd\u542fkylin\u3002 ``` cd /opt/apache-kylin-2.6.3-bin-hbase1x/bin ./kylin.sh stop ./kylin.sh start ```","title":"FAQ"},{"location":"SQL_Analytics/Kyligence/","text":"Kyligence Analytics Platform\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Kyligence Analytics Platform 2.2 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) Kyligence Analytics Platform 2.3 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) Kyligence Analytics Platform 2.4 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) Kyligence Analytics Platform 2.5 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) Kyligence Analytics Platform 3.0 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Yarn) \u8bf4\u660e \u00b6 \u53c2\u8003 \u5b98\u65b9\u4ea7\u54c1\u6587\u6863 \u7684 \u5feb\u901f\u5b89\u88c5 \u7ae0\u8282","title":"3.0 <--> C80"},{"location":"SQL_Analytics/Kyligence/#kyligence-analytics-platformfusioninsight","text":"","title":"Kyligence Analytics Platform\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Kyligence/#_1","text":"Kyligence Analytics Platform 2.2 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) Kyligence Analytics Platform 2.3 \u2194 FusionInsight HD V100R002C60U20 (HBase/Hive) Kyligence Analytics Platform 2.4 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) Kyligence Analytics Platform 2.5 \u2194 FusionInsight HD V100R002C70SPC200 (HBase/Hive) Kyligence Analytics Platform 3.0 \u2194 FusionInsight HD V100R002C80SPC200 (HBase/Hive/Yarn)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Kyligence/#_2","text":"\u53c2\u8003 \u5b98\u65b9\u4ea7\u54c1\u6587\u6863 \u7684 \u5feb\u901f\u5b89\u88c5 \u7ae0\u8282","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.155/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto 0.155 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive) \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eHive Connector \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.155/presto-server-0.155.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.155 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.155\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http.server.authentication.enabled=true http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml,/opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.155\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.155 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.155.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.155/plugin/hive-hadoop2/presto-hive-0.155.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.155/bin/launcher stop sh /opt/presto-server-0.155/bin/launcher start tailf /var/presto/data/var/log/server.log \u68c0\u67e5FusionInsight Manager\u4e2dHDFS\u670d\u52a1\u914d\u7f6e\u4e2dhadoop.rpc.protection\u7684\u914d\u7f6e\uff0c\u5fc5\u987b\u8bbe\u7f6e\u4e3aauthentacation\u3002 \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.155/presto-cli-0.155-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.155-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.155-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.155/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.155/presto-jdbc-0.155.jar \u53c2\u8003 https://prestodb.io/docs/0.155/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++ ) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"0.155 <--> C60"},{"location":"SQL_Analytics/Presto_0.155/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.155/#_1","text":"Presto 0.155 \u2194 FusionInsight HD V100R002C60U20 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.155/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.155/#hive-connector","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.155/presto-server-0.155.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.155 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.155\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http.server.authentication.enabled=true http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.155/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml,/opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.155\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.155 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.155.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.155/plugin/hive-hadoop2/presto-hive-0.155.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.155/bin/launcher stop sh /opt/presto-server-0.155/bin/launcher start tailf /var/presto/data/var/log/server.log \u68c0\u67e5FusionInsight Manager\u4e2dHDFS\u670d\u52a1\u914d\u7f6e\u4e2dhadoop.rpc.protection\u7684\u914d\u7f6e\uff0c\u5fc5\u987b\u8bbe\u7f6e\u4e3aauthentacation\u3002","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.155/#presto-clihive","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.155/presto-cli-0.155-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.155-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.155-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.155/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.155/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.155/presto-jdbc-0.155.jar \u53c2\u8003 https://prestodb.io/docs/0.155/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++ ) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.184/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto 0.184 \u2194 FusionInsight HD V100R002C70SPC100 (HDFS/Hive) Presto 0.196 \u2194 FusionInsight HD V100R002C80SPC100 (HDFS/Hive) \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5 \u914d\u7f6eHive Connector \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.184/presto-server-0.184.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.184 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C70SPC100\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.184\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.184/etc/catalog/core-site.xml,/opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.184/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.184/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.184/etc/catalog/ \u6309\u7167\u4e0b\u56fe\u4fee\u6539hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.184\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.184 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.184.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.184/plugin/hive-hadoop2/presto-hive-0.184.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.184/bin/launcher stop sh /opt/presto-server-0.184/bin/launcher start tailf /var/presto/data/var/log/server.log \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.184/presto-cli-0.184-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.184-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.184/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.184/presto-jdbc-0.184.jar \u53c2\u8003 https://prestodb.io/docs/0.184/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++ ) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"0.196 <--> C80"},{"location":"SQL_Analytics/Presto_0.184/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.184/#_1","text":"Presto 0.184 \u2194 FusionInsight HD V100R002C70SPC100 (HDFS/Hive) Presto 0.196 \u2194 FusionInsight HD V100R002C80SPC100 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.184/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u548cHDFS\u8fdb\u884c\u5bf9\u63a5","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.184/#hive-connector","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.184/presto-server-0.184.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.184 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C70SPC100\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto\u8be50.184\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool \u2013genkeypair \u2013alias testuser \u2013keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u9009\u62e9hadoop\u548chive\u7528\u6237\u7ec4\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.184/etc/catalog\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://162.1.93.101:21088,thrift://162.1.93.102:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.184/etc/catalog/core-site.xml,/opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.184/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.184/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.184/etc/catalog/ \u6309\u7167\u4e0b\u56fe\u4fee\u6539hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.184/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.184\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.184 \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP=auth\"\u4fee\u6539\u4e3a\"Sasl.QOP=auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.184.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.184/plugin/hive-hadoop2/presto-hive-0.184.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.184/bin/launcher stop sh /opt/presto-server-0.184/bin/launcher start tailf /var/presto/data/var/log/server.log","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.184/#presto-clihive","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.184/presto-cli-0.184-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.184-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --enable-authentication \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.184/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.184/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.184/presto-jdbc-0.184.jar \u53c2\u8003 https://prestodb.io/docs/0.184/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://162.1.115.71:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from workers_info limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++ ) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a \u767e\u4e07\u8bb0\u5f55\u6570\u8868web_sales\u67e5\u8be2\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/","text":"Apache Presto\u5bf9\u63a5FusionInsight \u00b6 \u9002\u7528\u573a\u666f \u00b6 Presto 0.210 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/ElasticSearch) Presto 0.210 \u2194 FusionInsight HD 6.5 (HDFS/Hive) Presto 0.210 \u2194 FusionInsight HD 8.0 (HDFS/Hive) \u8bf4\u660e \u00b6 Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u8fdb\u884c\u5bf9\u63a5,\u5728Presto0.210\u7248\u672c\u4e2d\u652f\u6301\u4e0eFusionInsight\u7684ES\u8fdb\u884c\u5bf9\u63a5\u3002 \u83b7\u53d6\u5e76\u914d\u7f6epresto server \u00b6 Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.210/presto-server-0.210.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.210 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C80SPC200\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto-0.210\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool -genkeypair -alias testuser -keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d,\u5e76\u4e14\u8981\u5ffd\u7565\u5927\u5c0f\u5199\uff0c\u7edf\u4e00\u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u9009\u62e9\u7528\u6237\u7ec4(hadoop\u548chive\u7ec4)\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d(\u4ecespark2x\u5ba2\u6237\u7aef\u76ee\u5f55\u4e0b\u83b7\u53d6)\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.210/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.210/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.210/etc/catalog/ \u5c06hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4fee\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.210\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.210 \u83b7\u53d6Presto CLI\u542f\u52a8\u5305 \u00b6 \u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.210/presto-cli-0.210-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.210-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h \u914d\u7f6eHive Connector \u00b6 \u8fdb\u5165\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://172.21.3.115:21088,thrift://172.21.3.116:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.210/etc/catalog/core-site.xml,/opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP\"\u7684\u503c\u4fee\u6539\u4e3a\u56fa\u5b9a\u7684\"auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.210.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.210/plugin/hive-hadoop2/presto-hive-0.210.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.210/bin/launcher start tailf /var/presto/data/var/log/server.log \u901a\u8fc7Presto CLI\u8fde\u63a5Hive \u00b6 \u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982/opt\uff1b \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.210/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a \u901a\u8fc7Presto JDBC\u8fde\u63a5Hive \u00b6 \u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.210/presto-jdbc-0.210.jar \u53c2\u8003 https://prestodb.io/docs/0.210/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://172.21.3.48:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from adult limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++ ) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a presto\u591aworker\u8282\u70b9\u914d\u7f6e \u00b6 presto \u670d\u52a1\u89d2\u8272\u5206\u5e03 \u670d\u52a1\u5668ip\u5730\u5740 \u89d2\u8272 172.16.9.107 Coordinator/Worker 172.16.2.120 Worker 172.16.2.121 Worker \u524d\u63d0\u6761\u4ef6 \uff1a\u6839\u636e\u4e0a\u8ff0\u90e8\u5206\u5df2\u7ecf\u5b8c\u6210coordinator/worker\u8282\u70b9\u5408\u90e8\u914d\u7f6e\u4ee5\u53ca\u5bf9\u63a5hive\u6210\u529f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5c06coordinator/worker\u8282\u70b9\u5df2\u7ecf\u914d\u7f6e\u597d\u7684presto\u6587\u4ef6\u62f7\u8d1d\u5230worker\u8282\u70b9 scp -r /opt/presto-server-0.210 root@172.16.2.120:/opt scp -r /opt/presto-server-0.210 root@172.16.2.121:/opt \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5c06coordinator/worker\u8282\u70b9\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230worker\u8282\u70b9 scp /opt/presto.keytab root@172.16.2.120:/opt scp /opt/presto.keytab root@172.16.2.121:/opt scp /opt/145_651hdclient/user.keytab root@172.16.2.120:/opt/145_651hdclient scp /opt/145_651hdclient/user.keytab root@172.16.2.121:/opt/145_651hdclient \u4fee\u6539worker\u8282\u70b9 /opt/presto-server-0.210/etc/config.properties \u914d\u7f6e\u6587\u4ef6 \u65b0\u589eworker\u8282\u70b9172.16.2.120, 172.16.2.121\u7684config.properties\u914d\u7f6e\u9700\u8981\u4fee\u6539\uff0c\u5982\u4e0b: coordinator=false http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery.uri=http://172-16-9-107:8080 Coordinator\uff0cworker\u8282\u70b9\u5408\u90e8\u8282\u70b9172.16.9.107\u7684config.properties\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u4e0d\u53d8\uff0c\u5982\u4e0b\uff1a coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://172-16-9-107:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=developuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/145_651hdclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 \u8bf4\u660e\uff1anode-scheduler.include-coordinator\u53c2\u6570\u4e3aCoordinator,worker\u8282\u70b9\u5408\u90e8\u53c2\u6570\uff0c\u503c\u4e3afalse\u4ee3\u8868\u8be5\u8282\u70b9\u53ea\u505acoordinator\u8282\u70b9 \u4fee\u6539worker\u8282\u70b9 /opt/presto-server-0.210/etc/node.properties 172.16.2.120\u8282\u70b9\u914d\u7f6e: node.environment=production node.id=ffffffff-ffff-ffff-ffff-fffffffffffw1 node.data-dir=/var/presto/data 172.16.2.121\u8282\u70b9\u914d\u7f6e: node.environment=production node.id=ffffffff-ffff-ffff-ffff-fffffffffffw2 node.data-dir=/var/presto/data \u5176\u4f59\u914d\u7f6e\u53c2\u6570\u4e0d\u7528\u4fee\u6539 \u5173\u95ed172.16.9.107\uff0c172.16.2.120\uff0c172.16.2.121\u8282\u70b9\u9632\u706b\u5899 \u767b\u9646172.16.9.107\uff0c172.16.2.120\uff0c172.16.2.121\u8282\u70b9\u542f\u52a8presto cd /opt/presto-server-0.210 bin/launcher start \u767b\u9646 http://172.16.9.107:8080/ui \u68c0\u67e5\u8282\u70b9\u60c5\u51b5 \u767b\u9646172.16.9.107\u8282\u70b9\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8presto cli: ./presto-cli \\ --server https://172-16-9-107:7778 \\ --krb5-config-path /opt/145_651hdclient/krb5.conf \\ --krb5-principal developuser/172-16-9-107 \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name developuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; \u68c0\u67e5\u7ed3\u679c\uff1a \u6309\u7167\u4e4b\u524d\u53d9\u8ff0\u5982\u679c172.16.9.107\u8282\u70b9\u53c2\u6570node-scheduler.include-coordinator\u914d\u7f6e\u4e3afalse,\u5bf9\u5e94\u7ed3\u679c\u5982\u4e0b: \u914d\u7f6eElasticSearch Connector \u00b6 presto\u548cES\u5b98\u65b9\u90fd\u6ca1\u6709\u7ed9\u51fa\u9002\u914d\u7684\u6587\u6863\u4ecb\u7ecd\uff0c\u8fd9\u91cc\u6211\u4eec\u91c7\u7528\u5f00\u6e90\u7684\u9002\u914d\u5305\u8fdb\u884c\u9002\u914d\u3002\u5728https://github.com/harbby/presto-connectors \u4e0b\u8f7d\u9002\u914d\u5305\u6e90\u7801\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\uff0c\u89e3\u538b\u3002 \u4fee\u6539presto-elasticsearch-connectors\u6e90\u7801\u4ee5\u53ca\u914d\u7f6e \u8fdb\u5165/opt/presto-connectors-master/presto-elasticsearch6/src/main/java/com/facebook/presto/elasticsearch6/functions\u76ee\u5f55\uff0c\u53c2\u8003\u4e0b\u56fe\uff0c\u4fee\u6539MatchQueryFunction.java\u6587\u4ef6\uff0c\u6dfb\u52a0\u6784\u9020\u51fd\u6570\uff0c\u5c06\u51fd\u6570\u58f0\u660e\u4e3apublic \u8fdb\u5165presto-connectors-master\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-connectors-master vi pom.xml \u5728\u6700\u540e\u7684' '\u4e4b\u524d\uff0c\u6dfb\u52a0\u4ee5\u4e0bplugin\u4f9d\u8d56 <build> <pluginManagement> <plugins> <plugin> <groupId>pl.project13.maven</groupId> <artifactId>git-commit-id-plugin</artifactId> <configuration> <skip>true</skip> </configuration> </plugin> </plugins> </pluginManagement> </build> \uff08\u53ef\u9009\u64cd\u4f5c\uff09\u5728modules\u4e2d\uff0c\u53bb\u6389\u9664presto-base-elasticsearch\u548cpresto-elasticsearch6\u4ee5\u5916\u7684module\uff0c\u5176\u4ed6\u7684module\u8fd9\u91cc\u5e76\u4e0d\u9700\u8981\uff0c\u53ef\u4ee5\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4 \u8fdb\u5165presto-elasticsearch6\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-elasticsearch6 vi pom.xml \u5c06'elasticsearch.version'\u4fee\u6539\u4e3a6.1.3 \u5c06'elasticsearch-x-content'\u548c'elasticsearch-core'\u7684\u4f9d\u8d56\u6ce8\u91ca\u6389 \u56de\u5230presto-connectors-master\u76ee\u5f55\uff0c\u7f16\u8bd1presto-connectors mvn clean install -DskipTests \u7f16\u8bd1\u6210\u529f\u540e\uff0c\u663e\u793a\u5982\u4e0b\uff1a \u83b7\u53d6'presto-connectors-master/presto-elasticsearch6/target'\u76ee\u5f55\u4e0b\u7684'presto-elasticsearch6-0.210'\u6587\u4ef6\u5939\uff0c\u5c06\u5176\u590d\u5236\u5230presto-server\u7684plugin\u76ee\u5f55\u4e0b cp -r /opt/presto-connectors-master/presto-elasticsearch6/target/presto-elasticsearch6-0.210 /opt/presto-server-0.210/plugin \u767b\u5f55\u96c6\u7fa4manager\u7ba1\u7406\u9875\u9762\uff0c\u5728'\u670d\u52a1\u7ba1\u7406->Elasticsearch \u670d\u52a1\u914d\u7f6e'\u9875\u9762\uff0c\u9009\u62e9\u5168\u90e8\u914d\u7f6e\uff0c\u641c\u7d22'port'\u5173\u952e\u8bcd,\u67e5\u770b'SERVER_PORT'\u914d\u7f6e\u4e3a24100\uff0c'TRANSPORT_TCP_PORT'\u914d\u7f6e\u4e3a24101 \u5728\u96c6\u7fa4\u5ba2\u6237\u7aef\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_evn curl -XGET http://172.21.3.115:24100/_cluster/health?pretty \u5176\u4e2d172.21.3.115\u662felasticsearch\u96c6\u7fa4\u8282\u70b9\uff0c24100\u4e3aSERVER_PORT\uff0c\u770b\u5230\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c { \"cluster_name\" : \"elasticsearch_cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 33, \"active_shards\" : 66, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } \u5728/opt/presto-server-0.210/etc/catalog\u76ee\u5f55\u4e0b\u521b\u5efaes.properties\u6587\u4ef6 connector.name=elasticsearch6 elasticsearch.cluster.name=elasticsearch_cluster elasticsearch.transport.hosts=172.21.3.115:24101 \u5176\u4e2d'elasticsearch.cluster.name'\u662f\u521a\u624d\u83b7\u53d6\u7684ES\u96c6\u7fa4\u7684\u540d\u5b57\uff0c'elasticsearch.transport.hosts'\u4e3aEsNode\u8282\u70b9IP\uff0c\u7aef\u53e3\u4e3a'TRANSPORT_TCP_PORT' \u91cd\u542fpresto-server sh /opt/presto-server-0.210/bin/launcher restart \u901a\u8fc7Presto CLI\u8fde\u63a5ElasticSearch \u00b6 \u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55,\u4f8b\u5982/opt \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog es \\ --schema default \\ --; catalog\u540e\u9762\u7684es\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684es.properties\u7684\u6587\u4ef6\u540d\u5339\u914d \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\u67e5\u8be2ES\u4e2d\u7684\u7d22\u5f15\u4fe1\u606f \u5f53\u524dconnector\u652f\u6301show,create,select,insert,drop\u64cd\u4f5c\uff0c\u6682\u4e0d\u652f\u6301delete,update,alter\u7b49\u64cd\u4f5c","title":"0.210 <--> 8.0"},{"location":"SQL_Analytics/Presto_0.210/#apache-prestofusioninsight","text":"","title":"Apache Presto\u5bf9\u63a5FusionInsight"},{"location":"SQL_Analytics/Presto_0.210/#_1","text":"Presto 0.210 \u2194 FusionInsight HD V100R002C80SPC200 (HDFS/Hive/ElasticSearch) Presto 0.210 \u2194 FusionInsight HD 6.5 (HDFS/Hive) Presto 0.210 \u2194 FusionInsight HD 8.0 (HDFS/Hive)","title":"\u9002\u7528\u573a\u666f"},{"location":"SQL_Analytics/Presto_0.210/#_2","text":"Presto\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5206\u5e03\u5f0fSQL\u67e5\u8be2\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u4ea4\u4e92\u5f0f\u5206\u6790\u67e5\u8be2\uff0c\u6570\u636e\u91cf\u652f\u6301GB\u5230PB\u5b57\u8282\u3002 Presto\u7684\u8bbe\u8ba1\u548c\u7f16\u5199\u5b8c\u5168\u662f\u4e3a\u4e86\u89e3\u51b3\u50cfFacebook\u8fd9\u6837\u89c4\u6a21\u7684\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u7684\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u5904\u7406\u901f\u5ea6\u7684\u95ee\u9898 Presto\u4e3b\u8981\u4e0eFusionInsight\u7684Hive\u8fdb\u884c\u5bf9\u63a5,\u5728Presto0.210\u7248\u672c\u4e2d\u652f\u6301\u4e0eFusionInsight\u7684ES\u8fdb\u884c\u5bf9\u63a5\u3002","title":"\u8bf4\u660e"},{"location":"SQL_Analytics/Presto_0.210/#presto-server","text":"Presto\u96c6\u7fa4\u5305\u62eccoordinator\u8282\u70b9\u548c\u4e0d\u9650\u6570\u91cf\u7684worker\u8282\u70b9(coordinator\u8282\u70b9\u4e5f\u53ef\u540c\u65f6\u4e3aworker\u8282\u70b9)\uff0c\u5176\u4e2d\u53ea\u9700\u8981\u5728coordinator\u8282\u70b9\u4e0a\u914d\u7f6eHive Connector\u5373\u53ef\u3002 \u672c\u6587\u6863\u4e2d\u914d\u7f6ecoordinator\u8282\u70b9\u540c\u65f6\u4e5f\u662fworker\u8282\u70b9\u3002 \u4ece\u8be5\u94fe\u63a5\u4e0b\u8f7dpresto-server\u7684\u5b89\u88c5\u5305\uff0c\u5e76\u4e0a\u4f20\u5230presto coordinator\u7684\u8282\u70b9 https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.210/presto-server-0.210.tar.gz \u5c06\u8be5\u538b\u7f29\u5305\u89e3\u538b\u7f29\u540e\u5f97\u5230\u76ee\u5f55 /opt/presto-server-0.210 \u3002 \u5728presto\u8282\u70b9\u4e0a\u5b89\u88c5\u534e\u4e3aFusionInsight HD V100R002C80SPC200\u7684\u5ba2\u6237\u7aef\uff0c\u9ed8\u8ba4\u5b89\u88c5\u76ee\u5f55 /opt/hadoopclient presto-0.210\u7248\u672c\u8981\u6c42jdk\u81f3\u5c11\u57281.8u60+\u4ee5\u4e0a\uff0c\u4fee\u6539 /etc/profile \u6587\u4ef6\u65b9\u5f0f\u914d\u7f6e\u7cfb\u7edf\u9ed8\u8ba4\u7684java\u4e3aFusionInsight HD\u5ba2\u6237\u7aef\u7684jdk\uff0c\u5e76source\u73af\u5883\u53d8\u91cf\uff0c\u547d\u4ee4\u53c2\u8003\u5982\u4e0b \u5728 /etc/profile \u4e2d\u589e\u52a0\u4ee5\u4e0b\u884c export JAVA_HOME=/opt/hadoopclient/JDK/jdk export JREHOME=/opt/hadoopclient/JDK/jdk/jre export PATH=$JAVA_HOME/bin:$PATH source \u73af\u5883\u53d8\u91cf source /etc/profile \u521b\u5efaJava keystore File for TLS\uff0c(\u540e\u7eed\u6b65\u9aa4\u9ed8\u8ba4\u90fd\u5728presto\u8282\u70b9\u4e0a\u6267\u884c)\u53c2\u8003\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_env keytool -genkeypair -alias testuser -keyalg RSA \u2013keystore /opt/presto.jks alias\u540e\u7684\u503c\u5fc5\u987b\u8981\u8ddf\u540e\u9762\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u4e00\u81f4 first and last name\u5fc5\u987b\u5199\u6210presto\u8282\u70b9\u7684\u4e3b\u673a\u540d,\u5e76\u4e14\u8981\u5ffd\u7565\u5927\u5c0f\u5199\uff0c\u7edf\u4e00\u4f7f\u7528\u5c0f\u5199\u5b57\u6bcd \u901a\u8fc7FusionInsight HD\u7684\u7ba1\u7406\u9875\u9762\u521b\u5efa\u4e00\u4e2a\u201c\u673a\u673a\u201d\u7528\u6237\uff0c\u5177\u4f53\u8bf7\u53c2\u89c1\u300aFusionInsight HD\u7ba1\u7406\u5458\u6307\u5357\u300b\u7684 \u521b\u5efa\u7528\u6237 \u7ae0\u8282\u3002\u4f8b\u5982\uff0c\u521b\u5efa\u7528\u6237testuser\uff0c\u5e76\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u9009\u62e9\u7528\u6237\u7ec4(hadoop\u548chive\u7ec4)\uff0c\u4e0b\u8f7d\u5bf9\u5e94\u7684\u79d8\u94a5\u6587\u4ef6user.keytab\u4ee5\u53cakrb5.conf\u6587\u4ef6\uff0c\u5e76\u4e0a\u4f20\u5230presto\u8282\u70b9\u7684 /opt/hadoopclient \u76ee\u5f55\u4e0b\uff0c\u5c06user.keytab\u6539\u540d\u4e3atestuser.keytab\u3002 \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\u5728Huawei FusionInsight HD\u7684Kerberos\u4e2d\u521b\u5efa\u4e00\u4e2a\u65b0\u7684principal\uff0c\u5176\u540d\u79f0\u4e3a\u201ctestuser/presto-server\u201d\uff0c\u5176\u4e2dpresto-server\u4e3apresto\u7684coordinator\u8282\u70b9\u7684\u4e3b\u673a\u540d\uff0c\u5bfc\u51fa\u8be5principal\u7684\u79d8\u94a5\u6587\u4ef6\u4e3a /opt/presto.keytab \u3002 \u6267\u884ckadmin \u2013p kadmin/admin\u547d\u4ee4\u65f6\u521d\u59cb\u5bc6\u7801Admin@123\uff0c\u4fee\u6539\u540e\u9700\u4e25\u683c\u7262\u8bb0\u65b0\u5bc6\u7801\u3002 \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc\uff0c\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efa\u5982\u4e0b\u6587\u4ef6 config.properties\u53c2\u8003\u5982\u4e0b coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://presto-server:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=testuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 jvm.config\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 -server -Xmx16G -XX:+UseG1GC -XX:G1HeapRegionSize=32M -XX:+UseGCOverheadLimit -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:OnOutOfMemoryError=kill -9 %p -Djava.security.krb5.conf=/opt/hadoopclient/KrbClient/kerberos/var/krb5kdc/krb5.conf node.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 node.environment=production node.id=ffffffff-ffff-ffff-ffff-ffffffffffff node.data-dir=/var/presto/data log.properties\u53c2\u8003\u5982\u4e0b\u5185\u5bb9 com.facebook.presto=INFO \u521b\u5efa\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5c06FusionInsight HD\u5ba2\u6237\u7aef\u4e2d(\u4ecespark2x\u5ba2\u6237\u7aef\u76ee\u5f55\u4e0b\u83b7\u53d6)\u7684core-site.xml\u548chdfs-site.xml\u590d\u5236\u5230 /opt/presto-server-0.210/etc/catalog \u4e2d cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/core-site.xml /opt/presto-server-0.210/etc/catalog/ cp /opt/hadoopclient/HDFS/hadoop/etc/hadoop/hdfs-site.xml /opt/presto-server-0.210/etc/catalog/ \u5c06hdfs-site.xml\u6587\u4ef6\u4e2d\u7684dfs.client.failover.proxy.provider.hacluster\u5c5e\u6027\u4fee\u6539\u4e3aorg.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider vi /opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u4fee\u6539/etc/hosts\u6587\u4ef6\uff0c\u5c06\u672c\u673a\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u4ee5\u53caHuawei FusionInsight HD\u96c6\u7fa4\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u89e3\u6790\u6dfb\u52a0\u8fdb\u53bb\uff0c\u4f8b\u5982 \u5b89\u88c5maven\uff1a wget http://apache.osuosl.org/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz tar -xzvf apache-maven-3.3.9-bin.tar.gz -C /opt/ \u4fee\u6539profile\u6587\u4ef6 vi /etc/profile ,\u589e\u52a0\u4ee5\u4e0b\u914d\u7f6e export PATH=$PATH:/opt/apache-maven-3.3.9/bin \u5bfc\u5165\u73af\u5883\u53d8\u91cf source /etc/profile \u6267\u884c mvn -v \u53ef\u4ee5\u6b63\u786e\u8f93\u51famvn\u7248\u672c \u5b89\u88c5git yum install -y git \u53c2\u8003\u5982\u4e0b\u547d\u4ee4\uff0c\u4e0b\u8f7dpresto-server-0.210\u7684\u6e90\u7801 git clone https://github.com/prestodb/presto.git git checkout 0.210","title":"\u83b7\u53d6\u5e76\u914d\u7f6epresto server"},{"location":"SQL_Analytics/Presto_0.210/#presto-cli","text":"\u4f7f\u7528Presto CLI\u8fde\u63a5Huawei FusionInsight HD\u7684Hive\uff0c\u4f7f\u7528presto\u81ea\u5e26\u7684\u547d\u4ee4\u884c\u5de5\u5177\u6267\u884cSQL\u8bed\u53e5\u3002 \u901a\u8fc7\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7dpresto cli\u542f\u52a8\u7684jar\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.210/presto-cli-0.210-executable.jar \u5e76\u5c06\u8be5jar\u5305\u4e0a\u4f20\u5230\u53ef\u4e0epresto\u8282\u70b9\u7f51\u7edc\u4e92\u901a\u7684\u8282\u70b9\u4e0a(\u4e5f\u53ef\u5c06presto coordinator\u8282\u70b9\u4f5c\u4e3acli\u4f7f\u7528\u8282\u70b9)\u3002 \u914d\u7f6ecli\u8282\u70b9\u7684jdk\u4e3a1.8u60+\u4ee5\u4e0a\u7248\u672c \u914d\u7f6ecli\u8282\u70b9\u7684/etc/hosts\u6587\u4ef6\uff0c\u5c06FI\u96c6\u7fa4\u548cpresto coordinator\u8282\u70b9\u7684IP\u4e0e\u4e3b\u673a\u540d\u5173\u7cfb\u914d\u7f6e\u5230cli\u8282\u70b9 \u4ecepresto\u8282\u70b9\u62f7\u8d1dpresto.jks\u3001presto.keytab\u3001krb5.conf\u4ee5\u53ca\u8fde\u63a5HDFS\u6240\u9700\u7684core-site.xml\u548chdfs-site.xml\u6587\u4ef6\u5230cli\u8282\u70b9 \u5c06presto-cli-0.210-executable.jar\u5305\u6539\u4e3a\u53ef\u6267\u884c\u6587\u4ef6 mv presto-cli-0.184-executable.jar presto chmod u+x presto ./presto -h","title":"\u83b7\u53d6Presto CLI\u542f\u52a8\u5305"},{"location":"SQL_Analytics/Presto_0.210/#hive-connector","text":"\u8fdb\u5165\u76ee\u5f55/opt/presto-server-0.210/etc/catalog,\u5728\u8be5\u76ee\u5f55\u4e0b\u521b\u5efahive.properties\u6587\u4ef6 connector.name=hive-hadoop2 hive.metastore.uri=thrift://172.21.3.115:21088,thrift://172.21.3.116:21088 hive.metastore.service.principal=hive/hadoop.hadoop.com@HADOOP.COM hive.metastore.authentication.type=KERBEROS hive.metastore.client.principal=testuser/presto-server hive.metastore.client.keytab=/opt/presto.keytab hive.hdfs.authentication.type=KERBEROS hive.hdfs.impersonation.enabled=false hive.hdfs.presto.principal=testuser hive.hdfs.presto.keytab=/opt/hadoopclient/testuser.keytab hive.config.resources=/opt/presto-server-0.210/etc/catalog/core-site.xml,/opt/presto-server-0.210/etc/catalog/hdfs-site.xml \u5176\u4e2dhive.metastore.uri\u7684\u503c\u4ece/opt/hadoopclient/Hive/config/hive-site.xml\u4e2d\u67e5\u627e \u4fee\u6539presto-hive/src/main/java/com/facebook/presto/hive/authentication/KerberosHiveMetastoreAuthentication.java\u7684\u4ee3\u7801\uff0c\u5c06\u4ee3\u7801\u4e2d\"Sasl.QOP\"\u7684\u503c\u4fee\u6539\u4e3a\u56fa\u5b9a\u7684\"auth-conf\" \u91cd\u65b0\u7f16\u8bd1presto cd presto-hive mvn clean install -DskipTests \u5c06\u7f16\u8bd1\u540etarget\u76ee\u5f55\u4e0b\u7684presto-hive-0.210.jar\u6587\u4ef6\u66ff\u6362/opt/presto-server-0.210/plugin/hive-hadoop2/presto-hive-0.210.jar\u6587\u4ef6 \u542f\u52a8presto server\uff0c\u8ddf\u8e2a/var/presto/data/var/log/server.log\u67e5\u770b\u542f\u52a8\u65e5\u5fd7 sh /opt/presto-server-0.210/bin/launcher start tailf /var/presto/data/var/log/server.log","title":"\u914d\u7f6eHive Connector"},{"location":"SQL_Analytics/Presto_0.210/#presto-clihive","text":"\u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55\uff0c\u4f8b\u5982/opt\uff1b \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; catalog\u540e\u9762\u7684hive\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684hive.properties\u7684\u6587\u4ef6\u540d\u5339\u914d\u7684\uff0c\u5982\u679chive.properties\u6539\u540d\u4e3ahivetest.properties\uff0c\u5219\u8fd9\u91cc\u6539\u4e3ahivetest \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\uff0c\u5176\u4ed6SQL\u8bed\u6cd5\u8bf7\u53c2\u8003 https://prestodb.io/docs/0.210/sql.html \u67e5\u8be2\u8868workers_info\u4e2d\u6570\u636e\uff1a","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/#presto-jdbchive","text":"\u4f7f\u7528Presto JDBC\u63a5\u53e3\u8fde\u63a5Huawei FusionInsight HD Hive \u4ece\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7djdbc\u7684\u9a71\u52a8\u5305 https://repo1.maven.org/maven2/com/facebook/presto/presto-jdbc/0.210/presto-jdbc-0.210.jar \u53c2\u8003 https://prestodb.io/docs/0.210/installation/jdbc.html \u8bbe\u7f6eJDBC URL\uff0c\u7528\u6237\u540d\u4e3a\u4efb\u610f\u5b57\u7b26\uff0c\u5bc6\u7801\u4e3a\u7a7a\uff0c\u5728eclipse\u4e2d\u8c03\u901a\u7684\u793a\u4f8b\u5982\u4e0b: import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.ResultSet ; import java.sql.SQLException ; import java.sql.Statement ; public class PrestoTest { public static void main ( String [] args ) throws SQLException , ClassNotFoundException { Class . forName ( \"com.facebook.presto.jdbc.PrestoDriver\" ); Connection connection = DriverManager . getConnection ( \"jdbc:presto://172.21.3.48:8080/hive/default\" , \"root\" , null ); Statement stmt = connection . createStatement (); ResultSet rs = stmt . executeQuery ( \"select * from adult limit 10\" ); int col = rs . getMetaData (). getColumnCount (); while ( rs . next ()) { for ( int i = 1 ; i <= col ; i ++ ) { System . out . print ( rs . getString ( i ) + \"\\t\" ); if (( i == 2 ) && ( rs . getString ( i ). length () < 8 )) { System . out . print ( \"\\t\" ); } } System . out . println ( \"\" ); } rs . close (); connection . close (); } } \u6d4b\u8bd5\u7ed3\u679c\uff1a","title":"\u901a\u8fc7Presto JDBC\u8fde\u63a5Hive"},{"location":"SQL_Analytics/Presto_0.210/#prestoworker","text":"presto \u670d\u52a1\u89d2\u8272\u5206\u5e03 \u670d\u52a1\u5668ip\u5730\u5740 \u89d2\u8272 172.16.9.107 Coordinator/Worker 172.16.2.120 Worker 172.16.2.121 Worker \u524d\u63d0\u6761\u4ef6 \uff1a\u6839\u636e\u4e0a\u8ff0\u90e8\u5206\u5df2\u7ecf\u5b8c\u6210coordinator/worker\u8282\u70b9\u5408\u90e8\u914d\u7f6e\u4ee5\u53ca\u5bf9\u63a5hive\u6210\u529f \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5c06coordinator/worker\u8282\u70b9\u5df2\u7ecf\u914d\u7f6e\u597d\u7684presto\u6587\u4ef6\u62f7\u8d1d\u5230worker\u8282\u70b9 scp -r /opt/presto-server-0.210 root@172.16.2.120:/opt scp -r /opt/presto-server-0.210 root@172.16.2.121:/opt \u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u5c06coordinator/worker\u8282\u70b9\u8ba4\u8bc1\u76f8\u5173\u914d\u7f6e\u6587\u4ef6\u62f7\u8d1d\u5230worker\u8282\u70b9 scp /opt/presto.keytab root@172.16.2.120:/opt scp /opt/presto.keytab root@172.16.2.121:/opt scp /opt/145_651hdclient/user.keytab root@172.16.2.120:/opt/145_651hdclient scp /opt/145_651hdclient/user.keytab root@172.16.2.121:/opt/145_651hdclient \u4fee\u6539worker\u8282\u70b9 /opt/presto-server-0.210/etc/config.properties \u914d\u7f6e\u6587\u4ef6 \u65b0\u589eworker\u8282\u70b9172.16.2.120, 172.16.2.121\u7684config.properties\u914d\u7f6e\u9700\u8981\u4fee\u6539\uff0c\u5982\u4e0b: coordinator=false http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery.uri=http://172-16-9-107:8080 Coordinator\uff0cworker\u8282\u70b9\u5408\u90e8\u8282\u70b9172.16.9.107\u7684config.properties\u914d\u7f6e\u6587\u4ef6\u5185\u5bb9\u4e0d\u53d8\uff0c\u5982\u4e0b\uff1a coordinator=true node-scheduler.include-coordinator=true http-server.http.port=8080 query.max-memory=50GB query.max-memory-per-node=1GB discovery-server.enabled=true discovery.uri=http://172-16-9-107:8080 http-server.authentication.type=KERBEROS http.server.authentication.krb5.service-name=developuser http.server.authentication.krb5.keytab=/opt/presto.keytab http.authentication.krb5.config=/opt/145_651hdclient/krb5.conf http-server.https.enabled=true http-server.https.port=7778 http-server.https.keystore.path=/opt/presto.jks http-server.https.keystore.key=Huawei@123 \u8bf4\u660e\uff1anode-scheduler.include-coordinator\u53c2\u6570\u4e3aCoordinator,worker\u8282\u70b9\u5408\u90e8\u53c2\u6570\uff0c\u503c\u4e3afalse\u4ee3\u8868\u8be5\u8282\u70b9\u53ea\u505acoordinator\u8282\u70b9 \u4fee\u6539worker\u8282\u70b9 /opt/presto-server-0.210/etc/node.properties 172.16.2.120\u8282\u70b9\u914d\u7f6e: node.environment=production node.id=ffffffff-ffff-ffff-ffff-fffffffffffw1 node.data-dir=/var/presto/data 172.16.2.121\u8282\u70b9\u914d\u7f6e: node.environment=production node.id=ffffffff-ffff-ffff-ffff-fffffffffffw2 node.data-dir=/var/presto/data \u5176\u4f59\u914d\u7f6e\u53c2\u6570\u4e0d\u7528\u4fee\u6539 \u5173\u95ed172.16.9.107\uff0c172.16.2.120\uff0c172.16.2.121\u8282\u70b9\u9632\u706b\u5899 \u767b\u9646172.16.9.107\uff0c172.16.2.120\uff0c172.16.2.121\u8282\u70b9\u542f\u52a8presto cd /opt/presto-server-0.210 bin/launcher start \u767b\u9646 http://172.16.9.107:8080/ui \u68c0\u67e5\u8282\u70b9\u60c5\u51b5 \u767b\u9646172.16.9.107\u8282\u70b9\uff0c\u4f7f\u7528\u5982\u4e0b\u547d\u4ee4\u542f\u52a8presto cli: ./presto-cli \\ --server https://172-16-9-107:7778 \\ --krb5-config-path /opt/145_651hdclient/krb5.conf \\ --krb5-principal developuser/172-16-9-107 \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name developuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog hive \\ --schema default \\ --; \u68c0\u67e5\u7ed3\u679c\uff1a \u6309\u7167\u4e4b\u524d\u53d9\u8ff0\u5982\u679c172.16.9.107\u8282\u70b9\u53c2\u6570node-scheduler.include-coordinator\u914d\u7f6e\u4e3afalse,\u5bf9\u5e94\u7ed3\u679c\u5982\u4e0b:","title":"presto\u591aworker\u8282\u70b9\u914d\u7f6e"},{"location":"SQL_Analytics/Presto_0.210/#elasticsearch-connector","text":"presto\u548cES\u5b98\u65b9\u90fd\u6ca1\u6709\u7ed9\u51fa\u9002\u914d\u7684\u6587\u6863\u4ecb\u7ecd\uff0c\u8fd9\u91cc\u6211\u4eec\u91c7\u7528\u5f00\u6e90\u7684\u9002\u914d\u5305\u8fdb\u884c\u9002\u914d\u3002\u5728https://github.com/harbby/presto-connectors \u4e0b\u8f7d\u9002\u914d\u5305\u6e90\u7801\uff0c\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\uff0c\u89e3\u538b\u3002 \u4fee\u6539presto-elasticsearch-connectors\u6e90\u7801\u4ee5\u53ca\u914d\u7f6e \u8fdb\u5165/opt/presto-connectors-master/presto-elasticsearch6/src/main/java/com/facebook/presto/elasticsearch6/functions\u76ee\u5f55\uff0c\u53c2\u8003\u4e0b\u56fe\uff0c\u4fee\u6539MatchQueryFunction.java\u6587\u4ef6\uff0c\u6dfb\u52a0\u6784\u9020\u51fd\u6570\uff0c\u5c06\u51fd\u6570\u58f0\u660e\u4e3apublic \u8fdb\u5165presto-connectors-master\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-connectors-master vi pom.xml \u5728\u6700\u540e\u7684' '\u4e4b\u524d\uff0c\u6dfb\u52a0\u4ee5\u4e0bplugin\u4f9d\u8d56 <build> <pluginManagement> <plugins> <plugin> <groupId>pl.project13.maven</groupId> <artifactId>git-commit-id-plugin</artifactId> <configuration> <skip>true</skip> </configuration> </plugin> </plugins> </pluginManagement> </build> \uff08\u53ef\u9009\u64cd\u4f5c\uff09\u5728modules\u4e2d\uff0c\u53bb\u6389\u9664presto-base-elasticsearch\u548cpresto-elasticsearch6\u4ee5\u5916\u7684module\uff0c\u5176\u4ed6\u7684module\u8fd9\u91cc\u5e76\u4e0d\u9700\u8981\uff0c\u53ef\u4ee5\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4 \u8fdb\u5165presto-elasticsearch6\u76ee\u5f55\uff0c\u4fee\u6539pom.xml\u6587\u4ef6 cd presto-elasticsearch6 vi pom.xml \u5c06'elasticsearch.version'\u4fee\u6539\u4e3a6.1.3 \u5c06'elasticsearch-x-content'\u548c'elasticsearch-core'\u7684\u4f9d\u8d56\u6ce8\u91ca\u6389 \u56de\u5230presto-connectors-master\u76ee\u5f55\uff0c\u7f16\u8bd1presto-connectors mvn clean install -DskipTests \u7f16\u8bd1\u6210\u529f\u540e\uff0c\u663e\u793a\u5982\u4e0b\uff1a \u83b7\u53d6'presto-connectors-master/presto-elasticsearch6/target'\u76ee\u5f55\u4e0b\u7684'presto-elasticsearch6-0.210'\u6587\u4ef6\u5939\uff0c\u5c06\u5176\u590d\u5236\u5230presto-server\u7684plugin\u76ee\u5f55\u4e0b cp -r /opt/presto-connectors-master/presto-elasticsearch6/target/presto-elasticsearch6-0.210 /opt/presto-server-0.210/plugin \u767b\u5f55\u96c6\u7fa4manager\u7ba1\u7406\u9875\u9762\uff0c\u5728'\u670d\u52a1\u7ba1\u7406->Elasticsearch \u670d\u52a1\u914d\u7f6e'\u9875\u9762\uff0c\u9009\u62e9\u5168\u90e8\u914d\u7f6e\uff0c\u641c\u7d22'port'\u5173\u952e\u8bcd,\u67e5\u770b'SERVER_PORT'\u914d\u7f6e\u4e3a24100\uff0c'TRANSPORT_TCP_PORT'\u914d\u7f6e\u4e3a24101 \u5728\u96c6\u7fa4\u5ba2\u6237\u7aef\u8282\u70b9\u6267\u884c\u5982\u4e0b\u547d\u4ee4 source /opt/hadoopclient/bigdata_evn curl -XGET http://172.21.3.115:24100/_cluster/health?pretty \u5176\u4e2d172.21.3.115\u662felasticsearch\u96c6\u7fa4\u8282\u70b9\uff0c24100\u4e3aSERVER_PORT\uff0c\u770b\u5230\u8fd4\u56de\u5982\u4e0b\u7ed3\u679c { \"cluster_name\" : \"elasticsearch_cluster\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 3, \"active_primary_shards\" : 33, \"active_shards\" : 66, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } \u5728/opt/presto-server-0.210/etc/catalog\u76ee\u5f55\u4e0b\u521b\u5efaes.properties\u6587\u4ef6 connector.name=elasticsearch6 elasticsearch.cluster.name=elasticsearch_cluster elasticsearch.transport.hosts=172.21.3.115:24101 \u5176\u4e2d'elasticsearch.cluster.name'\u662f\u521a\u624d\u83b7\u53d6\u7684ES\u96c6\u7fa4\u7684\u540d\u5b57\uff0c'elasticsearch.transport.hosts'\u4e3aEsNode\u8282\u70b9IP\uff0c\u7aef\u53e3\u4e3a'TRANSPORT_TCP_PORT' \u91cd\u542fpresto-server sh /opt/presto-server-0.210/bin/launcher restart","title":"\u914d\u7f6eElasticSearch Connector"},{"location":"SQL_Analytics/Presto_0.210/#presto-clielasticsearch","text":"\u8fdb\u5165Presto CLI\u542f\u52a8\u5305\u6240\u5728\u76ee\u5f55,\u4f8b\u5982/opt \u521b\u5efapresto cli\u542f\u52a8\u811a\u672c\uff0c\u7c7b\u4f3c\u5982\u4e0b\uff0c\u6ce8\u610f\u5c06\u76f8\u5173\u6587\u4ef6\u7684\u8def\u5f84\u6309\u5b9e\u9645\u4f4d\u7f6e\u66ff\u6362 ./presto \\ --server https://presto-server:7778 \\ --krb5-config-path /opt/hadoopclient/krb5.conf \\ --krb5-principal testuser/presto-server \\ --krb5-keytab-path /opt/presto.keytab \\ --krb5-remote-service-name testuser \\ --keystore-path /opt/presto.jks \\ --keystore-password Huawei@123 \\ --catalog es \\ --schema default \\ --; catalog\u540e\u9762\u7684es\u662f\u548cpresto coordinator\u8282\u70b9\u914d\u7f6e\u7684es.properties\u7684\u6587\u4ef6\u540d\u5339\u914d \u901a\u8fc7cli\u6267\u884cSQL\u8bed\u53e5\u67e5\u8be2ES\u4e2d\u7684\u7d22\u5f15\u4fe1\u606f \u5f53\u524dconnector\u652f\u6301show,create,select,insert,drop\u64cd\u4f5c\uff0c\u6682\u4e0d\u652f\u6301delete,update,alter\u7b49\u64cd\u4f5c","title":"\u901a\u8fc7Presto CLI\u8fde\u63a5ElasticSearch"}]}